FileHandleCache/com/sleepycat/je/log/FileManager.java:package com.sleepycat.je.log;
public class FileManager {
  private FileCache fileCache;
  Set getCacheKeys(){
    return fileCache.getCacheKeys();
  }
  /** 
 * Clear a file out of the file cache regardless of mode type.
 */
  private void clearFileCache(  long fileNum) throws IOException, DatabaseException {
    fileCache.remove(fileNum);
  }
  protected void hook451() throws IOException, DatabaseException {
    fileCache.clear();
  }
  protected void hook457(  DbConfigManager configManager) throws DatabaseException {
    fileCache=new FileCache(configManager);
    original(configManager);
  }
  protected void hook458(  long fileNum) throws DatabaseException, IOException {
    clearFileCache(fileNum);
    original(fileNum);
  }
  protected void hook459(  long fileNum) throws DatabaseException, IOException {
    clearFileCache(fileNum);
    original(fileNum);
  }
  protected void hook460(  long fileNum,  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    while (true) {
      original(fileNum,fileId,fileHandle);
    }
  }
  protected void hook461(  ByteBuffer data){
    data.position(0);
    original(data);
  }
  /** 
 * Close all file handles and empty the cache.
 */
  public void clear() throws IOException, DatabaseException {
{
      this.hook451();
    }
    original();
  }
  protected FileHandle hook462(  long fileNum,  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    fileHandle=fileCache.get(fileId);
    if (fileHandle == null) {
      fileHandle=original(fileNum,fileId,fileHandle);
    }
    return fileHandle;
  }
  protected FileHandle hook463(  long fileNum,  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    fileHandle=fileCache.get(fileId);
    if (fileHandle == null) {
      fileHandle=original(fileNum,fileId,fileHandle);
    }
    return fileHandle;
  }
  protected void hook464(  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    fileCache.add(fileId,fileHandle);
    original(fileId,fileHandle);
  }
}
\00CheckLeaks/com/sleepycat/je/dbi/EnvironmentImpl.java:package com.sleepycat.je.dbi;
public class EnvironmentImpl {
  /** 
 * Debugging support. Check for leaked locks and transactions.
 */
  private void checkLeaks() throws DatabaseException {
    new EnvironmentImpl_checkLeaks(this).execute();
  }
  protected void hook311() throws DatabaseException {
  }
  protected void hook325(  StringBuffer errors) throws DatabaseException {
    try {
      checkLeaks();
      this.hook311();
    }
 catch (    DatabaseException DBE) {
      errors.append("\nException performing validity checks: ");
      errors.append(DBE.toString()).append("\n");
    }
    original(errors);
  }
@MethodObject static class EnvironmentImpl_checkLeaks {
    EnvironmentImpl_checkLeaks(    EnvironmentImpl _this){
      this._this=_this;
    }
    void execute() throws DatabaseException {
      if (!_this.configManager.getBoolean(EnvironmentParams.ENV_CHECK_LEAKS)) {
        return;
      }
      clean=true;
      this.hook313();
      this.hook312();
      assert clean : "Lock, transaction, or latch left behind at environment close";
    }
    protected EnvironmentImpl _this;
    protected boolean clean;
    protected StatsConfig statsConfig;
    protected LockStats lockStat;
    protected TransactionStats txnStat;
    protected TransactionStats.Active[] active;
    protected void hook312() throws DatabaseException {
    }
    protected void hook313() throws DatabaseException {
    }
  }
}
\00IO/com/sleepycat/je/log/FileManager.java:package com.sleepycat.je.log;
public class FileManager {
@MethodObject static class FileManager_writeToFile {
    protected void hook447() throws IOException, DatabaseException {
      assert data.hasArray();
      assert data.arrayOffset() == 0;
      pos=data.position();
      size=data.limit() - pos;
      file.seek(destOffset);
      file.write(data.array(),pos,size);
      data.position(pos + size);
      totalBytesWritten=size;
    }
    int execute() throws IOException, DatabaseException {
      int result=original();
{
        this.hook447();
      }
      return result;
    }
  }
@MethodObject static class FileManager_readFromFile {
    protected void hook448() throws IOException {
      assert readBuffer.hasArray();
      assert readBuffer.arrayOffset() == 0;
      pos=readBuffer.position();
      size=readBuffer.limit() - pos;
      file.seek(offset);
      bytesRead2=file.read(readBuffer.array(),pos,size);
      if (bytesRead2 > 0) {
        readBuffer.position(pos + bytesRead2);
      }
    }
    void execute() throws IOException {
      original();
{
        this.hook448();
      }
    }
  }
}
\00Derivative_Statistics_Verifier/com/sleepycat/je/util/DbStat.java:package com.sleepycat.je.util;
import java.io.File;
import java.io.PrintStream;
import java.util.logging.Level;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DatabaseStats;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.Environment;
import com.sleepycat.je.JEVersion;
import com.sleepycat.je.StatsConfig;
import com.sleepycat.je.utilint.CmdUtil;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
public class DbStat extends DbVerify {
  private String usageString="usage: " + CmdUtil.getJavaCommand(DbStat.class) + "\n"+ "               [-V] -s database -h dbEnvHome [-v progressInterval]\n";
  private int progressInterval=0;
  static public void main(  String argv[]) throws DatabaseException {
    DbStat stat=new DbStat();
    stat.parseArgs(argv);
    int ret=0;
    try {
      if (!stat.stats(System.err)) {
        ret=1;
      }
    }
 catch (    Throwable T) {
      ret=1;
      T.printStackTrace(System.err);
    }
    try {
      stat.env.close();
    }
 catch (    Throwable ignored) {
    }
    System.exit(ret);
  }
  protected DbStat(){
  }
  public DbStat(  Environment env,  String dbName){
    super(env,dbName,false);
  }
  protected void printUsage(  String msg){
    System.err.println(msg);
    System.err.println(usageString);
    System.exit(-1);
  }
  protected void parseArgs(  String argv[]){
    int argc=0;
    int nArgs=argv.length;
    while (argc < nArgs) {
      String thisArg=argv[argc++];
      if (thisArg.equals("-V")) {
        System.out.println(JEVersion.CURRENT_VERSION);
        System.exit(0);
      }
 else       if (thisArg.equals("-h")) {
        if (argc < nArgs) {
          envHome=new File(argv[argc++]);
        }
 else {
          printUsage("-h requires an argument");
        }
      }
 else       if (thisArg.equals("-s")) {
        if (argc < nArgs) {
          dbName=argv[argc++];
        }
 else {
          printUsage("-s requires an argument");
        }
      }
 else       if (thisArg.equals("-v")) {
        if (argc < nArgs) {
          progressInterval=Integer.parseInt(argv[argc++]);
          if (progressInterval <= 0) {
            printUsage("-v requires a positive argument");
          }
        }
 else {
          printUsage("-v requires an argument");
        }
      }
    }
    if (envHome == null) {
      printUsage("-h is a required argument");
    }
    if (dbName == null) {
      printUsage("-s is a required argument");
    }
  }
  public boolean stats(  PrintStream out) throws DatabaseException {
    try {
      openEnv();
      this.hook850();
      DatabaseConfig dbConfig=new DatabaseConfig();
      dbConfig.setReadOnly(true);
      dbConfig.setAllowCreate(false);
      DbInternal.setUseExistingConfig(dbConfig,true);
      Database db=env.openDatabase(null,dbName,dbConfig);
      StatsConfig statsConfig=new StatsConfig();
      if (progressInterval > 0) {
        statsConfig.setShowProgressInterval(progressInterval);
        statsConfig.setShowProgressStream(out);
      }
      DatabaseStats stats=db.getStats(statsConfig);
      out.println(stats);
      db.close();
      this.hook849();
    }
 catch (    DatabaseException DE) {
      return false;
    }
    return true;
  }
  protected void hook849() throws DatabaseException {
  }
  protected void hook850() throws DatabaseException {
  }
}
\00Derivative_Statistics_Verifier/com/sleepycat/je/Database.java:package com.sleepycat.je;
public class Database {
  public DatabaseStats verify(  VerifyConfig config) throws DatabaseException {
    checkEnv();
    checkRequiredDbState(OPEN,"Can't call Database.verify");
    this.hook37();
    VerifyConfig useConfig=(config == null) ? VerifyConfig.DEFAULT : config;
    DatabaseStats stats=databaseImpl.getEmptyStats();
    databaseImpl.verify(useConfig,stats);
    return stats;
  }
  protected void hook37() throws DatabaseException {
  }
}
\00Latches/com/sleepycat/je/latch/LatchImpl.java:package com.sleepycat.je.latch;
import java.util.ArrayList;
import java.util.List;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.dbi.EnvironmentImpl;
import de.ovgu.cide.jakutil.*;
/** 
 * This implementation is used in non-Java5 JVMs.  In Java5 JVMs, the
 * Java5LockWrapperImpl class is used.  The switch hitting is performed in
 * LatchSupport.
 * Simple thread-based non-transactional exclusive non-nestable latch.
 * <p>
 * Latches provide simple exclusive transient locks on objects.  Latches are
 * expected to be held for short, defined periods of time.  No deadlock
 * detection is provided so it is the caller's responsibility to sequence latch
 * acquisition in an ordered fashion to avoid deadlocks.
 * <p>
 * A latch can be acquire in wait or no-wait modes.  In the former, the caller
 * will wait for any conflicting holders to release the latch.  In the latter,
 * if the latch is not available, control returns to the caller immediately.
 */
public class LatchImpl implements Latch {
  private static final String DEFAULT_LATCH_NAME="LatchImpl";
  private String name=null;
  private List waiters=null;
  private EnvironmentImpl env=null;
  private Thread owner=null;
  /** 
 * Create a latch.
 */
  public LatchImpl(  String name,  EnvironmentImpl env){
    this.name=name;
    this.env=env;
  }
  /** 
 * Create a latch with no name, more optimal for shortlived latches.
 */
  public LatchImpl(  EnvironmentImpl env){
    this.env=env;
    this.name=DEFAULT_LATCH_NAME;
  }
  /** 
 * Set the latch name, used for latches in objects instantiated from
 * the log.
 */
  synchronized public void setName(  String name){
    this.name=name;
  }
  /** 
 * Acquire a latch for exclusive/write access.
 * <p>Wait for the latch if some other thread is holding it.  If there are
 * threads waiting for access, they will be granted the latch on a FIFO
 * basis.  When the method returns, the latch is held for exclusive
 * access.</p>
 * @throws LatchException if the latch is already held by the calling
 * thread.
 * @throws RunRecoveryException if an InterruptedException exception
 * occurs.
 */
  public void acquire() throws DatabaseException {
    try {
      Thread thread=Thread.currentThread();
      LatchWaiter waitTarget=null;
synchronized (this) {
        if (thread == owner) {
          this.hook422();
          throw new LatchException(getNameString() + " already held");
        }
        if (owner == null) {
          this.hook423();
          owner=thread;
        }
 else {
          if (waiters == null) {
            waiters=new ArrayList();
          }
          waitTarget=new LatchWaiter(thread);
          waiters.add(waitTarget);
          this.hook424();
        }
      }
      if (waitTarget != null) {
synchronized (waitTarget) {
          while (true) {
            if (waitTarget.active) {
              if (thread == owner) {
                break;
              }
 else {
                throw new DatabaseException("waitTarget.active but not owner");
              }
            }
 else {
              waitTarget.wait();
              if (thread == owner) {
                break;
              }
 else {
                continue;
              }
            }
          }
        }
      }
      assert noteLatch();
    }
 catch (    InterruptedException e) {
      throw new RunRecoveryException(env,e);
    }
 finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Acquire a latch for exclusive/write access, but do not block if it's not
 * available.
 * @return true if the latch was acquired, false if it is not available.
 * @throws LatchException if the latch is already held by the calling
 * thread.
 */
  public synchronized boolean acquireNoWait() throws LatchException {
    try {
      Thread thread=Thread.currentThread();
      if (thread == owner) {
        this.hook425();
        throw new LatchException(getNameString() + " already held");
      }
      if (owner == null) {
        owner=thread;
        this.hook426();
        assert noteLatch();
        return true;
      }
 else {
        this.hook427();
        return false;
      }
    }
  finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Release the latch.  If there are other thread(s) waiting for the latch,
 * one is woken up and granted the latch. If the latch was not owned by 
 * the caller, just return;
 */
  public void releaseIfOwner(){
    doRelease(false);
  }
  /** 
 * Release the latch.  If there are other thread(s) waiting for the latch,
 * they are woken up and granted the latch.
 * @throws LatchNotHeldException if the latch is not currently held.
 */
  public void release() throws LatchNotHeldException {
    if (doRelease(true)) {
      throw new LatchNotHeldException(getNameString() + " not held");
    }
  }
  /** 
 * Do the work of releasing the latch. Wake up any waiters.
 * @returns true if this latch was not owned by the caller.
 */
  private boolean doRelease(  boolean checkHeld){
    LatchWaiter newOwner=null;
    try {
synchronized (this) {
        Thread thread=Thread.currentThread();
        if (thread != owner) {
          return true;
        }
        if (waiters != null && waiters.size() > 0) {
          newOwner=(LatchWaiter)waiters.remove(0);
          owner=(Thread)newOwner.thread;
        }
 else {
          owner=null;
        }
        this.hook428();
        assert unNoteLatch(checkHeld);
      }
    }
  finally {
      assert EnvironmentImpl.maybeForceYield();
    }
    if (newOwner != null) {
synchronized (newOwner) {
        newOwner.active=true;
        newOwner.notifyAll();
      }
    }
    return false;
  }
  /** 
 * Return true if the current thread holds this latch.
 * @return true if we hold this latch.  False otherwise.
 */
  public boolean isOwner(){
    return Thread.currentThread() == owner;
  }
  /** 
 * Used only for unit tests.
 * @return the thread that currently holds the latch for exclusive access.
 */
  public Thread owner(){
    return owner;
  }
  /** 
 * Return the number of threads waiting.
 * @return the number of threads waiting for the latch.
 */
  public synchronized int nWaiters(){
    return (waiters != null) ? waiters.size() : 0;
  }
  /** 
 * Formats a latch owner and waiters.
 */
  public synchronized String toString(){
    return LatchSupport.latchTable.toString(name,owner,waiters,0);
  }
  /** 
 * For concocting exception messages
 */
  private String getNameString(){
    return LatchSupport.latchTable.getNameString(name);
  }
  /** 
 * Only call under the assert system. This records latching by thread.
 */
  private boolean noteLatch() throws LatchException {
    return LatchSupport.latchTable.noteLatch(this);
  }
  /** 
 * Only call under the assert system. This records latching by thread.
 */
  private boolean unNoteLatch(  boolean checkHeld){
    if (checkHeld) {
      return LatchSupport.latchTable.unNoteLatch(this,name);
    }
 else {
      LatchSupport.latchTable.unNoteLatch(this,name);
      return true;
    }
  }
  /** 
 * Simple class that encapsulates a Thread to be 'notify()ed'.
 */
static private class LatchWaiter {
    boolean active;
    Thread thread;
    LatchWaiter(    Thread thread){
      this.thread=thread;
      active=false;
    }
    public String toString(){
      return "<LatchWaiter: " + thread + ">";
    }
  }
  protected void hook422() throws DatabaseException, InterruptedException {
  }
  protected void hook423() throws DatabaseException, InterruptedException {
  }
  protected void hook424() throws DatabaseException, InterruptedException {
  }
  protected void hook425() throws LatchException {
  }
  protected void hook426() throws LatchException {
  }
  protected void hook427() throws LatchException {
  }
  protected void hook428(){
  }
}
\00Latches/com/sleepycat/je/latch/Java5LatchImpl.java:package com.sleepycat.je.latch;
import java.util.concurrent.locks.ReentrantLock;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.dbi.EnvironmentImpl;
import de.ovgu.cide.jakutil.*;
/** 
 * Java5LatchImpl provides an implementation of the Latch interface.  By using
 * a wrapper class we can avoid link errors when we run in Java 1.4 JVMs.
 * LatchSupport.java will only reference this class if it knows that the
 * ReentrantLock class is available at runtime through Class.forName().
 * LatchSupport only references this class through the Latch interface and only
 * constructs an instance using
 * Class.forName("Java5LatchImpl").newInstance();
 */
class Java5LatchImpl implements Latch {
static private class JEReentrantLock extends ReentrantLock {
    JEReentrantLock(    boolean fair){
      super(fair);
    }
    protected Thread getOwner(){
      return super.getOwner();
    }
  }
  private JEReentrantLock lock;
  private String name;
  Java5LatchImpl(){
    lock=new JEReentrantLock(EnvironmentImpl.getFairLatches());
  }
  /** 
 * Set the latch name, used for latches in objects instantiated from
 * the log.
 */
  public void setName(  String name){
    this.name=name;
  }
  /** 
 * Acquire a latch for exclusive/write access.
 * <p>Wait for the latch if some other thread is holding it.  If there are
 * threads waiting for access, they will be granted the latch on a FIFO
 * basis.  When the method returns, the latch is held for exclusive
 * access.</p>
 * @throws LatchException if the latch is already held by the calling
 * thread.
 */
  public void acquire() throws DatabaseException {
    try {
      if (lock.isHeldByCurrentThread()) {
        this.hook417();
        throw new LatchException(name + " already held");
      }
      this.hook416();
      lock.lock();
      assert noteLatch();
    }
  finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Acquire a latch for exclusive/write access, but do not block if it's not
 * available.
 * @return true if the latch was acquired, false if it is not available.
 * @throws LatchException if the latch is already held by the calling
 * thread.
 */
  public boolean acquireNoWait() throws LatchException {
    try {
      if (lock.isHeldByCurrentThread()) {
        this.hook418();
        throw new LatchException(name + " already held");
      }
      boolean ret=lock.tryLock();
      if (ret) {
        assert noteLatch();
        this.hook419();
      }
 else {
        this.hook420();
      }
      return ret;
    }
  finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Release the latch.  If there are other thread(s) waiting for the latch,
 * one is woken up and granted the latch. If the latch was not owned by 
 * the caller, just return;
 */
  public void releaseIfOwner(){
    doRelease(false);
  }
  /** 
 * Release the latch.  If there are other thread(s) waiting for the latch,
 * they are woken up and granted the latch.
 * @throws LatchNotHeldException if the latch is not currently held.
 */
  public void release() throws LatchNotHeldException {
    if (doRelease(true)) {
      throw new LatchNotHeldException(name + " not held");
    }
  }
  /** 
 * Do the work of releasing the latch. Wake up any waiters.
 * @returns true if this latch was not owned by the caller.
 */
  private boolean doRelease(  boolean checkHeld){
    try {
      if (!lock.isHeldByCurrentThread()) {
        return true;
      }
      lock.unlock();
      this.hook421();
      assert unNoteLatch(checkHeld);
    }
 catch (    IllegalMonitorStateException IMSE) {
      return true;
    }
    return false;
  }
  /** 
 * Return true if the current thread holds this latch.
 * @return true if we hold this latch.  False otherwise.
 */
  public boolean isOwner(){
    return lock.isHeldByCurrentThread();
  }
  /** 
 * Used only for unit tests.
 * @return the thread that currently holds the latch for exclusive access.
 */
  public Thread owner(){
    return lock.getOwner();
  }
  /** 
 * Return the number of threads waiting.
 * @return the number of threads waiting for the latch.
 */
  public int nWaiters(){
    return lock.getQueueLength();
  }
  /** 
 * Formats a latch owner and waiters.
 */
  public String toString(){
    return lock.toString();
  }
  /** 
 * Only call under the assert system. This records latching by thread.
 */
  private boolean noteLatch() throws LatchException {
    return LatchSupport.latchTable.noteLatch(this);
  }
  /** 
 * Only call under the assert system. This records latching by thread.
 */
  private boolean unNoteLatch(  boolean checkHeld){
    if (checkHeld) {
      return LatchSupport.latchTable.unNoteLatch(this,name);
    }
 else {
      LatchSupport.latchTable.unNoteLatch(this,name);
      return true;
    }
  }
  protected void hook416() throws DatabaseException {
  }
  protected void hook417() throws DatabaseException {
  }
  protected void hook418() throws LatchException {
  }
  protected void hook419() throws LatchException {
  }
  protected void hook420() throws LatchException {
  }
  protected void hook421() throws IllegalMonitorStateException {
  }
}
\00Latches/com/sleepycat/je/latch/SharedLatchImpl.java:package com.sleepycat.je.latch;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.dbi.EnvironmentImpl;
import de.ovgu.cide.jakutil.*;
/** 
 * Simple thread-based non-transactional reader-writer/shared-exclusive latch.
 * Latches provide simple exclusive or shared transient locks on objects.
 * Latches are expected to be held for short, defined periods of time.  No
 * deadlock detection is provided so it is the caller's responsibility to
 * sequence latch acquisition in an ordered fashion to avoid deadlocks.
 * Nested latches for a single thread are supported, but upgrading a shared
 * latch to an exclusive latch is not.  This implementation is based on the
 * section Reader-Writer Locks in the book Java Threads by Scott Oaks, 2nd
 * Edition, Chapter 8.
 * This SharedLatch implementation is only used when Java 5
 * ReentrantReadWriteLocks are not available.
 */
public class SharedLatchImpl implements SharedLatch {
  private String name=null;
  private List waiters=new ArrayList();
  private EnvironmentImpl env=null;
  private boolean noteLatch;
  /** 
 * Create a shared latch.
 */
  public SharedLatchImpl(  String name,  EnvironmentImpl env){
    this.name=name;
    this.env=env;
  }
  /** 
 * Set the latch name, used for latches in objects instantiated from the
 * log.
 */
  synchronized public void setName(  String name){
    this.name=name;
  }
  /** 
 * If noteLatch is true, then track latch usage in the latchTable.
 */
  synchronized public void setNoteLatch(  boolean noteLatch){
    this.noteLatch=noteLatch;
  }
  /** 
 * Acquire a latch for exclusive/write access.  Nesting is allowed, that
 * is, the latch may be acquired more than once by the same thread for
 * exclusive access.  However, if the thread already holds the latch for
 * shared access, it cannot be upgraded and LatchException will be thrown.
 * Wait for the latch if some other thread is holding it.  If there are
 * threads waiting for access, they will be granted the latch on a FIFO
 * basis.  When the method returns, the latch is held for exclusive access.
 * @throws LatchException if the latch is already held by the current
 * thread for shared access.
 * @throws RunRecoveryException if an InterruptedException exception
 * occurs.
 */
  public synchronized void acquireExclusive() throws DatabaseException {
    try {
      Thread thread=Thread.currentThread();
      int index=indexOf(thread);
      Owner owner;
      if (index < 0) {
        owner=new Owner(thread,Owner.EXCLUSIVE);
        waiters.add(owner);
      }
 else {
        throw new LatchException(getNameString() + " reentrancy/upgrade not allowed");
      }
      if (waiters.size() == 1) {
        this.hook429();
      }
 else {
        this.hook430();
        while (waiters.get(0) != owner) {
          wait();
        }
      }
      owner.nAcquires+=1;
      assert (noteLatch ? noteLatch() : true);
    }
 catch (    InterruptedException e) {
      throw new RunRecoveryException(env,e);
    }
 finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  public boolean acquireExclusiveNoWait() throws DatabaseException {
    try {
      Thread thread=Thread.currentThread();
      int index=indexOf(thread);
      if (index < 0) {
        if (waiters.size() == 0) {
          Owner owner=new Owner(thread,Owner.EXCLUSIVE);
          waiters.add(owner);
          owner.nAcquires+=1;
          this.hook431();
          assert (noteLatch ? noteLatch() : true);
          return true;
        }
 else {
          return false;
        }
      }
 else {
        throw new LatchException(getNameString() + " reentrancy/upgrade not allowed");
      }
    }
  finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Acquire a latch for shared/read access.  Nesting is allowed, that is,
 * the latch may be acquired more than once by the same thread.
 * @throws RunRecoveryException if an InterruptedException exception
 * occurs.
 */
  public synchronized void acquireShared() throws DatabaseException {
    try {
      Thread thread=Thread.currentThread();
      int index=indexOf(thread);
      Owner owner;
      if (index < 0) {
        owner=new Owner(thread,Owner.SHARED);
        waiters.add(owner);
      }
 else {
        owner=(Owner)waiters.get(index);
      }
      while (indexOf(thread) > firstWriter()) {
        wait();
      }
      owner.nAcquires+=1;
      this.hook432();
      assert (noteLatch ? noteLatch() : true);
    }
 catch (    InterruptedException e) {
      throw new RunRecoveryException(env,e);
    }
 finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Release an exclusive or shared latch.  If there are other thread(s)
 * waiting for the latch, they are woken up and granted the latch.
 */
  public synchronized void release() throws LatchNotHeldException {
    try {
      Thread thread=Thread.currentThread();
      int index=indexOf(thread);
      if (index < 0 || index > firstWriter()) {
        return;
      }
      Owner owner=(Owner)waiters.get(index);
      owner.nAcquires-=1;
      if (owner.nAcquires == 0) {
        waiters.remove(index);
        assert (noteLatch ? unNoteLatch() : true);
        notifyAll();
      }
      this.hook433();
    }
  finally {
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Returns the index of the first Owner for the given thread, or -1 if
 * none.
 */
  private int indexOf(  Thread thread){
    Iterator i=waiters.iterator();
    for (int index=0; i.hasNext(); index+=1) {
      Owner owner=(Owner)i.next();
      if (owner.thread == thread) {
        return index;
      }
    }
    return -1;
  }
  /** 
 * Returns the index of the first Owner waiting for a write lock, or
 * Integer.MAX_VALUE if none.
 */
  private int firstWriter(){
    Iterator i=waiters.iterator();
    for (int index=0; i.hasNext(); index+=1) {
      Owner owner=(Owner)i.next();
      if (owner.type == Owner.EXCLUSIVE) {
        return index;
      }
    }
    return Integer.MAX_VALUE;
  }
  /** 
 * Holds the state of a single owner thread.
 */
private static class Owner {
    static final int SHARED=0;
    static final int EXCLUSIVE=1;
    Thread thread;
    int type;
    int nAcquires;
    Owner(    Thread thread,    int type){
      this.thread=thread;
      this.type=type;
    }
  }
  private String getNameString(){
    return LatchSupport.latchTable.getNameString(name);
  }
  /** 
 * Only call under the assert system. This records latching by thread.
 */
  private boolean noteLatch() throws LatchException {
    return LatchSupport.latchTable.noteLatch(this);
  }
  /** 
 * Only call under the assert system. This records latching by thread.
 */
  private boolean unNoteLatch() throws LatchNotHeldException {
    return LatchSupport.latchTable.unNoteLatch(this,name);
  }
  public synchronized boolean isWriteLockedByCurrentThread(){
    if (waiters.size() > 0) {
      Owner curOwner=(Owner)waiters.get(0);
      return (curOwner.thread == Thread.currentThread() && curOwner.type == Owner.EXCLUSIVE);
    }
 else {
      return false;
    }
  }
  protected void hook429() throws DatabaseException, InterruptedException {
  }
  protected void hook430() throws DatabaseException, InterruptedException {
  }
  protected void hook431() throws DatabaseException {
  }
  protected void hook432() throws DatabaseException, InterruptedException {
  }
  protected void hook433() throws LatchNotHeldException {
  }
}
\00Evictor/com/sleepycat/je/cleaner/UtilizationTracker.java:package com.sleepycat.je.cleaner;
public class UtilizationTracker {
  /** 
 * Evicts tracked detail if the budget for the tracker is exceeded. Evicts
 * only one file summary LN at most to keep eviction batches small. Returns
 * the number of bytes freed.
 * <p>
 * When flushFileSummary is called, the TrackedFileSummary is cleared via
 * its reset method, which is called by FileSummaryLN.writeToLog. This is
 * how memory is subtracted from the budget.
 * </p>
 */
  public long evictMemory() throws DatabaseException {
    return new UtilizationTracker_evictMemory(this).execute();
  }
@MethodObject static class UtilizationTracker_evictMemory {
    UtilizationTracker_evictMemory(    UtilizationTracker _this){
      this._this=_this;
    }
    long execute() throws DatabaseException {
      if (!_this.cleaner.trackDetail) {
        return 0;
      }
      if (!_this.env.isOpen()) {
        return 0;
      }
      mb=_this.env.getMemoryBudget();
      totalEvicted=0;
      totalBytes=0;
      largestBytes=0;
      bestFile=null;
      a=_this.snapshot;
      for (int i=0; i < a.length; i+=1) {
        tfs=a[i];
        this.hook198();
        b1=tfs.getAllowFlush();
        this.hook197();
        if (b1) {
          this.hook199();
          bestFile=tfs;
        }
      }
      b2=bestFile != null;
      this.hook196();
      if (b2) {
        _this.env.getUtilizationProfile().flushFileSummary(bestFile);
        totalEvicted+=largestBytes;
      }
      return totalEvicted;
    }
    protected UtilizationTracker _this;
    protected MemoryBudget mb;
    protected long totalEvicted;
    protected long totalBytes;
    protected int largestBytes;
    protected TrackedFileSummary bestFile;
    protected TrackedFileSummary[] a;
    protected TrackedFileSummary tfs;
    protected int mem;
    protected boolean b1;
    protected boolean b2;
    protected void hook196() throws DatabaseException {
    }
    protected void hook197() throws DatabaseException {
    }
    protected void hook198() throws DatabaseException {
    }
    protected void hook199() throws DatabaseException {
    }
  }
}
\00Evictor/com/sleepycat/je/evictor/Evictor.java:package com.sleepycat.je.evictor;
import java.text.NumberFormat;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.INList;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.tree.SearchResult;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.utilint.DaemonThread;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.TestHook;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * The Evictor looks through the INList for IN's and BIN's that are worthy of
 * eviction. Once the nodes are selected, it removes all references to them so
 * that they can be GC'd by the JVM.
 */
public class Evictor {
  public static final String SOURCE_DAEMON="daemon";
  public static final String SOURCE_MANUAL="manual";
  public static final String SOURCE_CRITICAL="critical";
  private static final boolean DEBUG=false;
  private EnvironmentImpl envImpl;
  private LogManager logManager;
  private volatile boolean active;
  private IN nextNode;
  private long currentRequiredEvictBytes;
  private int nodesPerScan;
  private long evictBytesSetting;
  private boolean evictByLruOnly;
  private NumberFormat formatter;
  private int nNodesScannedThisRun;
  EvictProfile evictProfile;
  private TestHook runnableHook;
  public Evictor(  EnvironmentImpl envImpl,  String name) throws DatabaseException {
    super(0,name,envImpl);
    this.envImpl=envImpl;
    logManager=envImpl.getLogManager();
    nextNode=null;
    DbConfigManager configManager=envImpl.getConfigManager();
    nodesPerScan=configManager.getInt(EnvironmentParams.EVICTOR_NODES_PER_SCAN);
    evictBytesSetting=configManager.getLong(EnvironmentParams.EVICTOR_EVICT_BYTES);
    evictByLruOnly=configManager.getBoolean(EnvironmentParams.EVICTOR_LRU_ONLY);
    this.hook373(envImpl);
    evictProfile=new EvictProfile();
    formatter=NumberFormat.getNumberInstance();
    active=false;
  }
  public String toString(){
    StringBuffer sb=new StringBuffer();
    sb.append("<Evictor name=\"").append(name).append("\"/>");
    return sb.toString();
  }
  synchronized public void clearEnv(){
    envImpl=null;
  }
  /** 
 * Wakeup the evictor only if it's not already active.
 */
  public void alert(){
    if (!active) {
      wakeup();
    }
  }
  /** 
 * May be called by the evictor thread on wakeup or programatically.
 */
  public void doEvict(  String source) throws DatabaseException {
    doEvict(source,false);
  }
  /** 
 * Allows performing eviction during shutdown, which is needed when during
 * checkpointing and cleaner log file deletion.
 */
  private synchronized void doEvict(  String source,  boolean evictDuringShutdown) throws DatabaseException {
    if (active) {
      return;
    }
    active=true;
    try {
      boolean progress=true;
      while (progress && (evictDuringShutdown || !isShutdownRequested()) && isRunnable(source)) {
        if (evictBatch(source,currentRequiredEvictBytes) == 0) {
          progress=false;
        }
      }
    }
  finally {
      active=false;
    }
  }
  /** 
 * Each iteration will latch and unlatch the major INList, and will attempt
 * to evict requiredEvictBytes, but will give up after a complete pass over
 * the major INList. Releasing the latch is important because it provides an
 * opportunity for to add the minor INList to the major INList.
 * @return the number of bytes evicted, or zero if no progress was made.
 */
  long evictBatch(  String source,  long requiredEvictBytes) throws DatabaseException {
    return new Evictor_evictBatch(this,source,requiredEvictBytes).execute();
  }
  /** 
 * Return true if eviction should happen.
 */
  boolean isRunnable(  String source) throws DatabaseException {
    return new Evictor_isRunnable(this,source).execute();
  }
  /** 
 * Select a single node to evict.
 */
  private IN selectIN(  INList inList,  ScanIterator scanIter) throws DatabaseException {
    IN target=null;
    long targetGeneration=Long.MAX_VALUE;
    int targetLevel=Integer.MAX_VALUE;
    boolean targetDirty=true;
    boolean envIsReadOnly=envImpl.isReadOnly();
    int scanned=0;
    boolean wrapped=false;
    while (scanned < nodesPerScan) {
      if (scanIter.hasNext()) {
        IN in=scanIter.next();
        nNodesScannedThisRun++;
        DatabaseImpl db=in.getDatabase();
        boolean b=db == null;
        b=this.hook387(db,b);
        if (b) {
          String inInfo=" IN type=" + in.getLogType() + " id="+ in.getNodeId()+ " not expected on INList";
          String errMsg=(db == null) ? inInfo : "Database " + db.getDebugName() + " id="+ db.getId()+ inInfo;
          throw new DatabaseException(errMsg);
        }
        boolean b2=false;
        b2=this.hook386(db,b2);
        if (b2) {
          continue;
        }
        if (db.getId().equals(DbTree.ID_DB_ID)) {
          continue;
        }
        if (envIsReadOnly && (target != null) && in.getDirty()) {
          continue;
        }
        int evictType=in.getEvictionType();
        if (evictType == IN.MAY_NOT_EVICT) {
          continue;
        }
        if (evictByLruOnly) {
          if (targetGeneration > in.getGeneration()) {
            targetGeneration=in.getGeneration();
            target=in;
          }
        }
 else {
          int level=normalizeLevel(in,evictType);
          if (targetLevel != level) {
            if (targetLevel > level) {
              targetLevel=level;
              targetDirty=in.getDirty();
              targetGeneration=in.getGeneration();
              target=in;
            }
          }
 else           if (targetDirty != in.getDirty()) {
            if (targetDirty) {
              targetDirty=false;
              targetGeneration=in.getGeneration();
              target=in;
            }
          }
 else {
            if (targetGeneration > in.getGeneration()) {
              targetGeneration=in.getGeneration();
              target=in;
            }
          }
        }
        scanned++;
      }
 else {
        if (wrapped) {
          break;
        }
 else {
          nextNode=inList.first();
          scanIter.reset(nextNode);
          wrapped=true;
        }
      }
    }
    this.hook380(target);
    return target;
  }
  /** 
 * Normalize the tree level of the given IN.
 * Is public for unit testing.
 * A BIN containing evictable LNs is given level 0, so it will be stripped
 * first. For non-duplicate and DBMAP trees, the high order bits are cleared
 * to make their levels correspond; that way, all bottom level nodes (BINs
 * and DBINs) are given the same eviction priority.
 * Note that BINs in a duplicate tree are assigned the same level as BINs in
 * a non-duplicate tree. This isn't always optimimal, but is the best we can
 * do considering that BINs in duplicate trees may contain a mix of LNs and
 * DINs.
 */
  public int normalizeLevel(  IN in,  int evictType){
    int level=in.getLevel() & IN.LEVEL_MASK;
    if (level == 1 && evictType == IN.MAY_EVICT_LNS) {
      level=0;
    }
    return level;
  }
  /** 
 * Strip or evict this node.
 * @return number of bytes evicted.
 */
  private long evict(  INList inList,  IN target,  ScanIterator scanIter) throws DatabaseException {
    boolean envIsReadOnly=envImpl.isReadOnly();
    long evictedBytes=0;
    if (target.latchNoWait(false)) {
      evictedBytes=this.hook374(inList,target,scanIter,envIsReadOnly,evictedBytes);
    }
    return evictedBytes;
  }
  /** 
 * Evict an IN. Dirty nodes are logged before they're evicted. inlist is
 * latched with the major latch by the caller.
 */
  private long evictIN(  IN child,  IN parent,  int index,  INList inlist,  ScanIterator scanIter,  boolean envIsReadOnly) throws DatabaseException {
    long evictBytes=0;
    evictBytes=this.hook375(child,parent,index,inlist,scanIter,envIsReadOnly,evictBytes);
    return evictBytes;
  }
  /** 
 * Used by unit tests.
 */
  IN getNextNode(){
    return nextNode;
  }
  public void setRunnableHook(  TestHook hook){
    runnableHook=hook;
  }
static public class EvictProfile {
    private List candidates=new ArrayList();
    public boolean count(    IN target){
      candidates.add(new Long(target.getNodeId()));
      return true;
    }
    public List getCandidates(){
      return candidates;
    }
    public boolean clear(){
      candidates.clear();
      return true;
    }
  }
private static class ScanIterator {
    private INList inList;
    private Iterator iter;
    private IN nextMark;
    ScanIterator(    IN startingIN,    INList inList) throws DatabaseException {
      this.inList=inList;
      reset(startingIN);
    }
    void reset(    IN startingIN) throws DatabaseException {
      iter=inList.tailSet(startingIN).iterator();
    }
    IN mark() throws DatabaseException {
      if (iter.hasNext()) {
        nextMark=(IN)iter.next();
      }
 else {
        nextMark=(IN)inList.first();
      }
      return (IN)nextMark;
    }
    void resetToMark() throws DatabaseException {
      reset(nextMark);
    }
    boolean hasNext(){
      return iter.hasNext();
    }
    IN next(){
      return (IN)iter.next();
    }
    void remove(){
      iter.remove();
    }
  }
@MethodObject static class Evictor_evictBatch {
    Evictor_evictBatch(    Evictor _this,    String source,    long requiredEvictBytes){
      this._this=_this;
      this.source=source;
      this.requiredEvictBytes=requiredEvictBytes;
    }
    long execute() throws DatabaseException {
      _this.nNodesScannedThisRun=0;
      this.hook381();
      assert _this.evictProfile.clear();
      nBatchSets=0;
      finished=false;
      evictBytes=0;
      evictBytes+=_this.envImpl.getUtilizationTracker().evictMemory();
      inList=_this.envImpl.getInMemoryINs();
      this.hook376();
      inListStartSize=inList.getSize();
      try {
        if (inListStartSize == 0) {
          _this.nextNode=null;
          return 0;
        }
 else {
          if (_this.nextNode == null) {
            _this.nextNode=inList.first();
          }
        }
        scanIter=new ScanIterator(_this.nextNode,inList);
        while ((evictBytes < requiredEvictBytes) && (_this.nNodesScannedThisRun <= inListStartSize)) {
          target=_this.selectIN(inList,scanIter);
          if (target == null) {
            break;
          }
 else {
            assert _this.evictProfile.count(target);
            evictBytes+=_this.evict(inList,target,scanIter);
          }
          nBatchSets++;
        }
        _this.nextNode=scanIter.mark();
        finished=true;
      }
  finally {
        this.hook382();
        this.hook377();
        this.hook371();
      }
      return evictBytes;
    }
    protected Evictor _this;
    protected String source;
    protected long requiredEvictBytes;
    protected int nBatchSets;
    protected boolean finished;
    protected long evictBytes;
    protected INList inList;
    protected int inListStartSize;
    protected ScanIterator scanIter;
    protected IN target;
    protected Logger logger;
    protected String msg;
    protected void hook371() throws DatabaseException {
    }
    protected void hook376() throws DatabaseException {
    }
    protected void hook377() throws DatabaseException {
    }
    protected void hook381() throws DatabaseException {
    }
    protected void hook382() throws DatabaseException {
    }
  }
@MethodObject static class Evictor_isRunnable {
    Evictor_isRunnable(    Evictor _this,    String source){
      this._this=_this;
      this.source=source;
    }
    boolean execute() throws DatabaseException {
      mb=_this.envImpl.getMemoryBudget();
      this.hook388();
      this.hook372();
      result=false;
      return result;
    }
    protected Evictor _this;
    protected String source;
    protected MemoryBudget mb;
    protected long currentUsage;
    protected long maxMem;
    protected boolean doRun;
    protected Logger logger;
    protected Runtime r;
    protected long totalBytes;
    protected long freeBytes;
    protected long usedBytes;
    protected StringBuffer sb;
    protected boolean result;
    protected void hook372() throws DatabaseException {
    }
    protected void hook388() throws DatabaseException {
    }
  }
  protected void hook373(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected long hook374(  INList inList,  IN target,  ScanIterator scanIter,  boolean envIsReadOnly,  long evictedBytes) throws DatabaseException {
    if (target instanceof BIN) {
      this.hook385(target);
      evictedBytes=((BIN)target).evictLNs();
      this.hook383(evictedBytes);
    }
    if (evictedBytes == 0 && target.isEvictable()) {
      Tree tree=target.getDatabase().getTree();
      SearchResult result=tree.getParentINForChildIN(target,true,false);
      if (result.exactParentFound) {
        evictedBytes=evictIN(target,result.parent,result.index,inList,scanIter,envIsReadOnly);
      }
    }
    return evictedBytes;
  }
  protected long hook375(  IN child,  IN parent,  int index,  INList inlist,  ScanIterator scanIter,  boolean envIsReadOnly,  long evictBytes) throws DatabaseException {
    this.hook378(parent);
    long oldGenerationCount=child.getGeneration();
    IN renewedChild=(IN)parent.getTarget(index);
    if ((renewedChild != null) && (renewedChild.getGeneration() <= oldGenerationCount) && renewedChild.latchNoWait(false)) {
      evictBytes=this.hook379(parent,index,inlist,scanIter,envIsReadOnly,evictBytes,renewedChild);
    }
    return evictBytes;
  }
  protected void hook378(  IN parent) throws DatabaseException {
  }
  protected long hook379(  IN parent,  int index,  INList inlist,  ScanIterator scanIter,  boolean envIsReadOnly,  long evictBytes,  IN renewedChild) throws DatabaseException {
    if (renewedChild.isEvictable()) {
      long renewedChildLsn=DbLsn.NULL_LSN;
      boolean newChildLsn=false;
      if (renewedChild.getDirty()) {
        if (!envIsReadOnly) {
          boolean logProvisional=(envImpl.getCheckpointer() != null && (renewedChild.getLevel() < envImpl.getCheckpointer().getHighestFlushLevel()));
          renewedChildLsn=renewedChild.log(logManager,false,logProvisional,true,parent);
          newChildLsn=true;
        }
      }
 else {
        renewedChildLsn=parent.getLsn(index);
      }
      if (renewedChildLsn != DbLsn.NULL_LSN) {
        scanIter.mark();
        inlist.removeLatchAlreadyHeld(renewedChild);
        scanIter.resetToMark();
        evictBytes=this.hook389(evictBytes,renewedChild);
        if (newChildLsn) {
          parent.updateEntry(index,null,renewedChildLsn);
        }
 else {
          parent.updateEntry(index,(Node)null);
        }
        this.hook384();
      }
    }
    return evictBytes;
  }
  protected void hook380(  IN target) throws DatabaseException {
  }
  protected void hook383(  long evictedBytes) throws DatabaseException {
  }
  protected void hook384() throws DatabaseException {
  }
  protected void hook385(  IN target) throws DatabaseException {
  }
  protected boolean hook386(  DatabaseImpl db,  boolean b2) throws DatabaseException {
    return b2;
  }
  protected boolean hook387(  DatabaseImpl db,  boolean b) throws DatabaseException {
    return b;
  }
  protected long hook389(  long evictBytes,  IN renewedChild) throws DatabaseException {
    return evictBytes;
  }
}
\00Evictor/com/sleepycat/je/util/DbRunAction.java:package com.sleepycat.je.util;
public class DbRunAction {
  private static final int EVICT=3;
  private static void doEvict(  Environment env) throws DatabaseException {
    new DbRunAction_doEvict(env).execute();
  }
@MethodObject static class DbRunAction_doEvict {
    DbRunAction_doEvict(    Environment env){
      this.env=env;
    }
    void execute() throws DatabaseException {
      envImpl=DbInternal.envGetEnvironmentImpl(env);
      this.hook837();
      c=new EnvironmentMutableConfig();
      this.hook836();
      env.setMutableConfig(c);
      start=System.currentTimeMillis();
      env.evictMemory();
      end=System.currentTimeMillis();
      f=new DecimalFormat();
      f.setMaximumFractionDigits(2);
      System.out.println("evict time=" + f.format(end - start));
    }
    protected Environment env;
    protected EnvironmentImpl envImpl;
    protected long cacheUsage;
    protected EnvironmentMutableConfig c;
    protected long start;
    protected long end;
    protected DecimalFormat f;
    protected void hook836() throws DatabaseException {
    }
    protected void hook837() throws DatabaseException {
    }
  }
@MethodObject static class DbRunAction_main {
    protected void hook844() throws Exception {
      if (doAction == EVICT) {
        preload(env,dbName);
      }
      original();
    }
    protected void hook845() throws Exception {
      if (doAction == EVICT) {
        envConfig.setConfigParam(EnvironmentParams.ENV_RUN_EVICTOR.getName(),"false");
        envConfig.setConfigParam(EnvironmentParams.EVICTOR_CRITICAL_PERCENTAGE.getName(),"1000");
      }
      original();
    }
    protected void hook846() throws Exception {
      if (action.equalsIgnoreCase("evict")) {
        doAction=EVICT;
      }
 else {
        original();
      }
    }
  }
}
\00Evictor/com/sleepycat/je/dbi/CursorImpl.java:package com.sleepycat.je.dbi;
public class CursorImpl {
  private boolean allowEviction=true;
  /** 
 * Disables or enables eviction during cursor operations for an internal
 * cursor. For example, a cursor used to implement eviction should not
 * itself perform eviction. Eviction is enabled by default.
 */
  public void setAllowEviction(  boolean allowed){
    allowEviction=allowed;
  }
  /** 
 * Evict the LN node at the cursor position. This is used for internal
 * databases only.
 */
  public void evict() throws DatabaseException {
    this.hook202();
    setTargetBin();
    targetBin.evictLN(targetIndex);
  }
  protected void hook202() throws DatabaseException {
  }
  protected void hook203() throws DatabaseException {
  }
  /** 
 * Shallow copy. addCursor() is optionally called. Allows inheriting the BIN
 * position from some other cursor.
 */
  public CursorImpl cloneCursor(  boolean addCursor,  CursorImpl usePosition) throws DatabaseException {
    CursorImpl result=original(addCursor,usePosition);
    if (allowEviction) {
      this.hook203();
    }
    return result;
  }
}
\00MemoryBudget/com/sleepycat/je/cleaner/FileProcessor.java:package com.sleepycat.je.cleaner;
class FileProcessor {
@MethodObject static class FileProcessor_processFile {
    protected void hook118() throws DatabaseException, IOException {
    }
    protected void hook161() throws DatabaseException, IOException {
      adjustMem=(2 * readBufferSize) + obsoleteOffsets.getLogSize();
      budget=_this.env.getMemoryBudget();
{
        this.hook118();
        budget.updateMiscMemoryUsage(adjustMem);
      }
      original();
    }
    protected void hook162() throws DatabaseException, IOException {
      budget.updateMiscMemoryUsage(0 - adjustMem);
      original();
    }
  }
}
\00MemoryBudget/com/sleepycat/je/util/DbCacheSize.java:package com.sleepycat.je.util;
import java.io.File;
import java.io.PrintStream;
import java.math.BigInteger;
import java.text.NumberFormat;
import java.util.Random;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.OperationStatus;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.utilint.CmdUtil;
import de.ovgu.cide.jakutil.*;
/** 
 * Estimating JE in-memory sizes as a function of key and data size is not
 * straightforward for two reasons. There is some fixed overhead for each btree
 * internal node, so tree fanout and degree of node sparseness impacts memory
 * consumption. In addition, JE compresses some of the internal nodes where
 * possible, but compression depends on on-disk layouts.
 * DbCacheSize is an aid for estimating cache sizes. To get an estimate of the
 * in-memory footprint for a given database, specify the number of records and
 * record characteristics and DbCacheSize will return a minimum and maximum
 * estimate of the cache size required for holding the database in memory. If
 * the user specifies the record's data size, the utility will return both
 * values for holding just the internal nodes of the btree, and for holding the
 * entire database in cache.
 * Note that "cache size" is a percentage more than "btree size", to cover
 * general environment resources like log buffers. Each invocation of the
 * utility returns an estimate for a single database in an environment. For an
 * environment with multiple databases, run the utility for each database, add
 * up the btree sizes, and then add 10 percent.
 * Note that the utility does not yet cover duplicate records and the API is
 * subject to change release to release.
 * The only required parameters are the number of records and key size. Data
 * size, non-tree cache overhead, btree fanout, and other parameters can also be
 * provided. For example:
 * $ java DbCacheSize -records 554719 -key 16 -data 100 Inputs: records=554719
 * keySize=16 dataSize=100 nodeMax=128 density=80% overhead=10%
 * Cache Size Btree Size Description -------------- -------------- -----------
 * 30,547,440 27,492,696 Minimum, internal nodes only 41,460,720 37,314,648
 * Maximum, internal nodes only 114,371,644 102,934,480 Minimum, internal nodes
 * and leaf nodes 125,284,924 112,756,432 Maximum, internal nodes and leaf nodes
 * Btree levels: 3
 * This says that the minimum cache size to hold only the internal nodes of the
 * btree in cache is approximately 30MB. The maximum size to hold the entire
 * database in cache, both internal nodes and datarecords, is 125Mb.
 */
public class DbCacheSize {
  private static final NumberFormat INT_FORMAT=NumberFormat.getIntegerInstance();
  private static final String HEADER="    Cache Size      Btree Size  Description\n" + "--------------  --------------  -----------";
  private static final int COLUMN_WIDTH=14;
  private static final int COLUMN_SEPARATOR=2;
  public static void main(  String[] args){
    try {
      long records=0;
      int keySize=0;
      int dataSize=0;
      int nodeMax=128;
      int density=80;
      long overhead=0;
      File measureDir=null;
      boolean measureRandom=false;
      for (int i=0; i < args.length; i+=1) {
        String name=args[i];
        String val=null;
        if (i < args.length - 1 && !args[i + 1].startsWith("-")) {
          i+=1;
          val=args[i];
        }
        if (name.equals("-records")) {
          if (val == null) {
            usage("No value after -records");
          }
          try {
            records=Long.parseLong(val);
          }
 catch (          NumberFormatException e) {
            usage(val + " is not a number");
          }
          if (records <= 0) {
            usage(val + " is not a positive integer");
          }
        }
 else         if (name.equals("-key")) {
          if (val == null) {
            usage("No value after -key");
          }
          try {
            keySize=Integer.parseInt(val);
          }
 catch (          NumberFormatException e) {
            usage(val + " is not a number");
          }
          if (keySize <= 0) {
            usage(val + " is not a positive integer");
          }
        }
 else         if (name.equals("-data")) {
          if (val == null) {
            usage("No value after -data");
          }
          try {
            dataSize=Integer.parseInt(val);
          }
 catch (          NumberFormatException e) {
            usage(val + " is not a number");
          }
          if (dataSize <= 0) {
            usage(val + " is not a positive integer");
          }
        }
 else         if (name.equals("-nodemax")) {
          if (val == null) {
            usage("No value after -nodemax");
          }
          try {
            nodeMax=Integer.parseInt(val);
          }
 catch (          NumberFormatException e) {
            usage(val + " is not a number");
          }
          if (nodeMax <= 0) {
            usage(val + " is not a positive integer");
          }
        }
 else         if (name.equals("-density")) {
          if (val == null) {
            usage("No value after -density");
          }
          try {
            density=Integer.parseInt(val);
          }
 catch (          NumberFormatException e) {
            usage(val + " is not a number");
          }
          if (density < 1 || density > 100) {
            usage(val + " is not betwen 1 and 100");
          }
        }
 else         if (name.equals("-overhead")) {
          if (val == null) {
            usage("No value after -overhead");
          }
          try {
            overhead=Long.parseLong(val);
          }
 catch (          NumberFormatException e) {
            usage(val + " is not a number");
          }
          if (overhead < 0) {
            usage(val + " is not a non-negative integer");
          }
        }
 else         if (name.equals("-measure")) {
          if (val == null) {
            usage("No value after -measure");
          }
          measureDir=new File(val);
        }
 else         if (name.equals("-measurerandom")) {
          measureRandom=true;
        }
 else {
          usage("Unknown arg: " + name);
        }
      }
      if (records == 0) {
        usage("-records not specified");
      }
      if (keySize == 0) {
        usage("-key not specified");
      }
      printCacheSizes(System.out,records,keySize,dataSize,nodeMax,density,overhead);
      if (measureDir != null) {
        measure(System.out,measureDir,records,keySize,dataSize,nodeMax,measureRandom);
      }
    }
 catch (    Throwable e) {
      e.printStackTrace(System.out);
    }
  }
  private static void usage(  String msg){
    if (msg != null) {
      System.out.println(msg);
    }
    System.out.println("usage:" + "\njava " + CmdUtil.getJavaCommand(DbCacheSize.class) + "\n   -records <count>"+ "\n      # Total records (key/data pairs); required"+ "\n   -key <bytes> "+ "\n      # Average key bytes per record; required"+ "\n  [-data <bytes>]"+ "\n      # Average data bytes per record; if omitted no leaf"+ "\n      # node sizes are included in the output"+ "\n  [-nodemax <entries>]"+ "\n      # Number of entries per Btree node; default: 128"+ "\n  [-density <percentage>]"+ "\n      # Percentage of node entries occupied; default: 80"+ "\n  [-overhead <bytes>]"+ "\n      # Overhead of non-Btree objects (log buffers, locks,"+ "\n      # etc); default: 10% of total cache size"+ "\n  [-measure <environmentHomeDirectory>]"+ "\n      # An empty directory used to write a database to find"+ "\n      # the actual cache size; default: do not measure"+ "\n  [-measurerandom"+ "\n      # With -measure insert randomly generated keys;"+ "\n      # default: insert sequential keys");
    System.exit(2);
  }
  private static void printCacheSizes(  PrintStream out,  long records,  int keySize,  int dataSize,  int nodeMax,  int density,  long overhead){
    out.println("Inputs:" + " records=" + records + " keySize="+ keySize+ " dataSize="+ dataSize+ " nodeMax="+ nodeMax+ " density="+ density+ '%'+ " overhead="+ ((overhead > 0) ? overhead : 10)+ "%");
    int nodeAvg=(nodeMax * density) / 100;
    long nBinEntries=(records * nodeMax) / nodeAvg;
    long nBinNodes=(nBinEntries + nodeMax - 1) / nodeMax;
    long nInNodes=0;
    int nLevels=1;
    for (long n=nBinNodes; n > 0; n/=nodeMax) {
      nInNodes+=n;
      nLevels+=1;
    }
    long minInSize=nInNodes * calcInSize(nodeMax,nodeAvg,keySize,true);
    long maxInSize=nInNodes * calcInSize(nodeMax,nodeAvg,keySize,false);
    long lnSize=0;
    if (dataSize > 0) {
      lnSize=records * calcLnSize(dataSize);
    }
    out.println();
    out.println(HEADER);
    out.println(line(minInSize,overhead,"Minimum, internal nodes only"));
    out.println(line(maxInSize,overhead,"Maximum, internal nodes only"));
    if (dataSize > 0) {
      out.println(line(minInSize + lnSize,overhead,"Minimum, internal nodes and leaf nodes"));
      out.println(line(maxInSize + lnSize,overhead,"Maximum, internal nodes and leaf nodes"));
    }
 else {
      out.println("\nTo get leaf node sizing specify -data");
    }
    out.println("\nBtree levels: " + nLevels);
  }
  private static int calcInSize(  int nodeMax,  int nodeAvg,  int keySize,  boolean lsnCompression){
    int size=MemoryBudget.IN_FIXED_OVERHEAD;
    size+=MemoryBudget.byteArraySize(nodeMax) + (nodeMax * (2 * MemoryBudget.ARRAY_ITEM_OVERHEAD));
    if (lsnCompression) {
      size+=MemoryBudget.byteArraySize(nodeMax * 2);
    }
 else {
      size+=MemoryBudget.BYTE_ARRAY_OVERHEAD + (nodeMax * MemoryBudget.LONG_OVERHEAD);
    }
    size+=(nodeAvg + 1) * MemoryBudget.byteArraySize(keySize);
    return size;
  }
  private static int calcLnSize(  int dataSize){
    return MemoryBudget.LN_OVERHEAD + MemoryBudget.byteArraySize(dataSize);
  }
  private static String line(  long btreeSize,  long overhead,  String comment){
    long cacheSize;
    if (overhead == 0) {
      cacheSize=(100 * btreeSize) / 90;
    }
 else {
      cacheSize=btreeSize + overhead;
    }
    StringBuffer buf=new StringBuffer(100);
    column(buf,INT_FORMAT.format(cacheSize));
    column(buf,INT_FORMAT.format(btreeSize));
    column(buf,comment);
    return buf.toString();
  }
  private static void column(  StringBuffer buf,  String str){
    int start=buf.length();
    while (buf.length() - start + str.length() < COLUMN_WIDTH) {
      buf.append(' ');
    }
    buf.append(str);
    for (int i=0; i < COLUMN_SEPARATOR; i+=1) {
      buf.append(' ');
    }
  }
  private static void measure(  PrintStream out,  File dir,  long records,  int keySize,  int dataSize,  int nodeMax,  boolean randomKeys) throws DatabaseException {
    String[] fileNames=dir.list();
    if (fileNames != null && fileNames.length > 0) {
      usage("Directory is not empty: " + dir);
    }
    Environment env=openEnvironment(dir,true);
    Database db=openDatabase(env,nodeMax,true);
    try {
      out.println("\nMeasuring with cache size: " + INT_FORMAT.format(env.getConfig().getCacheSize()));
      insertRecords(out,env,db,records,keySize,dataSize,randomKeys);
      hook832(out,env);
      db.close();
      env.close();
      env=openEnvironment(dir,false);
      db=openDatabase(env,nodeMax,false);
      out.println("\nPreloading with cache size: " + INT_FORMAT.format(env.getConfig().getCacheSize()));
      preloadRecords(out,db);
      hook831(out,env);
    }
  finally {
      try {
        db.close();
        env.close();
      }
 catch (      Exception e) {
        out.println("During close: " + e);
      }
    }
  }
  private static Environment openEnvironment(  File dir,  boolean allowCreate) throws DatabaseException {
    EnvironmentConfig envConfig=new EnvironmentConfig();
    envConfig.setAllowCreate(allowCreate);
    envConfig.setCachePercent(90);
    return new Environment(dir,envConfig);
  }
  private static Database openDatabase(  Environment env,  int nodeMax,  boolean allowCreate) throws DatabaseException {
    DatabaseConfig dbConfig=new DatabaseConfig();
    dbConfig.setAllowCreate(allowCreate);
    dbConfig.setNodeMaxEntries(nodeMax);
    return env.openDatabase(null,"foo",dbConfig);
  }
  private static void insertRecords(  PrintStream out,  Environment env,  Database db,  long records,  int keySize,  int dataSize,  boolean randomKeys) throws DatabaseException {
    new DbCacheSize_insertRecords(out,env,db,records,keySize,dataSize,randomKeys).execute();
  }
  private static void preloadRecords(  final PrintStream out,  final Database db) throws DatabaseException {
    Thread thread=new Thread(){
      public void run(){
        while (true) {
          try {
            out.print(".");
            out.flush();
            Thread.sleep(5 * 1000);
          }
 catch (          InterruptedException e) {
            break;
          }
        }
      }
    }
;
    thread.start();
    db.preload(0);
    thread.interrupt();
    try {
      thread.join();
    }
 catch (    InterruptedException e) {
      e.printStackTrace(out);
    }
  }
@MethodObject static class DbCacheSize_insertRecords {
    DbCacheSize_insertRecords(    PrintStream out,    Environment env,    Database db,    long records,    int keySize,    int dataSize,    boolean randomKeys){
      this.out=out;
      this.env=env;
      this.db=db;
      this.records=records;
      this.keySize=keySize;
      this.dataSize=dataSize;
      this.randomKeys=randomKeys;
    }
    void execute() throws DatabaseException {
      try {
        key=new DatabaseEntry();
        data=new DatabaseEntry(new byte[dataSize]);
        bigInt=BigInteger.ZERO;
        rnd=new Random(123);
        for (int i=0; i < records; i+=1) {
          if (randomKeys) {
            a=new byte[keySize];
            rnd.nextBytes(a);
            key.setData(a);
          }
 else {
            bigInt=bigInt.add(BigInteger.ONE);
            a2=bigInt.toByteArray();
            if (a2.length < keySize) {
              a3=new byte[keySize];
              System.arraycopy(a2,0,a3,a3.length - a2.length,a2.length);
              a2=a3;
            }
 else             if (a2.length > keySize) {
              out.println("*** Key doesn't fit value=" + bigInt + " byte length="+ a2.length);
              return;
            }
            key.setData(a2);
          }
          status=db.putNoOverwrite(null,key,data);
          if (status == OperationStatus.KEYEXIST && randomKeys) {
            i-=1;
            out.println("Random key already exists -- retrying");
            continue;
          }
          if (status != OperationStatus.SUCCESS) {
            out.println("*** " + status);
            return;
          }
          if (i % 10000 == 0) {
            this.hook833();
            out.print(".");
            out.flush();
          }
        }
      }
 catch (      ReturnVoid r) {
        return;
      }
    }
    protected PrintStream out;
    protected Environment env;
    protected Database db;
    protected long records;
    protected int keySize;
    protected int dataSize;
    protected boolean randomKeys;
    protected DatabaseEntry key;
    protected DatabaseEntry data;
    protected BigInteger bigInt;
    protected Random rnd;
    protected byte[] a;
    protected byte[] a2;
    protected byte[] a3;
    protected OperationStatus status;
    protected EnvironmentStats stats;
    protected void hook833() throws DatabaseException {
    }
  }
  protected static void hook831(  PrintStream out,  Environment env) throws DatabaseException {
  }
  protected static void hook832(  PrintStream out,  Environment env) throws DatabaseException {
  }
}
\00Statistics/com/sleepycat/je/dbi/CursorImpl.java:package com.sleepycat.je.dbi;
public class CursorImpl {
  public LockStats getLockStats() throws DatabaseException {
    return locker.collectStats(new LockStats());
  }
@MethodObject static class CursorImpl_getNextDuplicate {
    protected void hook200() throws DatabaseException {
      if (_this.index < 0) {
        throw new ReturnObject(OperationStatus.NOTFOUND);
      }
      duplicateRoot=(DIN)_this.bin.fetchTarget(_this.index);
      this.hook201();
    }
    protected void hook201() throws DatabaseException {
      dcl=duplicateRoot.getDupCountLN();
      if (dcl != null) {
        dcl.accumulateStats(treeStatsAccumulator);
      }
    }
    protected void hook275() throws DatabaseException {
      treeStatsAccumulator=_this.getTreeStatsAccumulator();
      if (treeStatsAccumulator != null) {
        this.hook200();
      }
      original();
    }
  }
}
\00Statistics/com/sleepycat/je/txn/LockManager.java:package com.sleepycat.je.txn;
import com.sleepycat.je.StatsConfig;
public abstract class LockManager {
  private long nRequests;
  private long nWaits;
  /** 
 * Statistics
 */
  public LockStats lockStat(  StatsConfig config) throws DatabaseException {
    return new LockManager_lockStat(this,config).execute();
  }
  protected void hook774() throws DatabaseException {
    nRequests=0;
    nWaits=0;
    original();
  }
  protected LockAttemptResult attemptLockInternal(  Long nodeId,  Locker locker,  LockType type,  boolean nonBlockingRequest,  int lockTableIndex) throws DatabaseException {
    nRequests++;
    return original(nodeId,locker,type,nonBlockingRequest,lockTableIndex);
  }
  protected void hook775() throws DatabaseException {
    nWaits++;
    original();
  }
  protected void hook776(  LockStats stats,  Map lockTable){
    stats.accumulateNTotalLocks(lockTable.size());
    original(stats,lockTable);
  }
  protected void hook777(  LockStats stats,  Lock lock){
    stats.setNWaiters(stats.getNWaiters() + lock.nWaiters());
    stats.setNOwners(stats.getNOwners() + lock.nOwners());
    original(stats,lock);
  }
  protected void hook778(  LockStats stats,  LockInfo info){
    if (info.getLockType().isWriteLock()) {
      stats.setNWriteLocks(stats.getNWriteLocks() + 1);
    }
 else {
      stats.setNReadLocks(stats.getNReadLocks() + 1);
    }
    original(stats,info);
  }
@MethodObject static class LockManager_lockStat {
    LockManager_lockStat(    LockManager _this,    StatsConfig config){
      this._this=_this;
      this.config=config;
    }
    LockStats execute() throws DatabaseException {
      stats=new LockStats();
      stats.setNRequests(_this.nRequests);
      stats.setNWaits(_this.nWaits);
      if (config.getClear()) {
        _this.nWaits=0;
        _this.nRequests=0;
      }
      this.hook769();
      if (!config.getFast()) {
        _this.dumpLockTable(stats);
      }
      return stats;
    }
    protected LockManager _this;
    protected StatsConfig config;
    protected LockStats stats;
    protected LatchStats latchStats;
    protected void hook769() throws DatabaseException {
    }
  }
}
\00Statistics/com/sleepycat/je/EnvironmentStats.java:package com.sleepycat.je;
import java.io.Serializable;
import com.sleepycat.je.utilint.DbLsn;
import de.ovgu.cide.jakutil.*;
/** 
 * Javadoc for this public class is generated
 * via the doc templates in the doc_src directory.
 */
public class EnvironmentStats implements Serializable {
  /** 
 * The number of bins encountered by the INCompressor that were split
 * between the time they were put on the compressor queue and when
 * the compressor ran.
 */
  private int splitBins;
  /** 
 * The number of bins encountered by the INCompressor that had their
 * database closed between the time they were put on the
 * compressor queue and when the compressor ran.
 */
  private int dbClosedBins;
  /** 
 * The number of bins encountered by the INCompressor that had cursors
 * referring to them when the compressor ran.
 */
  private int cursorsBins;
  /** 
 * The number of bins encountered by the INCompressor that were
 * not actually empty when the compressor ran.
 */
  private int nonEmptyBins;
  /** 
 * The number of bins that were successfully processed by the IN
 * Compressor.
 */
  private int processedBins;
  /** 
 * The number of entries in the INCompressor queue when the getStats()
 * call was made.
 */
  private int inCompQueueSize;
  /** 
 * The number of passes made to the evictor.
 */
  private int nEvictPasses;
  /** 
 * The accumulated number of nodes selected to evict.
 */
  private long nNodesSelected;
  /** 
 * The accumulated number of nodes scanned in order to select the
 * eviction set.
 */
  private long nNodesScanned;
  /** 
 * The accumulated number of nodes evicted.
 */
  private long nNodesExplicitlyEvicted;
  /** 
 * The number of BINs stripped by the evictor.
 */
  private long nBINsStripped;
  /** 
 * The number of bytes we need to evict in order to get under budget.
 */
  private long requiredEvictBytes;
  /** 
 * The total number of checkpoints run so far.
 */
  private int nCheckpoints;
  /** 
 * The Id of the last checkpoint.
 */
  private long lastCheckpointId;
  /** 
 * The accumulated number of full INs flushed to the log.
 */
  private int nFullINFlush;
  /** 
 * The accumulated number of full BINs flushed to the log.
 */
  private int nFullBINFlush;
  /** 
 * The accumulated number of Delta INs flushed to the log.
 */
  private int nDeltaINFlush;
  /** 
 * The location in the log of the last checkpoint start.
 */
  private long lastCheckpointStart;
  /** 
 * The location in the log of the last checkpoint end.
 */
  private long lastCheckpointEnd;
  /** 
 * The number of files to be cleaned to reach the target utilization. 
 */
  private int cleanerBacklog;
  /** 
 * The number of cleaner runs this session. 
 */
  private int nCleanerRuns;
  /** 
 * The number of cleaner file deletions this session. 
 */
  private int nCleanerDeletions;
  /** 
 * The accumulated number of INs obsolete.
 */
  private int nINsObsolete;
  /** 
 * The accumulated number of INs cleaned.
 */
  private int nINsCleaned;
  /** 
 * The accumulated number of INs that were not found in the tree anymore
 * (deleted).
 */
  private int nINsDead;
  /** 
 * The accumulated number of INs migrated.
 */
  private int nINsMigrated;
  /** 
 * The accumulated number of LNs obsolete.
 */
  private int nLNsObsolete;
  /** 
 * The accumulated number of LNs cleaned.
 */
  private int nLNsCleaned;
  /** 
 * The accumulated number of LNs that were not found in the tree anymore
 * (deleted).
 */
  private int nLNsDead;
  /** 
 * The accumulated number of LNs encountered that were locked.
 */
  private int nLNsLocked;
  /** 
 * The accumulated number of LNs encountered that were migrated forward
 * in the log.
 */
  private int nLNsMigrated;
  /** 
 * The accumulated number of LNs that were marked for migration during
 * cleaning.
 */
  private int nLNsMarked;
  /** 
 * The accumulated number of LNs processed without a tree lookup.
 */
  private int nLNQueueHits;
  /** 
 * The accumulated number of LNs processed because they were previously
 * locked.
 */
  private int nPendingLNsProcessed;
  /** 
 * The accumulated number of LNs processed because they were previously
 * marked for migration.
 */
  private int nMarkedLNsProcessed;
  /** 
 * The accumulated number of LNs processed because they are soon to be
 * cleaned.
 */
  private int nToBeCleanedLNsProcessed;
  /** 
 * The accumulated number of LNs processed because they qualify for
 * clustering.
 */
  private int nClusterLNsProcessed;
  /** 
 * The accumulated number of pending LNs that could not be locked for
 * migration because of a long duration application lock.
 */
  private int nPendingLNsLocked;
  /** 
 * The accumulated number of log entries read by the cleaner.
 */
  private int nCleanerEntriesRead;
  private long cacheDataBytes;
  private long nNotResident;
  private long nCacheMiss;
  private int nLogBuffers;
  private long bufferBytes;
  private long nRepeatFaultReads;
  private long nTempBufferWrites;
  private long nRepeatIteratorReads;
  /** 
 * Internal use only.
 */
  public EnvironmentStats(){
    reset();
  }
  /** 
 * Resets all stats.
 */
  private void reset(){
    splitBins=0;
    dbClosedBins=0;
    cursorsBins=0;
    nonEmptyBins=0;
    processedBins=0;
    inCompQueueSize=0;
    nEvictPasses=0;
    nNodesSelected=0;
    nNodesScanned=0;
    nNodesExplicitlyEvicted=0;
    nBINsStripped=0;
    requiredEvictBytes=0;
    nCheckpoints=0;
    lastCheckpointId=0;
    nFullINFlush=0;
    nFullBINFlush=0;
    nDeltaINFlush=0;
    lastCheckpointStart=DbLsn.NULL_LSN;
    lastCheckpointEnd=DbLsn.NULL_LSN;
    cleanerBacklog=0;
    nCleanerRuns=0;
    nCleanerDeletions=0;
    nINsObsolete=0;
    nINsCleaned=0;
    nINsDead=0;
    nINsMigrated=0;
    nLNsObsolete=0;
    nLNsCleaned=0;
    nLNsDead=0;
    nLNsLocked=0;
    nLNsMigrated=0;
    nLNsMarked=0;
    nLNQueueHits=0;
    nPendingLNsProcessed=0;
    nMarkedLNsProcessed=0;
    nToBeCleanedLNsProcessed=0;
    nClusterLNsProcessed=0;
    nPendingLNsLocked=0;
    nCleanerEntriesRead=0;
    cacheDataBytes=0;
    nNotResident=0;
    nCacheMiss=0;
    nLogBuffers=0;
    bufferBytes=0;
    this.hook60();
    nRepeatFaultReads=0;
    nTempBufferWrites=0;
    nRepeatIteratorReads=0;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getBufferBytes(){
    return bufferBytes;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getCursorsBins(){
    return cursorsBins;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getDbClosedBins(){
    return dbClosedBins;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getInCompQueueSize(){
    return inCompQueueSize;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getLastCheckpointId(){
    return lastCheckpointId;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNCacheMiss(){
    return nCacheMiss;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNCheckpoints(){
    return nCheckpoints;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getCleanerBacklog(){
    return cleanerBacklog;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNCleanerRuns(){
    return nCleanerRuns;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNCleanerDeletions(){
    return nCleanerDeletions;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNDeltaINFlush(){
    return nDeltaINFlush;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getLastCheckpointEnd(){
    return lastCheckpointEnd;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getLastCheckpointStart(){
    return lastCheckpointStart;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNCleanerEntriesRead(){
    return nCleanerEntriesRead;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNEvictPasses(){
    return nEvictPasses;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNFullINFlush(){
    return nFullINFlush;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNFullBINFlush(){
    return nFullBINFlush;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNINsObsolete(){
    return nINsObsolete;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNINsCleaned(){
    return nINsCleaned;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNINsDead(){
    return nINsDead;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNINsMigrated(){
    return nINsMigrated;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLNsObsolete(){
    return nLNsObsolete;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLNsCleaned(){
    return nLNsCleaned;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLNsDead(){
    return nLNsDead;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLNsLocked(){
    return nLNsLocked;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLNsMigrated(){
    return nLNsMigrated;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLNsMarked(){
    return nLNsMarked;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLNQueueHits(){
    return nLNQueueHits;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNPendingLNsProcessed(){
    return nPendingLNsProcessed;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNMarkedLNsProcessed(){
    return nMarkedLNsProcessed;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNToBeCleanedLNsProcessed(){
    return nToBeCleanedLNsProcessed;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNClusterLNsProcessed(){
    return nClusterLNsProcessed;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNPendingLNsLocked(){
    return nPendingLNsLocked;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNLogBuffers(){
    return nLogBuffers;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNNodesExplicitlyEvicted(){
    return nNodesExplicitlyEvicted;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNBINsStripped(){
    return nBINsStripped;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getRequiredEvictBytes(){
    return requiredEvictBytes;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNNodesScanned(){
    return nNodesScanned;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNNodesSelected(){
    return nNodesSelected;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getCacheTotalBytes(){
    return cacheDataBytes + bufferBytes;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getCacheDataBytes(){
    return cacheDataBytes;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNNotResident(){
    return nNotResident;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getNonEmptyBins(){
    return nonEmptyBins;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getProcessedBins(){
    return processedBins;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNRepeatFaultReads(){
    return nRepeatFaultReads;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNTempBufferWrites(){
    return nTempBufferWrites;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public long getNRepeatIteratorReads(){
    return nRepeatIteratorReads;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int getSplitBins(){
    return splitBins;
  }
  /** 
 * Internal use only.
 */
  public void setCacheDataBytes(  long cacheDataBytes){
    this.cacheDataBytes=cacheDataBytes;
  }
  /** 
 * Internal use only.
 */
  public void setNNotResident(  long nNotResident){
    this.nNotResident=nNotResident;
  }
  /** 
 * Internal use only.
 */
  public void setNCacheMiss(  long nCacheMiss){
    this.nCacheMiss=nCacheMiss;
  }
  /** 
 * Internal use only.
 */
  public void setNLogBuffers(  int nLogBuffers){
    this.nLogBuffers=nLogBuffers;
  }
  /** 
 * Internal use only.
 */
  public void setBufferBytes(  long bufferBytes){
    this.bufferBytes=bufferBytes;
  }
  /** 
 * Internal use only.
 */
  public void setCursorsBins(  int val){
    cursorsBins=val;
  }
  /** 
 * Internal use only.
 */
  public void setDbClosedBins(  int val){
    dbClosedBins=val;
  }
  /** 
 * Internal use only.
 */
  public void setInCompQueueSize(  int val){
    inCompQueueSize=val;
  }
  /** 
 * Internal use only.
 */
  public void setLastCheckpointId(  long l){
    lastCheckpointId=l;
  }
  /** 
 * Internal use only.
 */
  public void setNCheckpoints(  int val){
    nCheckpoints=val;
  }
  /** 
 * Internal use only.
 */
  public void setCleanerBacklog(  int val){
    cleanerBacklog=val;
  }
  /** 
 * Internal use only.
 */
  public void setNCleanerRuns(  int val){
    nCleanerRuns=val;
  }
  /** 
 * Internal use only.
 */
  public void setNCleanerDeletions(  int val){
    nCleanerDeletions=val;
  }
  /** 
 * Internal use only.
 */
  public void setNDeltaINFlush(  int val){
    nDeltaINFlush=val;
  }
  /** 
 * Internal use only.
 */
  public void setLastCheckpointEnd(  long lsn){
    lastCheckpointEnd=lsn;
  }
  /** 
 * Internal use only.
 */
  public void setLastCheckpointStart(  long lsn){
    lastCheckpointStart=lsn;
  }
  /** 
 * Internal use only.
 */
  public void setNCleanerEntriesRead(  int val){
    nCleanerEntriesRead=val;
  }
  /** 
 * Internal use only.
 */
  public void setNEvictPasses(  int val){
    nEvictPasses=val;
  }
  /** 
 * Internal use only.
 */
  public void setNFullINFlush(  int val){
    nFullINFlush=val;
  }
  /** 
 * Internal use only.
 */
  public void setNFullBINFlush(  int val){
    nFullBINFlush=val;
  }
  /** 
 * Internal use only.
 */
  public void setNINsObsolete(  int val){
    nINsObsolete=val;
  }
  /** 
 * Internal use only.
 */
  public void setNINsCleaned(  int val){
    nINsCleaned=val;
  }
  /** 
 * Internal use only.
 */
  public void setNINsDead(  int val){
    nINsDead=val;
  }
  /** 
 * Internal use only.
 */
  public void setNINsMigrated(  int val){
    nINsMigrated=val;
  }
  /** 
 * Internal use only.
 */
  public void setNLNsObsolete(  int val){
    nLNsObsolete=val;
  }
  /** 
 * Internal use only.
 */
  public void setNLNsCleaned(  int val){
    nLNsCleaned=val;
  }
  /** 
 * Internal use only.
 */
  public void setNLNsDead(  int val){
    nLNsDead=val;
  }
  /** 
 * Internal use only.
 */
  public void setNLNsLocked(  int val){
    nLNsLocked=val;
  }
  /** 
 * Internal use only.
 */
  public void setNLNsMigrated(  int val){
    nLNsMigrated=val;
  }
  /** 
 * Internal use only.
 */
  public void setNLNsMarked(  int val){
    nLNsMarked=val;
  }
  /** 
 * Internal use only.
 */
  public void setNLNQueueHits(  int val){
    nLNQueueHits=val;
  }
  /** 
 * Internal use only.
 */
  public void setNPendingLNsProcessed(  int val){
    nPendingLNsProcessed=val;
  }
  /** 
 * Internal use only.
 */
  public void setNMarkedLNsProcessed(  int val){
    nMarkedLNsProcessed=val;
  }
  /** 
 * Internal use only.
 */
  public void setNToBeCleanedLNsProcessed(  int val){
    nToBeCleanedLNsProcessed=val;
  }
  /** 
 * Internal use only.
 */
  public void setNClusterLNsProcessed(  int val){
    nClusterLNsProcessed=val;
  }
  /** 
 * Internal use only.
 */
  public void setNPendingLNsLocked(  int val){
    nPendingLNsLocked=val;
  }
  /** 
 * Internal use only.
 */
  public void setNNodesExplicitlyEvicted(  long l){
    nNodesExplicitlyEvicted=l;
  }
  /** 
 * Internal use only.
 */
  public void setRequiredEvictBytes(  long l){
    requiredEvictBytes=l;
  }
  /** 
 * Internal use only.
 */
  public void setNBINsStripped(  long l){
    nBINsStripped=l;
  }
  /** 
 * Internal use only.
 */
  public void setNNodesScanned(  long l){
    nNodesScanned=l;
  }
  /** 
 * Internal use only.
 */
  public void setNNodesSelected(  long l){
    nNodesSelected=l;
  }
  /** 
 * Internal use only.
 */
  public void setNonEmptyBins(  int val){
    nonEmptyBins=val;
  }
  /** 
 * Internal use only.
 */
  public void setProcessedBins(  int val){
    processedBins=val;
  }
  /** 
 * Internal use only.
 */
  public void setNRepeatFaultReads(  long val){
    nRepeatFaultReads=val;
  }
  /** 
 * Internal use only.
 */
  public void setNTempBufferWrites(  long val){
    nTempBufferWrites=val;
  }
  /** 
 * Internal use only.
 */
  public void setNRepeatIteratorReads(  long val){
    nRepeatIteratorReads=val;
  }
  /** 
 * Internal use only.
 */
  public void setSplitBins(  int val){
    splitBins=val;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public String toString(){
    StringBuffer sb=new StringBuffer();
    sb.append("splitBins=").append(splitBins).append('\n');
    sb.append("dbClosedBins=").append(dbClosedBins).append('\n');
    sb.append("cursorsBins=").append(cursorsBins).append('\n');
    sb.append("nonEmptyBins=").append(nonEmptyBins).append('\n');
    sb.append("processedBins=").append(processedBins).append('\n');
    sb.append("inCompQueueSize=").append(inCompQueueSize).append('\n');
    sb.append("nEvictPasses=").append(nEvictPasses).append('\n');
    sb.append("nNodesSelected=").append(nNodesSelected).append('\n');
    sb.append("nNodesScanned=").append(nNodesScanned).append('\n');
    sb.append("nNodesExplicitlyEvicted=").append(nNodesExplicitlyEvicted).append('\n');
    sb.append("nBINsStripped=").append(nBINsStripped).append('\n');
    sb.append("requiredEvictBytes=").append(requiredEvictBytes).append('\n');
    sb.append("nCheckpoints=").append(nCheckpoints).append('\n');
    sb.append("lastCheckpointId=").append(lastCheckpointId).append('\n');
    sb.append("nFullINFlush=").append(nFullINFlush).append('\n');
    sb.append("nFullBINFlush=").append(nFullBINFlush).append('\n');
    sb.append("nDeltaINFlush=").append(nDeltaINFlush).append('\n');
    sb.append("lastCheckpointStart=").append(DbLsn.getNoFormatString(lastCheckpointStart)).append('\n');
    sb.append("lastCheckpointEnd=").append(DbLsn.getNoFormatString(lastCheckpointEnd)).append('\n');
    sb.append("cleanerBacklog=").append(cleanerBacklog).append('\n');
    sb.append("nCleanerRuns=").append(nCleanerRuns).append('\n');
    sb.append("nCleanerDeletions=").append(nCleanerDeletions).append('\n');
    sb.append("nINsObsolete=").append(nINsObsolete).append('\n');
    sb.append("nINsCleaned=").append(nINsCleaned).append('\n');
    sb.append("nINsDead=").append(nINsDead).append('\n');
    sb.append("nINsMigrated=").append(nINsMigrated).append('\n');
    sb.append("nLNsObsolete=").append(nLNsObsolete).append('\n');
    sb.append("nLNsCleaned=").append(nLNsCleaned).append('\n');
    sb.append("nLNsDead=").append(nLNsDead).append('\n');
    sb.append("nLNsLocked=").append(nLNsLocked).append('\n');
    sb.append("nLNsMigrated=").append(nLNsMigrated).append('\n');
    sb.append("nLNsMarked=").append(nLNsMarked).append('\n');
    sb.append("nLNQueueHits=").append(nLNQueueHits).append('\n');
    sb.append("nPendingLNsProcessed=").append(nPendingLNsProcessed).append('\n');
    sb.append("nMarkedLNsProcessed=").append(nMarkedLNsProcessed).append('\n');
    sb.append("nToBeCleanedLNsProcessed=").append(nToBeCleanedLNsProcessed).append('\n');
    sb.append("nClusterLNsProcessed=").append(nClusterLNsProcessed).append('\n');
    sb.append("nPendingLNsLocked=").append(nPendingLNsLocked).append('\n');
    sb.append("nCleanerEntriesRead=").append(nCleanerEntriesRead).append('\n');
    sb.append("nNotResident=").append(nNotResident).append('\n');
    sb.append("nCacheMiss=").append(nCacheMiss).append('\n');
    sb.append("nLogBuffers=").append(nLogBuffers).append('\n');
    sb.append("bufferBytes=").append(bufferBytes).append('\n');
    sb.append("cacheDataBytes=").append(cacheDataBytes).append('\n');
    sb.append("cacheTotalBytes=").append(getCacheTotalBytes()).append('\n');
    this.hook61(sb);
    sb.append("nRepeatFaultReads=").append(nRepeatFaultReads).append('\n');
    sb.append("nTempBufferWrite=").append(nTempBufferWrites).append('\n');
    sb.append("nRepeatIteratorReads=").append(nRepeatIteratorReads).append('\n');
    return sb.toString();
  }
  protected void hook60(){
  }
  protected void hook61(  StringBuffer sb){
  }
}
\00Statistics/com/sleepycat/je/log/LogBufferPool.java:package com.sleepycat.je.log;
import com.sleepycat.je.EnvironmentStats;
import com.sleepycat.je.StatsConfig;
class LogBufferPool {
  private long nNotResident=0;
  private long nCacheMiss=0;
  void loadStats(  StatsConfig config,  EnvironmentStats stats) throws DatabaseException {
    stats.setNCacheMiss(nCacheMiss);
    stats.setNNotResident(nNotResident);
    if (config.getClear()) {
      nCacheMiss=0;
      nNotResident=0;
    }
    this.hook484();
    long bufferBytes=0;
    int nLogBuffers=0;
    this.hook483(bufferBytes,nLogBuffers);
    stats.setNLogBuffers(nLogBuffers);
    stats.setBufferBytes(bufferBytes);
  }
  protected void hook483(  long bufferBytes,  int nLogBuffers) throws DatabaseException {
    Iterator iter=bufferPool.iterator();
    while (iter.hasNext()) {
      LogBuffer l=(LogBuffer)iter.next();
      nLogBuffers++;
      bufferBytes+=l.getCapacity();
    }
  }
  protected void hook484() throws DatabaseException {
  }
  protected LogBuffer hook489(  long lsn,  LogBuffer foundBuffer) throws DatabaseException {
    nNotResident++;
    return original(lsn,foundBuffer);
  }
  protected void hook496() throws DatabaseException {
    nCacheMiss++;
    original();
  }
}
\00Statistics/com/sleepycat/je/Database.java:package com.sleepycat.je;
public class Database {
  public DatabaseStats getStats(  StatsConfig config) throws DatabaseException {
    checkEnv();
    checkRequiredDbState(OPEN,"Can't call Database.stat");
    StatsConfig useConfig=(config == null) ? StatsConfig.DEFAULT : config;
    if (databaseImpl != null) {
      this.hook38();
      return databaseImpl.stat(useConfig);
    }
    return null;
  }
  protected void hook38() throws DatabaseException {
  }
}
\00FSync/com/sleepycat/je/log/FSyncManager.java:package com.sleepycat.je.log;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.utilint.PropUtil;
import de.ovgu.cide.jakutil.*;
class FSyncManager {
  private EnvironmentImpl envImpl;
  private long timeout;
  private volatile boolean fsyncInProgress;
  private FSyncGroup nextFSyncWaiters;
  FSyncManager(  EnvironmentImpl envImpl) throws DatabaseException {
    timeout=PropUtil.microsToMillis(envImpl.getConfigManager().getLong(EnvironmentParams.LOG_FSYNC_TIMEOUT));
    this.envImpl=envImpl;
    this.hook434(envImpl);
    fsyncInProgress=false;
    nextFSyncWaiters=new FSyncGroup(timeout,envImpl);
  }
  /** 
 * Request that this file be fsynced to disk. This thread may or may not
 * actually execute the fsync, but will not return until a fsync has been
 * issued and executed on behalf of its write. There is a timeout period
 * specified by EnvironmentParam.LOG_FSYNC_TIMEOUT that ensures that no
 * thread gets stuck here indefinitely.
 * When a thread comes in, it will find one of two things. 
 * 1. There is no fsync going on right now. This thread should go
 * ahead and fsync.
 * 2. There is an active fsync, wait until it's over before
 * starting a new fsync.
 * When a fsync is going on, all those threads that come along are grouped
 * together as the nextFsyncWaiters. When the current fsync is finished,
 * one of those nextFsyncWaiters will be selected as a leader to issue the
 * next fsync. The other members of the group will merely wait until the
 * fsync done on their behalf is finished.
 * When a thread finishes a fsync, it has to:
 * 1. wake up all the threads that were waiting for its fsync call.
 * 2. wake up one member of the next group of waiting threads (the 
 * nextFsyncWaiters) so that thread can become the new leader
 * and issue the next fysnc call.
 * If a non-leader member of the nextFsyncWaiters times out, it will issue
 * its own fsync anyway, in case something happened to the leader.
 */
  void fsync() throws DatabaseException {
    boolean doFsync=false;
    boolean isLeader=false;
    boolean needToWait=false;
    FSyncGroup inProgressGroup=null;
    FSyncGroup myGroup=null;
synchronized (this) {
      this.hook435();
      if (fsyncInProgress) {
        needToWait=true;
        myGroup=nextFSyncWaiters;
      }
 else {
        isLeader=true;
        doFsync=true;
        fsyncInProgress=true;
        inProgressGroup=nextFSyncWaiters;
        nextFSyncWaiters=new FSyncGroup(timeout,envImpl);
      }
    }
    if (needToWait) {
      int waitStatus=myGroup.waitForFsync();
      if (waitStatus == FSyncGroup.DO_LEADER_FSYNC) {
synchronized (this) {
          if (!fsyncInProgress) {
            isLeader=true;
            doFsync=true;
            fsyncInProgress=true;
            inProgressGroup=myGroup;
            nextFSyncWaiters=new FSyncGroup(timeout,envImpl);
          }
        }
      }
 else       if (waitStatus == FSyncGroup.DO_TIMEOUT_FSYNC) {
        doFsync=true;
        this.hook436();
      }
    }
    if (doFsync) {
      executeFSync();
synchronized (this) {
        this.hook437();
        if (isLeader) {
          inProgressGroup.wakeupAll();
          nextFSyncWaiters.wakeupOne();
          fsyncInProgress=false;
        }
      }
    }
  }
  /** 
 * Put the fsync execution into this method so it can be overridden for
 * testing purposes.
 */
  protected void executeFSync() throws DatabaseException {
    envImpl.getFileManager().syncLogEnd();
  }
static class FSyncGroup {
    static int DO_TIMEOUT_FSYNC=0;
    static int DO_LEADER_FSYNC=1;
    static int NO_FSYNC_NEEDED=2;
    private volatile boolean fsyncDone;
    private long fsyncTimeout;
    private boolean leaderExists;
    private EnvironmentImpl envImpl;
    FSyncGroup(    long fsyncTimeout,    EnvironmentImpl envImpl){
      this.fsyncTimeout=fsyncTimeout;
      fsyncDone=false;
      leaderExists=false;
      this.envImpl=envImpl;
    }
    synchronized boolean getLeader(){
      if (fsyncDone) {
        return false;
      }
 else {
        if (leaderExists) {
          return false;
        }
 else {
          leaderExists=true;
          return true;
        }
      }
    }
    /** 
 * Wait for either a turn to execute a fsync, or to find out that a
 * fsync was done on your behalf.
 * @return true if the fsync wasn't done, and this thread needs to
 * execute a fsync when it wakes up. This may be true because it's the
 * leader of its group, or because the wait timed out.
 */
    synchronized int waitForFsync() throws RunRecoveryException {
      int status=0;
      if (!fsyncDone) {
        long startTime=System.currentTimeMillis();
        while (true) {
          try {
            wait(fsyncTimeout);
          }
 catch (          InterruptedException e) {
            throw new RunRecoveryException(envImpl,"Unexpected interrupt while waiting for fsync",e);
          }
          if (fsyncDone) {
            status=NO_FSYNC_NEEDED;
            break;
          }
 else {
            if (!leaderExists) {
              leaderExists=true;
              status=DO_LEADER_FSYNC;
              break;
            }
 else {
              long now=System.currentTimeMillis();
              if ((now - startTime) > fsyncTimeout) {
                status=DO_TIMEOUT_FSYNC;
                break;
              }
            }
          }
        }
      }
      return status;
    }
    synchronized void wakeupAll(){
      fsyncDone=true;
      notifyAll();
    }
    synchronized void wakeupOne(){
      notify();
    }
  }
  protected void hook434(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected void hook435() throws DatabaseException {
  }
  protected void hook436() throws DatabaseException {
  }
  protected void hook437() throws DatabaseException {
  }
}
\00LookAHEADCache/com/sleepycat/je/cleaner/FileProcessor.java:package com.sleepycat.je.cleaner;
class FileProcessor {
@MethodObject static class FileProcessor_processLN {
	protected LookAheadCache lookAheadCache;
	
    protected void hook117() throws DatabaseException {
    }
    void execute() throws DatabaseException {
      lookAheadCache=(LookAheadCache)lookAheadCachep;
      original();
    }
    protected void hook132() throws DatabaseException {
      offset=lookAheadCache.nextOffset();
      info=lookAheadCache.remove(offset);
      original();
    }
    protected void hook133() throws DatabaseException {
      if (!isDupCountLN) {
        for (int i=0; i < bin.getNEntries(); i+=1) {
          lsn=bin.getLsn(i);
          if (i != index && !bin.isEntryKnownDeleted(i) && !bin.isEntryPendingDeleted(i) && DbLsn.getFileNumber(lsn) == fileNum.longValue()) {
            myOffset=new Long(DbLsn.getFileOffset(lsn));
            myInfo=lookAheadCache.remove(myOffset);
            if (myInfo != null) {
              this.hook117();
              _this.processFoundLN(myInfo,lsn,lsn,bin,i,null);
            }
          }
        }
      }
      original();
    }
  }
@MethodObject static class FileProcessor_processFile {
    protected void hook116() throws DatabaseException, IOException {
    }
    protected void hook127() throws DatabaseException, IOException {
      lookAheadCache=new LookAheadCache(lookAheadCacheSize);
      original();
    }
    protected void hook128() throws DatabaseException, IOException {
      lookAheadCacheSize=_this.cleaner.lookAheadCacheSize;
      original();
    }
    protected void hook129() throws DatabaseException, IOException {
      while (!lookAheadCache.isEmpty()) {
        this.hook116();
        _this.processLN(fileNum,location,null,null,lookAheadCache,dbCache);
      }
      original();
    }
    protected void hook130() throws DatabaseException, IOException {
      lookAheadCache.add(aLsn,aLninfo);
      if (lookAheadCache.isFull()) {
        original();
      }
    }
    protected void hook131() throws DatabaseException, IOException {
      p=lookAheadCache;
      original();
    }
  }
}
\00CriticalEviction/com/sleepycat/je/cleaner/Cleaner.java:package com.sleepycat.je.cleaner;
public class Cleaner {
  /** 
 * Whether the cleaner should participate in critical eviction. Ideally the
 * cleaner would not participate in eviction, since that would reduce the
 * cost of cleaning. However, the cleaner can add large numbers of nodes to
 * the cache. By not participating in eviction, other threads could be kept
 * in a constant state of eviction and would effectively starve. Therefore,
 * this setting is currently enabled.
 */
  static final boolean DO_CRITICAL_EVICTION=true;
@MethodObject static class Cleaner_processPending {
    protected void hook86() throws DatabaseException {
    }
    protected void hook114() throws DatabaseException {
      if (_this.DO_CRITICAL_EVICTION) {
        this.hook86();
      }
      original();
    }
  }
}
\00INCompressor/com/sleepycat/je/incomp/INCompressor.java:package com.sleepycat.je.incomp;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.logging.Level;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.cleaner.TrackedFileSummary;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.BINReference;
import com.sleepycat.je.tree.CursorsExistException;
import com.sleepycat.je.tree.DBIN;
import com.sleepycat.je.tree.DIN;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Key;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.tree.NodeNotEmptyException;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.Tree.SearchType;
import com.sleepycat.je.utilint.DaemonThread;
import com.sleepycat.je.utilint.PropUtil;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * The IN Compressor.  JE compression consist of removing delete entries from
 * BINs, and pruning empty IN/BINs from the tree. Compression is carried out by
 * either a daemon thread or lazily by operations (namely checkpointing and
 * eviction) that are writing INS.
 */
public class INCompressor extends DaemonThread {
  private static final String TRACE_COMPRESS="INCompress:";
  private static final boolean DEBUG=false;
  private EnvironmentImpl env;
  private long lockTimeout;
  private Map binRefQueue;
  private Object binRefQueueSync;
  public INCompressor(  EnvironmentImpl env,  long waitTime,  String name) throws DatabaseException {
    super(waitTime,name,env);
    this.env=env;
    lockTimeout=PropUtil.microsToMillis(env.getConfigManager().getLong(EnvironmentParams.COMPRESSOR_LOCK_TIMEOUT));
    binRefQueue=new HashMap();
    binRefQueueSync=new Object();
  }
  public String toString(){
    StringBuffer sb=new StringBuffer();
    sb.append("<INCompressor name=\"").append(name).append("\"/>");
    return sb.toString();
  }
  synchronized public void clearEnv(){
    env=null;
  }
  /** 
 * The default daemon work queue is not used because we need a map, not a
 * set.
 */
  public void addToQueue(  Object o) throws DatabaseException {
    throw new DatabaseException("INCompressor.addToQueue should never be called.");
  }
  public int getBinRefQueueSize() throws DatabaseException {
    int size=0;
synchronized (binRefQueueSync) {
      size=binRefQueue.size();
    }
    return size;
  }
  /** 
 * Adds the BIN and deleted Key to the queue if the BIN is not already in
 * the queue, or adds the deleted key to an existing entry if one exists.
 */
  public void addBinKeyToQueue(  BIN bin,  Key deletedKey,  boolean doWakeup) throws DatabaseException {
synchronized (binRefQueueSync) {
      addBinKeyToQueueAlreadyLatched(bin,deletedKey);
    }
    if (doWakeup) {
      wakeup();
    }
  }
  /** 
 * Adds the BINReference to the queue if the BIN is not already in the
 * queue, or adds the deleted keys to an existing entry if one exists.
 */
  public void addBinRefToQueue(  BINReference binRef,  boolean doWakeup) throws DatabaseException {
synchronized (binRefQueueSync) {
      addBinRefToQueueAlreadyLatched(binRef);
    }
    if (doWakeup) {
      wakeup();
    }
  }
  /** 
 * Adds an entire collection of BINReferences to the queue at once.  Use
 * this to avoid latching for each add.
 */
  public void addMultipleBinRefsToQueue(  Collection binRefs,  boolean doWakeup) throws DatabaseException {
synchronized (binRefQueueSync) {
      Iterator it=binRefs.iterator();
      while (it.hasNext()) {
        BINReference binRef=(BINReference)it.next();
        addBinRefToQueueAlreadyLatched(binRef);
      }
    }
    if (doWakeup) {
      wakeup();
    }
  }
  /** 
 * Adds the BINReference with the latch held.
 */
  private void addBinRefToQueueAlreadyLatched(  BINReference binRef){
    Long node=new Long(binRef.getNodeId());
    BINReference existingRef=(BINReference)binRefQueue.get(node);
    if (existingRef != null) {
      existingRef.addDeletedKeys(binRef);
    }
 else {
      binRefQueue.put(node,binRef);
    }
  }
  /** 
 * Adds the BIN and deleted Key with the latch held.
 */
  private void addBinKeyToQueueAlreadyLatched(  BIN bin,  Key deletedKey){
    Long node=new Long(bin.getNodeId());
    BINReference existingRef=(BINReference)binRefQueue.get(node);
    if (existingRef != null) {
      if (deletedKey != null) {
        existingRef.addDeletedKey(deletedKey);
      }
    }
 else {
      BINReference binRef=bin.createReference();
      if (deletedKey != null) {
        binRef.addDeletedKey(deletedKey);
      }
      binRefQueue.put(node,binRef);
    }
  }
  public boolean exists(  long nodeId){
    Long node=new Long(nodeId);
synchronized (binRefQueueSync) {
      return (binRefQueue.get(node) != null);
    }
  }
  private BINReference removeCompressibleBinReference(  long nodeId){
    Long node=new Long(nodeId);
    BINReference foundRef=null;
synchronized (binRefQueueSync) {
      BINReference target=(BINReference)binRefQueue.remove(node);
      if (target != null) {
        if (target.deletedKeysExist()) {
          foundRef=target;
        }
 else {
          binRefQueue.put(node,target);
        }
      }
    }
    return foundRef;
  }
  /** 
 * Return the number of retries when a deadlock exception occurs.
 */
  protected int nDeadlockRetries() throws DatabaseException {
    return env.getConfigManager().getInt(EnvironmentParams.COMPRESSOR_RETRY);
  }
  public synchronized void onWakeup() throws DatabaseException {
    if (env.isClosed()) {
      return;
    }
    this.hook403();
    doCompress();
  }
  /** 
 * The real work to doing a compress. This may be called by the compressor
 * thread or programatically.
 */
  public synchronized void doCompress() throws DatabaseException {
    if (!isRunnable()) {
      return;
    }
    Map queueSnapshot=null;
    int binQueueSize=0;
synchronized (binRefQueueSync) {
      binQueueSize=binRefQueue.size();
      if (binQueueSize > 0) {
        queueSnapshot=binRefQueue;
        binRefQueue=new HashMap();
      }
    }
    if (binQueueSize > 0) {
      this.hook404();
      this.hook392(binQueueSize);
      this.hook393();
      UtilizationTracker tracker=new UtilizationTracker(env);
      Map dbCache=new HashMap();
      DbTree dbTree=env.getDbMapTree();
      BINSearch binSearch=new BINSearch();
      try {
        Iterator it=queueSnapshot.values().iterator();
        while (it.hasNext()) {
          if (env.isClosed()) {
            return;
          }
          BINReference binRef=(BINReference)it.next();
          if (!findDBAndBIN(binSearch,binRef,dbTree,dbCache)) {
            continue;
          }
          if (binRef.deletedKeysExist()) {
            boolean requeued=compressBin(binSearch.db,binSearch.bin,binRef,tracker);
            if (!requeued) {
              checkForRelocatedSlots(binSearch.db,binRef,tracker);
            }
          }
 else {
            BIN foundBin=binSearch.bin;
            byte[] idKey=foundBin.getIdentifierKey();
            boolean isDBIN=foundBin.containsDuplicates();
            byte[] dupKey=null;
            if (isDBIN) {
              dupKey=((DBIN)foundBin).getDupKey();
            }
            this.hook394(foundBin);
            pruneBIN(binSearch.db,binRef,idKey,isDBIN,dupKey,tracker);
          }
        }
        TrackedFileSummary[] summaries=tracker.getTrackedFiles();
        if (summaries.length > 0) {
          env.getUtilizationProfile().countAndLogSummaries(summaries);
        }
      }
  finally {
        this.hook395();
        this.hook405();
      }
    }
  }
  /** 
 * Compresses a single BIN and then deletes the BIN if it is empty.
 * @param bin is latched when this method is called, and unlatched when it
 * returns.
 * @return true if the BINReference was requeued by this method.
 */
  private boolean compressBin(  DatabaseImpl db,  BIN bin,  BINReference binRef,  UtilizationTracker tracker) throws DatabaseException {
    boolean empty=false;
    boolean requeued=false;
    byte[] idKey=bin.getIdentifierKey();
    byte[] dupKey=null;
    boolean isDBIN=bin.containsDuplicates();
    this.hook396(bin,binRef,empty,requeued,dupKey,isDBIN);
    if (empty) {
      requeued=pruneBIN(db,binRef,idKey,isDBIN,dupKey,tracker);
    }
    return requeued;
  }
  /** 
 * If the target BIN is empty, attempt to remove the empty branch of the 
 * tree.
 * @return true if the pruning was unable to proceed and the BINReference
 * was requeued.
 */
  private boolean pruneBIN(  DatabaseImpl dbImpl,  BINReference binRef,  byte[] idKey,  boolean containsDups,  byte[] dupKey,  UtilizationTracker tracker) throws DatabaseException {
    boolean requeued=false;
    try {
      Tree tree=dbImpl.getTree();
      if (containsDups) {
        tree.deleteDup(idKey,dupKey,tracker);
      }
 else {
        tree.delete(idKey,tracker);
      }
      this.hook406();
    }
 catch (    NodeNotEmptyException NNEE) {
      this.hook407();
    }
catch (    CursorsExistException e) {
      addBinRefToQueue(binRef,false);
      this.hook408();
      requeued=true;
    }
    return requeued;
  }
  private void checkForRelocatedSlots(  DatabaseImpl db,  BINReference binRef,  UtilizationTracker tracker) throws DatabaseException {
    Iterator iter=binRef.getDeletedKeyIterator();
    if (iter != null) {
      byte[] mainKey=binRef.getKey();
      boolean isDup=(binRef.getData() != null);
      while (iter.hasNext()) {
        Key key=(Key)iter.next();
        BIN splitBin=isDup ? searchForBIN(db,mainKey,key.getKey()) : searchForBIN(db,key.getKey(),null);
        if (splitBin != null) {
          BINReference splitBinRef=splitBin.createReference();
          splitBinRef.addDeletedKey(key);
          compressBin(db,splitBin,splitBinRef,tracker);
        }
      }
    }
  }
  private boolean isRunnable() throws DatabaseException {
    return true;
  }
  /** 
 * Search the tree for the BIN or DBIN that corresponds to this
 * BINReference.
 * @param binRef the BINReference that indicates the bin we want.
 * @return the BIN or DBIN that corresponds to this BINReference. The
 * node is latched upon return. Returns null if the BIN can't be found.
 */
  public BIN searchForBIN(  DatabaseImpl db,  BINReference binRef) throws DatabaseException {
    return searchForBIN(db,binRef.getKey(),binRef.getData());
  }
  private BIN searchForBIN(  DatabaseImpl db,  byte[] mainKey,  byte[] dupKey) throws DatabaseException {
    try {
      Tree tree=db.getTree();
      IN in=tree.search(mainKey,SearchType.NORMAL,-1,null,false);
      if (in == null) {
        return null;
      }
      if (dupKey == null) {
        return (BIN)in;
      }
      DIN duplicateRoot=null;
      DBIN duplicateBin=null;
      BIN bin=(BIN)in;
      this.hook397(mainKey,dupKey,tree,duplicateRoot,duplicateBin,bin);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (BIN)r.value;
    }
  }
  /** 
 * Lazily compress a single BIN. Do not do any pruning. The target IN
 * should be latched when we enter, and it will be remain latched.
 */
  public void lazyCompress(  IN in) throws DatabaseException {
    if (!in.isCompressible()) {
      return;
    }
    this.hook398(in);
    BIN bin=(BIN)in;
    int nCursors=bin.nCursors();
    if (nCursors > 0) {
      return;
    }
 else {
      BINReference binRef=removeCompressibleBinReference(bin.getNodeId());
      if ((binRef == null) || (!binRef.deletedKeysExist())) {
        return;
      }
 else {
        boolean requeued=bin.compress(binRef,false);
        this.hook409();
        if (!requeued && binRef.deletedKeysExist()) {
          addBinRefToQueue(binRef,false);
          this.hook410();
        }
 else {
          if (bin.getNEntries() == 0) {
            addBinRefToQueue(binRef,false);
            this.hook411();
          }
        }
      }
    }
  }
  private boolean findDBAndBIN(  BINSearch binSearch,  BINReference binRef,  DbTree dbTree,  Map dbCache) throws DatabaseException {
    binSearch.db=dbTree.getDb(binRef.getDatabaseId(),lockTimeout,dbCache);
    boolean close=binSearch.db == null;
    close=this.hook415(binSearch,close);
    if (close) {
      this.hook412();
      return false;
    }
    this.hook391();
    binSearch.bin=searchForBIN(binSearch.db,binRef);
    if ((binSearch.bin == null) || binSearch.bin.getNodeId() != binRef.getNodeId()) {
      this.hook399(binSearch);
      this.hook413();
      return false;
    }
    return true;
  }
private static class BINSearch {
    public DatabaseImpl db;
    public BIN bin;
  }
  protected void hook391() throws DatabaseException {
  }
  protected void hook392(  int binQueueSize) throws DatabaseException {
  }
  protected void hook393() throws DatabaseException {
  }
  protected void hook394(  BIN foundBin) throws DatabaseException {
  }
  protected void hook395() throws DatabaseException {
  }
  protected void hook396(  BIN bin,  BINReference binRef,  boolean empty,  boolean requeued,  byte[] dupKey,  boolean isDBIN) throws DatabaseException {
    int nCursors=bin.nCursors();
    if (nCursors > 0) {
      addBinRefToQueue(binRef,false);
      requeued=true;
      this.hook414();
    }
 else {
      requeued=bin.compress(binRef,true);
      if (!requeued) {
        empty=(bin.getNEntries() == 0);
        if (empty) {
          if (isDBIN) {
            dupKey=((DBIN)bin).getDupKey();
          }
        }
      }
    }
  }
  protected void hook397(  byte[] mainKey,  byte[] dupKey,  Tree tree,  DIN duplicateRoot,  DBIN duplicateBin,  BIN bin) throws DatabaseException {
    int index=bin.findEntry(mainKey,false,true);
    if (index >= 0) {
      Node node=null;
      if (!bin.isEntryKnownDeleted(index)) {
        node=bin.fetchTarget(index);
      }
      if (node == null) {
        this.hook400(bin);
        throw new ReturnObject(null);
      }
      if (node.containsDuplicates()) {
        duplicateRoot=(DIN)node;
        this.hook401(duplicateRoot,bin);
        duplicateBin=(DBIN)tree.searchSubTree(duplicateRoot,dupKey,SearchType.NORMAL,-1,null,false);
        throw new ReturnObject(duplicateBin);
      }
 else {
        throw new ReturnObject(bin);
      }
    }
 else {
      this.hook402(bin);
      throw new ReturnObject(null);
    }
  }
  protected void hook398(  IN in) throws DatabaseException {
  }
  protected void hook399(  BINSearch binSearch) throws DatabaseException {
  }
  protected void hook400(  BIN bin) throws DatabaseException {
  }
  protected void hook401(  DIN duplicateRoot,  BIN bin) throws DatabaseException {
  }
  protected void hook402(  BIN bin) throws DatabaseException {
  }
  protected void hook403() throws DatabaseException {
  }
  protected void hook404() throws DatabaseException {
  }
  protected void hook405() throws DatabaseException {
  }
  protected void hook406() throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook407() throws DatabaseException {
  }
  protected void hook408() throws DatabaseException {
  }
  protected void hook409() throws DatabaseException {
  }
  protected void hook410() throws DatabaseException {
  }
  protected void hook411() throws DatabaseException {
  }
  protected void hook412() throws DatabaseException {
  }
  protected void hook413() throws DatabaseException {
  }
  protected void hook414() throws DatabaseException {
  }
  protected boolean hook415(  BINSearch binSearch,  boolean close) throws DatabaseException {
    return close;
  }
}
\00DeleteOp/com/sleepycat/je/txn/Txn.java:package com.sleepycat.je.txn;
public class Txn {
  private Set deletedDatabases;
  /** 
 * @param dbImpl databaseImpl to remove
 * @param deleteAtCommit true if this databaseImpl should be cleaned on 
 * commit, false if it should be cleaned on abort.
 * @param mb environment memory budget.
 */
  public void markDeleteAtTxnEnd(  DatabaseImpl dbImpl,  boolean deleteAtCommit) throws DatabaseException {
    new Txn_markDeleteAtTxnEnd(this,dbImpl,deleteAtCommit).execute();
  }
  private void setDeletedDatabaseState(  boolean isCommit) throws DatabaseException {
    if (deletedDatabases != null) {
      Iterator iter=deletedDatabases.iterator();
      while (iter.hasNext()) {
        DatabaseCleanupInfo info=(DatabaseCleanupInfo)iter.next();
        if (info.deleteAtCommit == isCommit) {
          info.dbImpl.startDeleteProcessing();
        }
      }
    }
  }
  /** 
 * Cleanup leftover databaseImpls that are a by-product of database
 * operations like removeDatabase(), truncateDatabase().
 * This method must be called outside the synchronization on this txn,
 * because it calls deleteAndReleaseINs, which gets the TxnManager's
 * allTxns latch. The checkpointer also gets the allTxns latch, and within
 * that latch, needs to synchronize on individual txns, so we must avoid a
 * latching hiearchy conflict.
 */
  private void cleanupDatabaseImpls(  boolean isCommit) throws DatabaseException {
    if (deletedDatabases != null) {
      DatabaseCleanupInfo[] infoArray;
synchronized (this) {
        infoArray=new DatabaseCleanupInfo[deletedDatabases.size()];
        deletedDatabases.toArray(infoArray);
      }
      for (int i=0; i < infoArray.length; i+=1) {
        DatabaseCleanupInfo info=infoArray[i];
        if (info.deleteAtCommit == isCommit) {
          info.dbImpl.releaseDeletedINs();
        }
      }
      deletedDatabases=null;
    }
  }
  protected void hook805() throws DatabaseException, RunRecoveryException, Throwable {
    cleanupDatabaseImpls(true);
    original();
  }
  protected void hook806() throws DatabaseException, RunRecoveryException, Throwable {
    setDeletedDatabaseState(true);
    original();
  }
  protected void hook807() throws DatabaseException {
    cleanupDatabaseImpls(false);
    original();
  }
  protected void hook808() throws DatabaseException {
    setDeletedDatabaseState(false);
    original();
  }
@MethodObject static class Txn_markDeleteAtTxnEnd {
    Txn_markDeleteAtTxnEnd(    Txn _this,    DatabaseImpl dbImpl,    boolean deleteAtCommit){
      this._this=_this;
      this.dbImpl=dbImpl;
      this.deleteAtCommit=deleteAtCommit;
    }
    void execute() throws DatabaseException {
synchronized (_this) {
        this.hook797();
        if (_this.deletedDatabases == null) {
          _this.deletedDatabases=new HashSet();
          this.hook798();
        }
        _this.deletedDatabases.add(new DatabaseCleanupInfo(dbImpl,deleteAtCommit));
        this.hook796();
      }
    }
    protected Txn _this;
    protected DatabaseImpl dbImpl;
    protected boolean deleteAtCommit;
    protected int delta;
    protected void hook796() throws DatabaseException {
    }
    protected void hook797() throws DatabaseException {
    }
    protected void hook798() throws DatabaseException {
    }
  }
}
\00base/com/sleepycat/je/cleaner/FileSelector.java:package com.sleepycat.je.cleaner;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.SortedSet;
import java.util.TreeSet;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.tree.LN;
import de.ovgu.cide.jakutil.*;
/** 
 * Keeps track of the status of files for which cleaning is in progres.
 */
class FileSelector {
  private SortedSet toBeCleanedFiles;
  private Set beingCleanedFiles;
  private Set cleanedFiles;
  private Set checkpointedFiles;
  private Set fullyProcessedFiles;
  private Set safeToDeleteFiles;
  private Map pendingLNs;
  private boolean anyPendingDuringCheckpoint;
  private Set lowUtilizationFiles;
  FileSelector(){
    toBeCleanedFiles=new TreeSet();
    cleanedFiles=new HashSet();
    checkpointedFiles=new HashSet();
    fullyProcessedFiles=new HashSet();
    safeToDeleteFiles=new HashSet();
    pendingLNs=new HashMap();
    this.hook163();
    lowUtilizationFiles=Collections.EMPTY_SET;
    beingCleanedFiles=new HashSet();
  }
  /** 
 * Returns the best file that qualifies for cleaning, or null if no file
 * qualifies.  This method is not thread safe and should only be called
 * from the cleaner thread.
 * @param forceCleaning is true to always select a file, even if its
 * utilization is above the minimum utilization threshold.
 * @param calcLowUtilizationFiles whether to recalculate the set of files
 * that are below the minimum utilization threshold.
 * @param maxBatchFiles is the maximum number of files to be selected at
 * one time, or zero if there is no limit.
 * @return the next file to be cleaned, or null if no file needs cleaning.
 */
  Long selectFileForCleaning(  UtilizationProfile profile,  boolean forceCleaning,  boolean calcLowUtilizationFiles,  int maxBatchFiles) throws DatabaseException {
    Set newLowUtilizationFiles=calcLowUtilizationFiles ? (new HashSet()) : null;
    while (true) {
      if (maxBatchFiles > 0) {
synchronized (this) {
          if (toBeCleanedFiles.size() >= maxBatchFiles) {
            break;
          }
        }
      }
      Long fileNum=profile.getBestFileForCleaning(this,forceCleaning,newLowUtilizationFiles);
      if (fileNum == null) {
        break;
      }
synchronized (this) {
        toBeCleanedFiles.add(fileNum);
      }
    }
    if (newLowUtilizationFiles != null) {
      lowUtilizationFiles=newLowUtilizationFiles;
    }
    SortedSet availableFiles;
synchronized (this) {
      availableFiles=new TreeSet(toBeCleanedFiles);
    }
    Long file=profile.getCheapestFileToClean(availableFiles);
synchronized (this) {
      toBeCleanedFiles.remove(file);
      beingCleanedFiles.add(file);
    }
    return file;
  }
  /** 
 * Returns whether the file is in any stage of the cleaning process.
 */
  synchronized boolean isFileCleaningInProgress(  Long file){
    return toBeCleanedFiles.contains(file) || beingCleanedFiles.contains(file) || cleanedFiles.contains(file)|| checkpointedFiles.contains(file)|| fullyProcessedFiles.contains(file)|| safeToDeleteFiles.contains(file);
  }
  /** 
 * When file cleaning is aborted, move the file back from the being-cleaned
 * set to the to-be-cleaned set.
 */
  synchronized void putBackFileForCleaning(  Long fileNum){
    toBeCleanedFiles.add(fileNum);
    beingCleanedFiles.remove(fileNum);
  }
  /** 
 * When cleaning is complete, move the file from the being-cleaned set to
 * the cleaned set.
 */
  synchronized void addCleanedFile(  Long fileNum){
    cleanedFiles.add(fileNum);
    beingCleanedFiles.remove(fileNum);
  }
  /** 
 * Returns a read-only set of low utilization files that can be accessed
 * without synchronization.
 */
  Set getLowUtilizationFiles(){
    return lowUtilizationFiles;
  }
  /** 
 * Returns a read-only copy of to-be-cleaned and being-cleaned files that
 * can be accessed without synchronization.
 */
  synchronized Set getMustBeCleanedFiles(){
    Set set=new HashSet(toBeCleanedFiles);
    set.addAll(beingCleanedFiles);
    return set;
  }
  /** 
 * Returns the number of files waiting to-be-cleaned.
 */
  synchronized int getBacklog(){
    return toBeCleanedFiles.size();
  }
  /** 
 * Returns a copy of the cleaned and fully-processed files at the time a
 * checkpoint starts.
 */
  synchronized Set[] getFilesAtCheckpointStart(){
    anyPendingDuringCheckpoint=!pendingLNs.isEmpty();
    this.hook164();
    Set[] files=new Set[2];
    files[0]=(cleanedFiles.size() > 0) ? (new HashSet(cleanedFiles)) : null;
    files[1]=(fullyProcessedFiles.size() > 0) ? (new HashSet(fullyProcessedFiles)) : null;
    return (files[0] != null || files[1] != null) ? files : null;
  }
  /** 
 * When a checkpoint is complete, moves the previously cleaned and
 * fully-processed files to the checkpointed and safe-to-delete sets.
 */
  synchronized void updateFilesAtCheckpointEnd(  Set[] files){
    if (files != null) {
      Set previouslyCleanedFiles=files[0];
      if (previouslyCleanedFiles != null) {
        if (anyPendingDuringCheckpoint) {
          checkpointedFiles.addAll(previouslyCleanedFiles);
        }
 else {
          safeToDeleteFiles.addAll(previouslyCleanedFiles);
        }
        cleanedFiles.removeAll(previouslyCleanedFiles);
      }
      Set previouslyProcessedFiles=files[1];
      if (previouslyProcessedFiles != null) {
        safeToDeleteFiles.addAll(previouslyProcessedFiles);
        fullyProcessedFiles.removeAll(previouslyProcessedFiles);
      }
      updateProcessedFiles();
    }
  }
  /** 
 * Adds the given LN info to the pending LN set.
 */
  synchronized boolean addPendingLN(  LN ln,  DatabaseId dbId,  byte[] key,  byte[] dupKey){
    assert ln != null;
    boolean added=pendingLNs.put(new Long(ln.getNodeId()),new LNInfo(ln,dbId,key,dupKey)) != null;
    anyPendingDuringCheckpoint=true;
    return added;
  }
  /** 
 * Returns an array of LNInfo for LNs that could not be migrated in a
 * prior cleaning attempt, or null if no LNs are pending.
 */
  synchronized LNInfo[] getPendingLNs(){
    if (pendingLNs.size() > 0) {
      LNInfo[] lns=new LNInfo[pendingLNs.size()];
      pendingLNs.values().toArray(lns);
      return lns;
    }
 else {
      return null;
    }
  }
  /** 
 * Removes the LN for the given node ID from the pending LN set.
 */
  synchronized void removePendingLN(  long nodeId){
    pendingLNs.remove(new Long(nodeId));
    updateProcessedFiles();
  }
  /** 
 * Returns a copy of the safe-to-delete files.
 */
  synchronized Set copySafeToDeleteFiles(){
    if (safeToDeleteFiles.size() == 0) {
      return null;
    }
 else {
      return new HashSet(safeToDeleteFiles);
    }
  }
  /** 
 * Removes file from the safe-to-delete set after the file itself has
 * finally been deleted.
 */
  synchronized void removeDeletedFile(  Long fileNum){
    safeToDeleteFiles.remove(fileNum);
  }
  /** 
 * If there are no pending LNs or DBs outstanding, move the checkpointed
 * files to the fully-processed set.  The check for pending LNs/DBs and the
 * copying of the checkpointed files must be done atomically in a
 * synchronized block.  All methods that call this method are synchronized.
 */
  private void updateProcessedFiles(){
    boolean b=pendingLNs.isEmpty();
    b=this.hook165(b);
    if (b) {
      fullyProcessedFiles.addAll(checkpointedFiles);
      checkpointedFiles.clear();
    }
  }
  protected void hook163(){
  }
  protected void hook164(){
  }
  protected boolean hook165(  boolean b){
    return b;
  }
}
\00base/com/sleepycat/je/cleaner/FileProcessor.java:package com.sleepycat.je.cleaner;
import java.io.IOException;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import java.util.SortedMap;
import java.util.TreeMap;
import java.util.logging.Level;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.log.CleanerFileReader;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.ChildReference;
import com.sleepycat.je.tree.DIN;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.LN;
import com.sleepycat.je.tree.SearchResult;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.TreeLocation;
import com.sleepycat.je.tree.WithRootLatched;
import com.sleepycat.je.txn.BasicLocker;
import com.sleepycat.je.txn.LockGrantType;
import com.sleepycat.je.txn.LockResult;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.utilint.DaemonThread;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * Reads all entries in a log file and either determines them to be obsolete or
 * marks them for migration. LNs are marked for migration by setting the BIN
 * entry MIGRATE flag. INs are marked for migration by setting the dirty flag.
 * May be invoked explicitly by calling doClean, or woken up if used as a daemon
 * thread.
 */
class FileProcessor extends DaemonThread {
  /** 
 * The number of LN log entries after we process pending LNs. If we do this
 * too seldom, the pending LN queue may grow large, and it isn't budgeted
 * memory. If we process it too often, we will repeatedly request a
 * non-blocking lock for the same locked node.
 */
  private static final int PROCESS_PENDING_EVERY_N_LNS=100;
  /** 
 * Whether to prohibit BINDeltas for a BIN that is fetched by the cleaner.
 * The theory is that when fetching a BIN during cleaning we normally expect
 * that the BIN will be evicted soon, and a delta during checkpoint would be
 * wasted. However, this does not take into account use of the BIN by the
 * application after fetching; the BIN could become hot and then deltas may
 * be profitable. To be safe we currently allow deltas when fetching.
 */
  private static final boolean PROHIBIT_DELTAS_WHEN_FETCHING=false;
  private static final boolean DEBUG_TRACING=false;
  private EnvironmentImpl env;
  private Cleaner cleaner;
  private FileSelector fileSelector;
  private UtilizationProfile profile;
  FileProcessor(  String name,  EnvironmentImpl env,  Cleaner cleaner,  UtilizationProfile profile,  FileSelector fileSelector){
    super(0,name,env);
    this.env=env;
    this.cleaner=cleaner;
    this.fileSelector=fileSelector;
    this.profile=profile;
  }
  public void clearEnv(){
    env=null;
    cleaner=null;
    fileSelector=null;
    profile=null;
  }
  /** 
 * Return the number of retries when a deadlock exception occurs.
 */
  protected int nDeadlockRetries() throws DatabaseException {
    return cleaner.nDeadlockRetries;
  }
  /** 
 * Cleaner doesn't have a work queue so just throw an exception if it's ever
 * called.
 */
  public void addToQueue(  Object o) throws DatabaseException {
    throw new DatabaseException("Cleaner.addToQueue should never be called.");
  }
  /** 
 * Activates the cleaner. Is normally called when je.cleaner.byteInterval
 * bytes are written to the log.
 */
  public void onWakeup() throws DatabaseException {
    doClean(true,true,false);
  }
  /** 
 * Cleans selected files and returns the number of files cleaned. May be
 * called by the daemon thread or programatically.
 * @param invokedFromDaemoncurrently has no effect.
 * @param cleanMultipleFilesis true to clean until we're under budget, or false to clean
 * at most one file.
 * @param forceCleaningis true to clean even if we're not under the utilization
 * threshold.
 * @return the number of files cleaned, not including files cleaned
 * unsuccessfully.
 */
  public synchronized int doClean(  boolean invokedFromDaemon,  boolean cleanMultipleFiles,  boolean forceCleaning) throws DatabaseException {
    if (env.isClosed()) {
      return 0;
    }
    int nOriginalLogFiles=profile.getNumberOfFiles();
    int nFilesCleaned=0;
    while (true) {
      if (nFilesCleaned >= nOriginalLogFiles) {
        break;
      }
      if (env.isClosing()) {
        break;
      }
      cleaner.processPending();
      cleaner.deleteSafeToDeleteFiles();
      boolean needLowUtilizationSet=cleaner.clusterResident || cleaner.clusterAll;
      Long fileNum=fileSelector.selectFileForCleaning(profile,forceCleaning,needLowUtilizationSet,cleaner.maxBatchFiles);
      cleaner.updateReadOnlyFileCollections();
      if (fileNum == null) {
        break;
      }
      this.hook138();
      boolean finished=false;
      long fileNumValue=fileNum.longValue();
      int runId=++cleaner.nCleanerRuns;
      try {
        String traceMsg="CleanerRun " + runId + " on file 0x"+ Long.toHexString(fileNumValue);
        traceMsg=this.hook139(traceMsg);
        this.hook121(traceMsg);
        if (DEBUG_TRACING) {
          System.out.println("\n" + traceMsg);
        }
        if (processFile(fileNum)) {
          fileSelector.addCleanedFile(fileNum);
          nFilesCleaned+=1;
          this.hook140();
          finished=true;
        }
      }
 catch (      IOException IOE) {
        this.hook122(IOE);
        throw new DatabaseException(IOE);
      }
 finally {
        if (!finished) {
          fileSelector.putBackFileForCleaning(fileNum);
        }
        String traceMsg="CleanerRun " + runId + " on file 0x"+ Long.toHexString(fileNumValue)+ " invokedFromDaemon="+ invokedFromDaemon+ " finished="+ finished;
        traceMsg=this.hook141(traceMsg);
        this.hook123(traceMsg);
        if (DEBUG_TRACING) {
          System.out.println("\n" + traceMsg);
        }
      }
      if (!cleanMultipleFiles) {
        break;
      }
    }
    return nFilesCleaned;
  }
  /** 
 * Process all log entries in the given file.
 * Note that we check for obsolete entries using the active TFS
 * (TrackedFileSummary) for a file while it is being processed, and we
 * prohibit flushing (eviction) of that offset information until file
 * processing is complete. An entry could become obsolete because: 1- normal
 * application activity deletes or updates the entry, 2- proactive migration
 * migrates the entry before we process it, or 3- if trackDetail is false.
 * However, checking the TFS is expensive if it has many entries, because we
 * perform a linear search. There is a tradeoff between the cost of the TFS
 * lookup and its benefit, which is to avoid a tree search if the entry is
 * obsolete. Note that many more lookups for non-obsolete entries than
 * obsolete entries will typically be done. In spite of that we check the
 * tracked summary to avoid the situation where eviction does proactive
 * migration, and evicts a BIN that is very soon afterward fetched during
 * cleaning.
 * @return false if we aborted file processing because the environment is
 * being closed.
 */
  private boolean processFile(  Long fileNum) throws DatabaseException, IOException {
    return new FileProcessor_processFile(this,fileNum).execute();
  }
  /** 
 * Processes the first LN in the look ahead cache and removes it from the
 * cache. While the BIN is latched, look through the BIN for other LNs in
 * the cache; if any match, process them to avoid a tree search later.
 * @param info
 * @param offset
 */
  private void processLN(  Long fileNum,  TreeLocation location,  Long offset,  LNInfo info,  Object lookAheadCachep,  Map dbCache) throws DatabaseException {
    new FileProcessor_processLN(this,fileNum,location,offset,info,lookAheadCachep,dbCache).execute();
  }
  /** 
 * Processes an LN that was found in the tree. Lock the LN's node ID and
 * then set the entry's MIGRATE flag if the LSN of the LN log entry is the
 * active LSN in the tree.
 * @param infoidentifies the LN log entry.
 * @param logLsnis the LSN of the log entry.
 * @param treeLsnis the LSN found in the tree.
 * @param binis the BIN found in the tree; is latched on method entry and
 * exit.
 * @param indexis the BIN index found in the tree.
 * @param parentDINis non-null for a DupCountLN only; if non-null, is latched on
 * method entry and exit.
 */
  private void processFoundLN(  LNInfo info,  long logLsn,  long treeLsn,  BIN bin,  int index,  DIN parentDIN) throws DatabaseException {
    LN ln=info.getLN();
    byte[] key=info.getKey();
    byte[] dupKey=info.getDupKey();
    DatabaseImpl db=bin.getDatabase();
    boolean isDupCountLN=parentDIN != null;
    boolean obsolete=false;
    boolean migrated=false;
    boolean lockDenied=false;
    boolean completed=false;
    long nodeId=ln.getNodeId();
    BasicLocker locker=null;
    try {
      Tree tree=db.getTree();
      assert tree != null;
      if (treeLsn != logLsn) {
        locker=new BasicLocker(env);
        LockResult lockRet=locker.nonBlockingLock(nodeId,LockType.READ,db);
        if (lockRet.getLockGrant() == LockGrantType.DENIED) {
          this.hook142();
          lockDenied=true;
        }
 else {
          this.hook143();
          obsolete=true;
        }
      }
      if (!obsolete && !lockDenied) {
        if (isDupCountLN) {
          ChildReference dclRef=parentDIN.getDupCountLNRef();
          dclRef.setMigrate(true);
          parentDIN.setDirty(true);
          if (treeLsn == logLsn && dclRef.getTarget() == null) {
            ln.postFetchInit(db,logLsn);
            parentDIN.updateDupCountLN(ln);
          }
        }
 else {
          bin.setMigrate(index,true);
          bin.setDirty(true);
          if (treeLsn == logLsn && bin.getTarget(index) == null) {
            ln.postFetchInit(db,logLsn);
            bin.updateEntry(index,ln);
          }
          if (PROHIBIT_DELTAS_WHEN_FETCHING && bin.getGeneration() == 0) {
            bin.setProhibitNextDelta();
          }
          bin.setGeneration();
        }
        this.hook144();
        migrated=true;
      }
      completed=true;
    }
  finally {
      if (locker != null) {
        locker.operationEnd();
      }
      if (completed && lockDenied) {
        fileSelector.addPendingLN(ln,db.getId(),key,dupKey);
      }
      this.hook124(logLsn,ln,obsolete,migrated,completed);
    }
  }
  /** 
 * If an IN is still in use in the in-memory tree, dirty it. The checkpoint
 * invoked at the end of the cleaning run will end up rewriting it.
 */
  private void processIN(  IN inClone,  DatabaseImpl db,  long lsn) throws DatabaseException {
    try {
      boolean obsolete=false;
      boolean dirtied=false;
      boolean completed=false;
      this.hook125(inClone,db,lsn,obsolete,dirtied,completed);
    }
 catch (    ReturnVoid r) {
      return;
    }
  }
  /** 
 * Given a clone of an IN that has been taken out of the log, try to find it
 * in the tree and verify that it is the current one in the log. Returns the
 * node in the tree if it is found and it is current re: LSN's. Otherwise
 * returns null if the clone is not found in the tree or it's not the latest
 * version. Caller is responsible for unlatching the returned IN.
 */
  private IN findINInTree(  Tree tree,  DatabaseImpl db,  IN inClone,  long lsn) throws DatabaseException {
    try {
      if (inClone.isDbRoot()) {
        IN rootIN=isRoot(tree,db,inClone,lsn);
        if (rootIN == null) {
          return null;
        }
 else {
          return rootIN;
        }
      }
      inClone.latch(Cleaner.UPDATE_GENERATION);
      SearchResult result=null;
      this.hook134(tree,db,inClone,lsn,result);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (IN)r.value;
    }
  }
private static class RootDoWork implements WithRootLatched {
    private DatabaseImpl db;
    private IN inClone;
    private long lsn;
    RootDoWork(    DatabaseImpl db,    IN inClone,    long lsn){
      this.db=db;
      this.inClone=inClone;
      this.lsn=lsn;
    }
    public IN doWork(    ChildReference root) throws DatabaseException {
      if (root == null || root.fetchTarget(db,null).getNodeId() != inClone.getNodeId()) {
        return null;
      }
      if (DbLsn.compareTo(root.getLsn(),lsn) <= 0) {
        IN rootIN=(IN)root.fetchTarget(db,null);
        rootIN.latch(Cleaner.UPDATE_GENERATION);
        return rootIN;
      }
 else {
        return null;
      }
    }
  }
  /** 
 * Check if the cloned IN is the same node as the root in tree. Return the
 * real root if it is, null otherwise. If non-null is returned, the returned
 * IN (the root) is latched -- caller is responsible for unlatching it.
 */
  private IN isRoot(  Tree tree,  DatabaseImpl db,  IN inClone,  long lsn) throws DatabaseException {
    RootDoWork rdw=new RootDoWork(db,inClone,lsn);
    return tree.withRootLatchedShared(rdw);
  }
  /** 
 * XXX: Was this intended to override Thread.toString()? If so it no longer
 * does, because we separated Thread from DaemonThread.
 */
  public String toString(){
    StringBuffer sb=new StringBuffer();
    sb.append("<Cleaner name=\"").append(name).append("\"/>");
    return sb.toString();
  }
@MethodObject static class FileProcessor_processFile {
    FileProcessor_processFile(    FileProcessor _this,    Long fileNum){
      this._this=_this;
      this.fileNum=fileNum;
    }
    boolean execute() throws DatabaseException, IOException {
      obsoleteOffsets=new PackedOffsets();
      tfs=_this.profile.getObsoleteDetail(fileNum,obsoleteOffsets,true);
      obsoleteIter=obsoleteOffsets.iterator();
      nextObsolete=-1;
      readBufferSize=_this.cleaner.readBufferSize;
      this.hook128();
      this.hook161();
      this.hook119();
      this.hook127();
      this.hook154();
      dbCache=new HashMap();
      try {
        reader=new CleanerFileReader(_this.env,readBufferSize,DbLsn.NULL_LSN,fileNum);
        this.hook137();
        dbMapTree=_this.env.getDbMapTree();
        location=new TreeLocation();
        nProcessedLNs=0;
        while (reader.readNextEntry()) {
          this.hook146();
          lsn=reader.getLastLsn();
          fileOffset=DbLsn.getFileOffset(lsn);
          isLN=reader.isLN();
          isIN=reader.isIN();
          isRoot=reader.isRoot();
          isObsolete=false;
          if (_this.env.isClosing()) {
            return false;
          }
          while (nextObsolete < fileOffset && obsoleteIter.hasNext()) {
            nextObsolete=obsoleteIter.next();
          }
          if (nextObsolete == fileOffset) {
            isObsolete=true;
          }
          if (!isObsolete && !isLN && !isIN&& !isRoot) {
            isObsolete=true;
          }
          if (!isObsolete && isLN && reader.getLN().isDeleted()) {
            isObsolete=true;
          }
          if (!isObsolete && tfs != null && tfs.containsObsoleteOffset(fileOffset)) {
            isObsolete=true;
          }
          if (isObsolete) {
            this.hook147();
            this.hook156();
            continue;
          }
          this.hook120();
          if (isLN) {
            targetLN=reader.getLN();
            dbId2=reader.getDatabaseId();
            key=reader.getKey();
            dupKey=reader.getDupTreeKey();
            aLsn=new Long(DbLsn.getFileOffset(lsn));
            aLninfo=new LNInfo(targetLN,dbId2,key,dupKey);
            this.hook130();
            nProcessedLNs+=1;
            if (nProcessedLNs % _this.PROCESS_PENDING_EVERY_N_LNS == 0) {
              _this.cleaner.processPending();
            }
          }
 else           if (isIN) {
            targetIN=reader.getIN();
            dbId3=reader.getDatabaseId();
            db3=dbMapTree.getDb(dbId3,_this.cleaner.lockTimeout,dbCache);
            targetIN.setDatabase(db3);
            _this.processIN(targetIN,db3,lsn);
          }
 else           if (isRoot) {
            _this.env.rewriteMapTreeRoot(lsn);
          }
 else {
            assert false;
          }
        }
        this.hook129();
        this.hook155();
        this.hook145();
      }
  finally {
        this.hook162();
        if (tfs != null) {
          tfs.setAllowFlush(true);
        }
      }
      return true;
    }
    protected FileProcessor _this;
    protected Long fileNum;
    protected PackedOffsets obsoleteOffsets;
    protected TrackedFileSummary tfs;
    protected PackedOffsets.Iterator obsoleteIter;
    protected long nextObsolete;
    protected int readBufferSize;
    protected int lookAheadCacheSize;
    protected int adjustMem;
    protected MemoryBudget budget;
    protected Set checkPendingDbSet;
    protected Map dbCache;
    protected CleanerFileReader reader;
    protected DbTree dbMapTree;
    protected TreeLocation location;
    protected int nProcessedLNs;
    protected long lsn;
    protected long fileOffset;
    protected boolean isLN;
    protected boolean isIN;
    protected boolean isRoot;
    protected boolean isObsolete;
    protected DatabaseId dbId1;
    protected LN targetLN;
    protected DatabaseId dbId2;
    protected byte[] key;
    protected byte[] dupKey;
    protected Long aLsn;
    protected LNInfo aLninfo;
    protected Object p;
    protected IN targetIN;
    protected DatabaseId dbId3;
    protected DatabaseImpl db3;
    protected DatabaseId dbId;
    protected DatabaseImpl db;
    protected void hook119() throws DatabaseException, IOException {
    }
    protected void hook120() throws DatabaseException, IOException {
    }
    protected void hook127() throws DatabaseException, IOException {
    }
    protected void hook128() throws DatabaseException, IOException {
    }
    protected void hook129() throws DatabaseException, IOException {
    }
    protected void hook130() throws DatabaseException, IOException {
      p=null;
      this.hook131();
      _this.processLN(fileNum,location,aLsn,aLninfo,p,dbCache);
    }
    protected void hook131() throws DatabaseException, IOException {
    }
    protected void hook137() throws DatabaseException, IOException {
    }
    protected void hook145() throws DatabaseException, IOException {
    }
    protected void hook146() throws DatabaseException, IOException {
    }
    protected void hook147() throws DatabaseException, IOException {
    }
    protected void hook154() throws DatabaseException, IOException {
    }
    protected void hook155() throws DatabaseException, IOException {
    }
    protected void hook156() throws DatabaseException, IOException {
    }
    protected void hook161() throws DatabaseException, IOException {
    }
    protected void hook162() throws DatabaseException, IOException {
    }
  }
@MethodObject static class FileProcessor_processLN {
    FileProcessor_processLN(    FileProcessor _this,    Long fileNum,    TreeLocation location,    Long offset,    LNInfo info,    Object lookAheadCachep,    Map dbCache){
      this._this=_this;
      this.fileNum=fileNum;
      this.location=location;
      this.offset=offset;
      this.info=info;
      this.lookAheadCachep=lookAheadCachep;
      this.dbCache=dbCache;
    }
    void execute() throws DatabaseException {
      this.hook132();
      ln=info.getLN();
      key=info.getKey();
      dupKey=info.getDupKey();
      logLsn=DbLsn.makeLsn(fileNum.longValue(),offset.longValue());
      db=_this.env.getDbMapTree().getDb(info.getDbId(),_this.cleaner.lockTimeout,dbCache);
      processedHere=true;
      obsolete=false;
      completed=false;
      bin=null;
      parentDIN=null;
      try {
        b=db == null;
        this.hook157();
        if (b) {
          this.hook158();
          this.hook148();
          obsolete=true;
          completed=true;
          return;
        }
        tree=db.getTree();
        assert tree != null;
        parentFound=tree.getParentBINForChildLN(location,key,dupKey,ln,false,true,false,Cleaner.UPDATE_GENERATION);
        bin=location.bin;
        index=location.index;
        if (!parentFound) {
          this.hook149();
          obsolete=true;
          completed=true;
          return;
        }
        if (bin.isEntryKnownDeleted(index)) {
          this.hook150();
          obsolete=true;
          completed=true;
          return;
        }
        isDupCountLN=ln.containsDuplicates();
{
        }
        if (isDupCountLN) {
          parentDIN=(DIN)bin.fetchTarget(index);
          parentDIN.latch(Cleaner.UPDATE_GENERATION);
          dclRef=parentDIN.getDupCountLNRef();
          treeLsn=dclRef.getLsn();
        }
 else {
          treeLsn=bin.getLsn(index);
        }
        processedHere=false;
        _this.processFoundLN(info,logLsn,treeLsn,bin,index,parentDIN);
        completed=true;
        this.hook133();
        return;
      }
  finally {
        this.hook135();
        this.hook126();
      }
    }
    protected FileProcessor _this;
    protected Long fileNum;
    protected TreeLocation location;
    protected Long offset;
    protected LNInfo info;
    protected Object lookAheadCachep;
    protected Map dbCache;
    protected LN ln;
    protected byte[] key;
    protected byte[] dupKey;
    protected long logLsn;
    protected DatabaseImpl db;
    protected boolean processedHere;
    protected boolean obsolete;
    protected boolean completed;
    protected BIN bin;
    protected DIN parentDIN;
    protected boolean b;
    protected Tree tree;
    protected boolean parentFound;
    protected int index;
    protected boolean isDupCountLN;
    protected long treeLsn;
    protected ChildReference dclRef;
    protected long lsn;
    protected Long myOffset;
    protected LNInfo myInfo;
    protected void hook126() throws DatabaseException {
    }
    protected void hook132() throws DatabaseException {
    }
    protected void hook133() throws DatabaseException {
    }
    protected void hook135() throws DatabaseException {
    }
    protected void hook148() throws DatabaseException {
    }
    protected void hook149() throws DatabaseException {
    }
    protected void hook150() throws DatabaseException {
    }
    protected void hook157() throws DatabaseException {
    }
    protected void hook158() throws DatabaseException {
    }
  }
  protected void hook121(  String traceMsg) throws DatabaseException, IOException {
  }
  protected void hook122(  IOException IOE) throws DatabaseException {
  }
  protected void hook123(  String traceMsg) throws DatabaseException {
  }
  protected void hook124(  long logLsn,  LN ln,  boolean obsolete,  boolean migrated,  boolean completed) throws DatabaseException {
  }
  protected void hook125(  IN inClone,  DatabaseImpl db,  long lsn,  boolean obsolete,  boolean dirtied,  boolean completed) throws DatabaseException {
    boolean b=db == null;
    b=this.hook159(db,b);
    if (b) {
      this.hook160(db);
      this.hook151();
      obsolete=true;
      completed=true;
      throw new ReturnVoid();
    }
    Tree tree=db.getTree();
    assert tree != null;
    IN inInTree=findINInTree(tree,db,inClone,lsn);
    if (inInTree == null) {
      this.hook152();
      obsolete=true;
    }
 else {
      this.hook153();
      inInTree.setDirty(true);
      inInTree.setProhibitNextDelta();
      this.hook136(inInTree);
      dirtied=true;
    }
    completed=true;
  }
  protected void hook134(  Tree tree,  DatabaseImpl db,  IN inClone,  long lsn,  SearchResult result) throws DatabaseException {
    result=tree.getParentINForChildIN(inClone,true,Cleaner.UPDATE_GENERATION,inClone.getLevel(),null);
    if (!result.exactParentFound) {
      throw new ReturnObject(null);
    }
    int compareVal=DbLsn.compareTo(result.parent.getLsn(result.index),lsn);
    if (compareVal > 0) {
      throw new ReturnObject(null);
    }
 else {
      IN in;
      if (compareVal == 0) {
        in=(IN)result.parent.getTarget(result.index);
        if (in == null) {
          in=inClone;
          in.postFetchInit(db,lsn);
          result.parent.updateEntry(result.index,in);
        }
      }
 else {
        in=(IN)result.parent.fetchTarget(result.index);
      }
      in.latch(Cleaner.UPDATE_GENERATION);
      throw new ReturnObject(in);
    }
  }
  protected void hook136(  IN inInTree) throws DatabaseException {
  }
  protected void hook138() throws DatabaseException {
  }
  protected String hook139(  String traceMsg) throws DatabaseException, IOException {
    return traceMsg;
  }
  protected void hook140() throws DatabaseException, IOException {
  }
  protected String hook141(  String traceMsg) throws DatabaseException {
    return traceMsg;
  }
  protected void hook142() throws DatabaseException {
  }
  protected void hook143() throws DatabaseException {
  }
  protected void hook144() throws DatabaseException {
  }
  protected void hook151() throws DatabaseException {
  }
  protected void hook152() throws DatabaseException {
  }
  protected void hook153() throws DatabaseException {
  }
  protected boolean hook159(  DatabaseImpl db,  boolean b) throws DatabaseException {
    return b;
  }
  protected void hook160(  DatabaseImpl db) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/cleaner/UtilizationProfile.java:package com.sleepycat.je.cleaner;
import java.io.File;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;
import java.util.Set;
import java.util.SortedMap;
import java.util.SortedSet;
import java.util.StringTokenizer;
import java.util.TreeMap;
import java.util.logging.Level;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.OperationStatus;
import com.sleepycat.je.TransactionConfig;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.CursorImpl;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvConfigObserver;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.dbi.CursorImpl.SearchMode;
import com.sleepycat.je.log.FileManager;
import com.sleepycat.je.log.entry.LNLogEntry;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.FileSummaryLN;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.TreeLocation;
import com.sleepycat.je.txn.AutoTxn;
import com.sleepycat.je.txn.BasicLocker;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.txn.Locker;
import com.sleepycat.je.utilint.DbLsn;
import de.ovgu.cide.jakutil.*;
/** 
 * The UP tracks utilization summary information for all log files.
 * <p>Unlike the UtilizationTracker, the UP is not accessed under the log write
 * latch and is instead synchronized on itself for protecting the cache.  It is
 * not accessed during the primary data access path, except for when flushing
 * (writing) file summary LNs.  This occurs in the following cases:
 * <ol>
 * <li>The summary information is flushed at the end of a checkpoint.  This
 * allows tracking to occur in memory in between checkpoints, and replayed
 * during recovery.</li>
 * <li>When committing the truncateDatabase and removeDatabase operations, the
 * summary information is flushed because detail tracking for those operations
 * is not replayed during recovery</li>
 * <li>The evictor will ask the UtilizationTracker to flush the largest summary
 * if the memory taken by the tracker exeeds its budget.</li>
 * </ol>
 * <p>The cache is populated by the RecoveryManager just before performing the
 * initial checkpoint.  The UP must be open and populated in order to respond
 * to requests to flush summaries and to evict tracked detail, even if the
 * cleaner is disabled.</p>
 * <p>WARNING: While synchronized on this object, eviction is not permitted.
 * If it were, this could cause deadlocks because the order of locking would be
 * the UP object and then the evictor.  During normal eviction the order is to
 * first lock the evictor and then the UP, when evicting tracked detail.</p>
 * <p>The methods in this class synchronize to protect the cached summary
 * information.  Some methods also access the UP database.  However, because
 * eviction must not occur while synchronized, UP database access is not
 * performed while synchronized except in one case: when inserting a new
 * summary record.  In that case we disallow eviction during the database
 * operation.</p>
 */
public class UtilizationProfile implements EnvConfigObserver {
  private EnvironmentImpl env;
  private UtilizationTracker tracker;
  private DatabaseImpl fileSummaryDb;
  private SortedMap fileSummaryMap;
  private boolean cachePopulated;
  private boolean rmwFixEnabled;
  /** 
 * Minimum overall utilization threshold that triggers cleaning.  Is
 * non-private for unit tests.
 */
  int minUtilization;
  /** 
 * Minimum utilization threshold for an individual log file that triggers
 * cleaning.  Is non-private for unit tests.
 */
  int minFileUtilization;
  /** 
 * Minumum age to qualify for cleaning.  If the first active LSN file is 5
 * and the mininum age is 2, file 4 won't qualify but file 3 will.  Must be
 * greater than zero because we never clean the first active LSN file.  Is
 * non-private for unit tests.
 */
  int minAge;
  /** 
 * An array of pairs of file numbers, where each pair is a range of files
 * to be force cleaned.  Index i is the from value and i+1 is the to value,
 * both inclusive.
 */
  private long[] forceCleanFiles;
  /** 
 * Creates an empty UP.
 */
  public UtilizationProfile(  EnvironmentImpl env,  UtilizationTracker tracker) throws DatabaseException {
    this.env=env;
    this.tracker=tracker;
    fileSummaryMap=new TreeMap();
    rmwFixEnabled=env.getConfigManager().getBoolean(EnvironmentParams.CLEANER_RMW_FIX);
    parseForceCleanFiles(env.getConfigManager().get(EnvironmentParams.CLEANER_FORCE_CLEAN_FILES));
    envConfigUpdate(env.getConfigManager());
    env.addConfigObserver(this);
  }
  /** 
 * Process notifications of mutable property changes.
 */
  public void envConfigUpdate(  DbConfigManager cm) throws DatabaseException {
    minAge=cm.getInt(EnvironmentParams.CLEANER_MIN_AGE);
    minUtilization=cm.getInt(EnvironmentParams.CLEANER_MIN_UTILIZATION);
    minFileUtilization=cm.getInt(EnvironmentParams.CLEANER_MIN_FILE_UTILIZATION);
  }
  /** 
 * @see EnvironmentParams#CLEANER_RMW_FIX
 * @see FileSummaryLN#postFetchInit
 */
  public boolean isRMWFixEnabled(){
    return rmwFixEnabled;
  }
  /** 
 * Returns the number of files in the profile.
 */
  synchronized int getNumberOfFiles() throws DatabaseException {
    assert cachePopulated;
    return fileSummaryMap.size();
  }
  /** 
 * Returns the cheapest file to clean from the given list of files.  This
 * method is used to select the first file to be cleaned in the batch of
 * to-be-cleaned files.
 */
  synchronized Long getCheapestFileToClean(  SortedSet files) throws DatabaseException {
    if (files.size() == 1) {
      return (Long)files.first();
    }
    assert cachePopulated;
    Long bestFile=null;
    int bestCost=Integer.MAX_VALUE;
    for (Iterator iter=files.iterator(); iter.hasNext(); ) {
      Long file=(Long)iter.next();
      FileSummary summary=getFileSummary(file);
      int thisCost=summary.getNonObsoleteCount();
      if (bestFile == null || thisCost < bestCost) {
        bestFile=file;
        bestCost=thisCost;
      }
    }
    return bestFile;
  }
  /** 
 * Returns the best file that qualifies for cleaning, or null if no file
 * qualifies.
 * @param fileSelector is used to determine valid cleaning candidates.
 * @param forceCleaning is true to always select a file, even if its
 * utilization is above the minimum utilization threshold.
 * @param lowUtilizationFiles is a returned set of files that are below the
 * minimum utilization threshold.
 */
  synchronized Long getBestFileForCleaning(  FileSelector fileSelector,  boolean forceCleaning,  Set lowUtilizationFiles) throws DatabaseException {
    if (lowUtilizationFiles != null) {
      lowUtilizationFiles.clear();
    }
    assert cachePopulated;
    if (fileSummaryMap.size() == 0) {
      return null;
    }
    final int useMinUtilization=minUtilization;
    final int useMinFileUtilization=minFileUtilization;
    final int useMinAge=minAge;
    long firstActiveLsn=env.getCheckpointer().getFirstActiveLsn();
    if (firstActiveLsn == DbLsn.NULL_LSN) {
      return null;
    }
    Iterator iter=fileSummaryMap.keySet().iterator();
    Long bestFile=null;
    int bestUtilization=101;
    long totalSize=0;
    long totalObsoleteSize=0;
    while (iter.hasNext()) {
      Long file=(Long)iter.next();
      long fileNum=file.longValue();
      FileSummary summary=getFileSummary(file);
      int obsoleteSize=summary.getObsoleteSize();
      if (fileSelector.isFileCleaningInProgress(file)) {
        totalSize+=summary.totalSize - obsoleteSize;
        totalObsoleteSize+=estimateUPObsoleteSize(summary);
        continue;
      }
      totalSize+=summary.totalSize;
      totalObsoleteSize+=obsoleteSize;
      if (DbLsn.getFileNumber(firstActiveLsn) - fileNum < useMinAge) {
        continue;
      }
      int thisUtilization=utilization(obsoleteSize,summary.totalSize);
      if (bestFile == null || thisUtilization < bestUtilization) {
        bestFile=file;
        bestUtilization=thisUtilization;
      }
      if (lowUtilizationFiles != null && thisUtilization < useMinUtilization) {
        lowUtilizationFiles.add(file);
      }
    }
    int totalUtilization=utilization(totalObsoleteSize,totalSize);
    if (forceCleaning || totalUtilization < useMinUtilization || bestUtilization < useMinFileUtilization) {
      return bestFile;
    }
 else {
      return null;
    }
  }
  /** 
 * Calculate the utilization percentage.
 */
  public static int utilization(  long obsoleteSize,  long totalSize){
    if (totalSize != 0) {
      return (int)(((totalSize - obsoleteSize) * 100) / totalSize);
    }
 else {
      return 0;
    }
  }
  /** 
 * Estimate the log size that will be made obsolete when a log file is
 * deleted and we delete its UP records.
 * Note that we do not count the space taken by the deleted FileSummaryLN
 * records written during log file deletion.  These add the same amount to
 * the total log size and the obsolete log size, and therefore have a small
 * impact on total utilization.
 */
  private int estimateUPObsoleteSize(  FileSummary summary){
    if (true)     return 0;
    final int OVERHEAD=75;
    int OFFSETS_PER_LN=1000;
    int BYTES_PER_LN=OVERHEAD + (OFFSETS_PER_LN * 2);
    int totalNodes=summary.totalLNCount + summary.totalINCount;
    int logEntries=(totalNodes / OFFSETS_PER_LN) + 1;
    return logEntries * BYTES_PER_LN;
  }
  /** 
 * Gets the base summary from the cached map.  Add the tracked summary, if
 * one exists, to the base summary.  Sets all entries obsolete, if the file
 * is in the forceCleanFiles set.
 */
  private synchronized FileSummary getFileSummary(  Long file){
    FileSummary summary=(FileSummary)fileSummaryMap.get(file);
    long fileNum=file.longValue();
    TrackedFileSummary trackedSummary=tracker.getTrackedFile(fileNum);
    if (trackedSummary != null) {
      FileSummary totals=new FileSummary();
      totals.add(summary);
      totals.add(trackedSummary);
      summary=totals;
    }
    if (isForceCleanFile(fileNum)) {
      FileSummary allObsolete=new FileSummary();
      allObsolete.add(summary);
      allObsolete.obsoleteLNCount=allObsolete.totalLNCount;
      allObsolete.obsoleteINCount=allObsolete.totalINCount;
      summary=allObsolete;
    }
    return summary;
  }
  /** 
 * Returns whether the given file is in the forceCleanFiles set.
 */
  private boolean isForceCleanFile(  long file){
    if (forceCleanFiles != null) {
      for (int i=0; i < forceCleanFiles.length; i+=2) {
        long from=forceCleanFiles[i];
        long to=forceCleanFiles[i + 1];
        if (file >= from && file <= to) {
          return true;
        }
      }
    }
    return false;
  }
  /** 
 * Parses the je.cleaner.forceCleanFiles property value.
 */
  private void parseForceCleanFiles(  String propValue){
    if (propValue == null || propValue.length() == 0) {
      forceCleanFiles=null;
    }
 else {
      String errPrefix="Error in " + EnvironmentParams.CLEANER_FORCE_CLEAN_FILES.getName() + "="+ propValue+ ": ";
      StringTokenizer tokens=new StringTokenizer(propValue,",-",true);
      List list=new ArrayList();
      while (tokens.hasMoreTokens()) {
        String fromStr=tokens.nextToken();
        long fromNum;
        try {
          fromNum=Long.parseLong(fromStr,16);
        }
 catch (        NumberFormatException e) {
          throw new IllegalArgumentException(errPrefix + "Invalid hex file number: " + fromStr);
        }
        long toNum=-1;
        if (tokens.hasMoreTokens()) {
          String delim=tokens.nextToken();
          if (",".equals(delim)) {
            toNum=fromNum;
          }
 else           if ("-".equals(delim)) {
            if (tokens.hasMoreTokens()) {
              String toStr=tokens.nextToken();
              try {
                toNum=Long.parseLong(toStr,16);
              }
 catch (              NumberFormatException e) {
                throw new IllegalArgumentException(errPrefix + "Invalid hex file number: " + toStr);
              }
            }
 else {
              throw new IllegalArgumentException(errPrefix + "Expected file number: " + delim);
            }
          }
 else {
            throw new IllegalArgumentException(errPrefix + "Expected '-' or ',': " + delim);
          }
        }
 else {
          toNum=fromNum;
        }
        assert toNum != -1;
        list.add(new Long(fromNum));
        list.add(new Long(toNum));
      }
      forceCleanFiles=new long[list.size()];
      for (int i=0; i < forceCleanFiles.length; i+=1) {
        forceCleanFiles[i]=((Long)list.get(i)).longValue();
      }
    }
  }
  /** 
 * Count the given tracked info as obsolete and then log the summaries.
 */
  public void countAndLogSummaries(  TrackedFileSummary[] summaries) throws DatabaseException {
    env.getLogManager().countObsoleteNodes(summaries);
    if (!DbInternal.getCheckpointUP(env.getConfigManager().getEnvironmentConfig())) {
      return;
    }
    for (int i=0; i < summaries.length; i+=1) {
      long fileNum=summaries[i].getFileNumber();
      TrackedFileSummary tfs=tracker.getTrackedFile(fileNum);
      if (tfs != null) {
        flushFileSummary(tfs);
      }
    }
  }
  /** 
 * Returns a copy of the current file summary map, optionally including
 * tracked summary information, for use by the DbSpace utility and by unit
 * tests.  The returned map's key is a Long file number and its value is a
 * FileSummary.
 */
  public synchronized SortedMap getFileSummaryMap(  boolean includeTrackedFiles) throws DatabaseException {
    assert cachePopulated;
    if (includeTrackedFiles) {
      TreeMap map=new TreeMap();
      Iterator iter=fileSummaryMap.keySet().iterator();
      while (iter.hasNext()) {
        Long file=(Long)iter.next();
        FileSummary summary=getFileSummary(file);
        map.put(file,summary);
      }
      TrackedFileSummary[] trackedFiles=tracker.getTrackedFiles();
      for (int i=0; i < trackedFiles.length; i+=1) {
        TrackedFileSummary summary=trackedFiles[i];
        long fileNum=summary.getFileNumber();
        Long file=new Long(fileNum);
        if (!map.containsKey(file)) {
          map.put(file,summary);
        }
      }
      return map;
    }
 else {
      return new TreeMap(fileSummaryMap);
    }
  }
  /** 
 * Clears the cache of file summary info.  The cache starts out unpopulated
 * and is populated on the first call to getBestFileForCleaning.
 */
  public synchronized void clearCache(){
    new UtilizationProfile_clearCache(this).execute();
  }
  /** 
 * Removes a file from the utilization database and the profile, after it
 * has been deleted by the cleaner.
 */
  void removeFile(  Long fileNum) throws DatabaseException {
    new UtilizationProfile_removeFile(this,fileNum).execute();
  }
  /** 
 * For the LN at the cursor position deletes all LNs for the file.  This
 * method performs eviction and is not synchronized.
 */
  private void deleteFileSummary(  Long fileNum) throws DatabaseException {
    Locker locker=null;
    CursorImpl cursor=null;
    try {
      locker=new BasicLocker(env);
      cursor=new CursorImpl(fileSummaryDb,locker);
      DatabaseEntry keyEntry=new DatabaseEntry();
      DatabaseEntry dataEntry=new DatabaseEntry();
      long fileNumVal=fileNum.longValue();
      if (!getFirstFSLN(cursor,fileNumVal,keyEntry,dataEntry,LockType.WRITE)) {
        return;
      }
      OperationStatus status=OperationStatus.SUCCESS;
      while (status == OperationStatus.SUCCESS) {
        this.hook173();
        FileSummaryLN ln=(FileSummaryLN)cursor.getCurrentLN(LockType.NONE);
        if (ln != null) {
          if (fileNumVal != ln.getFileNumber(keyEntry.getData())) {
            break;
          }
          TrackedFileSummary tfs=tracker.getTrackedFile(fileNumVal);
          if (tfs != null) {
            ln.setTrackedSummary(tfs);
          }
          cursor.delete();
        }
        status=cursor.getNext(keyEntry,dataEntry,LockType.WRITE,true,false);
      }
    }
  finally {
      if (cursor != null) {
        this.hook178(cursor);
        cursor.close();
      }
      if (locker != null) {
        locker.operationEnd();
      }
    }
  }
  /** 
 * Updates and stores the FileSummary for a given tracked file, if flushing
 * of the summary is allowed.
 */
  public void flushFileSummary(  TrackedFileSummary tfs) throws DatabaseException {
    if (tfs.getAllowFlush()) {
      putFileSummary(tfs);
    }
  }
  /** 
 * Updates and stores the FileSummary for a given tracked file.  This
 * method is synchronized and may not perform eviction.
 */
  private synchronized PackedOffsets putFileSummary(  TrackedFileSummary tfs) throws DatabaseException {
    return new UtilizationProfile_putFileSummary(this,tfs).execute();
  }
  /** 
 * Returns the stored/packed obsolete offsets and the tracked obsolete
 * offsets for the given file.  The tracked summary object returned can be
 * used to test for obsolete offsets that are being added during cleaning
 * by other threads participating in lazy migration.  The caller must call
 * TrackedFileSummary.setAllowFlush(true) when cleaning is complete.
 * This method performs eviction and is not synchronized.
 * @param logUpdate if true, log any updates to the utilization profile. If
 * false, only retrieve the new information.
 */
  TrackedFileSummary getObsoleteDetail(  Long fileNum,  PackedOffsets packedOffsets,  boolean logUpdate) throws DatabaseException {
    if (!env.getCleaner().trackDetail) {
      return null;
    }
    assert cachePopulated;
    long fileNumVal=fileNum.longValue();
    List list=new ArrayList();
    TrackedFileSummary tfs=env.getLogManager().getUnflushableTrackedSummary(fileNumVal);
    Locker locker=null;
    CursorImpl cursor=null;
    try {
      locker=new BasicLocker(env);
      cursor=new CursorImpl(fileSummaryDb,locker);
      DatabaseEntry keyEntry=new DatabaseEntry();
      DatabaseEntry dataEntry=new DatabaseEntry();
      OperationStatus status=OperationStatus.SUCCESS;
      if (!getFirstFSLN(cursor,fileNumVal,keyEntry,dataEntry,LockType.NONE)) {
        status=OperationStatus.NOTFOUND;
      }
      while (status == OperationStatus.SUCCESS) {
        this.hook174();
        FileSummaryLN ln=(FileSummaryLN)cursor.getCurrentLN(LockType.NONE);
        if (ln != null) {
          if (fileNumVal != ln.getFileNumber(keyEntry.getData())) {
            break;
          }
          PackedOffsets offsets=ln.getObsoleteOffsets();
          if (offsets != null) {
            list.add(offsets.toArray());
          }
          this.hook187(cursor);
        }
        status=cursor.getNext(keyEntry,dataEntry,LockType.NONE,true,false);
      }
    }
  finally {
      this.hook179(cursor);
      if (locker != null) {
        locker.operationEnd();
      }
    }
    if (!tfs.isEmpty()) {
      PackedOffsets offsets=null;
      if (logUpdate) {
        offsets=putFileSummary(tfs);
        if (offsets != null) {
          list.add(offsets.toArray());
        }
      }
 else {
        long[] offsetList=tfs.getObsoleteOffsets();
        if (offsetList != null) {
          list.add(offsetList);
        }
      }
    }
    int size=0;
    for (int i=0; i < list.size(); i+=1) {
      long[] a=(long[])list.get(i);
      size+=a.length;
    }
    long[] offsets=new long[size];
    int index=0;
    for (int i=0; i < list.size(); i+=1) {
      long[] a=(long[])list.get(i);
      System.arraycopy(a,0,offsets,index,a.length);
      index+=a.length;
    }
    assert index == offsets.length;
    packedOffsets.pack(offsets);
    return tfs;
  }
  /** 
 * Populate the profile for file selection.  This method performs eviction
 * and is not synchronized.  It must be called before recovery is complete
 * so that synchronization is unnecessary.  It must be called before the
 * recovery checkpoint so that the checkpoint can flush file summary
 * information.
 */
  public boolean populateCache() throws DatabaseException {
    return new UtilizationProfile_populateCache(this).execute();
  }
  /** 
 * Positions at the most recent LN for the given file number.
 */
  private boolean getFirstFSLN(  CursorImpl cursor,  long fileNum,  DatabaseEntry keyEntry,  DatabaseEntry dataEntry,  LockType lockType) throws DatabaseException {
    byte[] keyBytes=FileSummaryLN.makePartialKey(fileNum);
    keyEntry.setData(keyBytes);
    int result=cursor.searchAndPosition(keyEntry,dataEntry,SearchMode.SET_RANGE,lockType);
    if ((result & CursorImpl.FOUND) == 0) {
      return false;
    }
    boolean exactKeyMatch=((result & CursorImpl.EXACT_KEY) != 0);
    if (exactKeyMatch && cursor.getCurrentAlreadyLatched(keyEntry,dataEntry,lockType,true) != OperationStatus.KEYEMPTY) {
      return true;
    }
    OperationStatus status=cursor.getNext(keyEntry,dataEntry,lockType,true,!exactKeyMatch);
    return status == OperationStatus.SUCCESS;
  }
  /** 
 * If the file summary db is already open, return, otherwise attempt to
 * open it.  If the environment is read-only and the database doesn't
 * exist, return false.  If the environment is read-write the database will
 * be created if it doesn't exist.
 */
  private boolean openFileSummaryDatabase() throws DatabaseException {
    if (fileSummaryDb != null) {
      return true;
    }
    DbTree dbTree=env.getDbMapTree();
    Locker autoTxn=null;
    boolean operationOk=false;
    try {
      autoTxn=new AutoTxn(env,new TransactionConfig());
      DatabaseImpl db=dbTree.getDb(autoTxn,DbTree.UTILIZATION_DB_NAME,null,true);
      if (db == null) {
        if (env.isReadOnly()) {
          return false;
        }
        db=dbTree.createDb(autoTxn,DbTree.UTILIZATION_DB_NAME,new DatabaseConfig(),null,true);
      }
      fileSummaryDb=db;
      operationOk=true;
      return true;
    }
  finally {
      if (autoTxn != null) {
        autoTxn.operationEnd(operationOk);
      }
    }
  }
  /** 
 * Insert the given LN with the given key values.  This method is
 * synchronized and may not perform eviction.
 */
  private synchronized void insertFileSummary(  FileSummaryLN ln,  long fileNum,  int sequence) throws DatabaseException {
    byte[] keyBytes=FileSummaryLN.makeFullKey(fileNum,sequence);
    Locker locker=null;
    CursorImpl cursor=null;
    try {
      locker=new BasicLocker(env);
      cursor=new CursorImpl(fileSummaryDb,locker);
      this.hook189(cursor);
      OperationStatus status=cursor.putLN(keyBytes,ln,false);
      this.hook177(fileNum,sequence,status);
      this.hook188(cursor);
    }
  finally {
      if (cursor != null) {
        cursor.close();
      }
      if (locker != null) {
        locker.operationEnd();
      }
    }
  }
  /** 
 * Checks that all FSLN offsets are indeed obsolete.  Assumes that the
 * system is quiesent (does not lock LNs).  This method is not synchronized
 * (because it doesn't access fileSummaryMap) and eviction is allowed.
 * @return true if no verification failures.
 */
  public boolean verifyFileSummaryDatabase() throws DatabaseException {
    DatabaseEntry key=new DatabaseEntry();
    DatabaseEntry data=new DatabaseEntry();
    openFileSummaryDatabase();
    Locker locker=null;
    CursorImpl cursor=null;
    boolean ok=true;
    try {
      locker=new BasicLocker(env);
      cursor=new CursorImpl(fileSummaryDb,locker);
      if (cursor.positionFirstOrLast(true,null)) {
        OperationStatus status=cursor.getCurrentAlreadyLatched(key,data,LockType.NONE,true);
        while (status == OperationStatus.SUCCESS) {
          this.hook175();
          FileSummaryLN ln=(FileSummaryLN)cursor.getCurrentLN(LockType.NONE);
          if (ln != null) {
            long fileNumVal=ln.getFileNumber(key.getData());
            PackedOffsets offsets=ln.getObsoleteOffsets();
            if (offsets != null) {
              long[] vals=offsets.toArray();
              for (int i=0; i < vals.length; i++) {
                long lsn=DbLsn.makeLsn(fileNumVal,vals[i]);
                if (!verifyLsnIsObsolete(lsn)) {
                  ok=false;
                }
              }
            }
            this.hook190(cursor);
            status=cursor.getNext(key,data,LockType.NONE,true,false);
          }
        }
      }
    }
  finally {
      if (cursor != null) {
        cursor.close();
      }
      if (locker != null) {
        locker.operationEnd();
      }
    }
    return ok;
  }
  private boolean verifyLsnIsObsolete(  long lsn) throws DatabaseException {
    try {
      Object o=env.getLogManager().getLogEntry(lsn);
      if (!(o instanceof LNLogEntry)) {
        return true;
      }
      LNLogEntry entry=(LNLogEntry)o;
      if (entry.getLN().isDeleted()) {
        return true;
      }
      DatabaseId dbId=entry.getDbId();
      DatabaseImpl db=env.getDbMapTree().getDb(dbId);
      boolean b=db == null;
      b=this.hook186(db,b);
      if (b) {
        return true;
      }
      BIN bin=null;
      this.hook180(lsn,entry,db,bin);
      throw ReturnHack.returnBoolean;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
@MethodObject static class UtilizationProfile_clearCache {
    UtilizationProfile_clearCache(    UtilizationProfile _this){
      this._this=_this;
    }
    void execute(){
      _this.fileSummaryMap=new TreeMap();
      _this.cachePopulated=false;
    }
    protected UtilizationProfile _this;
    protected int memorySize;
    protected MemoryBudget mb;
  }
@MethodObject static class UtilizationProfile_removeFile {
    UtilizationProfile_removeFile(    UtilizationProfile _this,    Long fileNum){
      this._this=_this;
      this.fileNum=fileNum;
    }
    void execute() throws DatabaseException {
synchronized (_this) {
        assert _this.cachePopulated;
        if (_this.fileSummaryMap.remove(fileNum) != null) {
          this.hook192();
        }
      }
      _this.deleteFileSummary(fileNum);
    }
    protected UtilizationProfile _this;
    protected Long fileNum;
    protected MemoryBudget mb;
    protected void hook192() throws DatabaseException {
    }
  }
@MethodObject static class UtilizationProfile_putFileSummary {
    UtilizationProfile_putFileSummary(    UtilizationProfile _this,    TrackedFileSummary tfs){
      this._this=_this;
      this.tfs=tfs;
    }
    PackedOffsets execute() throws DatabaseException {
      if (_this.env.isReadOnly()) {
        throw new DatabaseException("Cannot write file summary in a read-only environment");
      }
      if (tfs.isEmpty()) {
        return null;
      }
      if (!_this.cachePopulated) {
        return null;
      }
      fileNum=tfs.getFileNumber();
      fileNumLong=new Long(fileNum);
      summary=(FileSummary)_this.fileSummaryMap.get(fileNumLong);
      if (summary == null) {
        file=new File(_this.env.getFileManager().getFullFileName(fileNum,FileManager.JE_SUFFIX));
        if (!file.exists()) {
          return null;
        }
        summary=new FileSummary();
      }
      tmp=new FileSummary();
      tmp.add(summary);
      tmp.add(tfs);
      sequence=tmp.getEntriesCounted();
      ln=new FileSummaryLN(summary);
      ln.setTrackedSummary(tfs);
      _this.insertFileSummary(ln,fileNum,sequence);
      summary=ln.getBaseSummary();
      if (_this.fileSummaryMap.put(fileNumLong,summary) == null) {
        this.hook193();
      }
      return ln.getObsoleteOffsets();
    }
    protected UtilizationProfile _this;
    protected TrackedFileSummary tfs;
    protected long fileNum;
    protected Long fileNumLong;
    protected FileSummary summary;
    protected File file;
    protected FileSummary tmp;
    protected int sequence;
    protected FileSummaryLN ln;
    protected MemoryBudget mb;
    protected void hook193() throws DatabaseException {
    }
  }
@MethodObject static class UtilizationProfile_populateCache {
    UtilizationProfile_populateCache(    UtilizationProfile _this){
      this._this=_this;
    }
    boolean execute() throws DatabaseException {
      assert !_this.cachePopulated;
      if (!_this.openFileSummaryDatabase()) {
        return false;
      }
      this.hook194();
      existingFiles=_this.env.getFileManager().getAllFileNumbers();
      locker=null;
      cursor=null;
      try {
        locker=new BasicLocker(_this.env);
        cursor=new CursorImpl(_this.fileSummaryDb,locker);
        keyEntry=new DatabaseEntry();
        dataEntry=new DatabaseEntry();
        if (cursor.positionFirstOrLast(true,null)) {
          status=cursor.getCurrentAlreadyLatched(keyEntry,dataEntry,LockType.NONE,true);
          if (status != OperationStatus.SUCCESS) {
            status=cursor.getNext(keyEntry,dataEntry,LockType.NONE,true,false);
          }
          while (status == OperationStatus.SUCCESS) {
            this.hook176();
            ln=(FileSummaryLN)cursor.getCurrentLN(LockType.NONE);
            if (ln == null) {
              status=cursor.getNext(keyEntry,dataEntry,LockType.NONE,true,false);
              continue;
            }
            keyBytes=keyEntry.getData();
            isOldVersion=ln.hasStringKey(keyBytes);
            fileNum=ln.getFileNumber(keyBytes);
            fileNumLong=new Long(fileNum);
            if (Arrays.binarySearch(existingFiles,fileNumLong) >= 0) {
              _this.fileSummaryMap.put(fileNumLong,ln.getBaseSummary());
              if (isOldVersion) {
                _this.insertFileSummary(ln,fileNum,0);
                this.hook182();
                cursor.delete();
                this.hook181();
              }
 else {
                this.hook191();
              }
            }
 else {
              _this.fileSummaryMap.remove(fileNumLong);
              if (isOldVersion) {
                this.hook184();
                cursor.delete();
                this.hook183();
              }
 else {
                _this.deleteFileSummary(fileNumLong);
              }
            }
            if (isOldVersion) {
              status=cursor.getNext(keyEntry,dataEntry,LockType.NONE,true,false);
            }
 else {
              if (!_this.getFirstFSLN(cursor,fileNum + 1,keyEntry,dataEntry,LockType.NONE)) {
                status=OperationStatus.NOTFOUND;
              }
            }
          }
        }
      }
  finally {
        if (cursor != null) {
          this.hook185();
          cursor.close();
        }
        if (locker != null) {
          locker.operationEnd();
        }
        this.hook195();
      }
      _this.cachePopulated=true;
      return true;
    }
    protected UtilizationProfile _this;
    protected int oldMemorySize;
    protected Long[] existingFiles;
    protected Locker locker;
    protected CursorImpl cursor;
    protected DatabaseEntry keyEntry;
    protected DatabaseEntry dataEntry;
    protected OperationStatus status;
    protected FileSummaryLN ln;
    protected byte[] keyBytes;
    protected boolean isOldVersion;
    protected long fileNum;
    protected Long fileNumLong;
    protected int newMemorySize;
    protected MemoryBudget mb;
    protected void hook176() throws DatabaseException {
    }
    protected void hook181() throws DatabaseException {
    }
    protected void hook182() throws DatabaseException {
    }
    protected void hook183() throws DatabaseException {
    }
    protected void hook184() throws DatabaseException {
    }
    protected void hook185() throws DatabaseException {
    }
    protected void hook191() throws DatabaseException {
    }
    protected void hook194() throws DatabaseException {
    }
    protected void hook195() throws DatabaseException {
    }
  }
  protected void hook173() throws DatabaseException {
  }
  protected void hook174() throws DatabaseException {
  }
  protected void hook175() throws DatabaseException {
  }
  protected void hook177(  long fileNum,  int sequence,  OperationStatus status) throws DatabaseException {
  }
  protected void hook178(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook179(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook180(  long lsn,  LNLogEntry entry,  DatabaseImpl db,  BIN bin) throws DatabaseException {
    Tree tree=db.getTree();
    TreeLocation location=new TreeLocation();
    boolean parentFound=tree.getParentBINForChildLN(location,entry.getKey(),entry.getDupKey(),entry.getLN(),false,true,false,false);
    bin=location.bin;
    int index=location.index;
    if (!parentFound) {
      throw new ReturnBoolean(true);
    }
    if (bin.isEntryKnownDeleted(index)) {
      throw new ReturnBoolean(true);
    }
    if (bin.getLsn(index) != lsn) {
      throw new ReturnBoolean(true);
    }
    System.err.println("lsn " + DbLsn.getNoFormatString(lsn) + " was found in tree.");
    throw new ReturnBoolean(false);
  }
  protected boolean hook186(  DatabaseImpl db,  boolean b) throws DatabaseException {
    return b;
  }
  protected void hook187(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook188(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook189(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook190(  CursorImpl cursor) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/cleaner/Cleaner.java:package com.sleepycat.je.cleaner;
import java.io.IOException;
import java.util.Arrays;
import java.util.Collections;
import java.util.Comparator;
import java.util.Iterator;
import java.util.Set;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvConfigObserver;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.log.FileManager;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.ChildReference;
import com.sleepycat.je.tree.DIN;
import com.sleepycat.je.tree.LN;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.TreeLocation;
import com.sleepycat.je.txn.BasicLocker;
import com.sleepycat.je.txn.LockGrantType;
import com.sleepycat.je.txn.LockResult;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.utilint.DaemonRunner;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.PropUtil;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * The Cleaner is responsible for effectively garbage collecting the JE log. It
 * looks through log files and locates log records (IN's and LN's of all
 * flavors) that are superceded by later versions. Those that are "current" are
 * propagated to a newer log file so that older log files can be deleted.
 */
public class Cleaner implements EnvConfigObserver {
  static final String CLEAN_IN="CleanIN:";
  static final String CLEAN_LN="CleanLN:";
  static final String CLEAN_MIGRATE_LN="CleanMigrateLN:";
  static final String CLEAN_PENDING_LN="CleanPendingLN:";
  /** 
 * Whether to fetch LNs for files in the to-be-cleaned set during lazy
 * migration. This is currently enabled because we do not support the
 * dynamic addition of cleaner threads; that way, if the configured cleaner
 * threads cannot keep up, we use proactive migration to keep up.
 */
  static final boolean PROACTIVE_MIGRATION=true;
  /** 
 * Whether to update the IN generation count during searches. This is
 * currently disabled because 1) we update the generation of the BIN when we
 * set a MIGRATE flag and 2) if the BIN is not evicted its parents will not
 * be, so not updating the generation during the search has no benefit. By
 * not updating the generation during searches for which we do NOT set the
 * MIGRATE flag, we avoid holding INs in the cache that are not needed for
 * lazy migration. However, we do very few searches for obsolete LNs because
 * the obsolete tracking info prevents this, so the benefit of not updating
 * the generation during searches is questionable. In other words, changing
 * this setting will have little effect.
 */
  static final boolean UPDATE_GENERATION=false;
  int nCleanerRuns=0;
  long lockTimeout;
  int readBufferSize;
  int nDeadlockRetries;
  boolean expunge;
  boolean clusterResident;
  boolean clusterAll;
  int maxBatchFiles;
  long cleanerBytesInterval;
  boolean trackDetail;
  /** 
 * All files that are to-be-cleaning or being-cleaned. Used to perform
 * proactive migration. Is read-only after assignment, so no synchronization
 * is needed.
 */
  Set mustBeCleanedFiles=Collections.EMPTY_SET;
  /** 
 * All files that are below the minUtilization threshold. Used to perform
 * clustering migration. Is read-only after assignment, so no
 * synchronization is needed.
 */
  Set lowUtilizationFiles=Collections.EMPTY_SET;
  private String name;
  private EnvironmentImpl env;
  private UtilizationProfile profile;
  private UtilizationTracker tracker;
  private FileSelector fileSelector;
  private FileProcessor[] threads;
  private Object deleteFileLock;
  public Cleaner(  EnvironmentImpl env,  String name) throws DatabaseException {
    this.env=env;
    this.name=name;
    tracker=new UtilizationTracker(env,this);
    profile=new UtilizationProfile(env,tracker);
    fileSelector=new FileSelector();
    threads=new FileProcessor[0];
    deleteFileLock=new Object();
    trackDetail=env.getConfigManager().getBoolean(EnvironmentParams.CLEANER_TRACK_DETAIL);
    envConfigUpdate(env.getConfigManager());
    env.addConfigObserver(this);
  }
  /** 
 * Process notifications of mutable property changes.
 */
  public void envConfigUpdate(  DbConfigManager cm) throws DatabaseException {
    lockTimeout=PropUtil.microsToMillis(cm.getLong(EnvironmentParams.CLEANER_LOCK_TIMEOUT));
    readBufferSize=cm.getInt(EnvironmentParams.CLEANER_READ_SIZE);
    if (readBufferSize <= 0) {
      readBufferSize=cm.getInt(EnvironmentParams.LOG_ITERATOR_READ_SIZE);
    }
    this.hook94(cm);
    nDeadlockRetries=cm.getInt(EnvironmentParams.CLEANER_DEADLOCK_RETRY);
    expunge=cm.getBoolean(EnvironmentParams.CLEANER_REMOVE);
    clusterResident=cm.getBoolean(EnvironmentParams.CLEANER_CLUSTER);
    clusterAll=cm.getBoolean(EnvironmentParams.CLEANER_CLUSTER_ALL);
    maxBatchFiles=cm.getInt(EnvironmentParams.CLEANER_MAX_BATCH_FILES);
    this.hook90();
    if (clusterResident && clusterAll) {
      throw new IllegalArgumentException("Both " + EnvironmentParams.CLEANER_CLUSTER + " and "+ EnvironmentParams.CLEANER_CLUSTER_ALL+ " may not be set to true.");
    }
    int nThreads=cm.getInt(EnvironmentParams.CLEANER_THREADS);
    assert nThreads > 0;
    if (nThreads != threads.length) {
      for (int i=nThreads; i < threads.length; i+=1) {
        if (threads[i] != null) {
          threads[i].shutdown();
          threads[i]=null;
        }
      }
      FileProcessor[] newThreads=new FileProcessor[nThreads];
      for (int i=0; i < nThreads && i < threads.length; i+=1) {
        newThreads[i]=threads[i];
      }
      threads=newThreads;
      for (int i=0; i < nThreads; i+=1) {
        if (threads[i] == null) {
          threads[i]=new FileProcessor(name + '-' + (i + 1),env,this,profile,fileSelector);
        }
      }
    }
    cleanerBytesInterval=cm.getLong(EnvironmentParams.CLEANER_BYTES_INTERVAL);
    if (cleanerBytesInterval == 0) {
      cleanerBytesInterval=cm.getLong(EnvironmentParams.LOG_FILE_MAX) / 4;
    }
  }
  public UtilizationTracker getUtilizationTracker(){
    return tracker;
  }
  public UtilizationProfile getUtilizationProfile(){
    return profile;
  }
  public void wakeup(){
    for (int i=0; i < threads.length; i+=1) {
      if (threads[i] != null) {
        threads[i].wakeup();
      }
    }
  }
  private boolean areThreadsRunning(){
    for (int i=0; i < threads.length; i+=1) {
      if (threads[i] != null) {
        return threads[i].isRunning();
      }
    }
    return false;
  }
  /** 
 * Cleans selected files and returns the number of files cleaned. This
 * method is not invoked by a deamon thread, it is programatically.
 * @param cleanMultipleFilesis true to clean until we're under budget, or false to clean
 * at most one file.
 * @param forceCleaningis true to clean even if we're not under the utilization
 * threshold.
 * @return the number of files cleaned, not including files cleaned
 * unsuccessfully.
 */
  public int doClean(  boolean cleanMultipleFiles,  boolean forceCleaning) throws DatabaseException {
    FileProcessor processor=new FileProcessor("",env,this,profile,fileSelector);
    return processor.doClean(false,cleanMultipleFiles,forceCleaning);
  }
  /** 
 * Deletes all files that are safe-to-delete, if an exclusive lock on the
 * environment can be obtained.
 */
  void deleteSafeToDeleteFiles() throws DatabaseException {
    try {
synchronized (deleteFileLock) {
        Set safeFiles=fileSelector.copySafeToDeleteFiles();
        if (safeFiles == null) {
          return;
        }
        env.checkIfInvalid();
        if (env.mayNotWrite()) {
          return;
        }
        this.hook115(safeFiles);
      }
    }
 catch (    ReturnVoid r) {
      return;
    }
  }
  private void traceFileNotDeleted(  Exception e,  long fileNum){
  }
  /** 
 * Returns a copy of the cleaned and processed files at the time a
 * checkpoint starts.
 * <p>
 * If non-null is returned, the checkpoint should flush an extra level, and
 * addCheckpointedFiles() should be called when the checkpoint is complete.
 * </p>
 */
  public Set[] getFilesAtCheckpointStart() throws DatabaseException {
    processPending();
    return fileSelector.getFilesAtCheckpointStart();
  }
  /** 
 * When a checkpoint is complete, update the files that were returned at the
 * beginning of the checkpoint.
 */
  public void updateFilesAtCheckpointEnd(  Set[] files) throws DatabaseException {
    fileSelector.updateFilesAtCheckpointEnd(files);
    deleteSafeToDeleteFiles();
  }
  /** 
 * Update the lowUtilizationFiles and mustBeCleanedFiles fields with new
 * read-only collections, and update the backlog file count.
 */
  public void updateReadOnlyFileCollections(){
    mustBeCleanedFiles=fileSelector.getMustBeCleanedFiles();
    lowUtilizationFiles=fileSelector.getLowUtilizationFiles();
  }
  /** 
 * If any LNs are pending, process them. This method should be called often
 * enough to prevent the pending LN set from growing too large.
 */
  void processPending() throws DatabaseException {
    new Cleaner_processPending(this).execute();
  }
  /** 
 * Processes a pending LN, getting the lock first to ensure that the
 * overhead of retries is mimimal.
 */
  private void processPendingLN(  LN ln,  DatabaseImpl db,  byte[] key,  byte[] dupKey,  TreeLocation location) throws DatabaseException {
    boolean parentFound=false;
    boolean processedHere=true;
    boolean lockDenied=false;
    boolean obsolete=false;
    boolean completed=false;
    BasicLocker locker=null;
    BIN bin=null;
    DIN parentDIN=null;
    try {
      this.hook97();
      boolean c=db == null;
      c=this.hook112(db,c);
      if (c) {
        this.hook113(db);
        this.hook98();
        obsolete=true;
        completed=true;
        return;
      }
      Tree tree=db.getTree();
      assert tree != null;
      locker=new BasicLocker(env);
      LockResult lockRet=locker.nonBlockingLock(ln.getNodeId(),LockType.READ,db);
      if (lockRet.getLockGrant() == LockGrantType.DENIED) {
        this.hook99();
        lockDenied=true;
        completed=true;
        return;
      }
      parentFound=tree.getParentBINForChildLN(location,key,dupKey,ln,false,true,true,UPDATE_GENERATION);
      bin=location.bin;
      int index=location.index;
      if (!parentFound) {
        this.hook100();
        obsolete=true;
        completed=true;
        return;
      }
      if (ln.containsDuplicates()) {
        parentDIN=(DIN)bin.fetchTarget(index);
        parentDIN.latch(UPDATE_GENERATION);
        ChildReference dclRef=parentDIN.getDupCountLNRef();
        processedHere=false;
        migrateDupCountLN(db,dclRef.getLsn(),parentDIN,dclRef,true,true,ln.getNodeId(),CLEAN_PENDING_LN);
      }
 else {
        processedHere=false;
        migrateLN(db,bin.getLsn(index),bin,index,true,true,ln.getNodeId(),CLEAN_PENDING_LN);
      }
      completed=true;
    }
 catch (    DatabaseException DBE) {
      DBE.printStackTrace();
      this.hook89(DBE);
      throw DBE;
    }
 finally {
      this.hook95(bin,parentDIN);
      if (locker != null) {
        locker.operationEnd();
      }
      if (processedHere) {
        if (completed && !lockDenied) {
          fileSelector.removePendingLN(ln.getNodeId());
        }
        this.hook91(ln,obsolete,completed);
      }
    }
  }
  /** 
 * This method should be called just before logging a BIN. LNs will be
 * migrated if the MIGRATE flag is set, or if they are in a file to be
 * cleaned, or if the LNs qualify according to the rules for cluster and
 * clusterAll.
 * <p>
 * On return this method guarantees that no MIGRATE flag will be set on any
 * child entry. If this method is *not* called before logging a BIN, then
 * the addPendingLNs method must be called.
 * </p>
 * @param binis the latched BIN. The latch will not be released by this
 * method.
 * @param proactiveMigrationperform proactive migration if needed; this is false during a
 * split, to reduce the delay in the user operation.
 */
  public void lazyMigrateLNs(  final BIN bin,  boolean proactiveMigration) throws DatabaseException {
    DatabaseImpl db=bin.getDatabase();
    boolean isBinInDupDb=db.getSortedDuplicates() && !bin.containsDuplicates();
    Integer[] sortedIndices=null;
    int nSortedIndices=0;
    int nEntries=bin.getNEntries();
    for (int index=0; index < nEntries; index+=1) {
      boolean migrateFlag=bin.getMigrate(index);
      boolean isResident=(bin.getTarget(index) != null);
      long childLsn=bin.getLsn(index);
      if (shouldMigrateLN(migrateFlag,isResident,proactiveMigration,isBinInDupDb,childLsn)) {
        if (isResident) {
          migrateLN(db,childLsn,bin,index,migrateFlag,false,0,CLEAN_MIGRATE_LN);
        }
 else {
          if (sortedIndices == null) {
            sortedIndices=new Integer[nEntries];
          }
          sortedIndices[nSortedIndices++]=new Integer(index);
        }
      }
    }
    if (sortedIndices != null) {
      Arrays.sort(sortedIndices,0,nSortedIndices,new Comparator(){
        public int compare(        Object o1,        Object o2){
          int i1=((Integer)o1).intValue();
          int i2=((Integer)o2).intValue();
          return DbLsn.compareTo(bin.getLsn(i1),bin.getLsn(i2));
        }
      }
);
      for (int i=0; i < nSortedIndices; i+=1) {
        int index=sortedIndices[i].intValue();
        long childLsn=bin.getLsn(index);
        boolean migrateFlag=bin.getMigrate(index);
        migrateLN(db,childLsn,bin,index,migrateFlag,false,0,CLEAN_MIGRATE_LN);
      }
    }
  }
  /** 
 * This method should be called just before logging a root DIN. The
 * DupCountLN will be migrated if the MIGRATE flag is set, or if it is in a
 * file to be cleaned, or if the LN qualifies according to the rules for
 * cluster and clusterAll.
 * <p>
 * On return this method guarantees that the MIGRATE flag will not be set on
 * the child entry. If this method is *not* called before logging a root
 * DIN, then the addPendingDupCountLN method must be called.
 * </p>
 * @param dinis the latched DIN. The latch will not be released by this
 * method.
 * @param dclRefis the reference to the DupCountLN.
 * @param proactiveMigrationperform proactive migration if needed; this is false during a
 * split, to reduce the delay in the user operation.
 */
  public void lazyMigrateDupCountLN(  DIN din,  ChildReference dclRef,  boolean proactiveMigration) throws DatabaseException {
    DatabaseImpl db=din.getDatabase();
    boolean migrateFlag=dclRef.getMigrate();
    boolean isResident=(dclRef.getTarget() != null);
    boolean isBinInDupDb=false;
    long childLsn=dclRef.getLsn();
    if (shouldMigrateLN(migrateFlag,isResident,proactiveMigration,isBinInDupDb,childLsn)) {
      migrateDupCountLN(db,childLsn,din,dclRef,migrateFlag,false,0,CLEAN_MIGRATE_LN);
    }
  }
  /** 
 * Returns whether an LN entry should be migrated. Updates stats.
 * @param migrateFlagis whether the MIGRATE flag is set on the entry.
 * @param isResidentis whether the LN is currently resident.
 * @param proactiveMigrationperform proactive migration if needed; this is false during a
 * split, to reduce the delay in the user operation.
 * @param isBinInDupDbis whether this is a BIN entry in a database with duplicates
 * enabled.
 * @param childLsnis the LSN of the LN.
 * @return whether to migrate the LN.
 */
  private boolean shouldMigrateLN(  boolean migrateFlag,  boolean isResident,  boolean proactiveMigration,  boolean isBinInDupDb,  long childLsn){
    boolean doMigration=false;
    if (migrateFlag) {
      doMigration=true;
      this.hook101();
    }
 else     if (!proactiveMigration || isBinInDupDb || env.isClosing()) {
    }
 else {
      Long fileNum=new Long(DbLsn.getFileNumber(childLsn));
      if ((PROACTIVE_MIGRATION || isResident) && mustBeCleanedFiles.contains(fileNum)) {
        doMigration=true;
        this.hook102();
      }
 else       if ((clusterAll || (clusterResident && isResident)) && lowUtilizationFiles.contains(fileNum)) {
        doMigration=true;
        this.hook103();
      }
    }
    return doMigration;
  }
  /** 
 * Migrate an LN in the given BIN entry, if it is not obsolete. The BIN is
 * latched on entry to this method and is left latched when it returns.
 */
  private void migrateLN(  DatabaseImpl db,  long lsn,  BIN bin,  int index,  boolean wasCleaned,  boolean isPending,  long lockedPendingNodeId,  String cleanAction) throws DatabaseException {
    boolean obsolete=false;
    boolean migrated=false;
    boolean lockDenied=false;
    boolean completed=false;
    boolean clearTarget=false;
    BasicLocker locker=null;
    LN ln=null;
    try {
      if (!bin.isEntryKnownDeleted(index)) {
        ln=(LN)bin.getTarget(index);
        if (ln == null) {
          ln=(LN)bin.fetchTarget(index);
          clearTarget=!db.getId().equals(DbTree.ID_DB_ID);
        }
      }
      if (ln == null) {
        this.hook105(wasCleaned);
        obsolete=true;
        completed=true;
        return;
      }
      if (lockedPendingNodeId != ln.getNodeId()) {
        locker=new BasicLocker(env);
        LockResult lockRet=locker.nonBlockingLock(ln.getNodeId(),LockType.READ,db);
        if (lockRet.getLockGrant() == LockGrantType.DENIED) {
          this.hook106(wasCleaned);
          lockDenied=true;
          completed=true;
          return;
        }
      }
      if (ln.isDeleted()) {
        bin.setKnownDeletedLeaveTarget(index);
        this.hook107(wasCleaned);
        obsolete=true;
        completed=true;
        return;
      }
      if (bin.getMigrate(index)) {
        Long fileNum=new Long(DbLsn.getFileNumber(lsn));
        if (!fileSelector.isFileCleaningInProgress(fileNum)) {
          obsolete=true;
          completed=true;
          this.hook108(wasCleaned);
          return;
        }
      }
      byte[] key=getLNMainKey(bin,index);
      long newLNLsn=ln.log(env,db.getId(),key,lsn,locker);
      bin.updateEntry(index,newLNLsn);
      this.hook104();
      migrated=true;
      completed=true;
      return;
    }
  finally {
      if (isPending) {
        if (completed && !lockDenied) {
          fileSelector.removePendingLN(lockedPendingNodeId);
        }
      }
 else {
        if (bin.getMigrate(index) && (!completed || lockDenied)) {
          byte[] key=getLNMainKey(bin,index);
          byte[] dupKey=getLNDupKey(bin,index,ln);
          fileSelector.addPendingLN(ln,db.getId(),key,dupKey);
          if (!areThreadsRunning()) {
            env.getUtilizationTracker().activateCleaner();
          }
          clearTarget=false;
        }
      }
      bin.setMigrate(index,false);
      if (clearTarget) {
        bin.updateEntry(index,null);
      }
      if (locker != null) {
        locker.operationEnd();
      }
      this.hook92(lsn,cleanAction,obsolete,migrated,completed,ln);
    }
  }
  /** 
 * Migrate the DupCountLN for the given DIN. The DIN is latched on entry to
 * this method and is left latched when it returns.
 */
  private void migrateDupCountLN(  DatabaseImpl db,  long lsn,  DIN parentDIN,  ChildReference dclRef,  boolean wasCleaned,  boolean isPending,  long lockedPendingNodeId,  String cleanAction) throws DatabaseException {
    boolean obsolete=false;
    boolean migrated=false;
    boolean lockDenied=false;
    boolean completed=false;
    boolean clearTarget=false;
    BasicLocker locker=null;
    LN ln=null;
    try {
      ln=(LN)dclRef.getTarget();
      if (ln == null) {
        ln=(LN)dclRef.fetchTarget(db,parentDIN);
        assert ln != null;
        clearTarget=!db.getId().equals(DbTree.ID_DB_ID);
      }
      if (lockedPendingNodeId != ln.getNodeId()) {
        locker=new BasicLocker(env);
        LockResult lockRet=locker.nonBlockingLock(ln.getNodeId(),LockType.READ,db);
        if (lockRet.getLockGrant() == LockGrantType.DENIED) {
          this.hook110(wasCleaned);
          lockDenied=true;
          completed=true;
          return;
        }
      }
      Long fileNum=new Long(DbLsn.getFileNumber(lsn));
      if (!fileSelector.isFileCleaningInProgress(fileNum)) {
        obsolete=true;
        completed=true;
        this.hook111(wasCleaned);
        return;
      }
      byte[] key=parentDIN.getDupKey();
      long newLNLsn=ln.log(env,db.getId(),key,lsn,locker);
      parentDIN.updateDupCountLNRef(newLNLsn);
      this.hook109();
      migrated=true;
      completed=true;
      return;
    }
  finally {
      if (isPending) {
        if (completed && !lockDenied) {
          fileSelector.removePendingLN(lockedPendingNodeId);
        }
      }
 else {
        if (dclRef.getMigrate() && (!completed || lockDenied)) {
          byte[] key=parentDIN.getDupKey();
          byte[] dupKey=null;
          fileSelector.addPendingLN(ln,db.getId(),key,dupKey);
          if (!areThreadsRunning()) {
            env.getUtilizationTracker().activateCleaner();
          }
          clearTarget=false;
        }
      }
      dclRef.setMigrate(false);
      if (clearTarget) {
        parentDIN.updateDupCountLN(null);
      }
      if (locker != null) {
        locker.operationEnd();
      }
      this.hook93(lsn,cleanAction,obsolete,migrated,completed,ln);
    }
  }
  /** 
 * Returns the main key for a given BIN entry.
 */
  private byte[] getLNMainKey(  BIN bin,  int index) throws DatabaseException {
    if (bin.containsDuplicates()) {
      return bin.getDupKey();
    }
 else {
      return bin.getKey(index);
    }
  }
  /** 
 * Returns the duplicate key for a given BIN entry.
 */
  private byte[] getLNDupKey(  BIN bin,  int index,  LN ln) throws DatabaseException {
    DatabaseImpl db=bin.getDatabase();
    if (!db.getSortedDuplicates() || ln.containsDuplicates()) {
      return null;
    }
 else     if (bin.containsDuplicates()) {
      return bin.getKey(index);
    }
 else {
      return ln.getData();
    }
  }
@MethodObject static class Cleaner_processPending {
    Cleaner_processPending(    Cleaner _this){
      this._this=_this;
    }
    void execute() throws DatabaseException {
      dbMapTree=_this.env.getDbMapTree();
      pendingLNs=_this.fileSelector.getPendingLNs();
      if (pendingLNs != null) {
        location=new TreeLocation();
        for (int i=0; i < pendingLNs.length; i+=1) {
          info=pendingLNs[i];
          dbId1=info.getDbId();
          db1=dbMapTree.getDb(dbId1,_this.lockTimeout);
          key=info.getKey();
          dupKey=info.getDupKey();
          ln=info.getLN();
          this.hook114();
          _this.processPendingLN(ln,db1,key,dupKey,location);
        }
      }
    }
    protected Cleaner _this;
    protected DbTree dbMapTree;
    protected LNInfo[] pendingLNs;
    protected TreeLocation location;
    protected LNInfo info;
    protected DatabaseId dbId1;
    protected DatabaseImpl db1;
    protected byte[] key;
    protected byte[] dupKey;
    protected LN ln;
    protected DatabaseId[] pendingDBs;
    protected DatabaseId dbId2;
    protected DatabaseImpl db2;
    protected void hook114() throws DatabaseException {
    }
  }
  protected void hook88(  long fileNumValue) throws DatabaseException {
  }
  protected void hook89(  DatabaseException DBE) throws DatabaseException {
  }
  protected void hook90() throws DatabaseException {
  }
  protected void hook91(  LN ln,  boolean obsolete,  boolean completed) throws DatabaseException {
  }
  protected void hook92(  long lsn,  String cleanAction,  boolean obsolete,  boolean migrated,  boolean completed,  LN ln) throws DatabaseException {
  }
  protected void hook93(  long lsn,  String cleanAction,  boolean obsolete,  boolean migrated,  boolean completed,  LN ln) throws DatabaseException {
  }
  protected void hook94(  DbConfigManager cm) throws DatabaseException {
  }
  protected void hook95(  BIN bin,  DIN parentDIN) throws DatabaseException {
  }
  protected void hook96() throws DatabaseException {
  }
  protected void hook97() throws DatabaseException {
  }
  protected void hook98() throws DatabaseException {
  }
  protected void hook99() throws DatabaseException {
  }
  protected void hook100() throws DatabaseException {
  }
  protected void hook101(){
  }
  protected void hook102(){
  }
  protected void hook103(){
  }
  protected void hook104() throws DatabaseException {
  }
  protected void hook105(  boolean wasCleaned) throws DatabaseException {
  }
  protected void hook106(  boolean wasCleaned) throws DatabaseException {
  }
  protected void hook107(  boolean wasCleaned) throws DatabaseException {
  }
  protected void hook108(  boolean wasCleaned) throws DatabaseException {
  }
  protected void hook109() throws DatabaseException {
  }
  protected void hook110(  boolean wasCleaned) throws DatabaseException {
  }
  protected void hook111(  boolean wasCleaned) throws DatabaseException {
  }
  protected boolean hook112(  DatabaseImpl db,  boolean c) throws DatabaseException {
    return c;
  }
  protected void hook113(  DatabaseImpl db) throws DatabaseException {
  }
  protected void hook115(  Set safeFiles) throws DatabaseException {
    for (Iterator i=safeFiles.iterator(); i.hasNext(); ) {
      Long fileNum=(Long)i.next();
      long fileNumValue=fileNum.longValue();
      boolean deleted=false;
      try {
        if (expunge) {
          env.getFileManager().deleteFile(fileNumValue);
        }
 else {
          env.getFileManager().renameFile(fileNumValue,FileManager.DEL_SUFFIX);
        }
        deleted=true;
      }
 catch (      DatabaseException e) {
        traceFileNotDeleted(e,fileNumValue);
      }
catch (      IOException e) {
        traceFileNotDeleted(e,fileNumValue);
      }
      if (deleted) {
        this.hook88(fileNumValue);
        try {
          profile.removeFile(fileNum);
        }
  finally {
          fileSelector.removeDeletedFile(fileNum);
        }
      }
      this.hook96();
    }
  }
}
\00base/com/sleepycat/je/cleaner/TrackedFileSummary.java:package com.sleepycat.je.cleaner;
import com.sleepycat.je.dbi.MemoryBudget;
import de.ovgu.cide.jakutil.*;
/** 
 * Delta file summary info for a tracked file.  Tracked files are managed by
 * the UtilizationTracker.
 * <p>The methods in this class for reading obsolete offsets may be used by
 * multiple threads without synchronization even while another thread is adding
 * offsets.  This is possible because elements are never deleted from the
 * lists.  The thread adding obsolete offsets does so under the log write
 * latch to prevent multiple threads from adding concurrently.</p>
 */
public class TrackedFileSummary extends FileSummary {
  private UtilizationTracker tracker;
  private long fileNum;
  private OffsetList obsoleteOffsets;
  private boolean trackDetail;
  private boolean allowFlush=true;
  /** 
 * Creates an empty tracked summary.
 */
  TrackedFileSummary(  UtilizationTracker tracker,  long fileNum,  boolean trackDetail){
    this.tracker=tracker;
    this.fileNum=fileNum;
    this.trackDetail=trackDetail;
  }
  /** 
 * Returns whether this summary is allowed or prohibited from being flushed
 * or evicted during cleaning.  By default, flushing is allowed.
 */
  public boolean getAllowFlush(){
    return allowFlush;
  }
  /** 
 * Allows or prohibits this summary from being flushed or evicted during
 * cleaning.  By default, flushing is allowed.
 */
  void setAllowFlush(  boolean allowFlush){
    this.allowFlush=allowFlush;
  }
  /** 
 * Returns the file number being tracked.
 */
  public long getFileNumber(){
    return fileNum;
  }
  /** 
 * Overrides reset for a tracked file, and is called when a FileSummaryLN
 * is written to the log.
 * <p>Must be called under the log write latch.</p>
 */
  public void reset(){
    obsoleteOffsets=null;
    tracker.resetFile(this);
    this.hook168();
    super.reset();
  }
  /** 
 * Tracks the given offset as obsolete or non-obsolete.
 * <p>Must be called under the log write latch.</p>
 */
  void trackObsolete(  long offset){
    new TrackedFileSummary_trackObsolete(this,offset).execute();
  }
  /** 
 * Adds the obsolete offsets as well as the totals of the given object.
 */
  void addTrackedSummary(  TrackedFileSummary other){
    add(other);
    if (other.obsoleteOffsets != null) {
      if (obsoleteOffsets != null) {
        if (obsoleteOffsets.merge(other.obsoleteOffsets)) {
          this.hook169();
        }
      }
 else {
        obsoleteOffsets=other.obsoleteOffsets;
      }
    }
  }
  /** 
 * Returns obsolete offsets as an array of longs, or null if none.
 */
  public long[] getObsoleteOffsets(){
    if (obsoleteOffsets != null) {
      return obsoleteOffsets.toArray();
    }
 else {
      return null;
    }
  }
  /** 
 * Returns whether the given offset is present in the tracked offsets.
 * This does not indicate whether the offset is obsolete in general, but
 * only if it is known to be obsolete in this version of the tracked
 * information.
 */
  boolean containsObsoleteOffset(  long offset){
    if (obsoleteOffsets != null) {
      return obsoleteOffsets.contains(offset);
    }
 else {
      return false;
    }
  }
@MethodObject static class TrackedFileSummary_trackObsolete {
    TrackedFileSummary_trackObsolete(    TrackedFileSummary _this,    long offset){
      this._this=_this;
      this.offset=offset;
    }
    void execute(){
      if (!_this.trackDetail) {
        return;
      }
      this.hook170();
      if (_this.obsoleteOffsets == null) {
        _this.obsoleteOffsets=new OffsetList();
        this.hook171();
      }
      if (_this.obsoleteOffsets.add(offset,_this.tracker.getEnvironment().isOpen())) {
        this.hook172();
      }
    }
    protected TrackedFileSummary _this;
    protected long offset;
    protected int adjustMem;
    protected void hook170(){
    }
    protected void hook171(){
    }
    protected void hook172(){
    }
  }
  protected void hook168(){
  }
  protected void hook169(){
  }
}
\00base/com/sleepycat/je/SecondaryCursor.java:package com.sleepycat.je;
import java.util.HashSet;
import java.util.Set;
import java.util.logging.Level;
import com.sleepycat.je.dbi.GetMode;
import com.sleepycat.je.dbi.CursorImpl.SearchMode;
import com.sleepycat.je.txn.Locker;
import de.ovgu.cide.jakutil.*;
/** 
 * Javadoc for this public class is generated via
 * the doc templates in the doc_src directory.
 */
public class SecondaryCursor extends Cursor {
  private SecondaryDatabase secondaryDb;
  private Database primaryDb;
  /** 
 * Cursor constructor. Not public. To get a cursor, the user should
 * call SecondaryDatabase.cursor();
 */
  SecondaryCursor(  SecondaryDatabase dbHandle,  Transaction txn,  CursorConfig cursorConfig) throws DatabaseException {
    super(dbHandle,txn,cursorConfig);
    secondaryDb=dbHandle;
    primaryDb=dbHandle.getPrimaryDatabase();
  }
  /** 
 * Copy constructor.
 */
  private SecondaryCursor(  SecondaryCursor cursor,  boolean samePosition) throws DatabaseException {
    super(cursor,samePosition);
    secondaryDb=cursor.secondaryDb;
    primaryDb=cursor.primaryDb;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public Database getPrimaryDatabase(){
    return primaryDb;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public Cursor dup(  boolean samePosition) throws DatabaseException {
    checkState(true);
    return new SecondaryCursor(this,samePosition);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public SecondaryCursor dupSecondary(  boolean samePosition) throws DatabaseException {
    return (SecondaryCursor)dup(samePosition);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus delete() throws DatabaseException {
    checkState(true);
    checkUpdatesAllowed("delete");
    this.hook65();
    DatabaseEntry key=new DatabaseEntry();
    DatabaseEntry pKey=new DatabaseEntry();
    OperationStatus status=getCurrentInternal(key,pKey,LockMode.RMW);
    if (status == OperationStatus.SUCCESS) {
      status=primaryDb.deleteInternal(cursorImpl.getLocker(),pKey);
    }
    return status;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus put(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    throw SecondaryDatabase.notAllowedException();
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus putNoOverwrite(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    throw SecondaryDatabase.notAllowedException();
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus putNoDupData(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    throw SecondaryDatabase.notAllowedException();
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus putCurrent(  DatabaseEntry data) throws DatabaseException {
    throw SecondaryDatabase.notAllowedException();
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getCurrent(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getCurrent(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getCurrent(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(true);
    checkArgsNoValRequired(key,pKey,data);
    this.hook66(lockMode);
    return getCurrentInternal(key,pKey,data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getFirst(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getFirst(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getFirst(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,pKey,data);
    this.hook67(lockMode);
    return position(key,pKey,data,lockMode,true);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getLast(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getLast(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getLast(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,pKey,data);
    this.hook68(lockMode);
    return position(key,pKey,data,lockMode,false);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNext(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getNext(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNext(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,pKey,data);
    this.hook69(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,pKey,data,lockMode,true);
    }
 else {
      return retrieveNext(key,pKey,data,lockMode,GetMode.NEXT);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNextDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getNextDup(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNextDup(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(true);
    checkArgsNoValRequired(key,pKey,data);
    this.hook70(lockMode);
    return retrieveNext(key,pKey,data,lockMode,GetMode.NEXT_DUP);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNextNoDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getNextNoDup(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNextNoDup(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,pKey,data);
    this.hook71(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,pKey,data,lockMode,true);
    }
 else {
      return retrieveNext(key,pKey,data,lockMode,GetMode.NEXT_NODUP);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrev(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getPrev(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrev(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,pKey,data);
    this.hook72(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,pKey,data,lockMode,false);
    }
 else {
      return retrieveNext(key,pKey,data,lockMode,GetMode.PREV);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrevDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getPrevDup(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrevDup(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(true);
    checkArgsNoValRequired(key,pKey,data);
    this.hook73(lockMode);
    return retrieveNext(key,pKey,data,lockMode,GetMode.PREV_DUP);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrevNoDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getPrevNoDup(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrevNoDup(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,pKey,data);
    this.hook74(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,pKey,data,lockMode,false);
    }
 else {
      return retrieveNext(key,pKey,data,lockMode,GetMode.PREV_NODUP);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchKey(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getSearchKey(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchKey(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(pKey,"pKey",false);
    DatabaseUtil.checkForNullDbt(data,"data",false);
    this.hook75(key,lockMode);
    return search(key,pKey,data,lockMode,SearchMode.SET);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchKeyRange(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    return getSearchKeyRange(key,new DatabaseEntry(),data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchKeyRange(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(pKey,"pKey",false);
    DatabaseUtil.checkForNullDbt(data,"data",false);
    this.hook76(key,data,lockMode);
    return search(key,pKey,data,lockMode,SearchMode.SET_RANGE);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchBoth(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    throw SecondaryDatabase.notAllowedException();
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchBoth(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(pKey,"pKey",true);
    DatabaseUtil.checkForNullDbt(data,"data",false);
    this.hook77(key,data,lockMode);
    return search(key,pKey,data,lockMode,SearchMode.BOTH);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchBothRange(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    throw SecondaryDatabase.notAllowedException();
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchBothRange(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(pKey,"pKey",true);
    DatabaseUtil.checkForNullDbt(data,"data",false);
    this.hook78(key,data,lockMode);
    return search(key,pKey,data,lockMode,SearchMode.BOTH_RANGE);
  }
  /** 
 * Returns the current key and data.
 */
  private OperationStatus getCurrentInternal(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    OperationStatus status=getCurrentInternal(key,pKey,lockMode);
    if (status == OperationStatus.SUCCESS) {
      status=readPrimaryAfterGet(key,pKey,data,lockMode);
    }
    return status;
  }
  /** 
 * Calls search() and retrieves primary data.
 */
  OperationStatus search(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode,  SearchMode searchMode) throws DatabaseException {
    while (true) {
      OperationStatus status=search(key,pKey,lockMode,searchMode);
      if (status != OperationStatus.SUCCESS) {
        return status;
      }
      status=readPrimaryAfterGet(key,pKey,data,lockMode);
      if (status == OperationStatus.SUCCESS) {
        return status;
      }
    }
  }
  /** 
 * Calls position() and retrieves primary data.
 */
  OperationStatus position(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode,  boolean first) throws DatabaseException {
    while (true) {
      OperationStatus status=position(key,pKey,lockMode,first);
      if (status != OperationStatus.SUCCESS) {
        return status;
      }
      status=readPrimaryAfterGet(key,pKey,data,lockMode);
      if (status == OperationStatus.SUCCESS) {
        return status;
      }
    }
  }
  /** 
 * Calls retrieveNext() and retrieves primary data.
 */
  OperationStatus retrieveNext(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode,  GetMode getMode) throws DatabaseException {
    while (true) {
      OperationStatus status=retrieveNext(key,pKey,lockMode,getMode);
      if (status != OperationStatus.SUCCESS) {
        return status;
      }
      status=readPrimaryAfterGet(key,pKey,data,lockMode);
      if (status == OperationStatus.SUCCESS) {
        return status;
      }
    }
  }
  /** 
 * Reads the primary data for a primary key that was read via a secondary.
 * When SUCCESS is returned by this method, the caller should return
 * SUCCESS.  When KEYEMPTY is returned, the caller should treat this as a
 * deleted record and either retry the operation (in the case of position,
 * search, and retrieveNext) or return KEYEMPTY (in the case of
 * getCurrent).  KEYEMPTY is only returned when read-uncommitted is used.
 * @return SUCCESS if the primary was read succesfully, or KEYEMPTY if
 * using read-uncommitted and the primary has been deleted, or KEYEMPTY if
 * using read-uncommitted and the primary has been updated and no longer
 * contains the secondary key.
 * @throws DatabaseException to indicate a corrupt secondary reference if
 * the primary record is not found and read-uncommitted is not used (or
 * read-uncommitted is used, but we cannot verify that a valid deletion has
 * occured).
 */
  private OperationStatus readPrimaryAfterGet(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    Locker locker=cursorImpl.getLocker();
    Cursor cursor=null;
    try {
      cursor=new Cursor(primaryDb,locker,null);
      OperationStatus status=cursor.search(pKey,data,lockMode,SearchMode.SET);
      if (status != OperationStatus.SUCCESS) {
        if (isReadUncommittedMode(lockMode)) {
          status=getCurrentInternal(key,pKey,lockMode);
          if (status == OperationStatus.KEYEMPTY) {
            return status;
          }
        }
        SecondaryDatabase secDb=(SecondaryDatabase)getDatabase();
        throw secDb.secondaryCorruptException();
      }
      if (isReadUncommittedMode(lockMode)) {
        SecondaryConfig config=secondaryDb.getPrivateSecondaryConfig();
        if (config.getImmutableSecondaryKey()) {
        }
 else         if (config.getKeyCreator() != null) {
          DatabaseEntry secKey=new DatabaseEntry();
          if (!config.getKeyCreator().createSecondaryKey(secondaryDb,pKey,data,secKey) || !secKey.equals(key)) {
            return OperationStatus.KEYEMPTY;
          }
        }
 else         if (config.getMultiKeyCreator() != null) {
          Set results=new HashSet();
          config.getMultiKeyCreator().createSecondaryKeys(secondaryDb,pKey,data,results);
          if (!results.contains(key)) {
            return OperationStatus.KEYEMPTY;
          }
        }
      }
      return OperationStatus.SUCCESS;
    }
  finally {
      if (cursor != null) {
        cursor.close();
      }
    }
  }
  /** 
 * Note that this flavor of checkArgs doesn't require that the
 * dbt data is set.
 */
  private void checkArgsNoValRequired(  DatabaseEntry key,  DatabaseEntry pKey,  DatabaseEntry data){
    DatabaseUtil.checkForNullDbt(key,"key",false);
    DatabaseUtil.checkForNullDbt(pKey,"pKey",false);
    DatabaseUtil.checkForNullDbt(data,"data",false);
  }
  protected void hook65() throws DatabaseException {
  }
  protected void hook66(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook67(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook68(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook69(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook70(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook71(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook72(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook73(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook74(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook75(  DatabaseEntry key,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook76(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook77(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook78(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/tree/DIN.java:package com.sleepycat.je.tree;
import java.nio.ByteBuffer;
import java.util.Comparator;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.cleaner.Cleaner;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.log.LogEntryType;
import com.sleepycat.je.log.LogException;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.log.LogUtils;
import com.sleepycat.je.txn.LockResult;
import com.sleepycat.je.txn.Locker;
import de.ovgu.cide.jakutil.*;
/** 
 * An DIN represents an Duplicate Internal Node in the JE tree.
 */
public final class DIN extends IN {
  private static final String BEGIN_TAG="<din>";
  private static final String END_TAG="</din>";
  /** 
 * Full key for this set of duplicates.
 */
  private byte[] dupKey;
  /** 
 * Reference to DupCountLN which stores the count.
 */
  private ChildReference dupCountLNRef;
  /** 
 * Create an empty DIN, with no node id, to be filled in from the log.
 */
  public DIN(){
    super();
    dupCountLNRef=new ChildReference();
    init(null,Key.EMPTY_KEY,0,0);
  }
  /** 
 * Create a new DIN.
 */
  public DIN(  DatabaseImpl db,  byte[] identifierKey,  int capacity,  byte[] dupKey,  ChildReference dupCountLNRef,  int level){
    super(db,identifierKey,capacity,level);
    this.dupKey=dupKey;
    this.dupCountLNRef=dupCountLNRef;
  }
  protected int generateLevel(  DatabaseId dbId,  int newLevel){
    return newLevel;
  }
  /** 
 * Create a new DIN.  Need this because we can't call newInstance()
 * without getting a 0 node.
 */
  protected IN createNewInstance(  byte[] identifierKey,  int maxEntries,  int level){
    return new DIN(getDatabase(),identifierKey,maxEntries,dupKey,dupCountLNRef,level);
  }
  /** 
 * Return the key for this duplicate set.
 */
  public byte[] getDupKey(){
    return dupKey;
  }
  /** 
 * Get the key (dupe or identifier) in child that is used to locate
 * it in 'this' node.
 */
  public byte[] getChildKey(  IN child) throws DatabaseException {
    return child.getIdentifierKey();
  }
  public byte[] selectKey(  byte[] mainTreeKey,  byte[] dupTreeKey){
    return dupTreeKey;
  }
  /** 
 * Return the key for navigating through the duplicate tree.
 */
  public byte[] getDupTreeKey(){
    return getIdentifierKey();
  }
  /** 
 * Return the key for navigating through the main tree.
 */
  public byte[] getMainTreeKey(){
    return dupKey;
  }
  public ChildReference getDupCountLNRef(){
    return dupCountLNRef;
  }
  public DupCountLN getDupCountLN() throws DatabaseException {
    return (DupCountLN)dupCountLNRef.fetchTarget(getDatabase(),this);
  }
  /** 
 * Assign the Dup Count LN.
 */
  void setDupCountLN(  ChildReference dupCountLNRef){
    this.dupCountLNRef=dupCountLNRef;
  }
  /** 
 * Assign the Dup Count LN node.  Does not dirty the DIN.
 */
  public void updateDupCountLN(  Node target){
    new DIN_updateDupCountLN(this,target).execute();
  }
  /** 
 * Update Dup Count LN.
 */
  public void updateDupCountLNRefAndNullTarget(  long newLsn){
    new DIN_updateDupCountLNRefAndNullTarget(this,newLsn).execute();
  }
  /** 
 * Update dup count LSN.
 */
  public void updateDupCountLNRef(  long newLsn){
    setDirty(true);
    dupCountLNRef.setLsn(newLsn);
  }
  /** 
 * @return true if this node is a duplicate-bearing node type, false
 * if otherwise.
 */
  public boolean containsDuplicates(){
    return true;
  }
  public boolean isDbRoot(){
    return false;
  }
  /** 
 * Return the comparator function to be used for DINs.  This is
 * the user defined duplicate comparison function, if defined.
 */
  public final Comparator getKeyComparator(){
    return getDatabase().getDuplicateComparator();
  }
  /** 
 * Increment or decrement the DupCountLN, log the updated LN, and update
 * the lock result.
 * Preconditions: This DIN is latched and the DupCountLN is write locked.
 * Postconditions: Same as preconditions.
 */
  public void incrementDuplicateCount(  LockResult lockResult,  byte[] key,  Locker locker,  boolean increment) throws DatabaseException {
    long oldLsn=dupCountLNRef.getLsn();
    lockResult.setAbortLsn(oldLsn,dupCountLNRef.isKnownDeleted());
    DupCountLN dupCountLN=getDupCountLN();
    if (increment) {
      dupCountLN.incDupCount();
    }
 else {
      dupCountLN.decDupCount();
      assert dupCountLN.getDupCount() >= 0;
    }
    DatabaseImpl db=getDatabase();
    long newCountLSN=dupCountLN.log(db.getDbEnvironment(),db.getId(),key,oldLsn,locker);
    updateDupCountLNRef(newCountLSN);
  }
  boolean matchLNByNodeId(  TreeLocation location,  long nodeId) throws DatabaseException {
    for (int i=0; i < getNEntries(); i++) {
      Node n=fetchTarget(i);
      if (n != null) {
        boolean ret=n.matchLNByNodeId(location,nodeId);
        if (ret) {
          return true;
        }
      }
    }
    return false;
  }
  void accumulateStats(  TreeWalkerStatsAccumulator acc){
    acc.processDIN(this,new Long(getNodeId()),getLevel());
  }
  /** 
 * @see IN#getLogType
 */
  public LogEntryType getLogType(){
    return LogEntryType.LOG_DIN;
  }
  /** 
 * Handles lazy migration of DupCountLNs prior to logging a DIN.  See
 * BIN.logInternal for more information.
 */
  protected long logInternal(  LogManager logManager,  boolean allowDeltas,  boolean isProvisional,  boolean proactiveMigration,  IN parent) throws DatabaseException {
    if (dupCountLNRef != null) {
      Cleaner cleaner=getDatabase().getDbEnvironment().getCleaner();
      cleaner.lazyMigrateDupCountLN(this,dupCountLNRef,proactiveMigration);
    }
    return super.logInternal(logManager,allowDeltas,isProvisional,proactiveMigration,parent);
  }
  /** 
 * @see IN#getLogSize
 */
  public int getLogSize(){
    int size=super.getLogSize();
    size+=LogUtils.getByteArrayLogSize(dupKey);
    size+=LogUtils.getBooleanLogSize();
    if (dupCountLNRef != null) {
      size+=dupCountLNRef.getLogSize();
    }
    return size;
  }
  /** 
 * @see IN#writeToLog
 */
  public void writeToLog(  ByteBuffer logBuffer){
    super.writeToLog(logBuffer);
    LogUtils.writeByteArray(logBuffer,dupKey);
    boolean dupCountLNRefExists=(dupCountLNRef != null);
    LogUtils.writeBoolean(logBuffer,dupCountLNRefExists);
    if (dupCountLNRefExists) {
      dupCountLNRef.writeToLog(logBuffer);
    }
  }
  /** 
 * @see IN#readFromLog
 */
  public void readFromLog(  ByteBuffer itemBuffer,  byte entryTypeVersion) throws LogException {
    super.readFromLog(itemBuffer,entryTypeVersion);
    dupKey=LogUtils.readByteArray(itemBuffer);
    boolean dupCountLNRefExists=LogUtils.readBoolean(itemBuffer);
    if (dupCountLNRefExists) {
      dupCountLNRef.readFromLog(itemBuffer,entryTypeVersion);
    }
 else {
      dupCountLNRef=null;
    }
  }
  /** 
 * DINS need to dump their dup key
 */
  protected void dumpLogAdditional(  StringBuffer sb){
    super.dumpLogAdditional(sb);
    sb.append(Key.dumpString(dupKey,0));
    if (dupCountLNRef != null) {
      dupCountLNRef.dumpLog(sb,true);
    }
  }
  public String beginTag(){
    return BEGIN_TAG;
  }
  public String endTag(){
    return END_TAG;
  }
  /** 
 * For unit test support:
 * @return a string that dumps information about this DIN, without
 */
  public String dumpString(  int nSpaces,  boolean dumpTags){
    StringBuffer sb=new StringBuffer();
    if (dumpTags) {
      sb.append(TreeUtils.indent(nSpaces));
      sb.append(beginTag());
      sb.append('\n');
    }
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("<dupkey>");
    sb.append(dupKey == null ? "" : Key.dumpString(dupKey,0));
    sb.append("</dupkey>");
    sb.append('\n');
    if (dupCountLNRef == null) {
      sb.append(TreeUtils.indent(nSpaces + 2));
      sb.append("<dupCountLN/>");
    }
 else {
      sb.append(dupCountLNRef.dumpString(nSpaces + 4,true));
    }
    sb.append('\n');
    sb.append(super.dumpString(nSpaces,false));
    if (dumpTags) {
      sb.append(TreeUtils.indent(nSpaces));
      sb.append(endTag());
    }
    return sb.toString();
  }
  public String toString(){
    return dumpString(0,true);
  }
  public String shortClassName(){
    return "DIN";
  }
@MethodObject static class DIN_updateDupCountLN {
    DIN_updateDupCountLN(    DIN _this,    Node target){
      this._this=_this;
      this.target=target;
    }
    void execute(){
      _this.dupCountLNRef.setTarget(target);
    }
    protected DIN _this;
    protected Node target;
    protected long oldSize;
    protected long newSize;
  }
@MethodObject static class DIN_updateDupCountLNRefAndNullTarget {
    DIN_updateDupCountLNRefAndNullTarget(    DIN _this,    long newLsn){
      this._this=_this;
      this.newLsn=newLsn;
    }
    void execute(){
      _this.setDirty(true);
      this.hook614();
      _this.dupCountLNRef.setTarget(null);
      _this.dupCountLNRef.setLsn(newLsn);
    }
    protected DIN _this;
    protected long newLsn;
    protected long oldSize;
    protected long newSize;
    protected void hook614(){
    }
  }
}
\00base/com/sleepycat/je/tree/Tree.java:package com.sleepycat.je.tree;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.List;
import java.util.ListIterator;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.CursorImpl;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.INList;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.log.LogReadable;
import com.sleepycat.je.log.LogUtils;
import com.sleepycat.je.log.LogWritable;
import com.sleepycat.je.recovery.RecoveryManager;
import com.sleepycat.je.txn.BasicLocker;
import com.sleepycat.je.txn.LockGrantType;
import com.sleepycat.je.txn.LockResult;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.txn.Locker;
import com.sleepycat.je.txn.WriteLockInfo;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.TestHook;
import com.sleepycat.je.utilint.TestHookExecute;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * Tree implements the JE B+Tree.
 * A note on tree search patterns: There's a set of Tree.search* methods. Some
 * clients of the tree use those search methods directly, whereas other clients
 * of the tree tend to use methods built on top of search.
 * The semantics of search* are they leave you pointing at a BIN or IN they
 * don't tell you where the reference of interest is. they traverse a single
 * tree, to jump into the duplicate tree, the caller has to take explicit
 * action. The semantics of the get* methods are: they leave you pointing at a
 * BIN or IN they return the index of the slot of interest they traverse down to
 * whatever level is needed -- they'll take care of jumping into the duplicate
 * tree. they are built on top of search* methods. For the future: Over time, we
 * need to clarify which methods are to be used by clients of the tree.
 * Preferably clients that call the tree use get*, although their are cases
 * where they need visibility into the tree structure. For example, tee cursors
 * use search* because they want to add themselves to BIN before jumping into
 * the duplicate tree.
 * Also, search* should return the location of the slot to save us a second
 * binary search.
 */
public final class Tree implements LogWritable, LogReadable {
  private static final String TRACE_ROOT_SPLIT="RootSplit:";
  private static final String TRACE_DUP_ROOT_SPLIT="DupRootSplit:";
  private static final String TRACE_MUTATE="Mut:";
  private static final String TRACE_INSERT="Ins:";
  private static final String TRACE_INSERT_DUPLICATE="InsD:";
  private DatabaseImpl database;
  private ChildReference root;
  private int maxMainTreeEntriesPerNode;
  private int maxDupTreeEntriesPerNode;
  private boolean purgeRoot;
  private TreeStats treeStats;
  private ThreadLocal treeStatsAccumulatorTL=new ThreadLocal();
  private static SplitRequiredException splitRequiredException=new SplitRequiredException();
  /** 
 * Embodies an enum for the type of search being performed. NORMAL means do
 * a regular search down the tree. LEFT/RIGHT means search down the
 * left/right side to find the first/last node in the tree.
 */
public static class SearchType {
    public static final SearchType NORMAL=new SearchType();
    public static final SearchType LEFT=new SearchType();
    public static final SearchType RIGHT=new SearchType();
    private SearchType(){
    }
  }
  private TestHook waitHook;
  private TestHook searchHook;
  private TestHook ckptHook;
  /** 
 * Create a new tree.
 */
  public Tree(  DatabaseImpl database) throws DatabaseException {
    init(database);
    setDatabase(database);
  }
  /** 
 * Create a tree that's being read in from the log.
 */
  public Tree() throws DatabaseException {
    init(null);
    maxMainTreeEntriesPerNode=0;
    maxDupTreeEntriesPerNode=0;
  }
  /** 
 * constructor helper
 */
  private void init(  DatabaseImpl database){
    treeStats=new TreeStats();
    this.root=null;
    this.database=database;
  }
  /** 
 * Set the database for this tree. Used by recovery when recreating an
 * existing tree.
 */
  public void setDatabase(  DatabaseImpl database) throws DatabaseException {
    this.database=database;
    maxMainTreeEntriesPerNode=database.getNodeMaxEntries();
    maxDupTreeEntriesPerNode=database.getNodeMaxDupTreeEntries();
    DbConfigManager configManager=database.getDbEnvironment().getConfigManager();
    purgeRoot=configManager.getBoolean(EnvironmentParams.COMPRESSOR_PURGE_ROOT);
  }
  /** 
 * @return the database for this Tree.
 */
  public DatabaseImpl getDatabase(){
    return database;
  }
  /** 
 * Set the root for the tree. Should only be called within the root latch.
 */
  public void setRoot(  ChildReference newRoot,  boolean notLatched){
    root=newRoot;
  }
  public ChildReference makeRootChildReference(  Node target,  byte[] key,  long lsn){
    return new RootChildReference(target,key,lsn);
  }
  private ChildReference makeRootChildReference(){
    return new RootChildReference();
  }
private class RootChildReference extends ChildReference {
    private RootChildReference(){
      super();
    }
    private RootChildReference(    Node target,    byte[] key,    long lsn){
      super(target,key,lsn);
    }
    private RootChildReference(    Node target,    byte[] key,    long lsn,    byte existingState){
      super(target,key,lsn,existingState);
    }
    public Node fetchTarget(    DatabaseImpl database,    IN in) throws DatabaseException {
      this.hook666();
      return super.fetchTarget(database,in);
    }
    public void setTarget(    Node target){
      this.hook667();
      super.setTarget(target);
    }
    public void clearTarget(){
      this.hook668();
      super.clearTarget();
    }
    public void setLsn(    long lsn){
      this.hook669();
      super.setLsn(lsn);
    }
    protected void hook666() throws DatabaseException {
    }
    protected void hook667(){
    }
    protected void hook668(){
    }
    protected void hook669(){
    }
  }
  /** 
 * Get LSN of the rootIN. Obtained without latching, should only be accessed
 * while quiescent.
 */
  public long getRootLsn(){
    if (root == null) {
      return DbLsn.NULL_LSN;
    }
 else {
      return root.getLsn();
    }
  }
  /** 
 * @return the TreeStats for this tree.
 */
  TreeStats getTreeStats(){
    return treeStats;
  }
  private TreeWalkerStatsAccumulator getTreeStatsAccumulator(){
    if (EnvironmentImpl.getThreadLocalReferenceCount() > 0) {
      return (TreeWalkerStatsAccumulator)treeStatsAccumulatorTL.get();
    }
 else {
      return null;
    }
  }
  public void setTreeStatsAccumulator(  TreeWalkerStatsAccumulator tSA){
    treeStatsAccumulatorTL.set(tSA);
  }
  public IN withRootLatchedExclusive(  WithRootLatched wrl) throws DatabaseException {
    try {
      this.hook670(wrl);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (IN)r.value;
    }
  }
  public IN withRootLatchedShared(  WithRootLatched wrl) throws DatabaseException {
    try {
      this.hook671(wrl);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (IN)r.value;
    }
  }
  /** 
 * Deletes a BIN specified by key from the tree. If the BIN resides in a
 * subtree that can be pruned away, prune as much as possible, so we don't
 * leave a branch that has no BINs.
 * It's possible that the targeted BIN will now have entries, or will have
 * resident cursors. Either will prevent deletion.
 * @param idKey -
 * the identifier key of the node to delete.
 * @param trackeris used for tracking obsolete node info.
 */
  public void delete(  byte[] idKey,  UtilizationTracker tracker) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
    try {
      IN subtreeRootIN=null;
      ArrayList nodeLadder=new ArrayList();
      IN rootIN=null;
      boolean rootNeedsUpdating=false;
      this.hook672(idKey,tracker,subtreeRootIN,nodeLadder,rootIN,rootNeedsUpdating);
      if (subtreeRootIN != null) {
        EnvironmentImpl envImpl=database.getDbEnvironment();
        if (rootNeedsUpdating) {
          DbTree dbTree=envImpl.getDbMapTree();
          dbTree.modifyDbRoot(database);
          this.hook661();
        }
        INList inList=envImpl.getInMemoryINs();
        accountForSubtreeRemoval(inList,subtreeRootIN,tracker);
      }
    }
 catch (    ReturnVoid r) {
      return;
    }
  }
  /** 
 * This entire tree is empty, clear the root and log a new MapLN
 * @return the rootIN that has been detached, or null if there hasn't been
 * any removal.
 */
  private IN logTreeRemoval(  IN rootIN,  UtilizationTracker tracker) throws DatabaseException {
    IN detachedRootIN=null;
    if ((rootIN.getNEntries() <= 1) && (rootIN.validateSubtreeBeforeDelete(0))) {
      root=null;
      EnvironmentImpl envImpl=database.getDbEnvironment();
      LogManager logManager=envImpl.getLogManager();
      logManager.log(new INDeleteInfo(rootIN.getNodeId(),rootIN.getIdentifierKey(),database.getId()));
      detachedRootIN=rootIN;
    }
    return detachedRootIN;
  }
  /** 
 * Update nodes for a delete, going upwards. For example, suppose a node
 * ladder holds: INa, INb, index for INb in INa INb, INc, index for INc in
 * INb INc, BINd, index for BINd in INc
 * When we enter this method, BINd has already been removed from INc. We
 * need to - log INc - update INb, log INb - update INa, log INa
 * @param nodeLadderList of SplitInfos describing each node pair on the downward
 * path
 * @param binRootparent of the dup tree, or null if this is not for dups.
 * @param indexslot occupied by this din tree.
 * @return whether the DB root needs updating.
 */
  private boolean cascadeUpdates(  ArrayList nodeLadder,  BIN binRoot,  int index) throws DatabaseException {
    ListIterator iter=nodeLadder.listIterator(nodeLadder.size());
    EnvironmentImpl envImpl=database.getDbEnvironment();
    LogManager logManager=envImpl.getLogManager();
    long newLsn=DbLsn.NULL_LSN;
    SplitInfo info4=null;
    while (iter.hasPrevious()) {
      info4=(SplitInfo)iter.previous();
      if (newLsn != DbLsn.NULL_LSN) {
        info4.parent.updateEntry(info4.index,newLsn);
      }
      newLsn=info4.parent.log(logManager);
    }
    boolean rootNeedsUpdating=false;
    if (info4 != null) {
      if (info4.parent.isDbRoot()) {
        this.hook673();
        root.setLsn(newLsn);
        rootNeedsUpdating=true;
      }
 else       if ((binRoot != null) && info4.parent.isRoot()) {
        binRoot.updateEntry(index,newLsn);
      }
 else {
        assert false;
      }
    }
    return rootNeedsUpdating;
  }
  /** 
 * Delete a subtree of a duplicate tree. Find the duplicate tree using
 * mainKey in the top part of the tree and idKey in the duplicate tree.
 * @param idKeythe identifier key to be used in the duplicate subtree to find
 * the duplicate path.
 * @param mainKeythe key to be used in the main tree to find the duplicate
 * subtree.
 * @param trackeris used for tracking obsolete node info.
 * @return true if the delete succeeded, false if there were still cursors
 * present on the leaf DBIN of the subtree that was located.
 */
  public void deleteDup(  byte[] idKey,  byte[] mainKey,  UtilizationTracker tracker) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
    IN in=search(mainKey,SearchType.NORMAL,-1,null,false);
    IN deletedSubtreeRoot=null;
    deletedSubtreeRoot=this.hook674(idKey,mainKey,in,deletedSubtreeRoot);
    if (deletedSubtreeRoot != null) {
      EnvironmentImpl envImpl=database.getDbEnvironment();
      accountForSubtreeRemoval(envImpl.getInMemoryINs(),deletedSubtreeRoot,tracker);
    }
  }
  /** 
 * We enter and leave this method with 'bin' latched.
 * @return the root of the subtree we have deleted, so it can be properly
 * accounted for. May be null if nothing was deleted.
 */
  private IN deleteDupSubtree(  byte[] idKey,  BIN bin,  int index) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
    EnvironmentImpl envImpl=database.getDbEnvironment();
    boolean dupCountLNLocked=false;
    DupCountLN dcl=null;
    BasicLocker locker=new BasicLocker(envImpl);
    DIN duplicateRoot=(DIN)bin.fetchTarget(index);
    duplicateRoot.latch(false);
    ArrayList nodeLadder=new ArrayList();
    IN subtreeRootIN=null;
    try {
      ChildReference dclRef=duplicateRoot.getDupCountLNRef();
      dcl=(DupCountLN)dclRef.fetchTarget(database,duplicateRoot);
      LockResult lockResult=locker.nonBlockingLock(dcl.getNodeId(),LockType.READ,database);
      if (lockResult.getLockGrant() == LockGrantType.DENIED) {
        throw CursorsExistException.CURSORS_EXIST;
      }
 else {
        dupCountLNLocked=true;
      }
      searchDeletableSubTree(duplicateRoot,idKey,nodeLadder);
      LogManager logManager=envImpl.getLogManager();
      if (nodeLadder.size() == 0) {
        if (bin.nCursors() == 0) {
          boolean deleteOk=bin.deleteEntry(index,true);
          assert deleteOk;
          logManager.log(new INDupDeleteInfo(duplicateRoot.getNodeId(),duplicateRoot.getMainTreeKey(),duplicateRoot.getDupTreeKey(),database.getId()));
          subtreeRootIN=duplicateRoot;
          this.hook754(bin);
        }
 else {
          throw CursorsExistException.CURSORS_EXIST;
        }
      }
 else {
        SplitInfo detachPoint=(SplitInfo)nodeLadder.get(nodeLadder.size() - 1);
        boolean deleteOk=detachPoint.parent.deleteEntry(detachPoint.index,true);
        assert deleteOk;
        cascadeUpdates(nodeLadder,bin,index);
        subtreeRootIN=detachPoint.child;
      }
    }
  finally {
      this.hook676(nodeLadder);
      if (dupCountLNLocked) {
        locker.releaseLock(dcl.getNodeId());
      }
      this.hook675(duplicateRoot);
    }
    return subtreeRootIN;
  }
  /** 
 * Find the leftmost node (IN or BIN) in the tree. Do not descend into a
 * duplicate tree if the leftmost entry of the first BIN refers to one.
 * @return the leftmost node in the tree, null if the tree is empty. The
 * returned node is latched and the caller must release it.
 */
  public IN getFirstNode() throws DatabaseException {
    return search(null,SearchType.LEFT,-1,null,true);
  }
  /** 
 * Find the rightmost node (IN or BIN) in the tree. Do not descend into a
 * duplicate tree if the rightmost entry of the last BIN refers to one.
 * @return the rightmost node in the tree, null if the tree is empty. The
 * returned node is latched and the caller must release it.
 */
  public IN getLastNode() throws DatabaseException {
    return search(null,SearchType.RIGHT,-1,null,true);
  }
  /** 
 * Find the leftmost node (DBIN) in a duplicate tree.
 * @return the leftmost node in the tree, null if the tree is empty. The
 * returned node is latched and the caller must release it.
 */
  public DBIN getFirstNode(  DIN dupRoot) throws DatabaseException {
    if (dupRoot == null) {
      throw new IllegalArgumentException("getFirstNode passed null root");
    }
    this.hook677(dupRoot);
    IN ret=searchSubTree(dupRoot,null,SearchType.LEFT,-1,null,true);
    return (DBIN)ret;
  }
  /** 
 * Find the rightmost node (DBIN) in a duplicate tree.
 * @return the rightmost node in the tree, null if the tree is empty. The
 * returned node is latched and the caller must release it.
 */
  public DBIN getLastNode(  DIN dupRoot) throws DatabaseException {
    if (dupRoot == null) {
      throw new IllegalArgumentException("getLastNode passed null root");
    }
    this.hook678(dupRoot);
    IN ret=searchSubTree(dupRoot,null,SearchType.RIGHT,-1,null,true);
    return (DBIN)ret;
  }
  /** 
 * GetParentNode without optional tracking.
 */
  public SearchResult getParentINForChildIN(  IN child,  boolean requireExactMatch,  boolean updateGeneration) throws DatabaseException {
    return getParentINForChildIN(child,requireExactMatch,updateGeneration,-1,null);
  }
  /** 
 * Return a reference to the parent or possible parent of the child. Used by
 * objects that need to take a standalone node and find it in the tree, like
 * the evictor, checkpointer, and recovery.
 * @param childThe child node for which to find the parent. This node is
 * latched by the caller and is released by this function before
 * returning to the caller.
 * @param requireExactMatchif true, we must find the exact parent, not a potential
 * parent.
 * @param updateGenerationif true, set the generation count during latching. Pass false
 * when the LRU should not be impacted, such as during eviction
 * and checkpointing.
 * @param trackingListif not null, add the LSNs of the parents visited along the
 * way, as a debug tracing mechanism. This is meant to stay in
 * production, to add information to the log.
 * @return a SearchResult object. If the parent has been found,
 * result.foundExactMatch is true. If any parent, exact or potential
 * has been found, result.parent refers to that node.
 */
  public SearchResult getParentINForChildIN(  IN child,  boolean requireExactMatch,  boolean updateGeneration,  int targetLevel,  List trackingList) throws DatabaseException {
    if (child == null) {
      throw new IllegalArgumentException("getParentNode passed null");
    }
    this.hook680(child);
    byte[] mainTreeKey=child.getMainTreeKey();
    byte[] dupTreeKey=child.getDupTreeKey();
    boolean isRoot=child.isRoot();
    this.hook679(child);
    return getParentINForChildIN(child.getNodeId(),child.containsDuplicates(),isRoot,mainTreeKey,dupTreeKey,requireExactMatch,updateGeneration,targetLevel,trackingList,true);
  }
  /** 
 * Return a reference to the parent or possible parent of the child. Used by
 * objects that need to take a node id and find it in the tree, like the
 * evictor, checkpointer, and recovery.
 * @param requireExactMatchif true, we must find the exact parent, not a potential
 * parent.
 * @param updateGenerationif true, set the generation count during latching. Pass false
 * when the LRU should not be impacted, such as during eviction
 * and checkpointing.
 * @param trackingListif not null, add the LSNs of the parents visited along the
 * way, as a debug tracing mechanism. This is meant to stay in
 * production, to add information to the log.
 * @param doFetchif false, stop the search if we run into a non-resident child.
 * Used by the checkpointer to avoid conflicting with work done
 * by the evictor.
 * @param childThe child node for which to find the parent. This node is
 * latched by the caller and is released by this function before
 * returning to the caller.
 * @return a SearchResult object. If the parent has been found,
 * result.foundExactMatch is true. If any parent, exact or potential
 * has been found, result.parent refers to that node.
 */
  public SearchResult getParentINForChildIN(  long targetNodeId,  boolean targetContainsDuplicates,  boolean targetIsRoot,  byte[] targetMainTreeKey,  byte[] targetDupTreeKey,  boolean requireExactMatch,  boolean updateGeneration,  int targetLevel,  List trackingList,  boolean doFetch) throws DatabaseException {
    IN rootIN=getRootIN(updateGeneration);
    SearchResult result=new SearchResult();
    if (rootIN != null) {
      if (trackingList != null) {
        trackingList.add(new TrackingInfo(root.getLsn(),rootIN.getNodeId()));
      }
      IN potentialParent=rootIN;
      try {
        while (result.keepSearching) {
          assert TestHookExecute.doHookIfSet(searchHook);
          potentialParent.findParent(SearchType.NORMAL,targetNodeId,targetContainsDuplicates,targetIsRoot,targetMainTreeKey,targetDupTreeKey,result,requireExactMatch,updateGeneration,targetLevel,trackingList,doFetch);
          potentialParent=result.parent;
        }
      }
 catch (      Exception e) {
        this.hook681(potentialParent);
        throw new DatabaseException(e);
      }
    }
    return result;
  }
  /** 
 * Return a reference to the parent of this LN. This searches through the
 * main and duplicate tree and allows splits. Set the tree location to the
 * proper BIN parent whether or not the LN child is found. That's because if
 * the LN is not found, recovery or abort will need to place it within the
 * tree, and so we must point at the appropriate position.
 * <p>
 * When this method returns with location.bin non-null, the BIN is latched
 * and must be unlatched by the caller. Note that location.bin may be
 * non-null even if this method returns false.
 * </p>
 * @param locationa holder class to hold state about the location of our search.
 * Sort of an internal cursor.
 * @param mainKeykey to navigate through main key
 * @param dupKeykey to navigate through duplicate tree. May be null, since
 * deleted lns have no data.
 * @param lnthe node instantiated from the log
 * @param splitsAllowedtrue if this method is allowed to cause tree splits as a side
 * effect. In practice, recovery can cause splits, but abort
 * can't.
 * @param searchDupTreetrue if a search through the dup tree looking for a match on
 * the ln's node id should be made (only in the case where dupKey ==
 * null). See SR 8984.
 * @param updateGenerationif true, set the generation count during latching. Pass false
 * when the LRU should not be impacted, such as during eviction
 * and checkpointing.
 * @return true if node found in tree. If false is returned and there is the
 * possibility that we can insert the record into a plausible parent
 * we must also set - location.bin (may be null if no possible
 * parent found) - location.lnKey (don't need to set if no possible
 * parent).
 */
  public boolean getParentBINForChildLN(  TreeLocation location,  byte[] mainKey,  byte[] dupKey,  LN ln,  boolean splitsAllowed,  boolean findDeletedEntries,  boolean searchDupTree,  boolean updateGeneration) throws DatabaseException {
    try {
      IN searchResult=null;
      try {
        if (splitsAllowed) {
          searchResult=searchSplitsAllowed(mainKey,-1,updateGeneration);
        }
 else {
          searchResult=search(mainKey,SearchType.NORMAL,-1,null,updateGeneration);
        }
        location.bin=(BIN)searchResult;
      }
 catch (      Exception e) {
        StringBuffer msg=new StringBuffer();
        if (searchResult != null) {
          this.hook682(searchResult);
          msg.append("searchResult=" + searchResult.getClass() + " nodeId="+ searchResult.getNodeId()+ " nEntries="+ searchResult.getNEntries());
        }
        throw new DatabaseException(msg.toString(),e);
      }
      if (location.bin == null) {
        return false;
      }
      boolean exactSearch=false;
      boolean indicateIfExact=true;
      if (!findDeletedEntries) {
        exactSearch=true;
        indicateIfExact=false;
      }
      location.index=location.bin.findEntry(mainKey,indicateIfExact,exactSearch);
      boolean match=false;
      if (findDeletedEntries) {
        match=(location.index >= 0 && (location.index & IN.EXACT_MATCH) != 0);
        location.index&=~IN.EXACT_MATCH;
      }
 else {
        match=(location.index >= 0);
      }
      if (match) {
        if (!location.bin.isEntryKnownDeleted(location.index)) {
          if (database.getSortedDuplicates()) {
            Node childNode=location.bin.fetchTarget(location.index);
            this.hook683(location,mainKey,dupKey,ln,splitsAllowed,findDeletedEntries,searchDupTree,updateGeneration,exactSearch,indicateIfExact,childNode);
          }
        }
        location.childLsn=location.bin.getLsn(location.index);
      }
 else {
        location.lnKey=mainKey;
      }
      return match;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  /** 
 * For SR [#8984]: our prospective child is a deleted LN, and we're facing a
 * dup tree. Alas, the deleted LN has no data, and therefore nothing to
 * guide the search in the dup tree. Instead, we search by node id. This is
 * very expensive, but this situation is a very rare case.
 */
  private boolean searchDupTreeByNodeId(  TreeLocation location,  Node childNode,  LN ln,  boolean searchDupTree,  boolean updateGeneration) throws DatabaseException {
    if (searchDupTree) {
      BIN oldBIN=location.bin;
      if (childNode.matchLNByNodeId(location,ln.getNodeId())) {
        location.index&=~IN.EXACT_MATCH;
        this.hook684(oldBIN);
        location.bin.latch(updateGeneration);
        return true;
      }
 else {
        return false;
      }
    }
 else {
      return false;
    }
  }
  /** 
 * @return true if childNode is the DIN parent of this DupCountLN
 */
  private boolean searchDupTreeForDupCountLNParent(  TreeLocation location,  byte[] mainKey,  Node childNode) throws DatabaseException {
    location.lnKey=mainKey;
    if (childNode instanceof DIN) {
      DIN dupRoot=(DIN)childNode;
      location.childLsn=dupRoot.getDupCountLNRef().getLsn();
      return true;
    }
 else {
      return false;
    }
  }
  /** 
 * Search the dup tree for the DBIN parent of this ln.
 */
  private boolean searchDupTreeForDBIN(  TreeLocation location,  byte[] dupKey,  DIN dupRoot,  LN ln,  boolean findDeletedEntries,  boolean indicateIfExact,  boolean exactSearch,  boolean splitsAllowed,  boolean updateGeneration) throws DatabaseException {
    try {
      assert dupKey != null;
      this.hook685(location,dupKey,dupRoot,ln,findDeletedEntries,indicateIfExact,exactSearch,splitsAllowed,updateGeneration);
      throw ReturnHack.returnBoolean;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  /** 
 * Return a reference to the adjacent BIN.
 * @param binThe BIN to find the next BIN for. This BIN is latched.
 * @param traverseWithinDupTreeif true, only search within the dup tree and return null when
 * the traversal runs out of duplicates.
 * @return The next BIN, or null if there are no more. The returned node is
 * latched and the caller must release it. If null is returned, the
 * argument BIN remains latched.
 */
  public BIN getNextBin(  BIN bin,  boolean traverseWithinDupTree) throws DatabaseException {
    return getNextBinInternal(traverseWithinDupTree,bin,true);
  }
  /** 
 * Return a reference to the previous BIN.
 * @param binThe BIN to find the next BIN for. This BIN is latched.
 * @param traverseWithinDupTreeif true, only search within the dup tree and return null when
 * the traversal runs out of duplicates.
 * @return The previous BIN, or null if there are no more. The returned node
 * is latched and the caller must release it. If null is returned,
 * the argument bin remains latched.
 */
  public BIN getPrevBin(  BIN bin,  boolean traverseWithinDupTree) throws DatabaseException {
    return getNextBinInternal(traverseWithinDupTree,bin,false);
  }
  /** 
 * Helper routine for above two routines to iterate through BIN's.
 */
  private BIN getNextBinInternal(  boolean traverseWithinDupTree,  BIN bin,  boolean forward) throws DatabaseException {
    try {
      byte[] idKey=null;
      if (bin.getNEntries() == 0) {
        idKey=bin.getIdentifierKey();
      }
 else       if (forward) {
        idKey=bin.getKey(bin.getNEntries() - 1);
      }
 else {
        idKey=bin.getKey(0);
      }
      IN next=bin;
      this.hook687();
      IN parent=null;
      IN nextIN=null;
      this.hook686(traverseWithinDupTree,forward,idKey,next,parent,nextIN);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (BIN)r.value;
    }
  }
  /** 
 * Split the root of the tree.
 */
  private void splitRoot() throws DatabaseException {
    EnvironmentImpl env=database.getDbEnvironment();
    LogManager logManager=env.getLogManager();
    INList inMemoryINs=env.getInMemoryINs();
    IN curRoot=null;
    curRoot=(IN)root.fetchTarget(database,null);
    this.hook689(curRoot);
    long curRootLsn=0;
    long logLsn=0;
    IN newRoot=null;
    this.hook688(logManager,inMemoryINs,curRoot,curRootLsn,logLsn,newRoot);
    treeStats.nRootSplits++;
    this.hook662(curRoot,curRootLsn,logLsn,newRoot);
  }
  /** 
 * Search the tree, starting at the root. Depending on search type either
 * search using key, or search all the way down the right or left sides.
 * Stop the search either when the bottom of the tree is reached, or a node
 * matching nid is found (see below) in which case that node's parent is
 * returned.
 * Preemptive splitting is not done during the search.
 * @param key -
 * the key to search for, or null if searchType is LEFT or RIGHT.
 * @param searchType -
 * The type of tree search to perform. NORMAL means we're
 * searching for key in the tree. LEFT/RIGHT means we're
 * descending down the left or right side, resp. DELETE means
 * we're descending the tree and will return the lowest node in
 * the path that has > 1 entries.
 * @param nid -
 * The nodeid to search for in the tree. If found, returns its
 * parent. If the nodeid of the root is passed, null is returned.
 * @param binBoundary -
 * If non-null, information is returned about whether the BIN
 * found is the first or last BIN in the database.
 * @return - the Node that matches the criteria, if any. This is the node
 * that is farthest down the tree with a match. Returns null if the
 * root is null. Node is latched (unless it's null) and must be
 * unlatched by the caller. Only IN's and BIN's are returned, not
 * LN's. In a NORMAL search, It is the caller's responsibility to do
 * the findEntry() call on the key and BIN to locate the entry that
 * matches key. The return value node is latched upon return and it
 * is the caller's responsibility to unlatch it.
 */
  public IN search(  byte[] key,  SearchType searchType,  long nid,  BINBoundary binBoundary,  boolean updateGeneration) throws DatabaseException {
    IN rootIN=getRootIN(true);
    if (rootIN != null) {
      return searchSubTree(rootIN,key,searchType,nid,binBoundary,updateGeneration);
    }
 else {
      return null;
    }
  }
  /** 
 * Do a key based search, permitting pre-emptive splits. Returns the target
 * node's parent.
 */
  public IN searchSplitsAllowed(  byte[] key,  long nid,  boolean updateGeneration) throws DatabaseException {
    return new Tree_searchSplitsAllowed(this,key,nid,updateGeneration).execute();
  }
  /** 
 * Searches a portion of the tree starting at parent using key. If during
 * the search a node matching a non-null nid argument is found, its parent
 * is returned. If searchType is NORMAL, then key must be supplied to guide
 * the search. If searchType is LEFT (or RIGHT), then the tree is searched
 * down the left (or right) side to find the first (or last) leaf,
 * respectively.
 * <p>
 * Enters with parent latched, assuming it's not null. Exits with the return
 * value latched, assuming it's not null.
 * <p>
 * @param parent -
 * the root of the subtree to start the search at. This node
 * should be latched by the caller and will be unlatched prior to
 * return.
 * @param key -
 * the key to search for, unless searchType is LEFT or RIGHT
 * @param searchType -
 * NORMAL means search using key and, optionally, nid. LEFT means
 * find the first (leftmost) leaf RIGHT means find the last
 * (rightmost) leaf
 * @param nid -
 * The nodeid to search for in the tree. If found, returns its
 * parent. If the nodeid of the root is passed, null is returned.
 * Pass -1 if no nodeid based search is desired.
 * @return - the node matching the argument criteria, or null. The node is
 * latched and must be unlatched by the caller. The parent argument
 * and any other nodes that are latched during the search are
 * unlatched prior to return.
 */
  public IN searchSubTree(  IN parent,  byte[] key,  SearchType searchType,  long nid,  BINBoundary binBoundary,  boolean updateGeneration) throws DatabaseException {
    if (parent == null) {
      return null;
    }
    if ((searchType == SearchType.LEFT || searchType == SearchType.RIGHT) && key != null) {
      throw new IllegalArgumentException("searchSubTree passed key and left/right search");
    }
    this.hook690(parent);
    if (parent.getNodeId() == nid) {
      this.hook691(parent);
      return null;
    }
    if (binBoundary != null) {
      binBoundary.isLastBin=true;
      binBoundary.isFirstBin=true;
    }
    int index;
    IN child=null;
    TreeWalkerStatsAccumulator treeStatsAccumulator=getTreeStatsAccumulator();
    try {
      do {
        if (treeStatsAccumulator != null) {
          parent.accumulateStats(treeStatsAccumulator);
        }
        if (parent.getNEntries() == 0) {
          return parent;
        }
 else         if (searchType == SearchType.NORMAL) {
          index=parent.findEntry(key,false,false);
        }
 else         if (searchType == SearchType.LEFT) {
          index=0;
        }
 else         if (searchType == SearchType.RIGHT) {
          index=parent.getNEntries() - 1;
        }
 else {
          throw new IllegalArgumentException("Invalid value of searchType: " + searchType);
        }
        assert index >= 0;
        if (binBoundary != null) {
          if (index != parent.getNEntries() - 1) {
            binBoundary.isLastBin=false;
          }
          if (index != 0) {
            binBoundary.isFirstBin=false;
          }
        }
        child=(IN)parent.fetchTarget(index);
        child.latch(updateGeneration);
        if (treeStatsAccumulator != null) {
          child.accumulateStats(treeStatsAccumulator);
        }
        if (child.getNodeId() == nid) {
          this.hook693(child);
          return parent;
        }
        this.hook692(parent);
        parent=child;
      }
 while (!(parent instanceof BIN));
      return child;
    }
 catch (    Throwable t) {
      this.hook694(parent,child);
      if (t instanceof DatabaseException) {
        throw (DatabaseException)t;
      }
 else {
        throw new DatabaseException(t);
      }
    }
  }
  /** 
 * Search down the tree using a key, but instead of returning the BIN that
 * houses that key, find the point where we can detach a deletable subtree.
 * A deletable subtree is a branch where each IN has one child, and the
 * bottom BIN has no entries and no resident cursors. That point can be
 * found by saving a pointer to the lowest node in the path with more than
 * one entry.
 * INa / \ INb INc | | INd .. / \ INe .. | BINx (suspected of being empty)
 * In this case, we'd like to prune off the subtree headed by INe. INd is
 * the parent of this deletable subtree. As we descend, we must keep latches
 * for all the nodes that will be logged. In this case, we will need to keep
 * INa, INb and INd latched when we return from this method.
 * The method returns a list of parent/child/index structures. In this
 * example, the list will hold: INa/INb/index INb/INd/index INd/INe/index
 * Every node is latched, and every node except for the bottom most child
 * (INe) must be logged.
 */
  public void searchDeletableSubTree(  IN parent,  byte[] key,  ArrayList nodeLadder) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
    assert (parent != null);
    assert (key != null);
    this.hook695(parent);
    int index;
    IN child=null;
    IN lowestMultipleEntryIN=null;
    do {
      if (parent.getNEntries() == 0) {
        break;
      }
      if (parent.getNEntries() > 1) {
        lowestMultipleEntryIN=parent;
      }
      index=parent.findEntry(key,false,false);
      assert index >= 0;
      child=(IN)parent.fetchTarget(index);
      child.latch(false);
      nodeLadder.add(new SplitInfo(parent,child,index));
      parent=child;
    }
 while (!(parent instanceof BIN));
    if ((child != null) && (child instanceof BIN)) {
      if (child.getNEntries() != 0) {
        throw NodeNotEmptyException.NODE_NOT_EMPTY;
      }
      if (((BIN)child).nCursors() > 0) {
        throw CursorsExistException.CURSORS_EXIST;
      }
    }
    if (lowestMultipleEntryIN != null) {
      ListIterator iter=nodeLadder.listIterator(nodeLadder.size());
      while (iter.hasPrevious()) {
        SplitInfo info5=(SplitInfo)iter.previous();
        if (info5.parent == lowestMultipleEntryIN) {
          break;
        }
 else {
          this.hook696(info5);
          iter.remove();
        }
      }
    }
 else {
      this.hook697(nodeLadder);
      nodeLadder.clear();
    }
  }
  /** 
 * Search the portion of the tree starting at the parent, permitting
 * preemptive splits.
 */
  private IN searchSubTreeSplitsAllowed(  IN parent,  byte[] key,  long nid,  boolean updateGeneration) throws DatabaseException, SplitRequiredException {
    if (parent != null) {
      while (true) {
        try {
          return searchSubTreeUntilSplit(parent,key,nid,updateGeneration);
        }
 catch (        SplitRequiredException e) {
          if (waitHook != null) {
            waitHook.doHook();
          }
          forceSplit(parent,key);
        }
      }
    }
 else {
      return null;
    }
  }
  /** 
 * Search the subtree, but throw an exception when we see a node that has to
 * be split.
 */
  private IN searchSubTreeUntilSplit(  IN parent,  byte[] key,  long nid,  boolean updateGeneration) throws DatabaseException, SplitRequiredException {
    try {
      if (parent == null) {
        return null;
      }
      this.hook699(parent);
      if (parent.getNodeId() == nid) {
        this.hook700(parent);
        return null;
      }
      int index=0;
      IN child=null;
      this.hook698(parent,key,nid,updateGeneration,index,child);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (IN)r.value;
    }
  }
  /** 
 * Do pre-emptive splitting in the subtree topped by the "parent" node.
 * Search down the tree until we get to the BIN level, and split any nodes
 * that fit the splittable requirement.
 * Note that more than one node in the path may be splittable. For example,
 * a tree might have a level2 IN and a BIN that are both splittable, and
 * would be encountered by the same insert operation.
 */
  private void forceSplit(  IN parent,  byte[] key) throws DatabaseException, SplitRequiredException {
    new Tree_forceSplit(this,parent,key).execute();
  }
  /** 
 * Helper to obtain the root IN with proper root latching. Optionally
 * updates the generation of the root when latching it.
 */
  public IN getRootIN(  boolean updateGeneration) throws DatabaseException {
    try {
      this.hook702();
      IN rootIN=null;
      this.hook701(updateGeneration,rootIN);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (IN)r.value;
    }
  }
  /** 
 * Inserts a new LN into the tree.
 * @param lnThe LN to insert into the tree.
 * @param keyKey value for the node
 * @param allowDuplicateswhether to allow duplicates to be inserted
 * @param cursorthe cursor to update to point to the newly inserted key/data
 * pair, or null if no cursor should be updated.
 * @return true if LN was inserted, false if it was a duplicate duplicate or
 * if an attempt was made to insert a duplicate when allowDuplicates
 * was false.
 */
  public boolean insert(  LN ln,  byte[] key,  boolean allowDuplicates,  CursorImpl cursor,  LockResult lnLock) throws DatabaseException {
    try {
      validateInsertArgs(allowDuplicates);
      EnvironmentImpl env=database.getDbEnvironment();
      LogManager logManager=env.getLogManager();
      INList inMemoryINs=env.getInMemoryINs();
      BIN bin=null;
      this.hook703(ln,key,allowDuplicates,cursor,lnLock,env,logManager,inMemoryINs,bin);
      throw ReturnHack.returnBoolean;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  /** 
 * Attempts to insert a duplicate at the current cursor BIN position. If an
 * existing dup tree exists, insert into it; otherwise, create a new dup
 * tree and place the new LN and the existing LN into it. If the current BIN
 * entry contains an LN, the caller guarantees that it is not deleted.
 * @return true if duplicate inserted successfully, false if it was a
 * duplicate duplicate, false if a there is an existing LN and
 * allowDuplicates is false.
 */
  private boolean insertDuplicate(  byte[] key,  BIN bin,  LN newLN,  LogManager logManager,  INList inMemoryINs,  CursorImpl cursor,  LockResult lnLock,  boolean allowDuplicates) throws DatabaseException {
    try {
      EnvironmentImpl env=database.getDbEnvironment();
      int index=cursor.getIndex();
      boolean successfulInsert=false;
      DIN dupRoot=null;
      Node n=bin.fetchTarget(index);
      long binNid=bin.getNodeId();
      if (n instanceof DIN) {
        DBIN dupBin=null;
        this.hook704(key,bin,newLN,cursor,lnLock,allowDuplicates,env,index,successfulInsert,dupRoot,n,binNid,dupBin);
      }
 else       if (n instanceof LN) {
        if (!allowDuplicates) {
          return false;
        }
        try {
          lnLock.setAbortLsn(DbLsn.NULL_LSN,true,true);
          dupRoot=createDuplicateTree(key,logManager,inMemoryINs,newLN,cursor);
        }
  finally {
          if (dupRoot != null) {
            this.hook705(dupRoot);
            successfulInsert=true;
          }
 else {
            successfulInsert=false;
          }
        }
      }
 else {
        throw new InconsistentNodeException("neither LN or DIN found in BIN");
      }
      return successfulInsert;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  /** 
 * Check if the duplicate root needs to be split. The current duplicate root
 * is latched. Exit with the new root (even if it's unchanged) latched and
 * the old root (unless the root is unchanged) unlatched.
 * @param binthe BIN containing the duplicate root.
 * @param indexthe index of the duplicate root in bin.
 * @return true if the duplicate root was split.
 */
  private boolean maybeSplitDuplicateRoot(  BIN bin,  int index) throws DatabaseException {
    DIN curRoot=(DIN)bin.fetchTarget(index);
    if (curRoot.needsSplitting()) {
      EnvironmentImpl env=database.getDbEnvironment();
      LogManager logManager=env.getLogManager();
      INList inMemoryINs=env.getInMemoryINs();
      byte[] rootIdKey=curRoot.getKey(0);
      DIN newRoot=new DIN(database,rootIdKey,maxDupTreeEntriesPerNode,curRoot.getDupKey(),curRoot.getDupCountLNRef(),curRoot.getLevel() + 1);
      this.hook707(newRoot);
      long curRootLsn=0;
      long logLsn=0;
      this.hook706(bin,index,curRoot,logManager,inMemoryINs,rootIdKey,newRoot,curRootLsn,logLsn);
      this.hook663(curRoot,newRoot,curRootLsn,logLsn);
      return true;
    }
 else {
      return false;
    }
  }
  /** 
 * Convert an existing BIN entry from a single (non-duplicate) LN to a new
 * DIN/DupCountLN->DBIN->LN subtree.
 * @param keythe key of the entry which will become the duplicate key for
 * the duplicate subtree.
 * @param logManagerthe logManager
 * @param inMemoryINsthe in memory IN list
 * @param newLNthe new record to be inserted
 * @param cursorpoints to the target position for this new dup tree.
 * @return the new duplicate subtree root (a DIN). It is latched when it is
 * returned and the caller should unlatch it. If new entry to be
 * inserted is a duplicate of the existing LN, null is returned.
 */
  private DIN createDuplicateTree(  byte[] key,  LogManager logManager,  INList inMemoryINs,  LN newLN,  CursorImpl cursor) throws DatabaseException {
    EnvironmentImpl env=database.getDbEnvironment();
    DIN dupRoot=null;
    DBIN dupBin=null;
    BIN bin=cursor.getBIN();
    int index=cursor.getIndex();
    LN existingLN=(LN)bin.fetchTarget(index);
    boolean existingLNIsDeleted=bin.isEntryKnownDeleted(index) || existingLN.isDeleted();
    assert existingLN != null;
    byte[] existingKey=existingLN.getData();
    byte[] newLNKey=newLN.getData();
    boolean keysEqual=Key.compareKeys(newLNKey,existingKey,database.getDuplicateComparator()) == 0;
    if (keysEqual) {
      return null;
    }
    Locker locker=cursor.getLocker();
    long nodeId=existingLN.getNodeId();
    int startingCount=(locker.createdNode(nodeId) || existingLNIsDeleted || locker.getWriteLockInfo(nodeId).getAbortKnownDeleted()) ? 0 : 1;
    DupCountLN dupCountLN=new DupCountLN(startingCount);
    long firstDupCountLNLsn=dupCountLN.logProvisional(env,database.getId(),key,DbLsn.NULL_LSN);
    dupRoot=new DIN(database,existingKey,maxDupTreeEntriesPerNode,key,new ChildReference(dupCountLN,key,firstDupCountLNLsn),2);
    this.hook710(dupRoot);
    dupRoot.setIsRoot(true);
    dupBin=new DBIN(database,existingKey,maxDupTreeEntriesPerNode,key,1);
    this.hook709(dupBin);
    ChildReference newExistingLNRef=new ChildReference(existingLN,existingKey,bin.getLsn(index),bin.getState(index));
    boolean insertOk=dupBin.insertEntry(newExistingLNRef);
    assert insertOk;
    this.hook708(key,logManager,inMemoryINs,newLN,cursor,env,dupRoot,dupBin,bin,index,existingLN,newLNKey,locker,dupCountLN,firstDupCountLNLsn);
    return dupRoot;
  }
  /** 
 * Validate args passed to insert. Presently this just means making sure
 * that if they say duplicates are allowed that the database supports
 * duplicates.
 */
  private void validateInsertArgs(  boolean allowDuplicates) throws DatabaseException {
    if (allowDuplicates && !database.getSortedDuplicates()) {
      throw new DatabaseException("allowDuplicates passed to insert but database doesn't " + "have allow duplicates set.");
    }
  }
  /** 
 * Find the BIN that is relevant to the insert. If the tree doesn't exist
 * yet, then create the first IN and BIN.
 * @return the BIN that was found or created and return it latched.
 */
  private BIN findBinForInsert(  byte[] key,  LogManager logManager,  INList inMemoryINs,  CursorImpl cursor) throws DatabaseException {
    BIN bin;
    bin=cursor.latchBIN();
    if (bin != null) {
      if (!bin.needsSplitting() && bin.isKeyInBounds(key)) {
        return bin;
      }
 else {
        this.hook712(bin);
      }
    }
    boolean rootLatchIsHeld=false;
    bin=this.hook711(key,logManager,inMemoryINs,bin,rootLatchIsHeld);
    if (ckptHook != null) {
      ckptHook.doHook();
    }
    return bin;
  }
  private void accountForSubtreeRemoval(  INList inList,  IN subtreeRoot,  UtilizationTracker tracker) throws DatabaseException {
    this.hook713(inList,subtreeRoot,tracker);
    this.hook665(subtreeRoot);
  }
  /** 
 * @see LogWritable#getLogSize
 */
  public int getLogSize(){
    int size=LogUtils.getBooleanLogSize();
    if (root != null) {
      size+=root.getLogSize();
    }
    return size;
  }
  /** 
 * @see LogWritable#writeToLog
 */
  public void writeToLog(  ByteBuffer logBuffer){
    LogUtils.writeBoolean(logBuffer,(root != null));
    if (root != null) {
      root.writeToLog(logBuffer);
    }
  }
  /** 
 * @see LogReadable#readFromLog
 */
  public void readFromLog(  ByteBuffer itemBuffer,  byte entryTypeVersion){
    boolean rootExists=LogUtils.readBoolean(itemBuffer);
    if (rootExists) {
      root=makeRootChildReference();
      root.readFromLog(itemBuffer,entryTypeVersion);
    }
  }
  /** 
 * @see LogReadable#dumpLog
 */
  public void dumpLog(  StringBuffer sb,  boolean verbose){
    sb.append("<root>");
    if (root != null) {
      root.dumpLog(sb,verbose);
    }
    sb.append("</root>");
  }
  /** 
 * @see LogReadable#isTransactional
 */
  public boolean logEntryIsTransactional(){
    return false;
  }
  /** 
 * @see LogReadable#getTransactionId
 */
  public long getTransactionId(){
    return 0;
  }
  /** 
 * rebuildINList is used by recovery to add all the resident nodes to the IN
 * list.
 */
  public void rebuildINList() throws DatabaseException {
    INList inMemoryList=database.getDbEnvironment().getInMemoryINs();
    if (root != null) {
      this.hook714(inMemoryList);
    }
  }
  public void dump() throws DatabaseException {
    System.out.println(dumpString(0));
  }
  public String dumpString(  int nSpaces) throws DatabaseException {
    StringBuffer sb=new StringBuffer();
    sb.append(TreeUtils.indent(nSpaces));
    sb.append("<tree>");
    sb.append('\n');
    if (root != null) {
      sb.append(DbLsn.dumpString(root.getLsn(),nSpaces));
      sb.append('\n');
      IN rootIN=(IN)root.getTarget();
      if (rootIN == null) {
        sb.append("<in/>");
      }
 else {
        sb.append(rootIN.toString());
      }
      sb.append('\n');
    }
    sb.append(TreeUtils.indent(nSpaces));
    sb.append("</tree>");
    return sb.toString();
  }
  /** 
 * Unit test support to validate subtree pruning. Didn't want to make root
 * access public.
 */
  boolean validateDelete(  int index) throws DatabaseException {
    try {
      this.hook715(index);
      throw ReturnHack.returnBoolean;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  /** 
 * Debugging check that all resident nodes are on the INList and no stray
 * nodes are present in the unused portion of the IN arrays.
 */
  public void validateINList(  IN parent) throws DatabaseException {
    if (parent == null) {
      parent=(IN)root.getTarget();
    }
    if (parent != null) {
      INList inList=database.getDbEnvironment().getInMemoryINs();
      if (!inList.getINs().contains(parent)) {
        throw new DatabaseException("IN " + parent.getNodeId() + " missing from INList");
      }
      for (int i=0; ; i+=1) {
        try {
          Node node=parent.getTarget(i);
          if (i >= parent.getNEntries()) {
            if (node != null) {
              throw new DatabaseException("IN " + parent.getNodeId() + " has stray node "+ node.getNodeId()+ " at index "+ i);
            }
            byte[] key=parent.getKey(i);
            if (key != null) {
              throw new DatabaseException("IN " + parent.getNodeId() + " has stray key "+ key+ " at index "+ i);
            }
          }
          if (node instanceof IN) {
            validateINList((IN)node);
          }
        }
 catch (        ArrayIndexOutOfBoundsException e) {
          break;
        }
      }
    }
  }
  public void setWaitHook(  TestHook hook){
    waitHook=hook;
  }
  public void setSearchHook(  TestHook hook){
    searchHook=hook;
  }
  public void setCkptHook(  TestHook hook){
    ckptHook=hook;
  }
static private class SplitInfo {
    IN parent;
    IN child;
    int index;
    SplitInfo(    IN parent,    IN child,    int index){
      this.parent=parent;
      this.child=child;
      this.index=index;
    }
  }
@MethodObject static class Tree_searchSplitsAllowed {
    Tree_searchSplitsAllowed(    Tree _this,    byte[] key,    long nid,    boolean updateGeneration){
      this._this=_this;
      this.key=key;
      this.nid=nid;
      this.updateGeneration=updateGeneration;
    }
    IN execute() throws DatabaseException {
      insertTarget=null;
      while (insertTarget == null) {
        this.hook717();
        rootIN=null;
        this.hook716();
        if (rootIN == null) {
          break;
        }
        try {
          insertTarget=_this.searchSubTreeSplitsAllowed(rootIN,key,nid,updateGeneration);
        }
 catch (        SplitRequiredException e) {
          continue;
        }
      }
      return insertTarget;
    }
    protected Tree _this;
    protected byte[] key;
    protected long nid;
    protected boolean updateGeneration;
    protected IN insertTarget;
    protected boolean rootLatched;
    protected boolean rootLatchedExclusive;
    protected IN rootIN;
    protected boolean b;
    protected EnvironmentImpl env;
    protected void hook716() throws DatabaseException {
      while (true) {
        if (_this.root != null) {
          rootIN=(IN)_this.root.fetchTarget(_this.database,null);
          if (rootIN.needsSplitting()) {
            b=true;
            this.hook721();
            if (b)             continue;
            this.hook720();
            env=_this.database.getDbEnvironment();
            env.getDbMapTree().modifyDbRoot(_this.database);
            this.hook719();
            rootIN=(IN)_this.root.fetchTarget(_this.database,null);
          }
          this.hook718();
        }
        break;
      }
    }
    protected void hook717() throws DatabaseException {
    }
    protected void hook718() throws DatabaseException {
    }
    protected void hook719() throws DatabaseException {
    }
    protected void hook720() throws DatabaseException {
    }
    protected void hook721() throws DatabaseException {
    }
  }
@MethodObject static class Tree_forceSplit {
    Tree_forceSplit(    Tree _this,    IN parent,    byte[] key){
      this._this=_this;
      this.parent=parent;
      this.key=key;
    }
    void execute() throws DatabaseException, SplitRequiredException {
      nodeLadder=new ArrayList();
      allLeftSideDescent=true;
      allRightSideDescent=true;
{
      }
      child=null;
      origParent=parent;
      iter=null;
      this.hook722();
      success=false;
      try {
        this.hook723();
        if (origParent.needsSplitting() || !origParent.isRoot()) {
          throw _this.splitRequiredException;
        }
        do {
          if (parent.getNEntries() == 0) {
            break;
          }
 else {
            index=parent.findEntry(key,false,false);
            if (index != 0) {
              allLeftSideDescent=false;
            }
            if (index != (parent.getNEntries() - 1)) {
              allRightSideDescent=false;
            }
          }
          assert index >= 0;
          child=(IN)parent.getTarget(index);
          if (child == null) {
            break;
          }
 else {
            this.hook724();
            nodeLadder.add(new SplitInfo(parent,child,index));
          }
          parent=child;
        }
 while (!(parent instanceof BIN));
        startedSplits=false;
        logManager=_this.database.getDbEnvironment().getLogManager();
        iter=nodeLadder.listIterator(nodeLadder.size());
        lastParentForSplit=-1;
        while (iter.hasPrevious()) {
          info1=(SplitInfo)iter.previous();
          child=info1.child;
          parent=info1.parent;
          index=info1.index;
          if (child.needsSplitting()) {
            maxEntriesPerNode=(child.containsDuplicates() ? _this.maxDupTreeEntriesPerNode : _this.maxMainTreeEntriesPerNode);
            if (allLeftSideDescent || allRightSideDescent) {
              child.splitSpecial(parent,index,maxEntriesPerNode,key,allLeftSideDescent);
            }
 else {
              child.split(parent,index,maxEntriesPerNode);
            }
            lastParentForSplit=parent.getNodeId();
            startedSplits=true;
            if (parent.isDbRoot()) {
              this.hook726();
              _this.root.setLsn(parent.getLastFullVersion());
              parent.setDirty(true);
            }
          }
 else {
            if (startedSplits) {
              newLsn=0;
              if (lastParentForSplit == child.getNodeId()) {
                newLsn=child.getLastFullVersion();
              }
 else {
                newLsn=child.log(logManager);
              }
              parent.updateEntry(index,newLsn);
            }
          }
          this.hook725();
          child=null;
          iter.remove();
        }
        success=true;
      }
  finally {
        this.hook727();
      }
    }
    protected Tree _this;
    protected IN parent;
    protected byte[] key;
    protected ArrayList nodeLadder;
    protected boolean allLeftSideDescent;
    protected boolean allRightSideDescent;
    protected int index;
    protected IN child;
    protected IN origParent;
    protected ListIterator iter;
    protected boolean isRootLatched;
    protected boolean success;
    protected boolean startedSplits;
    protected LogManager logManager;
    protected long lastParentForSplit;
    protected SplitInfo info1;
    protected int maxEntriesPerNode;
    protected long newLsn;
    protected SplitInfo info2;
    protected void hook722() throws DatabaseException, SplitRequiredException {
    }
    protected void hook723() throws DatabaseException, SplitRequiredException {
    }
    protected void hook724() throws DatabaseException, SplitRequiredException {
    }
    protected void hook725() throws DatabaseException, SplitRequiredException {
    }
    protected void hook726() throws DatabaseException, SplitRequiredException {
    }
    protected void hook727() throws DatabaseException, SplitRequiredException {
    }
  }
  protected void hook657(  LN ln,  EnvironmentImpl env,  BIN bin,  int index,  long newLsn) throws DatabaseException {
  }
  protected void hook658(  LN ln,  EnvironmentImpl env,  BIN bin,  int index,  long newLsn) throws DatabaseException {
  }
  protected void hook659(  LN newLN,  long binNid,  DBIN dupBin,  long newLsn) throws DatabaseException {
  }
  protected void hook660(  LN newLN,  long binNid,  DBIN dupBin,  long newLsn) throws DatabaseException {
  }
  protected void hook661() throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook662(  IN curRoot,  long curRootLsn,  long logLsn,  IN newRoot) throws DatabaseException {
  }
  protected void hook663(  DIN curRoot,  DIN newRoot,  long curRootLsn,  long logLsn) throws DatabaseException {
  }
  protected void hook664(  LN newLN,  DIN dupRoot,  DBIN dupBin,  BIN bin,  LN existingLN,  DupCountLN dupCountLN,  long dbinLsn,  long dinLsn,  long dupCountLsn,  long newLsn) throws DatabaseException {
  }
  protected void hook665(  IN subtreeRoot) throws DatabaseException {
  }
  protected void hook670(  WithRootLatched wrl) throws DatabaseException {
    this.hook728();
    throw new ReturnObject(wrl.doWork(root));
  }
  protected void hook671(  WithRootLatched wrl) throws DatabaseException {
    this.hook729();
    throw new ReturnObject(wrl.doWork(root));
  }
  protected void hook672(  byte[] idKey,  UtilizationTracker tracker,  IN subtreeRootIN,  ArrayList nodeLadder,  IN rootIN,  boolean rootNeedsUpdating) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
    if (root == null) {
      throw new ReturnVoid();
    }
    rootIN=(IN)root.fetchTarget(database,null);
    rootIN.latch(false);
    searchDeletableSubTree(rootIN,idKey,nodeLadder);
    if (nodeLadder.size() == 0) {
      if (purgeRoot) {
        subtreeRootIN=logTreeRemoval(rootIN,tracker);
        if (subtreeRootIN != null) {
          rootNeedsUpdating=true;
        }
      }
    }
 else {
      SplitInfo detachPoint=(SplitInfo)nodeLadder.get(nodeLadder.size() - 1);
      boolean deleteOk=detachPoint.parent.deleteEntry(detachPoint.index,true);
      assert deleteOk;
      rootNeedsUpdating=cascadeUpdates(nodeLadder,null,-1);
      subtreeRootIN=detachPoint.child;
    }
  }
  protected void hook673() throws DatabaseException {
  }
  protected IN hook674(  byte[] idKey,  byte[] mainKey,  IN in,  IN deletedSubtreeRoot) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
    this.hook730(in);
    assert in instanceof BIN;
    assert in.getNEntries() > 0;
    int index=in.findEntry(mainKey,false,true);
    if (index >= 0) {
      deletedSubtreeRoot=deleteDupSubtree(idKey,(BIN)in,index);
    }
    return deletedSubtreeRoot;
  }
  protected void hook675(  DIN duplicateRoot) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook676(  ArrayList nodeLadder) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook677(  DIN dupRoot) throws DatabaseException {
  }
  protected void hook678(  DIN dupRoot) throws DatabaseException {
  }
  protected void hook679(  IN child) throws DatabaseException {
  }
  protected void hook680(  IN child) throws DatabaseException {
  }
  protected void hook681(  IN potentialParent) throws DatabaseException {
  }
  protected void hook682(  IN searchResult) throws DatabaseException {
  }
  protected void hook683(  TreeLocation location,  byte[] mainKey,  byte[] dupKey,  LN ln,  boolean splitsAllowed,  boolean findDeletedEntries,  boolean searchDupTree,  boolean updateGeneration,  boolean exactSearch,  boolean indicateIfExact,  Node childNode) throws DatabaseException {
    if (childNode == null) {
    }
 else     if (ln.containsDuplicates()) {
      throw new ReturnBoolean(searchDupTreeForDupCountLNParent(location,mainKey,childNode));
    }
 else {
      if (childNode.containsDuplicates()) {
        if (dupKey == null) {
          throw new ReturnBoolean(searchDupTreeByNodeId(location,childNode,ln,searchDupTree,updateGeneration));
        }
 else {
          throw new ReturnBoolean(searchDupTreeForDBIN(location,dupKey,(DIN)childNode,ln,findDeletedEntries,indicateIfExact,exactSearch,splitsAllowed,updateGeneration));
        }
      }
    }
  }
  protected void hook684(  BIN oldBIN) throws DatabaseException {
  }
  protected void hook685(  TreeLocation location,  byte[] dupKey,  DIN dupRoot,  LN ln,  boolean findDeletedEntries,  boolean indicateIfExact,  boolean exactSearch,  boolean splitsAllowed,  boolean updateGeneration) throws DatabaseException {
    if (maybeSplitDuplicateRoot(location.bin,location.index)) {
      dupRoot=(DIN)location.bin.fetchTarget(location.index);
    }
    this.hook731(location);
    location.lnKey=dupKey;
    if (splitsAllowed) {
      try {
        location.bin=(BIN)searchSubTreeSplitsAllowed(dupRoot,location.lnKey,ln.getNodeId(),updateGeneration);
      }
 catch (      SplitRequiredException e) {
        throw new DatabaseException(e);
      }
    }
 else {
      location.bin=(BIN)searchSubTree(dupRoot,location.lnKey,SearchType.NORMAL,ln.getNodeId(),null,updateGeneration);
    }
    location.index=location.bin.findEntry(location.lnKey,indicateIfExact,exactSearch);
    boolean match;
    if (findDeletedEntries) {
      match=(location.index >= 0 && (location.index & IN.EXACT_MATCH) != 0);
      location.index&=~IN.EXACT_MATCH;
    }
 else {
      match=(location.index >= 0);
    }
    if (match) {
      location.childLsn=location.bin.getLsn(location.index);
      throw new ReturnBoolean(true);
    }
 else {
      throw new ReturnBoolean(false);
    }
  }
  protected void hook686(  boolean traverseWithinDupTree,  boolean forward,  byte[] idKey,  IN next,  IN parent,  IN nextIN) throws DatabaseException {
    while (true) {
      SearchResult result=null;
      if (!traverseWithinDupTree) {
        result=getParentINForChildIN(next,true,true);
        if (result.exactParentFound) {
          parent=result.parent;
        }
 else {
          this.hook733();
          throw new ReturnObject(null);
        }
      }
 else {
        if (next.isRoot()) {
          this.hook734(next);
          throw new ReturnObject(null);
        }
 else {
          result=getParentINForChildIN(next,true,true);
          if (result.exactParentFound) {
            parent=result.parent;
          }
 else {
            throw new ReturnObject(null);
          }
        }
      }
      this.hook732();
      int index=parent.findEntry(idKey,false,false);
      boolean moreEntriesThisBin=false;
      if (forward) {
        index++;
        if (index < parent.getNEntries()) {
          moreEntriesThisBin=true;
        }
      }
 else {
        if (index > 0) {
          moreEntriesThisBin=true;
        }
        index--;
      }
      if (moreEntriesThisBin) {
        nextIN=(IN)parent.fetchTarget(index);
        this.hook735(nextIN);
        if (nextIN instanceof BIN) {
          this.hook736(parent);
          TreeWalkerStatsAccumulator treeStatsAccumulator=getTreeStatsAccumulator();
          if (treeStatsAccumulator != null) {
            nextIN.accumulateStats(treeStatsAccumulator);
          }
          throw new ReturnObject((BIN)nextIN);
        }
 else {
          IN ret=searchSubTree(nextIN,null,(forward ? SearchType.LEFT : SearchType.RIGHT),-1,null,true);
          this.hook737(parent);
          if (ret instanceof BIN) {
            throw new ReturnObject((BIN)ret);
          }
 else {
            throw new InconsistentNodeException("subtree did not have a BIN for leaf");
          }
        }
      }
      next=parent;
    }
  }
  protected void hook687() throws DatabaseException {
  }
  protected void hook688(  LogManager logManager,  INList inMemoryINs,  IN curRoot,  long curRootLsn,  long logLsn,  IN newRoot) throws DatabaseException {
    byte[] rootIdKey=curRoot.getKey(0);
    newRoot=new IN(database,rootIdKey,maxMainTreeEntriesPerNode,curRoot.getLevel() + 1);
    newRoot.setIsRoot(true);
    curRoot.setIsRoot(false);
    try {
      curRootLsn=curRoot.logProvisional(logManager,newRoot);
      boolean insertOk=newRoot.insertEntry(new ChildReference(curRoot,rootIdKey,curRootLsn));
      assert insertOk;
      logLsn=newRoot.log(logManager);
    }
 catch (    DatabaseException e) {
      curRoot.setIsRoot(true);
      throw e;
    }
    inMemoryINs.add(newRoot);
    root.setTarget(newRoot);
    root.setLsn(logLsn);
    curRoot.split(newRoot,0,maxMainTreeEntriesPerNode);
    root.setLsn(newRoot.getLastFullVersion());
  }
  protected void hook689(  IN curRoot) throws DatabaseException {
  }
  protected void hook690(  IN parent) throws DatabaseException {
  }
  protected void hook691(  IN parent) throws DatabaseException {
  }
  protected void hook692(  IN parent) throws DatabaseException, Throwable {
  }
  protected void hook693(  IN child) throws DatabaseException, Throwable {
  }
  protected void hook694(  IN parent,  IN child) throws DatabaseException {
  }
  protected void hook695(  IN parent) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook696(  SplitInfo info5) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook697(  ArrayList nodeLadder) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook698(  IN parent,  byte[] key,  long nid,  boolean updateGeneration,  int index,  IN child) throws DatabaseException, SplitRequiredException {
    do {
      if (parent.getNEntries() == 0) {
        throw new ReturnObject(parent);
      }
 else {
        index=parent.findEntry(key,false,false);
      }
      assert index >= 0;
      child=(IN)parent.fetchTarget(index);
      child.latch(updateGeneration);
      if (child.needsSplitting()) {
        this.hook739(parent,child);
        throw splitRequiredException;
      }
      if (child.getNodeId() == nid) {
        this.hook740(child);
        throw new ReturnObject(parent);
      }
      this.hook738(parent);
      parent=child;
    }
 while (!(parent instanceof BIN));
    throw new ReturnObject(parent);
  }
  protected void hook699(  IN parent) throws DatabaseException, SplitRequiredException {
  }
  protected void hook700(  IN parent) throws DatabaseException, SplitRequiredException {
  }
  protected void hook701(  boolean updateGeneration,  IN rootIN) throws DatabaseException {
    if (root != null) {
      rootIN=(IN)root.fetchTarget(database,null);
      rootIN.latch(updateGeneration);
    }
    throw new ReturnObject(rootIN);
  }
  protected void hook702() throws DatabaseException {
  }
  protected void hook703(  LN ln,  byte[] key,  boolean allowDuplicates,  CursorImpl cursor,  LockResult lnLock,  EnvironmentImpl env,  LogManager logManager,  INList inMemoryINs,  BIN bin) throws DatabaseException {
    bin=findBinForInsert(key,logManager,inMemoryINs,cursor);
    this.hook741(bin);
    ChildReference newLNRef=new ChildReference(ln,key,DbLsn.NULL_LSN);
    cursor.setBIN(bin);
    int index=bin.insertEntry1(newLNRef);
    if ((index & IN.INSERT_SUCCESS) != 0) {
      index&=~IN.INSERT_SUCCESS;
      cursor.updateBin(bin,index);
      long newLsn=DbLsn.NULL_LSN;
      try {
        newLsn=ln.log(env,database.getId(),key,DbLsn.NULL_LSN,cursor.getLocker());
      }
  finally {
        if (newLsn == DbLsn.NULL_LSN) {
          bin.setKnownDeleted(index);
        }
      }
      lnLock.setAbortLsn(DbLsn.NULL_LSN,true,true);
      bin.updateEntry(index,newLsn);
      this.hook657(ln,env,bin,index,newLsn);
      throw new ReturnBoolean(true);
    }
 else {
      index&=~IN.EXACT_MATCH;
      cursor.updateBin(bin,index);
      LN currentLN=null;
      boolean isDup=false;
      Node n=bin.fetchTarget(index);
      if (n == null || n instanceof LN) {
        currentLN=(LN)n;
      }
 else {
        isDup=true;
      }
      boolean isDeleted=false;
      LockResult currentLock=null;
      if (!isDup) {
        if (currentLN == null) {
          isDeleted=true;
        }
 else {
          currentLock=cursor.lockLNDeletedAllowed(currentLN,LockType.WRITE);
          currentLN=currentLock.getLN();
          bin=cursor.getBIN();
          index=cursor.getIndex();
          if (cursor.getDupBIN() != null) {
            cursor.clearDupBIN(true);
            isDup=true;
          }
 else           if (bin.isEntryKnownDeleted(index) || currentLN == null || currentLN.isDeleted()) {
            isDeleted=true;
          }
        }
      }
      if (isDeleted) {
        long abortLsn=bin.getLsn(index);
        boolean abortKnownDeleted=true;
        if (currentLN != null && currentLock.getLockGrant() == LockGrantType.EXISTING) {
          long nodeId=currentLN.getNodeId();
          Locker locker=cursor.getLocker();
          WriteLockInfo info6=locker.getWriteLockInfo(nodeId);
          abortLsn=info6.getAbortLsn();
          abortKnownDeleted=info6.getAbortKnownDeleted();
        }
        lnLock.setAbortLsn(abortLsn,abortKnownDeleted);
        long newLsn=ln.log(env,database.getId(),key,DbLsn.NULL_LSN,cursor.getLocker());
        bin.updateEntry(index,ln,newLsn,key);
        bin.clearKnownDeleted(index);
        bin.clearPendingDeleted(index);
        this.hook658(ln,env,bin,index,newLsn);
        throw new ReturnBoolean(true);
      }
 else {
        throw new ReturnBoolean(insertDuplicate(key,bin,ln,logManager,inMemoryINs,cursor,lnLock,allowDuplicates));
      }
    }
  }
  protected void hook704(  byte[] key,  BIN bin,  LN newLN,  CursorImpl cursor,  LockResult lnLock,  boolean allowDuplicates,  EnvironmentImpl env,  int index,  boolean successfulInsert,  DIN dupRoot,  Node n,  long binNid,  DBIN dupBin) throws DatabaseException {
    dupRoot=(DIN)n;
    this.hook744(dupRoot);
    LockResult dclLockResult=cursor.lockDupCountLN(dupRoot,LockType.WRITE);
    bin=cursor.getBIN();
    index=cursor.getIndex();
    if (!allowDuplicates) {
      DupCountLN dcl=(DupCountLN)dclLockResult.getLN();
      if (dcl.getDupCount() > 0) {
        throw new ReturnBoolean(false);
      }
    }
    maybeSplitDuplicateRoot(bin,index);
    dupRoot=(DIN)bin.fetchTarget(index);
    byte[] newLNKey=newLN.getData();
    long previousLsn=dupRoot.getLastFullVersion();
    try {
      dupBin=(DBIN)searchSubTreeSplitsAllowed(dupRoot,newLNKey,-1,true);
    }
 catch (    SplitRequiredException e) {
      throw new DatabaseException(e);
    }
    long currentLsn=dupRoot.getLastFullVersion();
    if (currentLsn != previousLsn) {
      bin.updateEntry(index,currentLsn);
    }
    this.hook743(cursor);
    bin=null;
    dupRoot=null;
    ChildReference newLNRef=new ChildReference(newLN,newLNKey,DbLsn.NULL_LSN);
    int dupIndex=dupBin.insertEntry1(newLNRef);
    if ((dupIndex & IN.INSERT_SUCCESS) != 0) {
      dupIndex&=~IN.INSERT_SUCCESS;
      cursor.updateDBin(dupBin,dupIndex);
      long newLsn=DbLsn.NULL_LSN;
      try {
        newLsn=newLN.log(env,database.getId(),key,DbLsn.NULL_LSN,cursor.getLocker());
      }
  finally {
        if (newLsn == DbLsn.NULL_LSN) {
          dupBin.setKnownDeleted(dupIndex);
        }
      }
      lnLock.setAbortLsn(DbLsn.NULL_LSN,true,true);
      dupBin.setLsn(dupIndex,newLsn);
      this.hook659(newLN,binNid,dupBin,newLsn);
      successfulInsert=true;
    }
 else {
      dupIndex&=~IN.EXACT_MATCH;
      cursor.updateDBin(dupBin,dupIndex);
      LN currentLN=(LN)dupBin.fetchTarget(dupIndex);
      boolean isDeleted=false;
      LockResult currentLock=null;
      if (currentLN == null) {
        isDeleted=true;
      }
 else {
        currentLock=cursor.lockLNDeletedAllowed(currentLN,LockType.WRITE);
        currentLN=currentLock.getLN();
        dupBin=cursor.getDupBIN();
        dupIndex=cursor.getDupIndex();
        if (dupBin.isEntryKnownDeleted(dupIndex) || currentLN == null || currentLN.isDeleted()) {
          isDeleted=true;
        }
      }
      if (isDeleted) {
        long abortLsn=dupBin.getLsn(dupIndex);
        boolean abortKnownDeleted=true;
        if (currentLN != null && currentLock.getLockGrant() == LockGrantType.EXISTING) {
          long nodeId=currentLN.getNodeId();
          Locker locker=cursor.getLocker();
          WriteLockInfo info7=locker.getWriteLockInfo(nodeId);
          abortLsn=info7.getAbortLsn();
          abortKnownDeleted=info7.getAbortKnownDeleted();
        }
        lnLock.setAbortLsn(abortLsn,abortKnownDeleted);
        long newLsn=newLN.log(env,database.getId(),key,DbLsn.NULL_LSN,cursor.getLocker());
        dupBin.updateEntry(dupIndex,newLN,newLsn,newLNKey);
        dupBin.clearKnownDeleted(dupIndex);
        dupBin.clearPendingDeleted(dupIndex);
        this.hook660(newLN,binNid,dupBin,newLsn);
        successfulInsert=true;
      }
 else {
        successfulInsert=false;
      }
    }
    this.hook742(dupBin);
    dupBin=null;
    if (successfulInsert) {
      this.hook746(cursor);
      dupRoot=cursor.getLatchedDupRoot(false);
      this.hook745(cursor);
      dupRoot.incrementDuplicateCount(dclLockResult,key,cursor.getLocker(),true);
    }
  }
  protected void hook705(  DIN dupRoot) throws DatabaseException {
  }
  protected void hook706(  BIN bin,  int index,  DIN curRoot,  LogManager logManager,  INList inMemoryINs,  byte[] rootIdKey,  DIN newRoot,  long curRootLsn,  long logLsn) throws DatabaseException {
    newRoot.setIsRoot(true);
    curRoot.setDupCountLN(null);
    curRoot.setIsRoot(false);
    try {
      curRootLsn=curRoot.logProvisional(logManager,newRoot);
      boolean insertOk=newRoot.insertEntry(new ChildReference(curRoot,rootIdKey,bin.getLsn(index)));
      assert insertOk;
      logLsn=newRoot.log(logManager);
    }
 catch (    DatabaseException e) {
      curRoot.setIsRoot(true);
      throw e;
    }
    inMemoryINs.add(newRoot);
    bin.updateEntry(index,newRoot,logLsn);
    curRoot.split(newRoot,0,maxDupTreeEntriesPerNode);
  }
  protected void hook707(  DIN newRoot) throws DatabaseException {
  }
  protected void hook708(  byte[] key,  LogManager logManager,  INList inMemoryINs,  LN newLN,  CursorImpl cursor,  EnvironmentImpl env,  DIN dupRoot,  DBIN dupBin,  BIN bin,  int index,  LN existingLN,  byte[] newLNKey,  Locker locker,  DupCountLN dupCountLN,  long firstDupCountLNLsn) throws DatabaseException {
    long dbinLsn=dupBin.logProvisional(logManager,dupRoot);
    inMemoryINs.add(dupBin);
    dupRoot.setEntry(0,dupBin,dupBin.getKey(0),dbinLsn,dupBin.getState(0));
    long dinLsn=dupRoot.log(logManager);
    inMemoryINs.add(dupRoot);
    LockResult lockResult=locker.lock(dupCountLN.getNodeId(),LockType.WRITE,false,database);
    lockResult.setAbortLsn(firstDupCountLNLsn,false);
    dupCountLN.setDupCount(2);
    long dupCountLsn=dupCountLN.log(env,database.getId(),key,firstDupCountLNLsn,locker);
    dupRoot.updateDupCountLNRef(dupCountLsn);
    long newLsn=newLN.log(env,database.getId(),key,DbLsn.NULL_LSN,locker);
    int dupIndex=dupBin.insertEntry1(new ChildReference(newLN,newLNKey,newLsn));
    dupIndex&=~IN.INSERT_SUCCESS;
    cursor.updateDBin(dupBin,dupIndex);
    bin.adjustCursorsForMutation(index,dupBin,dupIndex ^ 1,cursor);
    this.hook747(dupBin);
    bin.updateEntry(index,dupRoot,dinLsn);
    bin.setMigrate(index,false);
    this.hook664(newLN,dupRoot,dupBin,bin,existingLN,dupCountLN,dbinLsn,dinLsn,dupCountLsn,newLsn);
  }
  protected void hook709(  DBIN dupBin) throws DatabaseException {
  }
  protected void hook710(  DIN dupRoot) throws DatabaseException {
  }
  protected BIN hook711(  byte[] key,  LogManager logManager,  INList inMemoryINs,  BIN bin,  boolean rootLatchIsHeld) throws DatabaseException {
    long logLsn;
    while (true) {
      rootLatchIsHeld=true;
      this.hook748();
      if (root == null) {
        this.hook751();
        if (root != null) {
          this.hook752();
          rootLatchIsHeld=false;
          continue;
        }
        bin=new BIN(database,key,maxMainTreeEntriesPerNode,1);
        this.hook750(bin);
        logLsn=bin.logProvisional(logManager,null);
        IN rootIN=new IN(database,key,maxMainTreeEntriesPerNode,2);
        rootIN.setIsRoot(true);
        boolean insertOk=rootIN.insertEntry(new ChildReference(bin,key,logLsn));
        assert insertOk;
        logLsn=rootIN.log(logManager);
        rootIN.setDirty(true);
        root=new ChildReference(rootIN,new byte[0],logLsn);
        inMemoryINs.add(bin);
        inMemoryINs.add(rootIN);
        this.hook749();
        rootLatchIsHeld=false;
        break;
      }
 else {
        this.hook753();
        rootLatchIsHeld=false;
        IN in=searchSplitsAllowed(key,-1,true);
        if (in == null) {
          continue;
        }
 else {
          bin=(BIN)in;
          break;
        }
      }
    }
    return bin;
  }
  protected void hook712(  BIN bin) throws DatabaseException {
  }
  protected void hook713(  INList inList,  IN subtreeRoot,  UtilizationTracker tracker) throws DatabaseException {
    subtreeRoot.accountForSubtreeRemoval(inList,tracker);
  }
  protected void hook714(  INList inMemoryList) throws DatabaseException {
    Node rootIN=root.getTarget();
    if (rootIN != null) {
      rootIN.rebuildINList(inMemoryList);
    }
  }
  protected void hook715(  int index) throws DatabaseException {
    IN rootIN=(IN)root.fetchTarget(database,null);
    throw new ReturnBoolean(rootIN.validateSubtreeBeforeDelete(index));
  }
  protected void hook728() throws DatabaseException {
  }
  protected void hook729() throws DatabaseException {
  }
  protected void hook730(  IN in) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
  protected void hook731(  TreeLocation location) throws DatabaseException {
  }
  protected void hook732() throws DatabaseException {
  }
  protected void hook733() throws DatabaseException {
  }
  protected void hook734(  IN next) throws DatabaseException {
  }
  protected void hook735(  IN nextIN) throws DatabaseException {
  }
  protected void hook736(  IN parent) throws DatabaseException {
  }
  protected void hook737(  IN parent) throws DatabaseException {
  }
  protected void hook738(  IN parent) throws DatabaseException, SplitRequiredException {
  }
  protected void hook739(  IN parent,  IN child) throws DatabaseException, SplitRequiredException {
  }
  protected void hook740(  IN child) throws DatabaseException, SplitRequiredException {
  }
  protected void hook741(  BIN bin) throws DatabaseException {
  }
  protected void hook742(  DBIN dupBin) throws DatabaseException {
  }
  protected void hook743(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook744(  DIN dupRoot) throws DatabaseException {
  }
  protected void hook745(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook746(  CursorImpl cursor) throws DatabaseException {
  }
  protected void hook747(  DBIN dupBin) throws DatabaseException {
  }
  protected void hook748() throws DatabaseException {
  }
  protected void hook749() throws DatabaseException {
  }
  protected void hook750(  BIN bin) throws DatabaseException {
  }
  protected void hook751() throws DatabaseException {
  }
  protected void hook752() throws DatabaseException {
  }
  protected void hook753() throws DatabaseException {
  }
  protected void hook754(  BIN bin) throws DatabaseException, NodeNotEmptyException, CursorsExistException {
  }
}
\00base/com/sleepycat/je/tree/IN.java:package com.sleepycat.je.tree;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.INList;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.log.LogEntryType;
import com.sleepycat.je.log.LogException;
import com.sleepycat.je.log.LogFileNotFoundException;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.log.LogReadable;
import com.sleepycat.je.log.LogUtils;
import com.sleepycat.je.log.LoggableObject;
import com.sleepycat.je.log.entry.INLogEntry;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * An IN represents an Internal Node in the JE tree.
 */
public class IN extends Node implements Comparable, LoggableObject, LogReadable {
  private static final String BEGIN_TAG="<in>";
  private static final String END_TAG="</in>";
  private static final String TRACE_SPLIT="Split:";
  private static final String TRACE_DELETE="Delete:";
  private static final byte KNOWN_DELETED_BIT=0x1;
  private static final byte CLEAR_KNOWN_DELETED_BIT=~0x1;
  private static final byte DIRTY_BIT=0x2;
  private static final byte CLEAR_DIRTY_BIT=~0x2;
  private static final byte MIGRATE_BIT=0x4;
  private static final byte CLEAR_MIGRATE_BIT=~0x4;
  private static final byte PENDING_DELETED_BIT=0x8;
  private static final byte CLEAR_PENDING_DELETED_BIT=~0x8;
  private static final int BYTES_PER_LSN_ENTRY=4;
  private static final int MAX_FILE_OFFSET=0xfffffe;
  private static final int THREE_BYTE_NEGATIVE_ONE=0xffffff;
  private static final int GROWTH_INCREMENT=5;
  public static final int DBMAP_LEVEL=0x20000;
  public static final int MAIN_LEVEL=0x10000;
  public static final int LEVEL_MASK=0x0ffff;
  public static final int MIN_LEVEL=-1;
  public static final int MAX_LEVEL=Integer.MAX_VALUE;
  public static final int BIN_LEVEL=MAIN_LEVEL | 1;
  private long generation;
  private boolean dirty;
  private int nEntries;
  private byte[] identifierKey;
  private Node[] entryTargets;
  private byte[][] entryKeyVals;
  private long baseFileNumber;
  private byte[] entryLsnByteArray;
  private long[] entryLsnLongArray;
  private byte[] entryStates;
  private DatabaseImpl databaseImpl;
  private boolean isRoot;
  private int level;
  private long inMemorySize;
  private long lastFullVersion=DbLsn.NULL_LSN;
  private List provisionalObsolete;
  public static final int EXACT_MATCH=(1 << 16);
  public static final int INSERT_SUCCESS=(1 << 17);
  public static int ACCUMULATED_LIMIT=1000;
  /** 
 * Create an empty IN, with no node id, to be filled in from the log.
 */
  public IN(){
    super(false);
    init(null,Key.EMPTY_KEY,0,0);
  }
  /** 
 * Create a new IN.
 */
  public IN(  DatabaseImpl db,  byte[] identifierKey,  int capacity,  int level){
    super(true);
    init(db,identifierKey,capacity,generateLevel(db.getId(),level));
  }
  /** 
 * Initialize IN object.
 */
  protected void init(  DatabaseImpl db,  byte[] identifierKey,  int initialCapacity,  int level){
    setDatabase(db);
    EnvironmentImpl env=(databaseImpl == null) ? null : databaseImpl.getDbEnvironment();
    this.hook618(env);
    generation=0;
    dirty=false;
    nEntries=0;
    this.identifierKey=identifierKey;
    entryTargets=new Node[initialCapacity];
    entryKeyVals=new byte[initialCapacity][];
    baseFileNumber=-1;
    entryLsnByteArray=new byte[initialCapacity << 2];
    entryLsnLongArray=null;
    entryStates=new byte[initialCapacity];
    isRoot=false;
    this.level=level;
  }
  private long getEqualityKey(){
    int hash=System.identityHashCode(this);
    long hash2=(((long)hash) << 32) | hash;
    return hash2 ^ getNodeId();
  }
  public boolean equals(  Object obj){
    if (!(obj instanceof IN)) {
      return false;
    }
    IN in=(IN)obj;
    return (this.getEqualityKey() == in.getEqualityKey());
  }
  public int hashCode(){
    return (int)getEqualityKey();
  }
  /** 
 * Sort based on node id.
 */
  public int compareTo(  Object o){
    if (o == null) {
      throw new NullPointerException();
    }
    IN argIN=(IN)o;
    long argEqualityKey=argIN.getEqualityKey();
    long myEqualityKey=getEqualityKey();
    if (myEqualityKey < argEqualityKey) {
      return -1;
    }
 else     if (myEqualityKey > argEqualityKey) {
      return 1;
    }
 else {
      return 0;
    }
  }
  /** 
 * Create a new IN.  Need this because we can't call newInstance() without
 * getting a 0 for nodeid.
 */
  protected IN createNewInstance(  byte[] identifierKey,  int maxEntries,  int level){
    return new IN(databaseImpl,identifierKey,maxEntries,level);
  }
  /** 
 * Initialize a node that has been read in from the log.
 */
  public void postFetchInit(  DatabaseImpl db,  long sourceLsn) throws DatabaseException {
    setDatabase(db);
    setLastFullLsn(sourceLsn);
    EnvironmentImpl env=db.getDbEnvironment();
    this.hook637();
    env.getInMemoryINs().add(this);
  }
  /** 
 * Initialize a node read in during recovery.
 */
  public void postRecoveryInit(  DatabaseImpl db,  long sourceLsn){
    setDatabase(db);
    setLastFullLsn(sourceLsn);
  }
  /** 
 * Sets the last logged LSN.
 */
  void setLastFullLsn(  long lsn){
    lastFullVersion=lsn;
  }
  /** 
 * Returns the last logged LSN, or null if never logged.  Is public for
 * unit testing.
 */
  public long getLastFullVersion(){
    return lastFullVersion;
  }
  /** 
 * Latch this node, optionally setting the generation.
 */
  public void latch(  boolean updateGeneration) throws DatabaseException {
    if (updateGeneration) {
      setGeneration();
    }
  }
  /** 
 * Latch this node if it is not latched by another thread, optionally
 * setting the generation if the latch succeeds.
 */
  public boolean latchNoWait(  boolean updateGeneration) throws DatabaseException {
    try {
      this.hook619(updateGeneration);
      throw ReturnHack.returnBoolean;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  public long getGeneration(){
    return generation;
  }
  public void setGeneration(){
    generation=Generation.getNextGeneration();
  }
  public void setGeneration(  long newGeneration){
    generation=newGeneration;
  }
  public int getLevel(){
    return level;
  }
  protected int generateLevel(  DatabaseId dbId,  int newLevel){
    if (dbId.equals(DbTree.ID_DB_ID)) {
      return newLevel | DBMAP_LEVEL;
    }
 else {
      return newLevel | MAIN_LEVEL;
    }
  }
  public boolean getDirty(){
    return dirty;
  }
  public void setDirty(  boolean dirty){
    this.dirty=dirty;
  }
  public boolean isRoot(){
    return isRoot;
  }
  public boolean isDbRoot(){
    return isRoot;
  }
  void setIsRoot(  boolean isRoot){
    this.isRoot=isRoot;
    setDirty(true);
  }
  /** 
 * @return the identifier key for this node.
 */
  public byte[] getIdentifierKey(){
    return identifierKey;
  }
  /** 
 * Set the identifier key for this node.
 * @param key - the new identifier key for this node.
 */
  void setIdentifierKey(  byte[] key){
    identifierKey=key;
    setDirty(true);
  }
  /** 
 * Get the key (dupe or identifier) in child that is used to locate it in
 * 'this' node.
 */
  public byte[] getChildKey(  IN child) throws DatabaseException {
    return child.getIdentifierKey();
  }
  public byte[] selectKey(  byte[] mainTreeKey,  byte[] dupTreeKey){
    return mainTreeKey;
  }
  /** 
 * Return the key for this duplicate set.
 */
  public byte[] getDupKey() throws DatabaseException {
    throw new DatabaseException(shortClassName() + ".getDupKey() called");
  }
  /** 
 * Return the key for navigating through the duplicate tree.
 */
  public byte[] getDupTreeKey(){
    return null;
  }
  /** 
 * Return the key for navigating through the main tree.
 */
  public byte[] getMainTreeKey(){
    return getIdentifierKey();
  }
  /** 
 * Get the database for this IN.
 */
  public DatabaseImpl getDatabase(){
    return databaseImpl;
  }
  /** 
 * Set the database reference for this node.
 */
  public void setDatabase(  DatabaseImpl db){
    databaseImpl=db;
  }
  public DatabaseId getDatabaseId(){
    return databaseImpl.getId();
  }
  private void setEntryInternal(  int from,  int to){
    entryTargets[to]=entryTargets[from];
    entryKeyVals[to]=entryKeyVals[from];
    entryStates[to]=entryStates[from];
    if (entryLsnLongArray == null) {
      int fromOff=from << 2;
      int toOff=to << 2;
      entryLsnByteArray[toOff++]=entryLsnByteArray[fromOff++];
      entryLsnByteArray[toOff++]=entryLsnByteArray[fromOff++];
      entryLsnByteArray[toOff++]=entryLsnByteArray[fromOff++];
      entryLsnByteArray[toOff]=entryLsnByteArray[fromOff];
    }
 else {
      entryLsnLongArray[to]=entryLsnLongArray[from];
    }
  }
  private void clearEntry(  int idx){
    entryTargets[idx]=null;
    entryKeyVals[idx]=null;
    setLsnElement(idx,DbLsn.NULL_LSN);
    entryStates[idx]=0;
  }
  /** 
 * Return the idx'th key.
 */
  public byte[] getKey(  int idx){
    return entryKeyVals[idx];
  }
  /** 
 * Set the idx'th key.
 */
  private void setKey(  int idx,  byte[] keyVal){
    entryKeyVals[idx]=keyVal;
    entryStates[idx]|=DIRTY_BIT;
  }
  /** 
 * Get the idx'th migrate status.
 */
  public boolean getMigrate(  int idx){
    return (entryStates[idx] & MIGRATE_BIT) != 0;
  }
  /** 
 * Set the idx'th migrate status.
 */
  public void setMigrate(  int idx,  boolean migrate){
    if (migrate) {
      entryStates[idx]|=MIGRATE_BIT;
    }
 else {
      entryStates[idx]&=CLEAR_MIGRATE_BIT;
    }
  }
  public byte getState(  int idx){
    return entryStates[idx];
  }
  /** 
 * Return the idx'th target.
 */
  public Node getTarget(  int idx){
    return entryTargets[idx];
  }
  /** 
 * Sets the idx'th target. No need to make dirty, that state only applies
 * to key and LSN.
 * <p>WARNING: This method does not update the memory budget.  The caller
 * must update the budget.</p>
 */
  void setTarget(  int idx,  Node target){
    entryTargets[idx]=target;
  }
  /** 
 * Return the idx'th LSN for this entry.
 * @return the idx'th LSN for this entry.
 */
  public long getLsn(  int idx){
    if (entryLsnLongArray == null) {
      int offset=idx << 2;
      int fileOffset=getFileOffset(offset);
      if (fileOffset == -1) {
        return DbLsn.NULL_LSN;
      }
 else {
        return DbLsn.makeLsn((long)(baseFileNumber + getFileNumberOffset(offset)),fileOffset);
      }
    }
 else {
      return entryLsnLongArray[idx];
    }
  }
  /** 
 * Sets the idx'th target LSN.
 */
  public void setLsn(  int idx,  long lsn){
    new IN_setLsn(this,idx,lsn).execute();
  }
  long[] getEntryLsnLongArray(){
    return entryLsnLongArray;
  }
  byte[] getEntryLsnByteArray(){
    return entryLsnByteArray;
  }
  void initEntryLsn(  int capacity){
    entryLsnLongArray=null;
    entryLsnByteArray=new byte[capacity << 2];
    baseFileNumber=-1;
  }
  void setLsnElement(  int idx,  long value){
    int offset=idx << 2;
    if (entryLsnLongArray != null) {
      entryLsnLongArray[idx]=value;
      return;
    }
    if (value == DbLsn.NULL_LSN) {
      setFileNumberOffset(offset,(byte)0);
      setFileOffset(offset,-1);
      return;
    }
    long thisFileNumber=DbLsn.getFileNumber(value);
    if (baseFileNumber == -1) {
      baseFileNumber=thisFileNumber;
      setFileNumberOffset(offset,(byte)0);
    }
 else {
      if (thisFileNumber < baseFileNumber) {
        if (!adjustFileNumbers(thisFileNumber)) {
          mutateToLongArray(idx,value);
          return;
        }
        baseFileNumber=thisFileNumber;
      }
      long fileNumberDifference=thisFileNumber - baseFileNumber;
      if (fileNumberDifference > Byte.MAX_VALUE) {
        mutateToLongArray(idx,value);
        return;
      }
      setFileNumberOffset(offset,(byte)(thisFileNumber - baseFileNumber));
    }
    int fileOffset=(int)DbLsn.getFileOffset(value);
    if (fileOffset > MAX_FILE_OFFSET) {
      mutateToLongArray(idx,value);
      return;
    }
    setFileOffset(offset,fileOffset);
  }
  private void mutateToLongArray(  int idx,  long value){
    int nElts=entryLsnByteArray.length >> 2;
    long[] newArr=new long[nElts];
    for (int i=0; i < nElts; i++) {
      newArr[i]=getLsn(i);
    }
    newArr[idx]=value;
    entryLsnLongArray=newArr;
    entryLsnByteArray=null;
  }
  /** 
 * private void maybeAdjustCapacity(int offset) {
 * if (entryLsnLongArray == null) {
 * int bytesNeeded = offset + BYTES_PER_LSN_ENTRY;
 * int currentBytes = entryLsnByteArray.length;
 * if (currentBytes < bytesNeeded) {
 * int newBytes = bytesNeeded +
 * (GROWTH_INCREMENT * BYTES_PER_LSN_ENTRY);
 * byte[] newArr = new byte[newBytes];
 * System.arraycopy(entryLsnByteArray, 0, newArr, 0,
 * currentBytes);
 * entryLsnByteArray = newArr;
 * for (int i = currentBytes;
 * i < newBytes;
 * i += BYTES_PER_LSN_ENTRY) {
 * setFileNumberOffset(i, (byte) 0);
 * setFileOffset(i, -1);
 * }
 * }
 * } else {
 * int currentEntries = entryLsnLongArray.length;
 * int idx = offset >> 2;
 * if (currentEntries < idx + 1) {
 * int newEntries = idx + GROWTH_INCREMENT;
 * long[] newArr = new long[newEntries];
 * System.arraycopy(entryLsnLongArray, 0, newArr, 0,
 * currentEntries);
 * entryLsnLongArray = newArr;
 * for (int i = currentEntries; i < newEntries; i++) {
 * entryLsnLongArray[i] = DbLsn.NULL_LSN;
 * }
 * }
 * }
 * }
 */
  private boolean adjustFileNumbers(  long newBaseFileNumber){
    long oldBaseFileNumber=baseFileNumber;
    for (int i=0; i < entryLsnByteArray.length; i+=BYTES_PER_LSN_ENTRY) {
      if (getFileOffset(i) == -1) {
        continue;
      }
      long curEntryFileNumber=oldBaseFileNumber + getFileNumberOffset(i);
      long newCurEntryFileNumberOffset=(curEntryFileNumber - newBaseFileNumber);
      if (newCurEntryFileNumberOffset > Byte.MAX_VALUE) {
        long undoOffset=oldBaseFileNumber - newBaseFileNumber;
        for (int j=i - BYTES_PER_LSN_ENTRY; j >= 0; j-=BYTES_PER_LSN_ENTRY) {
          if (getFileOffset(j) == -1) {
            continue;
          }
          setFileNumberOffset(j,(byte)(getFileNumberOffset(j) - undoOffset));
        }
        return false;
      }
      setFileNumberOffset(i,(byte)newCurEntryFileNumberOffset);
    }
    return true;
  }
  private void setFileNumberOffset(  int offset,  byte fileNumberOffset){
    entryLsnByteArray[offset]=fileNumberOffset;
  }
  private byte getFileNumberOffset(  int offset){
    return entryLsnByteArray[offset];
  }
  private void setFileOffset(  int offset,  int fileOffset){
    put3ByteInt(offset + 1,fileOffset);
  }
  private int getFileOffset(  int offset){
    return get3ByteInt(offset + 1);
  }
  private void put3ByteInt(  int offset,  int value){
    entryLsnByteArray[offset++]=(byte)(value >>> 0);
    entryLsnByteArray[offset++]=(byte)(value >>> 8);
    entryLsnByteArray[offset]=(byte)(value >>> 16);
  }
  private int get3ByteInt(  int offset){
    int ret=(entryLsnByteArray[offset++] & 0xFF) << 0;
    ret+=(entryLsnByteArray[offset++] & 0xFF) << 8;
    ret+=(entryLsnByteArray[offset] & 0xFF) << 16;
    if (ret == THREE_BYTE_NEGATIVE_ONE) {
      ret=-1;
    }
    return ret;
  }
  /** 
 * @return true if the idx'th entry has been deleted, although the
 * transaction that performed the deletion may not be committed.
 */
  public boolean isEntryPendingDeleted(  int idx){
    return ((entryStates[idx] & PENDING_DELETED_BIT) != 0);
  }
  /** 
 * Set pendingDeleted to true.
 */
  public void setPendingDeleted(  int idx){
    entryStates[idx]|=PENDING_DELETED_BIT;
    entryStates[idx]|=DIRTY_BIT;
  }
  /** 
 * Set pendingDeleted to false.
 */
  public void clearPendingDeleted(  int idx){
    entryStates[idx]&=CLEAR_PENDING_DELETED_BIT;
    entryStates[idx]|=DIRTY_BIT;
  }
  /** 
 * @return true if the idx'th entry is deleted for sure.  If a transaction
 * performed the deletion, it has been committed.
 */
  public boolean isEntryKnownDeleted(  int idx){
    return ((entryStates[idx] & KNOWN_DELETED_BIT) != 0);
  }
  /** 
 * Set knownDeleted to true.
 */
  void setKnownDeleted(  int idx){
    entryStates[idx]|=KNOWN_DELETED_BIT;
    entryStates[idx]|=DIRTY_BIT;
  }
  /** 
 * Set knownDeleted to false.
 */
  void clearKnownDeleted(  int idx){
    entryStates[idx]&=CLEAR_KNOWN_DELETED_BIT;
    entryStates[idx]|=DIRTY_BIT;
  }
  /** 
 * @return true if the object is dirty.
 */
  boolean isDirty(  int idx){
    return ((entryStates[idx] & DIRTY_BIT) != 0);
  }
  /** 
 * @return the number of entries in this node.
 */
  public int getNEntries(){
    return nEntries;
  }
  /** 
 * Returns true if the given state is known deleted.
 */
  static boolean isStateKnownDeleted(  byte state){
    return ((state & KNOWN_DELETED_BIT) != 0);
  }
  /** 
 * Returns true if the given state is known deleted.
 */
  static boolean isStatePendingDeleted(  byte state){
    return ((state & KNOWN_DELETED_BIT) != 0);
  }
  /** 
 * @return the maximum number of entries in this node.
 */
  int getMaxEntries(){
    return entryTargets.length;
  }
  /** 
 * Returns the target of the idx'th entry or null if a pendingDeleted or
 * knownDeleted entry has been cleaned.  Note that null can only be
 * returned for a slot that could contain a deleted LN, not other node
 * types and not a DupCountLN since DupCountLNs are never deleted.  Null is
 * also returned for a KnownDeleted slot with a NULL_LSN.
 * @return the target node or null.
 */
  public Node fetchTarget(  int idx) throws DatabaseException {
    if (entryTargets[idx] == null) {
      long lsn=getLsn(idx);
      if (lsn == DbLsn.NULL_LSN) {
        if (!isEntryKnownDeleted(idx)) {
          throw new DatabaseException(makeFetchErrorMsg("NULL_LSN without KnownDeleted",this,lsn,entryStates[idx]));
        }
      }
 else {
        try {
          EnvironmentImpl env=databaseImpl.getDbEnvironment();
          Node node=(Node)env.getLogManager().get(lsn);
          node.postFetchInit(databaseImpl,lsn);
          entryTargets[idx]=node;
          this.hook638(node);
        }
 catch (        LogFileNotFoundException LNFE) {
          if (!isEntryKnownDeleted(idx) && !isEntryPendingDeleted(idx)) {
            throw new DatabaseException(makeFetchErrorMsg(LNFE.toString(),this,lsn,entryStates[idx]));
          }
        }
catch (        Exception e) {
          throw new DatabaseException(makeFetchErrorMsg(e.toString(),this,lsn,entryStates[idx]),e);
        }
      }
    }
    return entryTargets[idx];
  }
  static String makeFetchErrorMsg(  String msg,  IN in,  long lsn,  byte state){
    StringBuffer sb=new StringBuffer();
    sb.append("fetchTarget of ");
    if (lsn == DbLsn.NULL_LSN) {
      sb.append("null lsn");
    }
 else {
      sb.append(DbLsn.getNoFormatString(lsn));
    }
    if (in != null) {
      sb.append(" IN=").append(in.getNodeId());
    }
    sb.append(" state=").append(state);
    sb.append(" ").append(msg);
    return sb.toString();
  }
  /** 
 * Set the idx'th entry of this node.
 */
  public void setEntry(  int idx,  Node target,  byte[] keyVal,  long lsn,  byte state){
    new IN_setEntry(this,idx,target,keyVal,lsn,state).execute();
  }
  /** 
 * Update the idx'th entry of this node.
 * Note: does not dirty the node.
 */
  public void updateEntry(  int idx,  Node node){
    new IN_updateEntry(this,idx,node).execute();
  }
  /** 
 * Update the idx'th entry of this node.
 */
  public void updateEntry(  int idx,  Node node,  long lsn){
    new IN_updateEntry2(this,idx,node,lsn).execute();
  }
  /** 
 * Update the idx'th entry of this node.
 */
  public void updateEntry(  int idx,  Node node,  long lsn,  byte[] key){
    new IN_updateEntry3(this,idx,node,lsn,key).execute();
  }
  /** 
 * Update the idx'th entry of this node.
 */
  public void updateEntry(  int idx,  long lsn){
    setLsn(idx,lsn);
    setDirty(true);
  }
  /** 
 * Update the idx'th entry of this node.
 */
  public void updateEntry(  int idx,  long lsn,  byte state){
    setLsn(idx,lsn);
    entryStates[idx]=state;
    setDirty(true);
  }
  /** 
 * Update the idx'th entry of this node. This flavor is used when the
 * target LN is being modified, by an operation like a delete or update. We
 * don't have to check whether the LSN has been nulled or not, because we
 * know an LSN existed before. Also, the modification of the target is done
 * in the caller, so instead of passing in the old and new nodes, we pass
 * in the old and new node sizes.
 */
  public void updateEntry(  int idx,  long lsn,  long oldLNSize,  long newLNSize){
    setLsn(idx,lsn);
    setDirty(true);
  }
  /** 
 * Update the idx'th entry of this node.  Only update the key if the new
 * key is less than the existing key.
 */
  private void updateEntryCompareKey(  int idx,  Node node,  long lsn,  byte[] key){
    new IN_updateEntryCompareKey(this,idx,node,lsn,key).execute();
  }
  /** 
 * Returns whether the given key is greater than or equal to the first key
 * in the IN and less than or equal to the last key in the IN.  This method
 * is used to determine whether a key to be inserted belongs in this IN,
 * without doing a tree search.  If false is returned it is still possible
 * that the key belongs in this IN, but a tree search must be performed to
 * find out.
 */
  public boolean isKeyInBounds(  byte[] keyVal){
    if (nEntries < 2) {
      return false;
    }
    Comparator userCompareToFcn=getKeyComparator();
    int cmp;
    byte[] myKey;
    myKey=entryKeyVals[0];
    cmp=Key.compareKeys(keyVal,myKey,userCompareToFcn);
    if (cmp < 0) {
      return false;
    }
    myKey=entryKeyVals[nEntries - 1];
    cmp=Key.compareKeys(keyVal,myKey,userCompareToFcn);
    if (cmp > 0) {
      return false;
    }
    return true;
  }
  /** 
 * Find the entry in this IN for which key arg is >= the key.
 * Currently uses a binary search, but eventually, this may use binary or
 * linear search depending on key size, number of entries, etc.
 * Note that the 0'th entry's key is treated specially in an IN.  It always
 * compares lower than any other key.
 * This is public so that DbCursorTest can access it.
 * @param key - the key to search for.
 * @param indicateIfDuplicate - true if EXACT_MATCH should
 * be or'd onto the return value if key is already present in this node.
 * @param exact - true if an exact match must be found.
 * @return offset for the entry that has a key >= the arg.  0 if key
 * is less than the 1st entry.  -1 if exact is true and no exact match
 * is found.  If indicateIfDuplicate is true and an exact match was found
 * then EXACT_MATCH is or'd onto the return value.
 */
  public int findEntry(  byte[] key,  boolean indicateIfDuplicate,  boolean exact){
    int high=nEntries - 1;
    int low=0;
    int middle=0;
    Comparator userCompareToFcn=getKeyComparator();
    boolean entryZeroSpecialCompare=entryZeroKeyComparesLow() && !exact && !indicateIfDuplicate;
    assert nEntries >= 0;
    while (low <= high) {
      middle=(high + low) / 2;
      int s;
      byte[] middleKey=null;
      if (middle == 0 && entryZeroSpecialCompare) {
        s=1;
      }
 else {
        middleKey=entryKeyVals[middle];
        s=Key.compareKeys(key,middleKey,userCompareToFcn);
      }
      if (s < 0) {
        high=middle - 1;
      }
 else       if (s > 0) {
        low=middle + 1;
      }
 else {
        int ret;
        if (indicateIfDuplicate) {
          ret=middle | EXACT_MATCH;
        }
 else {
          ret=middle;
        }
        if ((ret >= 0) && exact && isEntryKnownDeleted(ret & 0xffff)) {
          return -1;
        }
 else {
          return ret;
        }
      }
    }
    if (exact) {
      return -1;
    }
 else {
      return high;
    }
  }
  /** 
 * Inserts the argument ChildReference into this IN.  Assumes this node is
 * already latched by the caller.
 * @param entry The ChildReference to insert into the IN.
 * @return true if the entry was successfully inserted, false
 * if it was a duplicate.
 * @throws InconsistentNodeException if the node is full
 * (it should have been split earlier).
 */
  public boolean insertEntry(  ChildReference entry) throws DatabaseException {
    return (insertEntry1(entry) & INSERT_SUCCESS) != 0;
  }
  /** 
 * Same as insertEntry except that it returns the index where the dup was
 * found instead of false.  The return value is |'d with either
 * INSERT_SUCCESS or EXACT_MATCH depending on whether the entry was
 * inserted or it was a duplicate, resp.
 * This returns a failure if there's a duplicate match. The caller must do
 * the processing to check if the entry is actually deleted and can be
 * overwritten. This is foisted upon the caller rather than handled in this
 * object because there may be some latch releasing/retaking in order to
 * check a child LN.
 * Inserts the argument ChildReference into this IN.  Assumes this node is
 * already latched by the caller.
 * @param entry The ChildReference to insert into the IN.
 * @return either (1) the index of location in the IN where the entry was
 * inserted |'d with INSERT_SUCCESS, or (2) the index of the duplicate in
 * the IN |'d with EXACT_MATCH if the entry was found to be a duplicate.
 * @throws InconsistentNodeException if the node is full (it should have
 * been split earlier).
 */
  public int insertEntry1(  ChildReference entry) throws DatabaseException {
    return new IN_insertEntry1(this,entry).execute();
  }
  /** 
 * Deletes the ChildReference with the key arg from this IN.  Assumes this
 * node is already latched by the caller.
 * This seems to only be used by INTest.
 * @param key The key of the reference to delete from the IN.
 * @param maybeValidate true if assert validation should occur prior to
 * delete.  Set this to false during recovery.
 * @return true if the entry was successfully deleted, false if it was not
 * found.
 */
  boolean deleteEntry(  byte[] key,  boolean maybeValidate) throws DatabaseException {
    if (nEntries == 0) {
      return false;
    }
    int index=findEntry(key,false,true);
    if (index < 0) {
      return false;
    }
    return deleteEntry(index,maybeValidate);
  }
  /** 
 * Deletes the ChildReference at index from this IN.  Assumes this node is
 * already latched by the caller.
 * @param index The index of the entry to delete from the IN.
 * @param maybeValidate true if asserts are enabled.
 * @return true if the entry was successfully deleted, false if it was not
 * found.
 */
  public boolean deleteEntry(  int index,  boolean maybeValidate) throws DatabaseException {
    return new IN_deleteEntry(this,index,maybeValidate).execute();
  }
  /** 
 * Do nothing since INs don't support deltas.
 */
  public void setProhibitNextDelta(){
  }
  public boolean compress(  BINReference binRef,  boolean canFetch) throws DatabaseException {
    return false;
  }
  public boolean isCompressible(){
    return false;
  }
  boolean validateSubtreeBeforeDelete(  int index) throws DatabaseException {
    return new IN_validateSubtreeBeforeDelete(this,index).execute();
  }
  /** 
 * Return true if this node needs splitting.  For the moment, needing to be
 * split is defined by there being no free entries available.
 */
  public boolean needsSplitting(){
    if ((entryTargets.length - nEntries) < 1) {
      return true;
    }
 else {
      return false;
    }
  }
  /** 
 * Indicates whether whether entry 0's key is "special" in that it always
 * compares less than any other key.  BIN's don't have the special key, but
 * IN's do.
 */
  boolean entryZeroKeyComparesLow(){
    return true;
  }
  /** 
 * Split this into two nodes.  Parent IN is passed in parent and should be
 * latched by the caller.
 * childIndex is the index in parent of where "this" can be found.
 * @return lsn of the newly logged parent
 */
  void split(  IN parent,  int childIndex,  int maxEntries) throws DatabaseException {
    splitInternal(parent,childIndex,maxEntries,-1);
  }
  protected void splitInternal(  IN parent,  int childIndex,  int maxEntries,  int splitIndex) throws DatabaseException {
    new IN_splitInternal(this,parent,childIndex,maxEntries,splitIndex).execute();
  }
  /** 
 * Called when we know we are about to split on behalf of a key that is the
 * minimum (leftSide) or maximum (!leftSide) of this node.  This is
 * achieved by just forcing the split to occur either one element in from
 * the left or the right (i.e. splitIndex is 1 or nEntries - 1).
 */
  void splitSpecial(  IN parent,  int parentIndex,  int maxEntriesPerNode,  byte[] key,  boolean leftSide) throws DatabaseException {
    int index=findEntry(key,false,false);
    if (leftSide && index == 0) {
      splitInternal(parent,parentIndex,maxEntriesPerNode,1);
    }
 else     if (!leftSide && index == (nEntries - 1)) {
      splitInternal(parent,parentIndex,maxEntriesPerNode,nEntries - 1);
    }
 else {
      split(parent,parentIndex,maxEntriesPerNode);
    }
  }
  void adjustCursors(  IN newSibling,  int newSiblingLow,  int newSiblingHigh){
  }
  void adjustCursorsForInsert(  int insertIndex){
  }
  /** 
 * Return the relevant user defined comparison function for this type of
 * node.  For IN's and BIN's, this is the BTree Comparison function.
 */
  public Comparator getKeyComparator(){
    return databaseImpl.getBtreeComparator();
  }
  /** 
 * Shift entries to the right starting with (and including) the entry at
 * index. Caller is responsible for incrementing nEntries.
 * @param index - The position to start shifting from.
 */
  private void shiftEntriesRight(  int index){
    for (int i=nEntries; i > index; i--) {
      setEntryInternal(i - 1,i);
    }
    clearEntry(index);
    setDirty(true);
  }
  /** 
 * Shift entries starting at the byHowMuch'th element to the left, thus
 * removing the first byHowMuch'th elements of the entries array.  This
 * always starts at the 0th entry.  Caller is responsible for decrementing
 * nEntries.
 * @param byHowMuch - The number of entries to remove from the left side
 * of the entries array.
 */
  private void shiftEntriesLeft(  int byHowMuch){
    for (int i=0; i < nEntries - byHowMuch; i++) {
      setEntryInternal(i + byHowMuch,i);
    }
    for (int i=nEntries - byHowMuch; i < nEntries; i++) {
      clearEntry(i);
    }
    setDirty(true);
  }
  /** 
 * Check that the IN is in a valid state.  For now, validity means that the
 * keys are in sorted order and that there are more than 0 entries.
 * maxKey, if non-null specifies that all keys in this node must be less
 * than maxKey.
 */
  public void verify(  byte[] maxKey) throws DatabaseException {
    new IN_verify(this,maxKey).execute();
  }
  /** 
 * Add self and children to this in-memory IN list. Called by recovery, can
 * run with no latching.
 */
  void rebuildINList(  INList inList) throws DatabaseException {
    inList.add(this);
    for (int i=0; i < nEntries; i++) {
      Node n=getTarget(i);
      if (n != null) {
        n.rebuildINList(inList);
      }
    }
  }
  /** 
 * Remove self and children from the in-memory IN list. The INList latch is
 * already held before this is called.  Also count removed nodes as
 * obsolete.
 */
  void accountForSubtreeRemoval(  INList inList,  UtilizationTracker tracker) throws DatabaseException {
    if (nEntries > 1) {
      throw new DatabaseException("Found non-deletable IN " + getNodeId() + " while flushing INList. nEntries = "+ nEntries);
    }
    inList.removeLatchAlreadyHeld(this);
    if (lastFullVersion != DbLsn.NULL_LSN) {
      tracker.countObsoleteNode(lastFullVersion,getLogType());
    }
    for (int i=0; i < nEntries; i++) {
      Node n=fetchTarget(i);
      if (n != null) {
        n.accountForSubtreeRemoval(inList,tracker);
      }
    }
  }
  /** 
 * Check if this node fits the qualifications for being part of a deletable
 * subtree. It can only have one IN child and no LN children.
 */
  boolean isValidForDelete() throws DatabaseException {
    return new IN_isValidForDelete(this).execute();
  }
  /** 
 * See if you are the parent of this child. If not, find a child of your's
 * that may be the parent, and return it. If there are no possiblities,
 * return null. Note that the keys of the target are passed in so we don't
 * have to latch the target to look at them. Also, this node is latched
 * upon entry.
 * @param doFetch If true, fetch the child in the pursuit of this search.
 * If false, give up if the child is not resident. In that case, we have
 * a potential ancestor, but are not sure if this is the parent.
 */
  void findParent(  Tree.SearchType searchType,  long targetNodeId,  boolean targetContainsDuplicates,  boolean targetIsRoot,  byte[] targetMainTreeKey,  byte[] targetDupTreeKey,  SearchResult result,  boolean requireExactMatch,  boolean updateGeneration,  int targetLevel,  List trackingList,  boolean doFetch) throws DatabaseException {
    if (getNodeId() == targetNodeId) {
      this.hook620();
      result.exactParentFound=false;
      result.keepSearching=false;
      result.parent=null;
      return;
    }
    if (getNEntries() == 0) {
      result.keepSearching=false;
      result.exactParentFound=false;
      if (requireExactMatch) {
        this.hook621();
        result.parent=null;
      }
 else {
        result.parent=this;
        result.index=-1;
      }
      return;
    }
 else {
      if (searchType == Tree.SearchType.NORMAL) {
        result.index=findEntry(selectKey(targetMainTreeKey,targetDupTreeKey),false,false);
      }
 else       if (searchType == Tree.SearchType.LEFT) {
        result.index=0;
      }
 else       if (searchType == Tree.SearchType.RIGHT) {
        result.index=nEntries - 1;
      }
 else {
        throw new IllegalArgumentException("Invalid value of searchType: " + searchType);
      }
      if (result.index < 0) {
        result.keepSearching=false;
        result.exactParentFound=false;
        if (requireExactMatch) {
          this.hook622();
          result.parent=null;
        }
 else {
          result.parent=this;
        }
        return;
      }
      Node child=null;
      boolean isDeleted=false;
      if (isEntryKnownDeleted(result.index)) {
        isDeleted=true;
      }
 else       if (doFetch) {
        child=fetchTarget(result.index);
        if (child == null) {
          isDeleted=true;
        }
      }
 else {
        child=getTarget(result.index);
      }
      if (isDeleted) {
        result.exactParentFound=false;
        result.keepSearching=false;
        if (requireExactMatch) {
          result.parent=null;
          this.hook623();
        }
 else {
          result.parent=this;
        }
        return;
      }
      if (targetLevel >= 0 && level == targetLevel + 1) {
        result.exactParentFound=true;
        result.parent=this;
        result.keepSearching=false;
        return;
      }
      if (child == null) {
        assert !doFetch;
        result.keepSearching=false;
        result.exactParentFound=false;
        result.parent=this;
        result.childNotResident=true;
        return;
      }
      long childLsn=getLsn(result.index);
      if (child.isSoughtNode(targetNodeId,updateGeneration)) {
        result.exactParentFound=true;
        result.parent=this;
        result.keepSearching=false;
        return;
      }
 else {
        descendOnParentSearch(result,targetContainsDuplicates,targetIsRoot,targetNodeId,child,requireExactMatch);
        if (trackingList != null) {
          if ((result.parent != this) && (result.parent != null)) {
            trackingList.add(new TrackingInfo(childLsn,child.getNodeId()));
          }
        }
        return;
      }
    }
  }
  protected void descendOnParentSearch(  SearchResult result,  boolean targetContainsDuplicates,  boolean targetIsRoot,  long targetNodeId,  Node child,  boolean requireExactMatch) throws DatabaseException {
    if (child.canBeAncestor(targetContainsDuplicates)) {
      this.hook624();
      result.parent=(IN)child;
    }
 else {
      this.hook625(child);
      result.exactParentFound=false;
      result.keepSearching=false;
      if (requireExactMatch) {
        this.hook626();
        result.parent=null;
      }
 else {
        result.parent=this;
      }
    }
  }
  protected boolean isSoughtNode(  long nid,  boolean updateGeneration) throws DatabaseException {
    latch(updateGeneration);
    if (getNodeId() == nid) {
      this.hook627();
      return true;
    }
 else {
      return false;
    }
  }
  protected boolean canBeAncestor(  boolean targetContainsDuplicates){
    return true;
  }
  /** 
 * Returns whether any resident children are not LNs (are INs).
 */
  boolean hasNonLNChildren(){
    return hasResidentChildren();
  }
  /** 
 * Returns whether any child is non-null.
 */
  private boolean hasResidentChildren(){
    for (int i=0; i < getNEntries(); i++) {
      if (getTarget(i) != null) {
        return true;
      }
    }
    return false;
  }
  void accumulateStats(  TreeWalkerStatsAccumulator acc){
    acc.processIN(this,new Long(getNodeId()),getLevel());
  }
  /** 
 * Log this IN and clear the dirty flag.
 */
  public long log(  LogManager logManager) throws DatabaseException {
    return logInternal(logManager,false,false,false,null);
  }
  /** 
 * Log this IN and clear the dirty flag.
 */
  public long log(  LogManager logManager,  boolean allowDeltas,  boolean proactiveMigration) throws DatabaseException {
    return logInternal(logManager,allowDeltas,false,proactiveMigration,null);
  }
  /** 
 * Log this node provisionally and clear the dirty flag.
 * @param item object to be logged
 * @return LSN of the new log entry
 */
  public long logProvisional(  LogManager logManager,  IN parent) throws DatabaseException {
    return logInternal(logManager,false,true,false,parent);
  }
  /** 
 * Log this node with all available options.
 */
  public long log(  LogManager logManager,  boolean allowDeltas,  boolean isProvisional,  boolean proactiveMigration,  IN parent) throws DatabaseException {
    return logInternal(logManager,allowDeltas,isProvisional,proactiveMigration,parent);
  }
  /** 
 * Decide how to log this node. INs are always logged in full.  Migration
 * never performed since it only applies to BINs.
 */
  protected long logInternal(  LogManager logManager,  boolean allowDeltas,  boolean isProvisional,  boolean proactiveMigration,  IN parent) throws DatabaseException {
    long lsn=logManager.log(new INLogEntry(this),isProvisional,isProvisional ? DbLsn.NULL_LSN : lastFullVersion);
    if (isProvisional) {
      if (parent != null) {
        parent.trackProvisionalObsolete(this,lastFullVersion,DbLsn.NULL_LSN);
      }
    }
 else {
      flushProvisionalObsolete(logManager);
    }
    setLastFullLsn(lsn);
    setDirty(false);
    return lsn;
  }
  /** 
 * Adds the given obsolete LSNs and any tracked obsolete LSNs for the given
 * child IN to this IN's tracking list.  This method is called to track
 * obsolete LSNs when a child IN is logged provisionally.  Such LSNs cannot
 * be considered obsolete until an ancestor IN is logged non-provisionally.
 */
  void trackProvisionalObsolete(  IN child,  long obsoleteLsn1,  long obsoleteLsn2){
    new IN_trackProvisionalObsolete(this,child,obsoleteLsn1,obsoleteLsn2).execute();
  }
  /** 
 * Adds the provisional obsolete tracking information in this node to the
 * live tracker.  This method is called when this node is logged
 * non-provisionally.
 */
  void flushProvisionalObsolete(  LogManager logManager) throws DatabaseException {
    new IN_flushProvisionalObsolete(this,logManager).execute();
  }
  /** 
 * @see LoggableObject#getLogType
 */
  public LogEntryType getLogType(){
    return LogEntryType.LOG_IN;
  }
  /** 
 * @see LoggableObject#getLogSize
 */
  public int getLogSize(){
    int size=super.getLogSize();
    size+=LogUtils.getByteArrayLogSize(identifierKey);
    size+=LogUtils.getBooleanLogSize();
    size+=LogUtils.INT_BYTES;
    size+=LogUtils.INT_BYTES;
    size+=LogUtils.INT_BYTES;
    size+=LogUtils.getBooleanLogSize();
    boolean compactLsnsRep=(entryLsnLongArray == null);
    if (compactLsnsRep) {
      size+=LogUtils.INT_BYTES;
    }
    for (int i=0; i < nEntries; i++) {
      size+=LogUtils.getByteArrayLogSize(entryKeyVals[i]) + (compactLsnsRep ? LogUtils.INT_BYTES : LogUtils.getLongLogSize()) + 1;
    }
    return size;
  }
  /** 
 * @see LoggableObject#writeToLog
 */
  public void writeToLog(  ByteBuffer logBuffer){
    super.writeToLog(logBuffer);
    LogUtils.writeByteArray(logBuffer,identifierKey);
    LogUtils.writeBoolean(logBuffer,isRoot);
    LogUtils.writeInt(logBuffer,nEntries);
    LogUtils.writeInt(logBuffer,level);
    LogUtils.writeInt(logBuffer,entryTargets.length);
    boolean compactLsnsRep=(entryLsnLongArray == null);
    LogUtils.writeBoolean(logBuffer,compactLsnsRep);
    if (compactLsnsRep) {
      LogUtils.writeInt(logBuffer,(int)baseFileNumber);
    }
    for (int i=0; i < nEntries; i++) {
      LogUtils.writeByteArray(logBuffer,entryKeyVals[i]);
      assert !(getLsn(i) == DbLsn.NULL_LSN && (entryStates[i] & KNOWN_DELETED_BIT) == 0);
      if (compactLsnsRep) {
        int offset=i << 2;
        int fileOffset=getFileOffset(offset);
        logBuffer.put(getFileNumberOffset(offset));
        logBuffer.put((byte)((fileOffset >>> 0) & 0xff));
        logBuffer.put((byte)((fileOffset >>> 8) & 0xff));
        logBuffer.put((byte)((fileOffset >>> 16) & 0xff));
      }
 else {
        LogUtils.writeLong(logBuffer,entryLsnLongArray[i]);
      }
      logBuffer.put(entryStates[i]);
      entryStates[i]&=CLEAR_DIRTY_BIT;
    }
  }
  /** 
 * @see LogReadable#readFromLog
 */
  public void readFromLog(  ByteBuffer itemBuffer,  byte entryTypeVersion) throws LogException {
    super.readFromLog(itemBuffer,entryTypeVersion);
    identifierKey=LogUtils.readByteArray(itemBuffer);
    isRoot=LogUtils.readBoolean(itemBuffer);
    nEntries=LogUtils.readInt(itemBuffer);
    level=LogUtils.readInt(itemBuffer);
    int length=LogUtils.readInt(itemBuffer);
    entryTargets=new Node[length];
    entryKeyVals=new byte[length][];
    baseFileNumber=-1;
    long storedBaseFileNumber=-1;
    entryLsnByteArray=new byte[length << 2];
    entryLsnLongArray=null;
    entryStates=new byte[length];
    boolean compactLsnsRep=false;
    if (entryTypeVersion > 1) {
      compactLsnsRep=LogUtils.readBoolean(itemBuffer);
      if (compactLsnsRep) {
        baseFileNumber=LogUtils.readInt(itemBuffer) & 0xffffffff;
        storedBaseFileNumber=baseFileNumber;
      }
    }
    for (int i=0; i < nEntries; i++) {
      entryKeyVals[i]=LogUtils.readByteArray(itemBuffer);
      long lsn;
      if (compactLsnsRep) {
        byte fileNumberOffset=itemBuffer.get();
        int fileOffset=(itemBuffer.get() & 0xff);
        fileOffset|=((itemBuffer.get() & 0xff) << 8);
        fileOffset|=((itemBuffer.get() & 0xff) << 16);
        if (fileOffset == THREE_BYTE_NEGATIVE_ONE) {
          lsn=DbLsn.NULL_LSN;
        }
 else {
          lsn=DbLsn.makeLsn(storedBaseFileNumber + fileNumberOffset,fileOffset);
        }
      }
 else {
        lsn=LogUtils.readLong(itemBuffer);
      }
      setLsnElement(i,lsn);
      byte entryState=itemBuffer.get();
      entryState&=CLEAR_DIRTY_BIT;
      entryState&=CLEAR_MIGRATE_BIT;
      if (lsn == DbLsn.NULL_LSN) {
        entryState|=KNOWN_DELETED_BIT;
      }
      entryStates[i]=entryState;
    }
  }
  /** 
 * @see LogReadable#dumpLog
 */
  public void dumpLog(  StringBuffer sb,  boolean verbose){
    sb.append(beginTag());
    super.dumpLog(sb,verbose);
    sb.append(Key.dumpString(identifierKey,0));
    sb.append("<isRoot val=\"");
    sb.append(isRoot);
    sb.append("\"/>");
    sb.append("<level val=\"");
    sb.append(Integer.toHexString(level));
    sb.append("\"/>");
    sb.append("<entries numEntries=\"");
    sb.append(nEntries);
    sb.append("\" length=\"");
    sb.append(entryTargets.length);
    boolean compactLsnsRep=(entryLsnLongArray == null);
    if (compactLsnsRep) {
      sb.append("\" baseFileNumber=\"");
      sb.append(baseFileNumber);
    }
    sb.append("\">");
    if (verbose) {
      for (int i=0; i < nEntries; i++) {
        sb.append("<ref knownDeleted=\"").append(isEntryKnownDeleted(i));
        sb.append("\" pendingDeleted=\"").append(isEntryPendingDeleted(i));
        sb.append("\">");
        sb.append(Key.dumpString(entryKeyVals[i],0));
        sb.append(DbLsn.toString(getLsn(i)));
        sb.append("</ref>");
      }
    }
    sb.append("</entries>");
    dumpLogAdditional(sb);
    sb.append(endTag());
  }
  /** 
 * @see LogReadable#logEntryIsTransactional.
 */
  public boolean logEntryIsTransactional(){
    return false;
  }
  /** 
 * @see LogReadable#getTransactionId
 */
  public long getTransactionId(){
    return 0;
  }
  /** 
 * Allows subclasses to add additional fields before the end tag. If they
 * just overload dumpLog, the xml isn't nested.
 */
  protected void dumpLogAdditional(  StringBuffer sb){
  }
  public String beginTag(){
    return BEGIN_TAG;
  }
  public String endTag(){
    return END_TAG;
  }
  void dumpKeys() throws DatabaseException {
    for (int i=0; i < nEntries; i++) {
      System.out.println(Key.dumpString(entryKeyVals[i],0));
    }
  }
  /** 
 * For unit test support:
 * @return a string that dumps information about this IN, without
 */
  public String dumpString(  int nSpaces,  boolean dumpTags){
    StringBuffer sb=new StringBuffer();
    if (dumpTags) {
      sb.append(TreeUtils.indent(nSpaces));
      sb.append(beginTag());
      sb.append('\n');
    }
    sb.append(super.dumpString(nSpaces + 2,true));
    sb.append('\n');
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("<idkey>");
    sb.append(identifierKey == null ? "" : Key.dumpString(identifierKey,0));
    sb.append("</idkey>");
    sb.append('\n');
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("<dirty val=\"").append(dirty).append("\"/>");
    sb.append('\n');
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("<generation val=\"").append(generation).append("\"/>");
    sb.append('\n');
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("<level val=\"");
    sb.append(Integer.toHexString(level)).append("\"/>");
    sb.append('\n');
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("<isRoot val=\"").append(isRoot).append("\"/>");
    sb.append('\n');
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("<entries nEntries=\"");
    sb.append(nEntries);
    sb.append("\">");
    sb.append('\n');
    for (int i=0; i < nEntries; i++) {
      sb.append(TreeUtils.indent(nSpaces + 4));
      sb.append("<entry id=\"" + i + "\">");
      sb.append('\n');
      if (getLsn(i) == DbLsn.NULL_LSN) {
        sb.append(TreeUtils.indent(nSpaces + 6));
        sb.append("<lsn/>");
      }
 else {
        sb.append(DbLsn.dumpString(getLsn(i),nSpaces + 6));
      }
      sb.append('\n');
      if (entryKeyVals[i] == null) {
        sb.append(TreeUtils.indent(nSpaces + 6));
        sb.append("<key/>");
      }
 else {
        sb.append(Key.dumpString(entryKeyVals[i],(nSpaces + 6)));
      }
      sb.append('\n');
      if (entryTargets[i] == null) {
        sb.append(TreeUtils.indent(nSpaces + 6));
        sb.append("<target/>");
      }
 else {
        sb.append(entryTargets[i].dumpString(nSpaces + 6,true));
      }
      sb.append('\n');
      sb.append(TreeUtils.indent(nSpaces + 6));
      dumpDeletedState(sb,getState(i));
      sb.append("<dirty val=\"").append(isDirty(i)).append("\"/>");
      sb.append('\n');
      sb.append(TreeUtils.indent(nSpaces + 4));
      sb.append("</entry>");
      sb.append('\n');
    }
    sb.append(TreeUtils.indent(nSpaces + 2));
    sb.append("</entries>");
    sb.append('\n');
    if (dumpTags) {
      sb.append(TreeUtils.indent(nSpaces));
      sb.append(endTag());
    }
    return sb.toString();
  }
  /** 
 * Utility method for output of knownDeleted and pendingDelete.
 */
  static void dumpDeletedState(  StringBuffer sb,  byte state){
    sb.append("<knownDeleted val=\"");
    sb.append(isStateKnownDeleted(state)).append("\"/>");
    sb.append("<pendingDeleted val=\"");
    sb.append(isStatePendingDeleted(state)).append("\"/>");
  }
  public String toString(){
    return dumpString(0,true);
  }
  public String shortClassName(){
    return "IN";
  }
@MethodObject static class IN_setLsn {
    IN_setLsn(    IN _this,    int idx,    long lsn){
      this._this=_this;
      this.idx=idx;
      this.lsn=lsn;
    }
    void execute(){
      _this.setLsnElement(idx,lsn);
      this.hook639();
      _this.entryStates[idx]|=_this.DIRTY_BIT;
    }
    protected IN _this;
    protected int idx;
    protected long lsn;
    protected int oldSize;
    protected void hook639(){
    }
  }
@MethodObject static class IN_setEntry {
    IN_setEntry(    IN _this,    int idx,    Node target,    byte[] keyVal,    long lsn,    byte state){
      this._this=_this;
      this.idx=idx;
      this.target=target;
      this.keyVal=keyVal;
      this.lsn=lsn;
      this.state=state;
    }
    void execute(){
      newNEntries=idx + 1;
      if (newNEntries > _this.nEntries) {
        _this.nEntries=newNEntries;
        this.hook641();
      }
      _this.entryTargets[idx]=target;
      _this.entryKeyVals[idx]=keyVal;
      _this.setLsnElement(idx,lsn);
      _this.entryStates[idx]=state;
      this.hook640();
      _this.setDirty(true);
    }
    protected IN _this;
    protected int idx;
    protected Node target;
    protected byte[] keyVal;
    protected long lsn;
    protected byte state;
    protected long oldSize;
    protected int newNEntries;
    protected long newSize;
    protected void hook640(){
    }
    protected void hook641(){
    }
  }
@MethodObject static class IN_updateEntry {
    IN_updateEntry(    IN _this,    int idx,    Node node){
      this._this=_this;
      this.idx=idx;
      this.node=node;
    }
    void execute(){
      _this.setTarget(idx,node);
    }
    protected IN _this;
    protected int idx;
    protected Node node;
    protected long oldSize;
    protected long newSize;
  }
@MethodObject static class IN_updateEntry2 {
    IN_updateEntry2(    IN _this,    int idx,    Node node,    long lsn){
      this._this=_this;
      this.idx=idx;
      this.node=node;
      this.lsn=lsn;
    }
    void execute(){
      _this.setLsn(idx,lsn);
      _this.setTarget(idx,node);
      this.hook642();
      _this.setDirty(true);
    }
    protected IN _this;
    protected int idx;
    protected Node node;
    protected long lsn;
    protected long oldSize;
    protected long newSize;
    protected void hook642(){
    }
  }
@MethodObject static class IN_updateEntry3 {
    IN_updateEntry3(    IN _this,    int idx,    Node node,    long lsn,    byte[] key){
      this._this=_this;
      this.idx=idx;
      this.node=node;
      this.lsn=lsn;
      this.key=key;
    }
    void execute(){
      _this.setLsn(idx,lsn);
      _this.setTarget(idx,node);
      _this.setKey(idx,key);
      this.hook643();
      _this.setDirty(true);
    }
    protected IN _this;
    protected int idx;
    protected Node node;
    protected long lsn;
    protected byte[] key;
    protected long oldSize;
    protected long newSize;
    protected void hook643(){
    }
  }
@MethodObject static class IN_updateEntryCompareKey {
    IN_updateEntryCompareKey(    IN _this,    int idx,    Node node,    long lsn,    byte[] key){
      this._this=_this;
      this.idx=idx;
      this.node=node;
      this.lsn=lsn;
      this.key=key;
    }
    void execute(){
      _this.setLsn(idx,lsn);
      _this.setTarget(idx,node);
      existingKey=_this.getKey(idx);
      s=Key.compareKeys(key,existingKey,_this.getKeyComparator());
      if (s < 0) {
        _this.setKey(idx,key);
      }
      this.hook644();
      _this.setDirty(true);
    }
    protected IN _this;
    protected int idx;
    protected Node node;
    protected long lsn;
    protected byte[] key;
    protected long oldSize;
    protected byte[] existingKey;
    protected int s;
    protected long newSize;
    protected void hook644(){
    }
  }
@MethodObject static class IN_insertEntry1 {
    IN_insertEntry1(    IN _this,    ChildReference entry){
      this._this=_this;
      this.entry=entry;
    }
    int execute() throws DatabaseException {
      if (_this.nEntries >= _this.entryTargets.length) {
        _this.compress(null,true);
      }
      if (_this.nEntries < _this.entryTargets.length) {
        key=entry.getKey();
        index=_this.findEntry(key,true,false);
        if (index >= 0 && (index & _this.EXACT_MATCH) != 0) {
          return index;
        }
 else {
          index++;
        }
        if (index < _this.nEntries) {
          this.hook647();
          _this.shiftEntriesRight(index);
          this.hook646();
        }
        _this.entryTargets[index]=entry.getTarget();
        _this.entryKeyVals[index]=entry.getKey();
        _this.setLsnElement(index,entry.getLsn());
        _this.entryStates[index]=entry.getState();
        _this.nEntries++;
        _this.adjustCursorsForInsert(index);
        this.hook645();
        _this.setDirty(true);
        return (index | _this.INSERT_SUCCESS);
      }
 else {
        throw new InconsistentNodeException("Node " + _this.getNodeId() + " should have been split before calling insertEntry");
      }
    }
    protected IN _this;
    protected ChildReference entry;
    protected byte[] key;
    protected int index;
    protected int oldSize;
    protected void hook645() throws DatabaseException {
    }
    protected void hook646() throws DatabaseException {
    }
    protected void hook647() throws DatabaseException {
    }
  }
@MethodObject static class IN_deleteEntry {
    IN_deleteEntry(    IN _this,    int index,    boolean maybeValidate){
      this._this=_this;
      this.index=index;
      this.maybeValidate=maybeValidate;
    }
    boolean execute() throws DatabaseException {
      if (_this.nEntries == 0) {
        return false;
      }
      assert maybeValidate ? _this.validateSubtreeBeforeDelete(index) : true;
      if (index < _this.nEntries) {
        this.hook649();
        for (int i=index; i < _this.nEntries - 1; i++) {
          _this.setEntryInternal(i + 1,i);
        }
        _this.clearEntry(_this.nEntries - 1);
        this.hook648();
        _this.nEntries--;
        _this.setDirty(true);
        _this.setProhibitNextDelta();
        this.hook616();
        return true;
      }
 else {
        return false;
      }
    }
    protected IN _this;
    protected int index;
    protected boolean maybeValidate;
    protected int oldLSNArraySize;
    protected void hook616() throws DatabaseException {
    }
    protected void hook648() throws DatabaseException {
    }
    protected void hook649() throws DatabaseException {
    }
  }
@MethodObject static class IN_validateSubtreeBeforeDelete {
    IN_validateSubtreeBeforeDelete(    IN _this,    int index){
      this._this=_this;
      this.index=index;
    }
    boolean execute() throws DatabaseException {
      try {
        this.hook628();
        throw ReturnHack.returnBoolean;
      }
 catch (      ReturnBoolean r) {
        return r.value;
      }
    }
    protected IN _this;
    protected int index;
    protected boolean needToLatch;
    protected Node child;
    protected void hook628() throws DatabaseException {
      this.hook629();
      if (index >= _this.nEntries) {
        throw new ReturnBoolean(true);
      }
 else {
        child=_this.fetchTarget(index);
        throw new ReturnBoolean(child != null && child.isValidForDelete());
      }
    }
    protected void hook629() throws DatabaseException {
    }
  }
@MethodObject static class IN_splitInternal {
    IN_splitInternal(    IN _this,    IN parent,    int childIndex,    int maxEntries,    int splitIndex){
      this._this=_this;
      this.parent=parent;
      this.childIndex=childIndex;
      this.maxEntries=maxEntries;
      this.splitIndex=splitIndex;
    }
    void execute() throws DatabaseException {
      if (_this.identifierKey == null) {
        throw new InconsistentNodeException("idkey is null");
      }
      idKeyIndex=_this.findEntry(_this.identifierKey,false,false);
      if (splitIndex < 0) {
        splitIndex=_this.nEntries / 2;
      }
{
      }
      newSibling=null;
      if (idKeyIndex < splitIndex) {
        low=splitIndex;
        high=_this.nEntries;
      }
 else {
        low=0;
        high=splitIndex;
      }
      newIdKey=_this.entryKeyVals[low];
      parentLsn=DbLsn.NULL_LSN;
      newSibling=_this.createNewInstance(newIdKey,maxEntries,_this.level);
      this.hook631();
      oldMemorySize=_this.inMemorySize;
      this.hook630();
    }
    protected IN _this;
    protected IN parent;
    protected int childIndex;
    protected int maxEntries;
    protected int splitIndex;
    protected int idKeyIndex;
    protected int low;
    protected int high;
    protected IN newSibling;
    protected byte[] newIdKey;
    protected long parentLsn;
    protected long oldMemorySize;
    protected int toIdx;
    protected boolean deletedEntrySeen;
    protected BINReference binRef;
    protected byte[] thisKey;
    protected int newSiblingNEntries;
    protected EnvironmentImpl env;
    protected LogManager logManager;
    protected INList inMemoryINs;
    protected long newSiblingLsn;
    protected long myNewLsn;
    protected boolean insertOk1;
    protected boolean insertOk2;
    protected long newSize;
    protected void hook617() throws DatabaseException {
    }
    protected void hook630() throws DatabaseException {
      toIdx=0;
      deletedEntrySeen=false;
      binRef=null;
      for (int i=low; i < high; i++) {
        thisKey=_this.entryKeyVals[i];
        if (_this.isEntryPendingDeleted(i)) {
          if (!deletedEntrySeen) {
            deletedEntrySeen=true;
            binRef=new BINReference(newSibling.getNodeId(),_this.databaseImpl.getId(),newIdKey);
          }
          binRef.addDeletedKey(new Key(thisKey));
        }
        newSibling.setEntry(toIdx++,_this.entryTargets[i],thisKey,_this.getLsn(i),_this.entryStates[i]);
        _this.clearEntry(i);
      }
      this.hook636();
      newSiblingNEntries=(high - low);
      if (low == 0) {
        _this.shiftEntriesLeft(newSiblingNEntries);
      }
      newSibling.nEntries=toIdx;
      _this.nEntries-=newSiblingNEntries;
      _this.setDirty(true);
      _this.adjustCursors(newSibling,low,high);
      env=_this.databaseImpl.getDbEnvironment();
      logManager=env.getLogManager();
      inMemoryINs=env.getInMemoryINs();
      newSiblingLsn=newSibling.logProvisional(logManager,parent);
      myNewLsn=_this.logProvisional(logManager,parent);
      if (low == 0) {
        if (childIndex == 0) {
          parent.updateEntryCompareKey(childIndex,newSibling,newSiblingLsn,newIdKey);
        }
 else {
          parent.updateEntry(childIndex,newSibling,newSiblingLsn);
        }
        insertOk1=parent.insertEntry(new ChildReference(_this,_this.entryKeyVals[0],myNewLsn));
        assert insertOk1;
      }
 else {
        if (childIndex == 0) {
          parent.updateEntryCompareKey(childIndex,_this,myNewLsn,_this.entryKeyVals[0]);
        }
 else {
          parent.updateEntry(childIndex,_this,myNewLsn);
        }
        insertOk2=parent.insertEntry(new ChildReference(newSibling,newIdKey,newSiblingLsn));
        assert insertOk2;
      }
      parentLsn=parent.log(logManager);
      if (parent.isRoot()) {
        parent.setDirty(true);
      }
      this.hook650();
      inMemoryINs.add(newSibling);
      this.hook617();
    }
    protected void hook631() throws DatabaseException {
    }
    protected void hook636() throws DatabaseException {
    }
    protected void hook650() throws DatabaseException {
    }
  }
@MethodObject static class IN_verify {
    IN_verify(    IN _this,    byte[] maxKey){
      this._this=_this;
      this.maxKey=maxKey;
    }
    void execute() throws DatabaseException {
      try {
        this.hook632();
        userCompareToFcn=(_this.databaseImpl == null ? null : _this.getKeyComparator());
        key1=null;
        for (int i=1; i < _this.nEntries; i++) {
          key1=_this.entryKeyVals[i];
          key2=_this.entryKeyVals[i - 1];
          s=Key.compareKeys(key1,key2,userCompareToFcn);
          if (s <= 0) {
            throw new InconsistentNodeException("IN " + _this.getNodeId() + " key "+ (i - 1)+ " ("+ Key.dumpString(key2,0)+ ") and "+ i+ " ("+ Key.dumpString(key1,0)+ ") are out of order");
          }
        }
        inconsistent=false;
        if (maxKey != null && key1 != null) {
          if (Key.compareKeys(key1,maxKey,userCompareToFcn) >= 0) {
            inconsistent=true;
          }
        }
        if (inconsistent) {
          throw new InconsistentNodeException("IN " + _this.getNodeId() + " has entry larger than next entry in parent.");
        }
      }
 catch (      DatabaseException DE) {
        DE.printStackTrace(System.out);
      }
 finally {
        this.hook633();
      }
    }
    protected IN _this;
    protected byte[] maxKey;
    protected boolean unlatchThis;
    protected Comparator userCompareToFcn;
    protected byte[] key1;
    protected byte[] key2;
    protected int s;
    protected boolean inconsistent;
    protected void hook632() throws DatabaseException {
    }
    protected void hook633() throws DatabaseException {
    }
  }
@MethodObject static class IN_isValidForDelete {
    IN_isValidForDelete(    IN _this){
      this._this=_this;
    }
    boolean execute() throws DatabaseException {
      try {
        this.hook634();
        throw ReturnHack.returnBoolean;
      }
 catch (      ReturnBoolean r) {
        return r.value;
      }
    }
    protected IN _this;
    protected boolean needToLatch;
    protected Node child;
    protected void hook634() throws DatabaseException {
      this.hook635();
      if (_this.nEntries > 1) {
        throw new ReturnBoolean(false);
      }
 else       if (_this.nEntries == 1) {
        child=_this.fetchTarget(0);
        throw new ReturnBoolean(child != null && child.isValidForDelete());
      }
 else {
        throw new ReturnBoolean(true);
      }
    }
    protected void hook635() throws DatabaseException {
    }
  }
@MethodObject static class IN_trackProvisionalObsolete {
    IN_trackProvisionalObsolete(    IN _this,    IN child,    long obsoleteLsn1,    long obsoleteLsn2){
      this._this=_this;
      this.child=child;
      this.obsoleteLsn1=obsoleteLsn1;
      this.obsoleteLsn2=obsoleteLsn2;
    }
    void execute(){
      memDelta=0;
      if (child.provisionalObsolete != null) {
        this.hook652();
        if (_this.provisionalObsolete != null) {
          _this.provisionalObsolete.addAll(child.provisionalObsolete);
        }
 else {
          _this.provisionalObsolete=child.provisionalObsolete;
        }
        child.provisionalObsolete=null;
        this.hook651();
      }
      if (obsoleteLsn1 != DbLsn.NULL_LSN || obsoleteLsn2 != DbLsn.NULL_LSN) {
        if (_this.provisionalObsolete == null) {
          _this.provisionalObsolete=new ArrayList();
        }
        if (obsoleteLsn1 != DbLsn.NULL_LSN) {
          _this.provisionalObsolete.add(new Long(obsoleteLsn1));
          this.hook653();
        }
        if (obsoleteLsn2 != DbLsn.NULL_LSN) {
          _this.provisionalObsolete.add(new Long(obsoleteLsn2));
          this.hook654();
        }
      }
    }
    protected IN _this;
    protected IN child;
    protected long obsoleteLsn1;
    protected long obsoleteLsn2;
    protected int memDelta;
    protected int childMemDelta;
    protected void hook651(){
    }
    protected void hook652(){
    }
    protected void hook653(){
    }
    protected void hook654(){
    }
  }
@MethodObject static class IN_flushProvisionalObsolete {
    IN_flushProvisionalObsolete(    IN _this,    LogManager logManager){
      this._this=_this;
      this.logManager=logManager;
    }
    void execute() throws DatabaseException {
      if (_this.provisionalObsolete != null) {
        this.hook656();
        logManager.countObsoleteINs(_this.provisionalObsolete);
        _this.provisionalObsolete=null;
        this.hook655();
      }
    }
    protected IN _this;
    protected LogManager logManager;
    protected int memDelta;
    protected void hook655() throws DatabaseException {
    }
    protected void hook656() throws DatabaseException {
    }
  }
  protected void hook618(  EnvironmentImpl env){
  }
  protected void hook619(  boolean updateGeneration) throws DatabaseException {
    if (updateGeneration) {
      setGeneration();
    }
    throw new ReturnBoolean(true);
  }
  protected void hook620() throws DatabaseException {
  }
  protected void hook621() throws DatabaseException {
  }
  protected void hook622() throws DatabaseException {
  }
  protected void hook623() throws DatabaseException {
  }
  protected void hook624() throws DatabaseException {
  }
  protected void hook625(  Node child) throws DatabaseException {
  }
  protected void hook626() throws DatabaseException {
  }
  protected void hook627() throws DatabaseException {
  }
  protected void hook637() throws DatabaseException {
  }
  protected void hook638(  Node node) throws DatabaseException, LogFileNotFoundException, Exception {
  }
}
\00base/com/sleepycat/je/tree/TreeIterator.java:package com.sleepycat.je.tree;
import java.util.Iterator;
import java.util.NoSuchElementException;
import com.sleepycat.je.DatabaseException;
import de.ovgu.cide.jakutil.*;
/** 
 * TreeIterator implements an Iterator over Tree's.  Not protected
 * against insertions like cursors.
 */
public final class TreeIterator implements Iterator {
  private Tree tree;
  private BIN nextBin;
  private int index;
  public TreeIterator(  Tree tree) throws DatabaseException {
    nextBin=(BIN)tree.getFirstNode();
    this.hook755();
    index=-1;
    this.tree=tree;
  }
  public boolean hasNext(){
    boolean ret=false;
    try {
      this.hook756();
      advance();
      ret=(nextBin != null) && (index < nextBin.getNEntries());
    }
 catch (    DatabaseException e) {
    }
 finally {
      this.hook757();
    }
    return ret;
  }
  public Object next(){
    Object ret=null;
    try {
      if (nextBin == null) {
        throw new NoSuchElementException();
      }
      this.hook758();
      ret=nextBin.getKey(index);
    }
 catch (    DatabaseException e) {
    }
 finally {
      this.hook759();
    }
    return ret;
  }
  public void remove(){
    throw new UnsupportedOperationException();
  }
  private void advance() throws DatabaseException {
    while (nextBin != null) {
      if (++index < nextBin.getNEntries()) {
        return;
      }
      nextBin=tree.getNextBin(nextBin,false);
      index=-1;
    }
  }
  protected void hook755() throws DatabaseException {
  }
  protected void hook756() throws DatabaseException {
  }
  protected void hook757(){
  }
  protected void hook758() throws DatabaseException {
  }
  protected void hook759(){
  }
}
\00base/com/sleepycat/je/tree/BIN.java:package com.sleepycat.je.tree;
import java.util.Comparator;
import java.util.Iterator;
import java.util.Set;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.cleaner.Cleaner;
import com.sleepycat.je.dbi.CursorImpl;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.log.LogEntryType;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.log.LoggableObject;
import com.sleepycat.je.txn.BasicLocker;
import com.sleepycat.je.txn.LockGrantType;
import com.sleepycat.je.txn.LockResult;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.TinyHashSet;
import de.ovgu.cide.jakutil.*;
/** 
 * A BIN represents a Bottom Internal Node in the JE tree.
 */
public class BIN extends IN implements LoggableObject {
  private static final String BEGIN_TAG="<bin>";
  private static final String END_TAG="</bin>";
  private TinyHashSet cursorSet;
  private long lastDeltaVersion=DbLsn.NULL_LSN;
  private int numDeltasSinceLastFull;
  private boolean prohibitNextDelta;
  public BIN(){
    cursorSet=new TinyHashSet();
    numDeltasSinceLastFull=0;
    prohibitNextDelta=false;
  }
  public BIN(  DatabaseImpl db,  byte[] identifierKey,  int maxEntriesPerNode,  int level){
    super(db,identifierKey,maxEntriesPerNode,level);
    cursorSet=new TinyHashSet();
    numDeltasSinceLastFull=0;
    prohibitNextDelta=false;
  }
  /** 
 * Create a holder object that encapsulates information about this BIN for
 * the INCompressor.
 */
  public BINReference createReference(){
    return new BINReference(getNodeId(),getDatabase().getId(),getIdentifierKey());
  }
  /** 
 * Create a new BIN. Need this because we can't call newInstance() without
 * getting a 0 for nodeid.
 */
  protected IN createNewInstance(  byte[] identifierKey,  int maxEntries,  int level){
    return new BIN(getDatabase(),identifierKey,maxEntries,level);
  }
  /** 
 * Get the key (dupe or identifier) in child that is used to locate it in
 * 'this' node. For BIN's, the child node has to be a DIN so we use the Dup
 * Key to cross the main-tree/dupe-tree boundary.
 */
  public byte[] getChildKey(  IN child) throws DatabaseException {
    return child.getDupKey();
  }
  /** 
 * @return the log entry type to use for bin delta log entries.
 */
  LogEntryType getBINDeltaType(){
    return LogEntryType.LOG_BIN_DELTA;
  }
  /** 
 * @return location of last logged delta version. If never set, return null.
 */
  public long getLastDeltaVersion(){
    return lastDeltaVersion;
  }
  /** 
 * If cleaned or compressed, must log full version.
 * @Override
 */
  public void setProhibitNextDelta(){
    prohibitNextDelta=true;
  }
  protected void descendOnParentSearch(  SearchResult result,  boolean targetContainsDuplicates,  boolean targetIsRoot,  long targetNodeId,  Node child,  boolean requireExactMatch) throws DatabaseException {
    if (child.canBeAncestor(targetContainsDuplicates)) {
      if (targetContainsDuplicates && targetIsRoot) {
        long childNid=child.getNodeId();
        this.hook603(child);
        result.keepSearching=false;
        if (childNid == targetNodeId) {
          result.exactParentFound=true;
        }
 else {
          result.exactParentFound=false;
        }
        if (requireExactMatch && !result.exactParentFound) {
          result.parent=null;
          this.hook604();
        }
 else {
          result.parent=this;
        }
      }
 else {
        this.hook605();
        result.parent=(IN)child;
      }
    }
 else {
      result.exactParentFound=false;
      result.keepSearching=false;
      if (!requireExactMatch && targetContainsDuplicates) {
        result.parent=this;
      }
 else {
        this.hook606();
        result.parent=null;
      }
    }
  }
  protected boolean canBeAncestor(  boolean targetContainsDuplicates){
    return targetContainsDuplicates;
  }
  /** 
 * @Override
 */
  boolean isEvictionProhibited(){
    return (nCursors() > 0);
  }
  /** 
 * @Override
 */
  boolean hasNonLNChildren(){
    for (int i=0; i < getNEntries(); i++) {
      Node node=getTarget(i);
      if (node != null) {
        if (!(node instanceof LN)) {
          return true;
        }
      }
    }
    return false;
  }
  /** 
 * Indicates whether entry 0's key is "special" in that it always compares
 * less than any other key. BIN's don't have the special key, but IN's do.
 */
  boolean entryZeroKeyComparesLow(){
    return false;
  }
  /** 
 * Mark this entry as deleted, using the delete flag. Only BINS may do this.
 * @param indexindicates target entry
 */
  public void setKnownDeleted(  int index){
    super.setKnownDeleted(index);
    this.hook610(index);
    setMigrate(index,false);
    super.setTarget(index,null);
    setDirty(true);
  }
  /** 
 * Mark this entry as deleted, using the delete flag. Only BINS may do this.
 * Don't null the target field.
 * This is used so that an LN can still be locked by the compressor even if
 * the entry is knownDeleted. See BIN.compress.
 * @param indexindicates target entry
 */
  public void setKnownDeletedLeaveTarget(  int index){
    setMigrate(index,false);
    super.setKnownDeleted(index);
    setDirty(true);
  }
  /** 
 * Clear the known deleted flag. Only BINS may do this.
 * @param indexindicates target entry
 */
  public void clearKnownDeleted(  int index){
    super.clearKnownDeleted(index);
    setDirty(true);
  }
  public Set getCursorSet(){
    return cursorSet.copy();
  }
  /** 
 * Register a cursor with this bin. Caller has this bin already latched.
 * @param cursorCursor to register.
 */
  public void addCursor(  CursorImpl cursor){
    cursorSet.add(cursor);
  }
  /** 
 * Unregister a cursor with this bin. Caller has this bin already latched.
 * @param cursorCursor to unregister.
 */
  public void removeCursor(  CursorImpl cursor){
    cursorSet.remove(cursor);
  }
  /** 
 * @return the number of cursors currently referring to this BIN.
 */
  public int nCursors(){
    return cursorSet.size();
  }
  /** 
 * The following four methods access the correct fields in a cursor
 * depending on whether "this" is a BIN or DBIN. For BIN's, the
 * CursorImpl.index and CursorImpl.bin fields should be used. For DBIN's,
 * the CursorImpl.dupIndex and CursorImpl.dupBin fields should be used.
 */
  BIN getCursorBIN(  CursorImpl cursor){
    return cursor.getBIN();
  }
  BIN getCursorBINToBeRemoved(  CursorImpl cursor){
    return cursor.getBINToBeRemoved();
  }
  int getCursorIndex(  CursorImpl cursor){
    return cursor.getIndex();
  }
  void setCursorBIN(  CursorImpl cursor,  BIN bin){
    cursor.setBIN(bin);
  }
  void setCursorIndex(  CursorImpl cursor,  int index){
    cursor.setIndex(index);
  }
  /** 
 * Called when we know we are about to split on behalf of a key that is the
 * minimum (leftSide) or maximum (!leftSide) of this node. This is achieved
 * by just forcing the split to occur either one element in from the left or
 * the right (i.e. splitIndex is 1 or nEntries - 1).
 */
  void splitSpecial(  IN parent,  int parentIndex,  int maxEntriesPerNode,  byte[] key,  boolean leftSide) throws DatabaseException {
    int index=findEntry(key,true,false);
    int nEntries=getNEntries();
    boolean exact=(index & IN.EXACT_MATCH) != 0;
    index&=~IN.EXACT_MATCH;
    if (leftSide && index < 0) {
      splitInternal(parent,parentIndex,maxEntriesPerNode,1);
    }
 else     if (!leftSide && !exact && index == (nEntries - 1)) {
      splitInternal(parent,parentIndex,maxEntriesPerNode,nEntries - 1);
    }
 else {
      split(parent,parentIndex,maxEntriesPerNode);
    }
  }
  /** 
 * Adjust any cursors that are referring to this BIN. This method is called
 * during a split operation. "this" is the BIN being split. newSibling is
 * the new BIN into which the entries from "this" between newSiblingLow and
 * newSiblingHigh have been copied.
 * @param newSibling -
 * the newSibling into which "this" has been split.
 * @param newSiblingLow,
 * newSiblingHigh - the low and high entry of "this" that were
 * moved into newSibling.
 */
  void adjustCursors(  IN newSibling,  int newSiblingLow,  int newSiblingHigh){
    int adjustmentDelta=(newSiblingHigh - newSiblingLow);
    Iterator iter=cursorSet.iterator();
    while (iter.hasNext()) {
      CursorImpl cursor=(CursorImpl)iter.next();
      if (getCursorBINToBeRemoved(cursor) == this) {
        continue;
      }
      int cIdx=getCursorIndex(cursor);
      BIN cBin=getCursorBIN(cursor);
      assert cBin == this : "nodeId=" + getNodeId() + " cursor="+ cursor.dumpToString(true);
      assert newSibling instanceof BIN;
      BIN ns=(BIN)newSibling;
      if (newSiblingLow == 0) {
        if (cIdx < newSiblingHigh) {
          setCursorBIN(cursor,ns);
          iter.remove();
          ns.addCursor(cursor);
        }
 else {
          setCursorIndex(cursor,cIdx - adjustmentDelta);
        }
      }
 else {
        if (cIdx >= newSiblingLow) {
          setCursorIndex(cursor,cIdx - newSiblingLow);
          setCursorBIN(cursor,ns);
          iter.remove();
          ns.addCursor(cursor);
        }
      }
    }
  }
  /** 
 * Adjust cursors referring to this BIN following an insert.
 * @param insertIndex -
 * The index of the new entry.
 */
  void adjustCursorsForInsert(  int insertIndex){
    if (cursorSet != null) {
      Iterator iter=cursorSet.iterator();
      while (iter.hasNext()) {
        CursorImpl cursor=(CursorImpl)iter.next();
        if (getCursorBINToBeRemoved(cursor) != this) {
          int cIdx=getCursorIndex(cursor);
          if (insertIndex <= cIdx) {
            setCursorIndex(cursor,cIdx + 1);
          }
        }
      }
    }
  }
  /** 
 * Adjust cursors referring to the given binIndex in this BIN following a
 * mutation of the entry from an LN to a DIN. The entry was moved from a BIN
 * to a newly created DBIN so each cursor must be added to the new DBIN.
 * @param binIndex -
 * The index of the DIN (previously LN) entry in the BIN.
 * @param dupBin -
 * The DBIN into which the LN entry was moved.
 * @param dupBinIndex -
 * The index of the moved LN entry in the DBIN.
 * @param excludeCursor -
 * The cursor being used for insertion and that should not be
 * updated.
 */
  void adjustCursorsForMutation(  int binIndex,  DBIN dupBin,  int dupBinIndex,  CursorImpl excludeCursor){
    if (cursorSet != null) {
      Iterator iter=cursorSet.iterator();
      while (iter.hasNext()) {
        CursorImpl cursor=(CursorImpl)iter.next();
        if (getCursorBINToBeRemoved(cursor) != this && cursor != excludeCursor && cursor.getIndex() == binIndex) {
          assert cursor.getDupBIN() == null;
          cursor.addCursor(dupBin);
          cursor.updateDBin(dupBin,dupBinIndex);
        }
      }
    }
  }
  /** 
 * Compress this BIN by removing any entries that are deleted. Deleted
 * entries are those that have LN's marked deleted or if the knownDeleted
 * flag is set. Caller is responsible for latching and unlatching this node.
 * @param binRefis used to determine the set of keys to be checked for
 * deletedness, or is null to check all keys.
 * @param canFetchif false, don't fetch any non-resident children. We don't want
 * some callers of compress, such as the evictor, to fault in
 * other nodes.
 * @return true if we had to requeue the entry because we were unable to get
 * locks, false if all entries were processed and therefore any
 * remaining deleted keys in the BINReference must now be in some
 * other BIN because of a split.
 */
  public boolean compress(  BINReference binRef,  boolean canFetch) throws DatabaseException {
    boolean ret=false;
    boolean setNewIdKey=false;
    boolean anyLocksDenied=false;
    DatabaseImpl db=getDatabase();
    BasicLocker lockingTxn=new BasicLocker(db.getDbEnvironment());
    try {
      for (int i=0; i < getNEntries(); i++) {
        boolean deleteEntry=false;
        if (binRef == null || isEntryPendingDeleted(i) || isEntryKnownDeleted(i) || binRef.hasDeletedKey(new Key(getKey(i)))) {
          Node n=null;
          if (canFetch) {
            n=fetchTarget(i);
          }
 else {
            n=getTarget(i);
            if (n == null) {
              continue;
            }
          }
          if (n == null) {
            deleteEntry=true;
          }
 else           if (isEntryKnownDeleted(i)) {
            LockResult lockRet=lockingTxn.nonBlockingLock(n.getNodeId(),LockType.READ,db);
            if (lockRet.getLockGrant() == LockGrantType.DENIED) {
              anyLocksDenied=true;
              continue;
            }
            deleteEntry=true;
          }
 else {
            if (!n.containsDuplicates()) {
              LN ln=(LN)n;
              LockResult lockRet=lockingTxn.nonBlockingLock(ln.getNodeId(),LockType.READ,db);
              if (lockRet.getLockGrant() == LockGrantType.DENIED) {
                anyLocksDenied=true;
                continue;
              }
              if (ln.isDeleted()) {
                deleteEntry=true;
              }
            }
          }
          if (binRef != null) {
            binRef.removeDeletedKey(new Key(getKey(i)));
          }
        }
        if (deleteEntry) {
          boolean entryIsIdentifierKey=Key.compareKeys(getKey(i),getIdentifierKey(),getKeyComparator()) == 0;
          if (entryIsIdentifierKey) {
            setNewIdKey=true;
          }
          boolean deleteSuccess=deleteEntry(i,true);
          assert deleteSuccess;
          i--;
        }
      }
    }
  finally {
      if (lockingTxn != null) {
        lockingTxn.operationEnd();
      }
    }
    if (anyLocksDenied && binRef != null) {
      this.hook609(binRef,db);
      ret=true;
    }
    if (getNEntries() != 0 && setNewIdKey) {
      setIdentifierKey(getKey(0));
    }
    if (getNEntries() == 0) {
      setGeneration(0);
    }
    return ret;
  }
  public boolean isCompressible(){
    return true;
  }
  boolean validateSubtreeBeforeDelete(  int index) throws DatabaseException {
    return true;
  }
  /** 
 * Check if this node fits the qualifications for being part of a deletable
 * subtree. It can only have one IN child and no LN children.
 */
  boolean isValidForDelete() throws DatabaseException {
    try {
      int validIndex=0;
      int numValidEntries=0;
      boolean needToLatch=false;
      this.hook607(validIndex,numValidEntries,needToLatch);
      throw ReturnHack.returnBoolean;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  void accumulateStats(  TreeWalkerStatsAccumulator acc){
    acc.processBIN(this,new Long(getNodeId()),getLevel());
  }
  /** 
 * Return the relevant user defined comparison function for this type of
 * node. For IN's and BIN's, this is the BTree Comparison function.
 * Overriden by DBIN.
 */
  public Comparator getKeyComparator(){
    return getDatabase().getBtreeComparator();
  }
  public String beginTag(){
    return BEGIN_TAG;
  }
  public String endTag(){
    return END_TAG;
  }
  /** 
 * @see LoggableObject#getLogType
 */
  public LogEntryType getLogType(){
    return LogEntryType.LOG_BIN;
  }
  public String shortClassName(){
    return "BIN";
  }
  /** 
 * Decide how to log this node. BINs may be logged provisionally. If logging
 * a delta, return an null for the LSN.
 */
  protected long logInternal(  LogManager logManager,  boolean allowDeltas,  boolean isProvisional,  boolean proactiveMigration,  IN parent) throws DatabaseException {
    boolean doDeltaLog=false;
    long lastFullVersion=getLastFullVersion();
    Cleaner cleaner=getDatabase().getDbEnvironment().getCleaner();
    cleaner.lazyMigrateLNs(this,proactiveMigration);
    BINDelta deltaInfo=null;
    if ((allowDeltas) && (lastFullVersion != DbLsn.NULL_LSN) && !prohibitNextDelta) {
      deltaInfo=new BINDelta(this);
      doDeltaLog=doDeltaLog(deltaInfo);
    }
    long returnLsn=DbLsn.NULL_LSN;
    if (doDeltaLog) {
      lastDeltaVersion=logManager.log(deltaInfo);
      returnLsn=DbLsn.NULL_LSN;
      numDeltasSinceLastFull++;
    }
 else {
      returnLsn=super.logInternal(logManager,allowDeltas,isProvisional,proactiveMigration,parent);
      lastDeltaVersion=DbLsn.NULL_LSN;
      numDeltasSinceLastFull=0;
    }
    prohibitNextDelta=false;
    return returnLsn;
  }
  /** 
 * Decide whether to log a full or partial BIN, depending on the ratio of
 * the delta size to full BIN size, and the number of deltas that have been
 * logged since the last full.
 * @return true if we should log the deltas of this BIN
 */
  private boolean doDeltaLog(  BINDelta deltaInfo) throws DatabaseException {
    int maxDiffs=(getNEntries() * getDatabase().getBinDeltaPercent()) / 100;
    if ((deltaInfo.getNumDeltas() <= maxDiffs) && (numDeltasSinceLastFull < getDatabase().getBinMaxDeltas())) {
      return true;
    }
 else {
      return false;
    }
  }
  protected void hook603(  Node child) throws DatabaseException {
  }
  protected void hook604() throws DatabaseException {
  }
  protected void hook605() throws DatabaseException {
  }
  protected void hook606() throws DatabaseException {
  }
  protected void hook607(  int validIndex,  int numValidEntries,  boolean needToLatch) throws DatabaseException {
    this.hook608(needToLatch);
    for (int i=0; i < getNEntries(); i++) {
      if (!isEntryKnownDeleted(i)) {
        numValidEntries++;
        validIndex=i;
      }
    }
    if (numValidEntries > 1) {
      throw new ReturnBoolean(false);
    }
 else {
      if (nCursors() > 0) {
        throw new ReturnBoolean(false);
      }
      if (numValidEntries == 1) {
        Node child=fetchTarget(validIndex);
        throw new ReturnBoolean(child != null && child.isValidForDelete());
      }
 else {
        throw new ReturnBoolean(true);
      }
    }
  }
  protected void hook608(  boolean needToLatch) throws DatabaseException {
  }
  protected void hook609(  BINReference binRef,  DatabaseImpl db) throws DatabaseException {
  }
  protected void hook610(  int index){
  }
}
\00base/com/sleepycat/je/util/DbRunAction.java:package com.sleepycat.je.util;
import java.io.File;
import java.text.DecimalFormat;
import com.sleepycat.je.CheckpointConfig;
import com.sleepycat.je.Cursor;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.EnvironmentMutableConfig;
import com.sleepycat.je.LockMode;
import com.sleepycat.je.OperationStatus;
import com.sleepycat.je.Transaction;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.utilint.CmdUtil;
import de.ovgu.cide.jakutil.*;
/** 
 * DbRunAction is a debugging aid that runs one of the background activities
 * (cleaning, compressing, evicting, checkpointing).
 */
public class DbRunAction {
  private static final int CLEAN=1;
  private static final int CHECKPOINT=4;
  public static void main(  String[] argv){
    new DbRunAction_main(argv).execute();
  }
  private static String getSecs(  long start,  long end){
    return (end - start) / 1000 + " secs";
  }
  private static void preload(  Environment env,  String dbName) throws DatabaseException {
    System.out.println("Preload starting");
    Database db=env.openDatabase(null,dbName,null);
    Cursor cursor=db.openCursor(null,null);
    try {
      DatabaseEntry key=new DatabaseEntry();
      DatabaseEntry data=new DatabaseEntry();
      int count=0;
      while (cursor.getNext(key,data,LockMode.DEFAULT) == OperationStatus.SUCCESS) {
        count++;
        if ((count % 50000) == 0) {
          System.out.println(count + "...");
        }
      }
      System.out.println("Preloaded " + count + " records");
    }
  finally {
      cursor.close();
      db.close();
    }
  }
  private static void usage(){
    System.out.println("Usage: \n " + CmdUtil.getJavaCommand(DbRunAction.class));
    System.out.println("  -h <environment home> ");
    System.out.println("  -a <clean|compress|evict|checkpoint|removedb>");
    System.out.println("  -ro (read-only - defaults to read-write)");
    System.out.println("  -s <dbName> (for preloading of evict or db remove)");
  }
@MethodObject static class DbRunAction_main {
    DbRunAction_main(    String[] argv){
      this.argv=argv;
    }
    void execute(){
      recoveryStart=0;
      actionStart=0;
      actionEnd=0;
      try {
        whichArg=0;
        if (argv.length == 0) {
          usage();
          System.exit(1);
        }
        dbName=null;
        doAction=0;
        envHome=".";
        readOnly=false;
        while (whichArg < argv.length) {
          nextArg=argv[whichArg];
          if (nextArg.equals("-h")) {
            whichArg++;
            envHome=CmdUtil.getArg(argv,whichArg);
          }
 else           if (nextArg.equals("-a")) {
            whichArg++;
            action=CmdUtil.getArg(argv,whichArg);
            if (action.equalsIgnoreCase("clean")) {
              doAction=CLEAN;
            }
 else {
              this.hook841();
            }
          }
 else           if (nextArg.equals("-ro")) {
            readOnly=true;
          }
 else           if (nextArg.equals("-s")) {
            dbName=argv[++whichArg];
          }
 else {
            throw new IllegalArgumentException(nextArg + " is not a supported option.");
          }
          whichArg++;
        }
        envConfig=new EnvironmentConfig();
        this.hook848();
        this.hook847();
        this.hook845();
        recoveryStart=System.currentTimeMillis();
        env=new Environment(new File(envHome),envConfig);
        forceConfig=new CheckpointConfig();
        forceConfig.setForce(true);
        actionStart=System.currentTimeMillis();
        if (doAction == CLEAN) {
          while (true) {
            nFiles=env.cleanLog();
            System.out.println("Files cleaned: " + nFiles);
            if (nFiles == 0) {
              break;
            }
          }
          env.checkpoint(forceConfig);
        }
        this.hook840();
        this.hook844();
        if (doAction == CHECKPOINT) {
          env.checkpoint(forceConfig);
        }
        this.hook842();
        this.hook838();
        actionEnd=System.currentTimeMillis();
        env.close();
      }
 catch (      Exception e) {
        e.printStackTrace();
        System.out.println(e.getMessage());
        usage();
        System.exit(1);
      }
 finally {
        f=new DecimalFormat();
        f.setMaximumFractionDigits(2);
        recoveryDuration=actionStart - recoveryStart;
        System.out.println("\nrecovery time = " + f.format(recoveryDuration) + " millis "+ f.format((double)recoveryDuration / 60000)+ " minutes");
        actionDuration=actionEnd - actionStart;
        System.out.println("action time = " + f.format(actionDuration) + " millis "+ f.format(actionDuration / 60000)+ " minutes");
      }
    }
    protected String[] argv;
    protected long recoveryStart;
    protected long actionStart;
    protected long actionEnd;
    protected int whichArg;
    protected String dbName;
    protected int doAction;
    protected String envHome;
    protected boolean readOnly;
    protected String nextArg;
    protected String action;
    protected EnvironmentConfig envConfig;
    protected Environment env;
    protected CheckpointConfig forceConfig;
    protected int nFiles;
    protected DatabaseConfig dbConfig;
    protected Database db;
    protected DecimalFormat f;
    protected long recoveryDuration;
    protected long actionDuration;
    protected void hook838() throws Exception {
    }
    protected void hook839() throws Exception {
      usage();
      System.exit(1);
    }
    protected void hook840() throws Exception {
    }
    protected void hook841() throws Exception {
      if (action.equalsIgnoreCase("checkpoint")) {
        doAction=CHECKPOINT;
      }
 else {
        this.hook846();
      }
    }
    protected void hook842() throws Exception {
    }
    protected void hook843() throws Exception {
      this.hook839();
    }
    protected void hook844() throws Exception {
    }
    protected void hook845() throws Exception {
    }
    protected void hook846() throws Exception {
      this.hook843();
    }
    protected void hook847() throws Exception {
    }
    protected void hook848() throws Exception {
    }
  }
}
\00base/com/sleepycat/je/util/DbLoad.java:package com.sleepycat.je.util;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.util.Date;
import java.util.logging.Level;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.JEVersion;
import com.sleepycat.je.OperationStatus;
import com.sleepycat.je.utilint.CmdUtil;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
public class DbLoad {
  private static final boolean DEBUG=false;
  protected Environment env;
  private boolean formatUsingPrintable;
  private String dbName;
  private BufferedReader reader;
  private boolean noOverwrite;
  private boolean textFileMode;
  private boolean dupSort;
  private boolean ignoreUnknownConfig;
  private boolean commandLine;
  private long progressInterval;
  private long totalLoadBytes;
  private static final String usageString="usage: " + CmdUtil.getJavaCommand(DbLoad.class) + "\n"+ "       -h <dir>             # environment home directory\n"+ "       [-f <fileName>]      # input file\n"+ "       [-n ]                # no overwrite mode\n"+ "       [-T]                 # input file is in text mode\n"+ "       [-I]                 # ignore unknown parameters\n"+ "       [-c name=value]      # config values\n"+ "       [-s <databaseName> ] # database to load\n"+ "       [-v]                 # show progress\n"+ "       [-V]                 # print JE version number";
  static public void main(  String argv[]) throws DatabaseException, IOException {
    DbLoad loader=parseArgs(argv);
    try {
      loader.load();
    }
 catch (    Throwable e) {
      e.printStackTrace();
    }
    loader.env.close();
  }
  static private void printUsage(  String msg){
    System.err.println(msg);
    System.err.println(usageString);
    System.exit(-1);
  }
  static private DbLoad parseArgs(  String argv[]) throws IOException, DatabaseException {
    boolean noOverwrite=false;
    boolean textFileMode=false;
    boolean ignoreUnknownConfig=false;
    boolean showProgressInterval=false;
    int argc=0;
    int nArgs=argv.length;
    String inputFileName=null;
    File envHome=null;
    String dbName=null;
    long progressInterval=0;
    DbLoad ret=new DbLoad();
    ret.setCommandLine(true);
    while (argc < nArgs) {
      String thisArg=argv[argc++].trim();
      if (thisArg.equals("-n")) {
        noOverwrite=true;
      }
 else       if (thisArg.equals("-T")) {
        textFileMode=true;
      }
 else       if (thisArg.equals("-I")) {
        ignoreUnknownConfig=true;
      }
 else       if (thisArg.equals("-V")) {
        System.out.println(JEVersion.CURRENT_VERSION);
        System.exit(0);
      }
 else       if (thisArg.equals("-f")) {
        if (argc < nArgs) {
          inputFileName=argv[argc++];
        }
 else {
          printUsage("-f requires an argument");
        }
      }
 else       if (thisArg.equals("-h")) {
        if (argc < nArgs) {
          envHome=new File(argv[argc++]);
        }
 else {
          printUsage("-h requires an argument");
        }
      }
 else       if (thisArg.equals("-s")) {
        if (argc < nArgs) {
          dbName=argv[argc++];
        }
 else {
          printUsage("-s requires an argument");
        }
      }
 else       if (thisArg.equals("-c")) {
        if (argc < nArgs) {
          try {
            ret.loadConfigLine(argv[argc++]);
          }
 catch (          IllegalArgumentException e) {
            printUsage("-c: " + e.getMessage());
          }
        }
 else {
          printUsage("-c requires an argument");
        }
      }
 else       if (thisArg.equals("-v")) {
        showProgressInterval=true;
      }
    }
    if (envHome == null) {
      printUsage("-h is a required argument");
    }
    long totalLoadBytes=0;
    InputStream is;
    if (inputFileName == null) {
      is=System.in;
      if (showProgressInterval) {
        printUsage("-v requires -f");
      }
    }
 else {
      is=new FileInputStream(inputFileName);
      if (showProgressInterval) {
        totalLoadBytes=((FileInputStream)is).getChannel().size();
        progressInterval=totalLoadBytes / 20;
      }
    }
    BufferedReader reader=new BufferedReader(new InputStreamReader(is));
    EnvironmentConfig envConfig=new EnvironmentConfig();
    envConfig.setAllowCreate(true);
    Environment env=new Environment(envHome,envConfig);
    ret.setEnv(env);
    ret.setDbName(dbName);
    ret.setInputReader(reader);
    ret.setNoOverwrite(noOverwrite);
    ret.setTextFileMode(textFileMode);
    ret.setIgnoreUnknownConfig(ignoreUnknownConfig);
    ret.setProgressInterval(progressInterval);
    ret.setTotalLoadBytes(totalLoadBytes);
    return ret;
  }
  public DbLoad(){
  }
  /** 
 * If true, enables output of warning messages.  Command line behavior is
 * not available via the public API.
 */
  private void setCommandLine(  boolean commandLine){
    this.commandLine=commandLine;
  }
  public void setEnv(  Environment env){
    this.env=env;
  }
  public void setDbName(  String dbName){
    this.dbName=dbName;
  }
  public void setInputReader(  BufferedReader reader){
    this.reader=reader;
  }
  public void setNoOverwrite(  boolean noOverwrite){
    this.noOverwrite=noOverwrite;
  }
  public void setTextFileMode(  boolean textFileMode){
    this.textFileMode=textFileMode;
  }
  public void setIgnoreUnknownConfig(  boolean ignoreUnknownConfigMode){
    this.ignoreUnknownConfig=ignoreUnknownConfigMode;
  }
  public void setProgressInterval(  long progressInterval){
    this.progressInterval=progressInterval;
  }
  public void setTotalLoadBytes(  long totalLoadBytes){
    this.totalLoadBytes=totalLoadBytes;
  }
  public boolean load() throws IOException, DatabaseException {
    if (progressInterval > 0) {
      System.out.println("Load start: " + new Date());
    }
    if (textFileMode) {
      formatUsingPrintable=true;
    }
 else {
      loadHeader();
    }
    if (dbName == null) {
      throw new IllegalArgumentException("Must supply a database name if -l not supplied.");
    }
    DatabaseConfig dbConfig=new DatabaseConfig();
    dbConfig.setSortedDuplicates(dupSort);
    dbConfig.setAllowCreate(true);
    Database db=env.openDatabase(null,dbName,dbConfig);
    loadData(db);
    db.close();
    this.hook835();
    if (progressInterval > 0) {
      System.out.println("Load end: " + new Date());
    }
    return true;
  }
  private void loadConfigLine(  String line) throws DatabaseException {
    int equalsIdx=line.indexOf('=');
    if (equalsIdx < 0) {
      throw new IllegalArgumentException("Invalid header parameter: " + line);
    }
    String keyword=line.substring(0,equalsIdx).trim().toLowerCase();
    String value=line.substring(equalsIdx + 1).trim();
    if (keyword.equals("version")) {
      if (DEBUG) {
        System.out.println("Found version: " + line);
      }
      if (!value.equals("3")) {
        throw new IllegalArgumentException("Version " + value + " is not supported.");
      }
    }
 else     if (keyword.equals("format")) {
      value=value.toLowerCase();
      if (value.equals("print")) {
        formatUsingPrintable=true;
      }
 else       if (value.equals("bytevalue")) {
        formatUsingPrintable=false;
      }
 else {
        throw new IllegalArgumentException(value + " is an unknown value for the format keyword");
      }
      if (DEBUG) {
        System.out.println("Found format: " + formatUsingPrintable);
      }
    }
 else     if (keyword.equals("dupsort")) {
      value=value.toLowerCase();
      if (value.equals("true") || value.equals("1")) {
        dupSort=true;
      }
 else       if (value.equals("false") || value.equals("0")) {
        dupSort=false;
      }
 else {
        throw new IllegalArgumentException(value + " is an unknown value for the dupsort keyword");
      }
      if (DEBUG) {
        System.out.println("Found dupsort: " + dupSort);
      }
    }
 else     if (keyword.equals("type")) {
      value=value.toLowerCase();
      if (!value.equals("btree")) {
        throw new IllegalArgumentException(value + " is not a supported database type.");
      }
      if (DEBUG) {
        System.out.println("Found type: " + line);
      }
    }
 else     if (keyword.equals("database")) {
      if (dbName == null) {
        dbName=value;
      }
      if (DEBUG) {
        System.out.println("DatabaseImpl: " + dbName);
      }
    }
 else     if (!ignoreUnknownConfig) {
      throw new IllegalArgumentException("'" + line + "' is not understood.");
    }
  }
  private void loadHeader() throws IOException, DatabaseException {
    if (DEBUG) {
      System.out.println("loading header");
    }
    String line=reader.readLine();
    while (line != null && !line.equals("HEADER=END")) {
      loadConfigLine(line);
      line=reader.readLine();
    }
  }
  private void loadData(  Database db) throws DatabaseException, IOException {
    String keyLine=reader.readLine();
    String dataLine=null;
    int count=0;
    long totalBytesRead=0;
    long lastTime=System.currentTimeMillis();
    long bytesReadThisInterval=0;
    while (keyLine != null && !keyLine.equals("DATA=END")) {
      dataLine=reader.readLine();
      if (dataLine == null) {
        throw new DatabaseException("No data to match key " + keyLine);
      }
      bytesReadThisInterval+=dataLine.length() + 1;
      byte[] keyBytes=loadLine(keyLine.trim());
      byte[] dataBytes=loadLine(dataLine.trim());
      DatabaseEntry key=new DatabaseEntry(keyBytes);
      DatabaseEntry data=new DatabaseEntry(dataBytes);
      if (noOverwrite) {
        if (db.putNoOverwrite(null,key,data) == OperationStatus.KEYEXIST) {
          if (commandLine) {
            System.err.println("Key exists: " + key);
          }
        }
      }
 else {
        db.put(null,key,data);
      }
      count++;
      if ((progressInterval > 0) && (bytesReadThisInterval > progressInterval)) {
        totalBytesRead+=bytesReadThisInterval;
        bytesReadThisInterval-=progressInterval;
        long now=System.currentTimeMillis();
        System.out.println("loaded " + count + " records  "+ (now - lastTime)+ " ms - % completed: "+ ((100 * totalBytesRead) / totalLoadBytes));
        lastTime=now;
      }
      keyLine=reader.readLine();
      if (keyLine == null) {
        throw new DatabaseException("No \"DATA=END\"");
      }
      bytesReadThisInterval+=keyLine.length() + 1;
    }
  }
  private byte[] loadLine(  String line) throws DatabaseException {
    if (formatUsingPrintable) {
      return readPrintableLine(line);
    }
    int nBytes=line.length() / 2;
    byte[] ret=new byte[nBytes];
    int charIdx=0;
    for (int i=0; i < nBytes; i++, charIdx+=2) {
      int b2=Character.digit(line.charAt(charIdx),16);
      b2<<=4;
      b2+=Character.digit(line.charAt(charIdx + 1),16);
      ret[i]=(byte)b2;
    }
    return ret;
  }
  static private byte backSlashValue=(byte)(new Character('\\').charValue() & 0xff);
  private byte[] readPrintableLine(  String line) throws DatabaseException {
    int maxNBytes=line.length();
    byte[] ba=new byte[maxNBytes];
    int actualNBytes=0;
    for (int charIdx=0; charIdx < maxNBytes; charIdx++) {
      char c=line.charAt(charIdx);
      if (c == '\\') {
        if (++charIdx < maxNBytes) {
          char c1=line.charAt(charIdx);
          if (c1 == '\\') {
            ba[actualNBytes++]=backSlashValue;
          }
 else {
            if (++charIdx < maxNBytes) {
              char c2=line.charAt(charIdx);
              int b=Character.digit(c1,16);
              b<<=4;
              b+=Character.digit(c2,16);
              ba[actualNBytes++]=(byte)b;
            }
 else {
              throw new DatabaseException("Corrupted file");
            }
          }
        }
 else {
          throw new DatabaseException("Corrupted file");
        }
      }
 else {
        ba[actualNBytes++]=(byte)(c & 0xff);
      }
    }
    if (maxNBytes == actualNBytes) {
      return ba;
    }
 else {
      byte[] ret=new byte[actualNBytes];
      System.arraycopy(ba,0,ret,0,actualNBytes);
      return ret;
    }
  }
  protected void hook835() throws IOException, DatabaseException {
  }
}
\00base/com/sleepycat/je/util/DbDump.java:package com.sleepycat.je.util;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintStream;
import java.util.Iterator;
import java.util.List;
import java.util.logging.Level;
import com.sleepycat.je.Cursor;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.JEVersion;
import com.sleepycat.je.LockMode;
import com.sleepycat.je.OperationStatus;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.utilint.CmdUtil;
import com.sleepycat.je.utilint.DbScavenger;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
public class DbDump {
  private static final int VERSION=3;
  protected File envHome=null;
  protected Environment env;
  protected String dbName=null;
  protected boolean formatUsingPrintable;
  private boolean dupSort;
  private String outputFileName=null;
  protected String outputDirectory=null;
  protected PrintStream outputFile=null;
  protected boolean doScavengerRun=false;
  protected boolean doAggressiveScavengerRun=false;
  protected boolean verbose=false;
  private static final String usageString="usage: " + CmdUtil.getJavaCommand(DbDump.class) + "\n"+ "  -h <dir> # environment home directory\n"+ "  [-f <fileName>]     # output file, for non -rR dumps\n"+ "  [-l]                # list databases in the environment\n"+ "  [-p]                # output printable characters\n"+ "  [-r]                # salvage mode\n"+ "  [-R]                # aggressive salvage mode\n"+ "  [-d] <directory>    # directory for *.dump files (salvage mode)\n"+ "  [-s <databaseName>] # database to dump\n"+ "  [-v]                # verbose in salvage mode\n"+ "  [-V]                # print JE version number\n";
  private DbDump(){
  }
  public DbDump(  Environment env,  String dbName,  PrintStream outputFile,  String outputDirectory,  boolean formatUsingPrintable){
    try {
      this.envHome=env.getHome();
    }
 catch (    DatabaseException e) {
      IllegalArgumentException iae=new IllegalArgumentException();
      iae.initCause(e);
      throw iae;
    }
    this.env=env;
    this.dbName=dbName;
    this.outputFile=outputFile;
    this.outputDirectory=outputDirectory;
    this.formatUsingPrintable=formatUsingPrintable;
  }
  public static void main(  String argv[]) throws DatabaseException, IOException {
    DbDump dumper=new DbDump();
    boolean listDbs=dumper.parseArgs(argv);
    if (dumper.doScavengerRun) {
      dumper.openEnv(false);
      dumper=new DbScavenger(dumper.env,dumper.outputFile,dumper.outputDirectory,dumper.formatUsingPrintable,dumper.doAggressiveScavengerRun,dumper.verbose);
      ((DbScavenger)dumper).setDumpCorruptedBounds(true);
    }
    if (listDbs) {
      dumper.listDbs();
      System.exit(0);
    }
    try {
      dumper.dump();
    }
 catch (    Throwable T) {
      T.printStackTrace();
    }
 finally {
      dumper.env.close();
      if (dumper.outputFile != null && dumper.outputFile != System.out) {
        dumper.outputFile.close();
      }
    }
  }
  private void listDbs() throws DatabaseException {
    openEnv(true);
    List dbNames=env.getDatabaseNames();
    Iterator iter=dbNames.iterator();
    while (iter.hasNext()) {
      String name=(String)iter.next();
      System.out.println(name);
    }
  }
  protected void printUsage(  String msg){
    System.err.println(msg);
    System.err.println(usageString);
    System.exit(-1);
  }
  protected boolean parseArgs(  String argv[]) throws IOException {
    int argc=0;
    int nArgs=argv.length;
    boolean listDbs=false;
    while (argc < nArgs) {
      String thisArg=argv[argc++];
      if (thisArg.equals("-p")) {
        formatUsingPrintable=true;
      }
 else       if (thisArg.equals("-V")) {
        System.out.println(JEVersion.CURRENT_VERSION);
        System.exit(0);
      }
 else       if (thisArg.equals("-l")) {
        listDbs=true;
      }
 else       if (thisArg.equals("-r")) {
        doScavengerRun=true;
      }
 else       if (thisArg.equals("-R")) {
        doScavengerRun=true;
        doAggressiveScavengerRun=true;
      }
 else       if (thisArg.equals("-f")) {
        if (argc < nArgs) {
          outputFileName=argv[argc++];
        }
 else {
          printUsage("-f requires an argument");
        }
      }
 else       if (thisArg.equals("-h")) {
        if (argc < nArgs) {
          String envDir=argv[argc++];
          envHome=new File(envDir);
        }
 else {
          printUsage("-h requires an argument");
        }
      }
 else       if (thisArg.equals("-d")) {
        if (argc < nArgs) {
          outputDirectory=argv[argc++];
        }
 else {
          printUsage("-d requires an argument");
        }
      }
 else       if (thisArg.equals("-s")) {
        if (argc < nArgs) {
          dbName=argv[argc++];
        }
 else {
          printUsage("-s requires an argument");
        }
      }
 else       if (thisArg.equals("-v")) {
        verbose=true;
      }
 else {
        printUsage(thisArg + " is not a valid option.");
      }
    }
    if (envHome == null) {
      printUsage("-h is a required argument");
    }
    if (!listDbs && !doScavengerRun) {
      if (dbName == null) {
        printUsage("Must supply a database name if -l not supplied.");
      }
    }
    if (outputFileName == null) {
      outputFile=System.out;
    }
 else {
      outputFile=new PrintStream(new FileOutputStream(outputFileName));
    }
    return listDbs;
  }
  protected void openEnv(  boolean doRecovery) throws DatabaseException {
    if (env == null) {
      EnvironmentConfig envConfiguration=new EnvironmentConfig();
      envConfiguration.setReadOnly(true);
      envConfiguration.setConfigParam(EnvironmentParams.ENV_RECOVERY.getName(),doRecovery ? "true" : "false");
      env=new Environment(envHome,envConfiguration);
    }
  }
  public void dump() throws IOException, DatabaseException {
    openEnv(true);
    this.hook834();
    DatabaseEntry foundKey=new DatabaseEntry();
    DatabaseEntry foundData=new DatabaseEntry();
    DatabaseConfig dbConfig=new DatabaseConfig();
    dbConfig.setReadOnly(true);
    DbInternal.setUseExistingConfig(dbConfig,true);
    Database db=env.openDatabase(null,dbName,dbConfig);
    dupSort=db.getConfig().getSortedDuplicates();
    printHeader(outputFile,dupSort,formatUsingPrintable);
    Cursor cursor=db.openCursor(null,null);
    while (cursor.getNext(foundKey,foundData,LockMode.DEFAULT) == OperationStatus.SUCCESS) {
      dumpOne(outputFile,foundKey.getData(),formatUsingPrintable);
      dumpOne(outputFile,foundData.getData(),formatUsingPrintable);
    }
    cursor.close();
    db.close();
    outputFile.println("DATA=END");
  }
  protected void printHeader(  PrintStream o,  boolean dupSort,  boolean formatUsingPrintable){
    o.println("VERSION=" + VERSION);
    if (formatUsingPrintable) {
      o.println("format=print");
    }
 else {
      o.println("format=bytevalue");
    }
    o.println("type=btree");
    o.println("dupsort=" + (dupSort ? "1" : "0"));
    o.println("HEADER=END");
  }
  protected void dumpOne(  PrintStream o,  byte[] ba,  boolean formatUsingPrintable){
    StringBuffer sb=new StringBuffer();
    sb.append(' ');
    CmdUtil.formatEntry(sb,ba,formatUsingPrintable);
    o.println(sb.toString());
  }
  protected void hook834() throws IOException, DatabaseException {
  }
}
\00base/com/sleepycat/je/Cursor.java:package com.sleepycat.je;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.dbi.CursorImpl;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.GetMode;
import com.sleepycat.je.dbi.PutMode;
import com.sleepycat.je.dbi.RangeRestartException;
import com.sleepycat.je.dbi.CursorImpl.KeyChangeStatus;
import com.sleepycat.je.dbi.CursorImpl.SearchMode;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.DBIN;
import com.sleepycat.je.tree.Key;
import com.sleepycat.je.tree.LN;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.txn.BuddyLocker;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.txn.Locker;
import com.sleepycat.je.txn.LockerFactory;
import com.sleepycat.je.utilint.InternalException;
import de.ovgu.cide.jakutil.*;
/** 
 * Javadoc for this public class is generated
 * via the doc templates in the doc_src directory.
 */
public class Cursor {
  /** 
 * The underlying cursor.
 */
  CursorImpl cursorImpl;
  /** 
 * The CursorConfig used to configure this cursor.
 */
  CursorConfig config;
  /** 
 * True if update operations are prohibited through this cursor.  Update
 * operations are prohibited if the database is read-only or:
 * (1) The database is transactional,
 * and
 * (2) The user did not supply a txn to the cursor ctor (meaning, the
 * locker is non-transactional).
 */
  private boolean updateOperationsProhibited;
  /** 
 * Handle under which this cursor was created; may be null.
 */
  private Database dbHandle;
  /** 
 * Database implementation.
 */
  private DatabaseImpl dbImpl;
  private boolean readUncommittedDefault;
  private boolean serializableIsolationDefault;
  /** 
 * Creates a cursor for a given user transaction.
 * <p>If txn is null, a non-transactional cursor will be created that
 * releases locks for the prior operation when the next operation
 * suceeds.</p>
 */
  Cursor(  Database dbHandle,  Transaction txn,  CursorConfig cursorConfig) throws DatabaseException {
    if (cursorConfig == null) {
      cursorConfig=CursorConfig.DEFAULT;
    }
    Locker locker=LockerFactory.getReadableLocker(dbHandle.getEnvironment(),txn,dbHandle.isTransactional(),false,cursorConfig.getReadCommitted());
    init(dbHandle,dbHandle.getDatabaseImpl(),locker,dbHandle.isWritable(),cursorConfig);
  }
  /** 
 * Creates a cursor for a given locker.
 * <p>If locker is null or is non-transactional, a non-transactional cursor
 * will be created that releases locks for the prior operation when the
 * next operation suceeds.</p>
 */
  Cursor(  Database dbHandle,  Locker locker,  CursorConfig cursorConfig) throws DatabaseException {
    if (cursorConfig == null) {
      cursorConfig=CursorConfig.DEFAULT;
    }
    locker=LockerFactory.getReadableLocker(dbHandle.getEnvironment(),dbHandle,locker,false,cursorConfig.getReadCommitted());
    init(dbHandle,dbHandle.getDatabaseImpl(),locker,dbHandle.isWritable(),cursorConfig);
  }
  /** 
 * Creates a cursor for a given locker and no db handle.
 * <p>The locker parameter must be non-null.  With this constructor, we use
 * the given locker without applying any special rules for different
 * isolation levels -- the caller must supply the correct locker.</p>
 */
  Cursor(  DatabaseImpl dbImpl,  Locker locker,  CursorConfig cursorConfig) throws DatabaseException {
    if (cursorConfig == null) {
      cursorConfig=CursorConfig.DEFAULT;
    }
    init(null,dbImpl,locker,true,cursorConfig);
  }
  private void init(  Database dbHandle,  DatabaseImpl dbImpl,  Locker locker,  boolean isWritable,  CursorConfig cursorConfig) throws DatabaseException {
    assert locker != null;
    assert dbImpl != null;
    cursorImpl=new CursorImpl(dbImpl,locker,false);
    readUncommittedDefault=cursorConfig.getReadUncommitted() || locker.isReadUncommittedDefault();
    serializableIsolationDefault=cursorImpl.getLocker().isSerializableIsolation();
    updateOperationsProhibited=(dbImpl.isTransactional() && !locker.isTransactional()) || !isWritable;
    this.dbImpl=dbImpl;
    this.dbHandle=dbHandle;
    if (dbHandle != null) {
      dbHandle.addCursor(this);
    }
    this.config=cursorConfig;
    this.hook36(dbImpl);
  }
  /** 
 * Copy constructor.
 */
  Cursor(  Cursor cursor,  boolean samePosition) throws DatabaseException {
    readUncommittedDefault=cursor.readUncommittedDefault;
    serializableIsolationDefault=cursor.serializableIsolationDefault;
    updateOperationsProhibited=cursor.updateOperationsProhibited;
    cursorImpl=cursor.cursorImpl.dup(samePosition);
    dbImpl=cursor.dbImpl;
    dbHandle=cursor.dbHandle;
    if (dbHandle != null) {
      dbHandle.addCursor(this);
    }
    config=cursor.config;
  }
  /** 
 * Internal entrypoint.
 */
  CursorImpl getCursorImpl(){
    return cursorImpl;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public Database getDatabase(){
    return dbHandle;
  }
  /** 
 * Always returns non-null, while getDatabase() returns null if no handle
 * is associated with this cursor.
 */
  DatabaseImpl getDatabaseImpl(){
    return dbImpl;
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public CursorConfig getConfig(){
    return config.cloneConfig();
  }
  void setNonCloning(  boolean nonCloning){
    cursorImpl.setNonCloning(nonCloning);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public synchronized void close() throws DatabaseException {
    checkState(false);
    cursorImpl.close();
    if (dbHandle != null) {
      dbHandle.removeCursor(this);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public int count() throws DatabaseException {
    checkState(true);
    this.hook0();
    return countInternal(null);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public Cursor dup(  boolean samePosition) throws DatabaseException {
    checkState(false);
    return new Cursor(this,samePosition);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus delete() throws DatabaseException {
    checkState(true);
    checkUpdatesAllowed("delete");
    this.hook1();
    return deleteInternal();
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus put(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    DatabaseUtil.checkForPartialKey(key);
    checkUpdatesAllowed("put");
    this.hook2(key,data);
    return putInternal(key,data,PutMode.OVERWRITE);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus putNoOverwrite(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    DatabaseUtil.checkForPartialKey(key);
    checkUpdatesAllowed("putNoOverwrite");
    this.hook3(key,data);
    return putInternal(key,data,PutMode.NOOVERWRITE);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus putNoDupData(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    DatabaseUtil.checkForPartialKey(key);
    checkUpdatesAllowed("putNoDupData");
    this.hook4(key,data);
    return putInternal(key,data,PutMode.NODUP);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus putCurrent(  DatabaseEntry data) throws DatabaseException {
    checkState(true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    checkUpdatesAllowed("putCurrent");
    this.hook5(data);
    return putInternal(null,data,PutMode.CURRENT);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getCurrent(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(true);
    checkArgsNoValRequired(key,data);
    this.hook6(lockMode);
    return getCurrentInternal(key,data,lockMode);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getFirst(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,data);
    this.hook7(lockMode);
    return position(key,data,lockMode,true);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getLast(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,data);
    this.hook8(lockMode);
    return position(key,data,lockMode,false);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNext(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,data);
    this.hook9(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,data,lockMode,true);
    }
 else {
      return retrieveNext(key,data,lockMode,GetMode.NEXT);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNextDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(true);
    checkArgsNoValRequired(key,data);
    this.hook10(lockMode);
    return retrieveNext(key,data,lockMode,GetMode.NEXT_DUP);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getNextNoDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,data);
    this.hook11(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,data,lockMode,true);
    }
 else {
      return retrieveNext(key,data,lockMode,GetMode.NEXT_NODUP);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrev(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,data);
    this.hook12(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,data,lockMode,false);
    }
 else {
      return retrieveNext(key,data,lockMode,GetMode.PREV);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrevDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(true);
    checkArgsNoValRequired(key,data);
    this.hook13(lockMode);
    return retrieveNext(key,data,lockMode,GetMode.PREV_DUP);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getPrevNoDup(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsNoValRequired(key,data);
    this.hook14(lockMode);
    if (cursorImpl.isNotInitialized()) {
      return position(key,data,lockMode,false);
    }
 else {
      return retrieveNext(key,data,lockMode,GetMode.PREV_NODUP);
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchKey(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",false);
    this.hook15(key,lockMode);
    return search(key,data,lockMode,SearchMode.SET);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchKeyRange(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",false);
    this.hook16(key,lockMode);
    return search(key,data,lockMode,SearchMode.SET_RANGE);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchBoth(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsValRequired(key,data);
    this.hook17(key,data,lockMode);
    return search(key,data,lockMode,SearchMode.BOTH);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public OperationStatus getSearchBothRange(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkState(false);
    checkArgsValRequired(key,data);
    this.hook18(key,data,lockMode);
    return search(key,data,lockMode,SearchMode.BOTH_RANGE);
  }
  /** 
 * Counts duplicates without parameter checking.
 */
  int countInternal(  LockMode lockMode) throws DatabaseException {
    CursorImpl origCursor=null;
    CursorImpl dup=null;
    try {
      origCursor=cursorImpl;
      dup=origCursor.cloneCursor(true);
      return dup.count(getLockType(lockMode,false));
    }
  finally {
      if (dup != origCursor) {
        dup.close();
      }
    }
  }
  /** 
 * Internal version of delete() that does no parameter checking.  Calls
 * deleteNoNotify() and notifies triggers (performs secondary updates).
 */
  OperationStatus deleteInternal() throws DatabaseException {
    DatabaseEntry oldKey=null;
    DatabaseEntry oldData=null;
    boolean doNotifyTriggers=dbHandle != null && dbHandle.hasTriggers();
    if (doNotifyTriggers) {
      oldKey=new DatabaseEntry();
      oldData=new DatabaseEntry();
      OperationStatus status=getCurrentInternal(oldKey,oldData,LockMode.RMW);
      if (status != OperationStatus.SUCCESS) {
        return OperationStatus.KEYEMPTY;
      }
    }
    if (doNotifyTriggers) {
      dbHandle.notifyTriggers(cursorImpl.getLocker(),oldKey,oldData,null);
    }
    OperationStatus status=deleteNoNotify();
    return status;
  }
  /** 
 * Clone the cursor, delete at current position, and if successful, swap
 * cursors.  Does not notify triggers (does not perform secondary updates).
 */
  OperationStatus deleteNoNotify() throws DatabaseException {
    CursorImpl origCursor=null;
    CursorImpl dup=null;
    OperationStatus status=OperationStatus.KEYEMPTY;
    try {
      origCursor=cursorImpl;
      dup=origCursor.cloneCursor(true);
      this.hook19(dup);
      status=dup.delete();
      return status;
    }
  finally {
      this.hook20(origCursor,dup);
      boolean success=(status == OperationStatus.SUCCESS);
      if (cursorImpl == dup) {
        if (!success) {
          cursorImpl.reset();
        }
      }
 else {
        if (success) {
          origCursor.close();
          cursorImpl=dup;
        }
 else {
          dup.close();
        }
      }
    }
  }
  /** 
 * Internal version of put() that does no parameter checking.  Calls
 * putNoNotify() and notifies triggers (performs secondary updates).
 * Prevents phantoms.
 */
  OperationStatus putInternal(  DatabaseEntry key,  DatabaseEntry data,  PutMode putMode) throws DatabaseException {
    DatabaseEntry oldData=null;
    boolean doNotifyTriggers=dbHandle != null && dbHandle.hasTriggers();
    if (doNotifyTriggers && (putMode == PutMode.CURRENT || putMode == PutMode.OVERWRITE)) {
      oldData=new DatabaseEntry();
      if (key == null && putMode == PutMode.CURRENT) {
        key=new DatabaseEntry();
      }
    }
    OperationStatus commitStatus=putNoNotify(key,data,putMode,oldData);
    if (doNotifyTriggers && commitStatus == OperationStatus.SUCCESS) {
      if (oldData != null && oldData.getData() == null) {
        oldData=null;
      }
      dbHandle.notifyTriggers(cursorImpl.getLocker(),key,oldData,data);
    }
    return commitStatus;
  }
  /** 
 * Performs the put operation but does not notify triggers (does not
 * perform secondary updates).  Prevents phantoms.
 */
  OperationStatus putNoNotify(  DatabaseEntry key,  DatabaseEntry data,  PutMode putMode,  DatabaseEntry returnOldData) throws DatabaseException {
    Locker nextKeyLocker=null;
    CursorImpl nextKeyCursor=null;
    try {
      Locker cursorLocker=cursorImpl.getLocker();
      if (putMode != PutMode.CURRENT && dbImpl.getDbEnvironment().getTxnManager().areOtherSerializableTransactionsActive(cursorLocker)) {
        nextKeyLocker=new BuddyLocker(dbImpl.getDbEnvironment(),cursorLocker);
        nextKeyCursor=new CursorImpl(dbImpl,nextKeyLocker);
        nextKeyCursor.lockNextKeyForInsert(key,data);
      }
      return putAllowPhantoms(key,data,putMode,returnOldData,nextKeyCursor);
    }
  finally {
      if (nextKeyCursor != null) {
        nextKeyCursor.close();
      }
      if (nextKeyLocker != null) {
        nextKeyLocker.operationEnd();
      }
    }
  }
  /** 
 * Clone the cursor, put key/data according to PutMode, and if successful,
 * swap cursors.  Does not notify triggers (does not perform secondary
 * updates).  Does not prevent phantoms.
 * @param nextKeyCursor is the cursor used to lock the next key during
 * phantom prevention.  If this cursor is non-null and initialized, it's
 * BIN will be used to initialize the dup cursor used to perform insertion.
 * This enables an optimization that skips the search for the BIN.
 */
  private OperationStatus putAllowPhantoms(  DatabaseEntry key,  DatabaseEntry data,  PutMode putMode,  DatabaseEntry returnOldData,  CursorImpl nextKeyCursor) throws DatabaseException {
    if (data == null) {
      throw new NullPointerException("put passed a null DatabaseEntry arg");
    }
    if (putMode != PutMode.CURRENT && key == null) {
      throw new IllegalArgumentException("put passed a null DatabaseEntry arg");
    }
    CursorImpl origCursor=null;
    OperationStatus status=OperationStatus.NOTFOUND;
    CursorImpl dup=null;
    try {
      origCursor=cursorImpl;
      if (putMode == PutMode.CURRENT) {
        dup=origCursor.cloneCursor(true);
      }
 else {
        dup=origCursor.cloneCursor(false,nextKeyCursor);
      }
      if (putMode == PutMode.CURRENT) {
        status=dup.putCurrent(data,key,returnOldData);
      }
 else       if (putMode == PutMode.OVERWRITE) {
        status=dup.put(key,data,returnOldData);
      }
 else       if (putMode == PutMode.NOOVERWRITE) {
        status=dup.putNoOverwrite(key,data);
      }
 else       if (putMode == PutMode.NODUP) {
        status=dup.putNoDupData(key,data);
      }
 else {
        throw new InternalException("unknown PutMode");
      }
      return status;
    }
  finally {
      this.hook21(origCursor);
      boolean success=(status == OperationStatus.SUCCESS);
      if (cursorImpl == dup) {
        if (!success) {
          cursorImpl.reset();
        }
      }
 else {
        if (success) {
          origCursor.close();
          cursorImpl=dup;
        }
 else {
          if (dup != null) {
            dup.close();
          }
        }
      }
    }
  }
  /** 
 * Position the cursor at the first or last record of the database.
 * Prevents phantoms.
 */
  OperationStatus position(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode,  boolean first) throws DatabaseException {
    if (!isSerializableIsolation(lockMode)) {
      return positionAllowPhantoms(key,data,getLockType(lockMode,false),first);
    }
    while (true) {
      try {
        if (!first) {
          cursorImpl.lockEofNode(LockType.RANGE_READ);
        }
        LockType lockType=getLockType(lockMode,first);
        OperationStatus status=positionAllowPhantoms(key,data,lockType,first);
        if (first && status != OperationStatus.SUCCESS) {
          cursorImpl.lockEofNode(LockType.RANGE_READ);
        }
        return status;
      }
 catch (      RangeRestartException e) {
        continue;
      }
    }
  }
  /** 
 * Position without preventing phantoms.
 */
  private OperationStatus positionAllowPhantoms(  DatabaseEntry key,  DatabaseEntry data,  LockType lockType,  boolean first) throws DatabaseException {
    assert (key != null && data != null);
    OperationStatus status=OperationStatus.NOTFOUND;
    CursorImpl dup=null;
    try {
      dup=beginRead(false);
      if (!dup.positionFirstOrLast(first,null)) {
        status=OperationStatus.NOTFOUND;
        this.hook22();
      }
 else {
        this.hook23();
        status=dup.getCurrentAlreadyLatched(key,data,lockType,first);
        if (status == OperationStatus.SUCCESS) {
          if (dup.getDupBIN() != null) {
            dup.incrementLNCount();
          }
        }
 else {
          status=dup.getNext(key,data,lockType,first,false);
        }
      }
    }
  finally {
      this.hook24();
      endRead(dup,status == OperationStatus.SUCCESS);
    }
    return status;
  }
  /** 
 * Perform search by key, data, or both.  Prevents phantoms.
 */
  OperationStatus search(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode,  SearchMode searchMode) throws DatabaseException {
    if (!isSerializableIsolation(lockMode)) {
      LockType lockType=getLockType(lockMode,false);
      KeyChangeStatus result=searchAllowPhantoms(key,data,lockType,lockType,searchMode);
      return result.status;
    }
    while (true) {
      try {
        LockType searchLockType=getLockType(lockMode,false);
        LockType advanceLockType=getLockType(lockMode,true);
        DatabaseEntry tryKey=new DatabaseEntry(key.getData(),key.getOffset(),key.getSize());
        DatabaseEntry tryData=new DatabaseEntry(data.getData(),data.getOffset(),data.getSize());
        KeyChangeStatus result;
        if (searchMode.isExactSearch()) {
          result=searchExactAndRangeLock(tryKey,tryData,searchLockType,advanceLockType,searchMode);
        }
 else {
          result=searchAllowPhantoms(tryKey,tryData,searchLockType,advanceLockType,searchMode);
          if (result.status != OperationStatus.SUCCESS) {
            cursorImpl.lockEofNode(LockType.RANGE_READ);
          }
        }
        if (result.status == OperationStatus.SUCCESS) {
          key.setData(tryKey.getData(),0,tryKey.getSize());
          data.setData(tryData.getData(),0,tryData.getSize());
        }
        return result.status;
      }
 catch (      RangeRestartException e) {
        continue;
      }
    }
  }
  /** 
 * For an exact search, perform a range search and return NOTFOUND if the
 * key changes (or if the data changes for BOTH) during the search.
 * If no exact match is found the range search will range lock the
 * following key for phantom prevention.  Importantly, the cursor position
 * is not changed if an exact match is not found, even though we advance to
 * the following key in order to range lock it.
 */
  private KeyChangeStatus searchExactAndRangeLock(  DatabaseEntry key,  DatabaseEntry data,  LockType searchLockType,  LockType advanceLockType,  SearchMode searchMode) throws DatabaseException {
    searchMode=(searchMode == SearchMode.SET) ? SearchMode.SET_RANGE : SearchMode.BOTH_RANGE;
    KeyChangeStatus result=null;
    boolean noNextKeyFound;
    CursorImpl dup=beginRead(false);
    try {
      result=searchInternal(dup,key,data,searchLockType,advanceLockType,searchMode,true);
      noNextKeyFound=!result.keyChange;
      if (result.keyChange && result.status == OperationStatus.SUCCESS) {
        result.status=OperationStatus.NOTFOUND;
      }
    }
  finally {
      endRead(dup,result != null && result.status == OperationStatus.SUCCESS);
    }
    if (noNextKeyFound) {
      cursorImpl.lockEofNode(LockType.RANGE_READ);
    }
    return result;
  }
  /** 
 * Perform search without preventing phantoms.
 */
  private KeyChangeStatus searchAllowPhantoms(  DatabaseEntry key,  DatabaseEntry data,  LockType searchLockType,  LockType advanceLockType,  SearchMode searchMode) throws DatabaseException {
    OperationStatus status=OperationStatus.NOTFOUND;
    CursorImpl dup=beginRead(false);
    try {
      KeyChangeStatus result=searchInternal(dup,key,data,searchLockType,advanceLockType,searchMode,false);
      status=result.status;
      return result;
    }
  finally {
      endRead(dup,status == OperationStatus.SUCCESS);
    }
  }
  /** 
 * Perform search for a given CursorImpl.
 */
  private KeyChangeStatus searchInternal(  CursorImpl dup,  DatabaseEntry key,  DatabaseEntry data,  LockType searchLockType,  LockType advanceLockType,  SearchMode searchMode,  boolean advanceAfterRangeSearch) throws DatabaseException {
    assert key != null && data != null;
    OperationStatus status=OperationStatus.NOTFOUND;
    boolean keyChange=false;
    this.hook25(dup,key,data,searchLockType,advanceLockType,searchMode,advanceAfterRangeSearch,status,keyChange);
    return new KeyChangeStatus(status,keyChange);
  }
  /** 
 * Retrieve the next or previous record.  Prevents phantoms.
 */
  OperationStatus retrieveNext(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode,  GetMode getMode) throws DatabaseException {
    if (!isSerializableIsolation(lockMode)) {
      return retrieveNextAllowPhantoms(key,data,getLockType(lockMode,false),getMode);
    }
    while (true) {
      try {
        OperationStatus status;
        if (getMode == GetMode.NEXT_DUP) {
          status=getNextDupAndRangeLock(key,data,lockMode);
        }
 else {
          if (!getMode.isForward()) {
            rangeLockCurrentPosition(getMode);
          }
          LockType lockType=getLockType(lockMode,getMode.isForward());
          status=retrieveNextAllowPhantoms(key,data,lockType,getMode);
          if (getMode.isForward() && status != OperationStatus.SUCCESS) {
            cursorImpl.lockEofNode(LockType.RANGE_READ);
          }
        }
        return status;
      }
 catch (      RangeRestartException e) {
        continue;
      }
    }
  }
  /** 
 * Retrieve the next dup; if no next dup is found then range lock the
 * following key for phantom prevention.  Importantly, the cursor position
 * is not changed if there are no more dups, even though we advance to the
 * following key in order to range lock it.
 */
  private OperationStatus getNextDupAndRangeLock(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    DatabaseEntry tryKey=new DatabaseEntry();
    DatabaseEntry tryData=new DatabaseEntry();
    LockType lockType=getLockType(lockMode,true);
    OperationStatus status;
    boolean noNextKeyFound;
    while (true) {
      this.hook26();
      CursorImpl dup=beginRead(true);
      try {
        KeyChangeStatus result=dup.getNextWithKeyChangeStatus(tryKey,tryData,lockType,true,false);
        status=result.status;
        noNextKeyFound=(status != OperationStatus.SUCCESS);
        if (result.keyChange && status == OperationStatus.SUCCESS) {
          status=OperationStatus.NOTFOUND;
        }
      }
 catch (      DatabaseException DBE) {
        endRead(dup,false);
        throw DBE;
      }
      if (checkForInsertion(GetMode.NEXT,cursorImpl,dup)) {
        endRead(dup,false);
        continue;
      }
 else {
        endRead(dup,status == OperationStatus.SUCCESS);
        this.hook27();
        break;
      }
    }
    if (noNextKeyFound) {
      cursorImpl.lockEofNode(LockType.RANGE_READ);
    }
    if (status == OperationStatus.SUCCESS) {
      key.setData(tryKey.getData(),0,tryKey.getSize());
      data.setData(tryData.getData(),0,tryData.getSize());
    }
    return status;
  }
  /** 
 * For 'prev' operations, upgrade to a range lock at the current position.
 * For PREV_NODUP, range lock the first duplicate instead.  If there are no
 * records at the current position, get a range lock on the next record or,
 * if not found, on the logical EOF node.  Do not modify the current
 * cursor position, use a separate cursor.
 */
  private void rangeLockCurrentPosition(  GetMode getMode) throws DatabaseException {
    DatabaseEntry tempKey=new DatabaseEntry();
    DatabaseEntry tempData=new DatabaseEntry();
    tempKey.setPartial(0,0,true);
    tempData.setPartial(0,0,true);
    OperationStatus status;
    CursorImpl dup=cursorImpl.cloneCursor(true);
    try {
      if (getMode == GetMode.PREV_NODUP) {
        status=dup.getFirstDuplicate(tempKey,tempData,LockType.RANGE_READ);
      }
 else {
        status=dup.getCurrent(tempKey,tempData,LockType.RANGE_READ);
      }
      if (status != OperationStatus.SUCCESS) {
        while (true) {
          this.hook28();
          status=dup.getNext(tempKey,tempData,LockType.RANGE_READ,true,false);
          if (checkForInsertion(GetMode.NEXT,cursorImpl,dup)) {
            dup.close();
            dup=cursorImpl.cloneCursor(true);
            continue;
          }
 else {
            this.hook29();
            break;
          }
        }
      }
    }
  finally {
      if (cursorImpl == dup) {
        dup.reset();
      }
 else {
        dup.close();
      }
    }
    if (status != OperationStatus.SUCCESS) {
      cursorImpl.lockEofNode(LockType.RANGE_READ);
    }
  }
  /** 
 * Retrieve without preventing phantoms.
 */
  private OperationStatus retrieveNextAllowPhantoms(  DatabaseEntry key,  DatabaseEntry data,  LockType lockType,  GetMode getMode) throws DatabaseException {
    assert (key != null && data != null);
    OperationStatus status;
    while (true) {
      this.hook30();
      CursorImpl dup=beginRead(true);
      try {
        if (getMode == GetMode.NEXT) {
          status=dup.getNext(key,data,lockType,true,false);
        }
 else         if (getMode == GetMode.PREV) {
          status=dup.getNext(key,data,lockType,false,false);
        }
 else         if (getMode == GetMode.NEXT_DUP) {
          status=dup.getNextDuplicate(key,data,lockType,true,false);
        }
 else         if (getMode == GetMode.PREV_DUP) {
          status=dup.getNextDuplicate(key,data,lockType,false,false);
        }
 else         if (getMode == GetMode.NEXT_NODUP) {
          status=dup.getNextNoDup(key,data,lockType,true,false);
        }
 else         if (getMode == GetMode.PREV_NODUP) {
          status=dup.getNextNoDup(key,data,lockType,false,false);
        }
 else {
          throw new InternalException("unknown GetMode");
        }
      }
 catch (      DatabaseException DBE) {
        endRead(dup,false);
        throw DBE;
      }
      if (checkForInsertion(getMode,cursorImpl,dup)) {
        endRead(dup,false);
        continue;
      }
 else {
        endRead(dup,status == OperationStatus.SUCCESS);
        this.hook31();
        break;
      }
    }
    return status;
  }
  /** 
 * Returns the current key and data.  There is no need to prevent phantoms.
 */
  OperationStatus getCurrentInternal(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    LockType lockType=getLockType(lockMode,false);
    return cursorImpl.getCurrent(key,data,lockType);
  }
  private boolean checkForInsertion(  GetMode getMode,  CursorImpl origCursor,  CursorImpl dupCursor) throws DatabaseException {
    BIN origBIN=origCursor.getBIN();
    BIN dupBIN=dupCursor.getBIN();
    DBIN origDBIN=origCursor.getDupBIN();
    boolean forward=true;
    if (getMode == GetMode.PREV || getMode == GetMode.PREV_DUP || getMode == GetMode.PREV_NODUP) {
      forward=false;
    }
    boolean ret=false;
    if (origBIN != dupBIN) {
      this.hook33(origCursor);
      if (origDBIN == null) {
        if (forward) {
          if (origBIN.getNEntries() - 1 > origCursor.getIndex()) {
            for (int i=origCursor.getIndex() + 1; i < origBIN.getNEntries(); i++) {
              if (!origBIN.isEntryKnownDeleted(i)) {
                Node n=origBIN.fetchTarget(i);
                if (n != null && !n.containsDuplicates()) {
                  LN ln=(LN)n;
                  if (!ln.isDeleted()) {
                    ret=true;
                    break;
                  }
                }
              }
 else {
              }
            }
          }
        }
 else {
          if (origCursor.getIndex() > 0) {
            for (int i=0; i < origCursor.getIndex(); i++) {
              if (!origBIN.isEntryKnownDeleted(i)) {
                Node n=origBIN.fetchTarget(i);
                if (n != null && !n.containsDuplicates()) {
                  LN ln=(LN)n;
                  if (!ln.isDeleted()) {
                    ret=true;
                    break;
                  }
                }
 else {
                }
              }
            }
          }
        }
      }
      this.hook32(origCursor);
      return ret;
    }
    if (origDBIN != dupCursor.getDupBIN() && origCursor.getIndex() == dupCursor.getIndex() && getMode != GetMode.NEXT_NODUP && getMode != GetMode.PREV_NODUP) {
      this.hook35(origCursor);
      if (forward) {
        if (origDBIN.getNEntries() - 1 > origCursor.getDupIndex()) {
          for (int i=origCursor.getDupIndex() + 1; i < origDBIN.getNEntries(); i++) {
            if (!origDBIN.isEntryKnownDeleted(i)) {
              Node n=origDBIN.fetchTarget(i);
              LN ln=(LN)n;
              if (n != null && !ln.isDeleted()) {
                ret=true;
                break;
              }
            }
          }
        }
      }
 else {
        if (origCursor.getDupIndex() > 0) {
          for (int i=0; i < origCursor.getDupIndex(); i++) {
            if (!origDBIN.isEntryKnownDeleted(i)) {
              Node n=origDBIN.fetchTarget(i);
              LN ln=(LN)n;
              if (n != null && !ln.isDeleted()) {
                ret=true;
                break;
              }
            }
          }
        }
      }
      this.hook34(origCursor);
      return ret;
    }
    return false;
  }
  /** 
 * If the cursor is initialized, dup it and return the dup; otherwise,
 * return the original.  This avoids the overhead of duping when the
 * original is uninitialized.  The cursor returned must be passed to
 * endRead() to close the correct cursor.
 */
  private CursorImpl beginRead(  boolean addCursor) throws DatabaseException {
    CursorImpl dup;
    if (cursorImpl.isNotInitialized()) {
      dup=cursorImpl;
    }
 else {
      dup=cursorImpl.cloneCursor(addCursor);
    }
    return dup;
  }
  /** 
 * If the operation is successful, swaps cursors and closes the original
 * cursor; otherwise, closes the duped cursor.  In the case where the
 * original cursor was not duped by beginRead because it was uninitialized,
 * just resets the original cursor if the operation did not succeed.
 */
  private void endRead(  CursorImpl dup,  boolean success) throws DatabaseException {
    if (dup == cursorImpl) {
      if (!success) {
        cursorImpl.reset();
      }
    }
 else {
      if (success) {
        cursorImpl.close();
        cursorImpl=dup;
      }
 else {
        dup.close();
      }
    }
  }
  boolean advanceCursor(  DatabaseEntry key,  DatabaseEntry data){
    return cursorImpl.advanceCursor(key,data);
  }
  private LockType getLockType(  LockMode lockMode,  boolean rangeLock){
    if (isReadUncommittedMode(lockMode)) {
      return LockType.NONE;
    }
 else     if (lockMode == null || lockMode == LockMode.DEFAULT) {
      return rangeLock ? LockType.RANGE_READ : LockType.READ;
    }
 else     if (lockMode == LockMode.RMW) {
      return rangeLock ? LockType.RANGE_WRITE : LockType.WRITE;
    }
 else     if (lockMode == LockMode.READ_COMMITTED) {
      throw new IllegalArgumentException(lockMode.toString() + " not allowed with Cursor methods");
    }
 else {
      assert false : lockMode;
      return LockType.NONE;
    }
  }
  /** 
 * Returns whether the given lock mode will cause a read-uncommitted when
 * used with this cursor, taking into account the default cursor
 * configuration.
 */
  boolean isReadUncommittedMode(  LockMode lockMode){
    return (lockMode == LockMode.READ_UNCOMMITTED || (readUncommittedDefault && (lockMode == null || lockMode == LockMode.DEFAULT)));
  }
  private boolean isSerializableIsolation(  LockMode lockMode){
    return serializableIsolationDefault && !isReadUncommittedMode(lockMode);
  }
  protected void checkUpdatesAllowed(  String operation) throws DatabaseException {
    if (updateOperationsProhibited) {
      throw new DatabaseException("A transaction was not supplied when opening this cursor: " + operation);
    }
  }
  /** 
 * Note that this flavor of checkArgs doesn't require that the dbt data is
 * set.
 */
  private void checkArgsNoValRequired(  DatabaseEntry key,  DatabaseEntry data){
    DatabaseUtil.checkForNullDbt(key,"key",false);
    DatabaseUtil.checkForNullDbt(data,"data",false);
  }
  /** 
 * Note that this flavor of checkArgs requires that the dbt data is set.
 */
  private void checkArgsValRequired(  DatabaseEntry key,  DatabaseEntry data){
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
  }
  /** 
 * Check the environment and cursor state.
 */
  void checkState(  boolean mustBeInitialized) throws DatabaseException {
    checkEnv();
    cursorImpl.checkCursorState(mustBeInitialized);
  }
  /** 
 * @throws RunRecoveryException if the underlying environment is invalid.
 */
  void checkEnv() throws RunRecoveryException {
    cursorImpl.checkEnv();
  }
  private void traceCursorImpl(  StringBuffer sb){
    sb.append(" locker=").append(cursorImpl.getLocker().getId());
    if (cursorImpl.getBIN() != null) {
      sb.append(" bin=").append(cursorImpl.getBIN().getNodeId());
    }
    sb.append(" idx=").append(cursorImpl.getIndex());
    if (cursorImpl.getDupBIN() != null) {
      sb.append(" Dbin=").append(cursorImpl.getDupBIN().getNodeId());
    }
    sb.append(" dupIdx=").append(cursorImpl.getDupIndex());
  }
  protected void hook0() throws DatabaseException {
  }
  protected void hook1() throws DatabaseException {
  }
  protected void hook2(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
  }
  protected void hook3(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
  }
  protected void hook4(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
  }
  protected void hook5(  DatabaseEntry data) throws DatabaseException {
  }
  protected void hook6(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook7(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook8(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook9(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook10(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook11(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook12(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook13(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook14(  LockMode lockMode) throws DatabaseException {
  }
  protected void hook15(  DatabaseEntry key,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook16(  DatabaseEntry key,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook17(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook18(  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook19(  CursorImpl dup) throws DatabaseException {
  }
  protected void hook20(  CursorImpl origCursor,  CursorImpl dup) throws DatabaseException {
  }
  protected void hook21(  CursorImpl origCursor) throws DatabaseException {
  }
  protected void hook22() throws DatabaseException {
  }
  protected void hook23() throws DatabaseException {
  }
  protected void hook24() throws DatabaseException {
  }
  protected void hook25(  CursorImpl dup,  DatabaseEntry key,  DatabaseEntry data,  LockType searchLockType,  LockType advanceLockType,  SearchMode searchMode,  boolean advanceAfterRangeSearch,  OperationStatus status,  boolean keyChange) throws DatabaseException {
    int searchResult=dup.searchAndPosition(key,data,searchMode,searchLockType);
    if ((searchResult & CursorImpl.FOUND) != 0) {
      boolean exactKeyMatch=((searchResult & CursorImpl.EXACT_KEY) != 0);
      boolean exactDataMatch=((searchResult & CursorImpl.EXACT_DATA) != 0);
      boolean foundLast=((searchResult & CursorImpl.FOUND_LAST) != 0);
      boolean rangeMatch=false;
      if (searchMode == SearchMode.SET_RANGE && !exactKeyMatch) {
        rangeMatch=true;
      }
      if (searchMode == SearchMode.BOTH_RANGE && (!exactKeyMatch || !exactDataMatch)) {
        rangeMatch=true;
      }
      DatabaseEntry useKey=(searchMode == SearchMode.SET) ? null : key;
      if (rangeMatch || (status=dup.getCurrentAlreadyLatched(useKey,data,searchLockType,true)) == OperationStatus.KEYEMPTY) {
        if (foundLast) {
          status=OperationStatus.NOTFOUND;
        }
 else         if (searchMode == SearchMode.SET) {
          status=dup.getNextDuplicate(key,data,advanceLockType,true,rangeMatch);
        }
 else         if (searchMode == SearchMode.BOTH) {
          if (status == OperationStatus.KEYEMPTY) {
            status=OperationStatus.NOTFOUND;
          }
        }
 else {
          assert !searchMode.isExactSearch();
          byte[] searchKey=null;
          if (searchMode.isDataSearch()) {
            searchKey=Key.makeKey(key);
          }
          if (exactKeyMatch) {
            KeyChangeStatus result=dup.getNextWithKeyChangeStatus(key,data,advanceLockType,true,rangeMatch);
            status=result.status;
            keyChange=searchMode.isDataSearch() ? (status == OperationStatus.SUCCESS) : result.keyChange;
          }
 else           if (searchMode.isDataSearch() && !advanceAfterRangeSearch) {
            status=OperationStatus.NOTFOUND;
          }
 else {
            status=dup.getNextNoDup(key,data,advanceLockType,true,rangeMatch);
            keyChange=(status == OperationStatus.SUCCESS);
          }
          if (status == OperationStatus.SUCCESS && searchMode.isDataSearch()) {
            if (Key.compareKeys(key.getData(),searchKey,dbImpl.getDuplicateComparator()) != 0) {
              status=OperationStatus.NOTFOUND;
            }
          }
        }
      }
    }
  }
  protected void hook26() throws DatabaseException {
  }
  protected void hook27() throws DatabaseException {
  }
  protected void hook28() throws DatabaseException {
  }
  protected void hook29() throws DatabaseException {
  }
  protected void hook30() throws DatabaseException {
  }
  protected void hook31() throws DatabaseException {
  }
  protected void hook32(  CursorImpl origCursor) throws DatabaseException {
  }
  protected void hook33(  CursorImpl origCursor) throws DatabaseException {
  }
  protected void hook34(  CursorImpl origCursor) throws DatabaseException {
  }
  protected void hook35(  CursorImpl origCursor) throws DatabaseException {
  }
  protected void hook36(  DatabaseImpl dbImpl) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/dbi/EnvironmentImpl.java:package com.sleepycat.je.dbi;
import java.io.File;
import java.io.IOException;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.logging.ConsoleHandler;
import java.util.logging.FileHandler;
import java.util.logging.Handler;
import java.util.logging.Level;
import java.util.logging.Logger;
import java.util.logging.SimpleFormatter;
import com.sleepycat.je.CheckpointConfig;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.EnvironmentMutableConfig;
import com.sleepycat.je.LockStats;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.Transaction;
import com.sleepycat.je.TransactionConfig;
import com.sleepycat.je.cleaner.Cleaner;
import com.sleepycat.je.cleaner.UtilizationProfile;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.log.FileManager;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.log.SyncedLogManager;
import com.sleepycat.je.recovery.Checkpointer;
import com.sleepycat.je.recovery.RecoveryInfo;
import com.sleepycat.je.recovery.RecoveryManager;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.BINReference;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Key;
import com.sleepycat.je.txn.Locker;
import com.sleepycat.je.txn.Txn;
import com.sleepycat.je.txn.TxnManager;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.PropUtil;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * Underlying Environment implementation. There is a single instance for any
 * database environment opened by the application.
 */
public class EnvironmentImpl implements EnvConfigObserver {
  private static final boolean TEST_NO_LOCKING_MODE=false;
  private DbEnvState envState;
  private boolean closing;
  private File envHome;
  private int referenceCount;
  private boolean isTransactional;
  private boolean isNoLocking;
  private boolean isReadOnly;
  private MemoryBudget memoryBudget;
  private long lockTimeout;
  private long txnTimeout;
  private DbTree dbMapTree;
  private long mapTreeRootLsn=DbLsn.NULL_LSN;
  private INList inMemoryINs;
  private DbConfigManager configManager;
  private List configObservers;
  protected LogManager logManager;
  private FileManager fileManager;
  private TxnManager txnManager;
  private Checkpointer checkpointer;
  private Cleaner cleaner;
  private RecoveryInfo lastRecoveryInfo;
  private RunRecoveryException savedInvalidatingException;
  private static boolean forcedYield=false;
  private static int threadLocalReferenceCount=0;
  /** 
 * DbPrintLog doesn't need btree and dup comparators to function properly
 * don't require any instantiations. This flag, if true, indicates that
 * we've been called from DbPrintLog.
 */
  private static boolean noComparators=false;
  public static final boolean JAVA5_AVAILABLE;
  private static final String DISABLE_JAVA_ADLER32="je.disable.java.adler32";
static {
    boolean ret=false;
    if (System.getProperty(DISABLE_JAVA_ADLER32) == null) {
      String javaVersion=System.getProperty("java.version");
      if (javaVersion != null && !javaVersion.startsWith("1.4.")) {
        ret=true;
      }
    }
    JAVA5_AVAILABLE=ret;
  }
  /** 
 * Create a database environment to represent the data in envHome. dbHome.
 * Properties from the je.properties file in that directory are used to
 * initialize the system wide property bag. Properties passed to this method
 * are used to influence the open itself.
 * @param envHomeabsolute path of the database environment home directory
 * @param envConfig
 * @throws DatabaseExceptionon all other failures
 */
  public EnvironmentImpl(  File envHome,  EnvironmentConfig envConfig) throws DatabaseException {
    try {
      this.envHome=envHome;
      envState=DbEnvState.INIT;
      this.hook323();
      configManager=new DbConfigManager(envConfig);
      configObservers=new ArrayList();
      addConfigObserver(this);
      memoryBudget=new MemoryBudget(this,configManager);
      this.hook336(envHome);
      forcedYield=configManager.getBoolean(EnvironmentParams.ENV_FORCED_YIELD);
      isTransactional=configManager.getBoolean(EnvironmentParams.ENV_INIT_TXN);
      isNoLocking=!(configManager.getBoolean(EnvironmentParams.ENV_INIT_LOCKING));
      if (isTransactional && isNoLocking) {
        if (TEST_NO_LOCKING_MODE) {
          isNoLocking=!isTransactional;
        }
 else {
          throw new IllegalArgumentException("Can't set 'je.env.isNoLocking' and " + "'je.env.isTransactional';");
        }
      }
      this.hook322();
      isReadOnly=configManager.getBoolean(EnvironmentParams.ENV_RDONLY);
      fileManager=new FileManager(this,envHome,isReadOnly);
      if (!envConfig.getAllowCreate() && !fileManager.filesExist()) {
        throw new DatabaseException("Enviroment creation isn't allowed, " + " but there is no pre-existing " + " environment in "+ envHome);
      }
      this.hook321();
      inMemoryINs=new INList(this);
      txnManager=new TxnManager(this);
      createDaemons();
      dbMapTree=new DbTree(this);
      referenceCount=0;
      this.hook320();
      if (configManager.getBoolean(EnvironmentParams.ENV_RECOVERY)) {
        try {
          RecoveryManager recoveryManager=new RecoveryManager(this);
          lastRecoveryInfo=recoveryManager.recover(isReadOnly);
        }
  finally {
          try {
            logManager.flush();
            fileManager.clear();
          }
 catch (          IOException e) {
            throw new DatabaseException(e.getMessage());
          }
        }
      }
 else {
        isReadOnly=true;
        noComparators=true;
      }
      runOrPauseDaemons(configManager);
      lockTimeout=PropUtil.microsToMillis(configManager.getLong(EnvironmentParams.LOCK_TIMEOUT));
      txnTimeout=PropUtil.microsToMillis(configManager.getLong(EnvironmentParams.TXN_TIMEOUT));
      this.hook335();
      open();
    }
 catch (    DatabaseException e) {
      if (fileManager != null) {
        try {
          fileManager.close();
        }
 catch (        IOException IOE) {
        }
      }
      throw e;
    }
  }
  /** 
 * Respond to config updates.
 */
  public void envConfigUpdate(  DbConfigManager mgr) throws DatabaseException {
    runOrPauseDaemons(mgr);
  }
  /** 
 * Read configurations for daemons, instantiate.
 */
  private void createDaemons() throws DatabaseException {
    new EnvironmentImpl_createDaemons(this).execute();
  }
  /** 
 * Run or pause daemons, depending on config properties.
 */
  private void runOrPauseDaemons(  DbConfigManager mgr) throws DatabaseException {
    if (!isReadOnly) {
      this.hook330(mgr);
      this.hook333(mgr);
      this.hook326(mgr);
    }
    this.hook317(mgr);
  }
  /** 
 * Returns the UtilizationTracker.
 */
  public UtilizationTracker getUtilizationTracker(){
    return cleaner.getUtilizationTracker();
  }
  /** 
 * Returns the UtilizationProfile.
 */
  public UtilizationProfile getUtilizationProfile(){
    return cleaner.getUtilizationProfile();
  }
  /** 
 * Log the map tree root and save the LSN.
 */
  public void logMapTreeRoot() throws DatabaseException {
    mapTreeRootLsn=logManager.log(dbMapTree);
  }
  /** 
 * Force a rewrite of the map tree root if required.
 */
  public void rewriteMapTreeRoot(  long cleanerTargetLsn) throws DatabaseException {
    if (DbLsn.compareTo(cleanerTargetLsn,mapTreeRootLsn) == 0) {
      mapTreeRootLsn=logManager.log(dbMapTree);
    }
  }
  /** 
 * @return the mapping tree root LSN.
 */
  public long getRootLsn(){
    return mapTreeRootLsn;
  }
  /** 
 * Set the mapping tree from the log. Called during recovery.
 */
  public void readMapTreeFromLog(  long rootLsn) throws DatabaseException {
    dbMapTree=(DbTree)logManager.get(rootLsn);
    dbMapTree.setEnvironmentImpl(this);
    this.hook324(rootLsn);
  }
  /** 
 * Not much to do, mark state.
 */
  public void open(){
    envState=DbEnvState.OPEN;
  }
  /** 
 * Invalidate the environment. Done when a fatal exception
 * (RunRecoveryException) is thrown.
 */
  public void invalidate(  RunRecoveryException e){
    savedInvalidatingException=e;
    envState=DbEnvState.INVALID;
    requestShutdownDaemons();
  }
  /** 
 * @return true if environment is open.
 */
  public boolean isOpen(){
    return (envState == DbEnvState.OPEN);
  }
  /** 
 * @return true if close has begun, although the state may still be open.
 */
  public boolean isClosing(){
    return closing;
  }
  public boolean isClosed(){
    return (envState == DbEnvState.CLOSED);
  }
  /** 
 * When a RunRecoveryException occurs or the environment is closed, further
 * writing can cause log corruption.
 */
  public boolean mayNotWrite(){
    return (envState == DbEnvState.INVALID) || (envState == DbEnvState.CLOSED);
  }
  public void checkIfInvalid() throws RunRecoveryException {
    if (envState == DbEnvState.INVALID) {
      savedInvalidatingException.setAlreadyThrown();
      throw savedInvalidatingException;
    }
  }
  public void checkNotClosed() throws DatabaseException {
    if (envState == DbEnvState.CLOSED) {
      throw new DatabaseException("Attempt to use a Environment that has been closed.");
    }
  }
  public synchronized void close() throws DatabaseException {
    if (--referenceCount <= 0) {
      doClose(true);
    }
  }
  public synchronized void close(  boolean doCheckpoint) throws DatabaseException {
    if (--referenceCount <= 0) {
      doClose(doCheckpoint);
    }
  }
  private void doClose(  boolean doCheckpoint) throws DatabaseException {
    StringBuffer errors=new StringBuffer();
    try {
      this.hook319();
      try {
        envState.checkState(DbEnvState.VALID_FOR_CLOSE,DbEnvState.CLOSED);
      }
 catch (      DatabaseException DBE) {
        throw DBE;
      }
      requestShutdownDaemons();
      if (doCheckpoint && !isReadOnly && (envState != DbEnvState.INVALID)&& logManager.getLastLsnAtRecovery() != fileManager.getLastUsedLsn()) {
        CheckpointConfig ckptConfig=new CheckpointConfig();
        ckptConfig.setForce(true);
        ckptConfig.setMinimizeRecoveryTime(true);
        try {
          invokeCheckpoint(ckptConfig,false,"close");
        }
 catch (        DatabaseException IE) {
          errors.append("\nException performing checkpoint: ");
          errors.append(IE.toString()).append("\n");
        }
      }
      try {
        shutdownDaemons();
      }
 catch (      InterruptedException IE) {
        errors.append("\nException shutting down daemon threads: ");
        errors.append(IE.toString()).append("\n");
      }
      this.hook318();
      try {
        logManager.flush();
      }
 catch (      DatabaseException DBE) {
        errors.append("\nException flushing log manager: ");
        errors.append(DBE.toString()).append("\n");
      }
      try {
        fileManager.clear();
      }
 catch (      IOException IOE) {
        errors.append("\nException clearing file manager: ");
        errors.append(IOE.toString()).append("\n");
      }
catch (      DatabaseException DBE) {
        errors.append("\nException clearing file manager: ");
        errors.append(DBE.toString()).append("\n");
      }
      try {
        fileManager.close();
      }
 catch (      IOException IOE) {
        errors.append("\nException clearing file manager: ");
        errors.append(IOE.toString()).append("\n");
      }
catch (      DatabaseException DBE) {
        errors.append("\nException clearing file manager: ");
        errors.append(DBE.toString()).append("\n");
      }
      try {
        inMemoryINs.clear();
      }
 catch (      DatabaseException DBE) {
        errors.append("\nException closing file manager: ");
        errors.append(DBE.toString()).append("\n");
      }
      this.hook337();
      DbEnvPool.getInstance().remove(envHome);
      this.hook325(errors);
    }
  finally {
      envState=DbEnvState.CLOSED;
    }
    if (errors.length() > 0 && savedInvalidatingException == null) {
      throw new RunRecoveryException(this,errors.toString());
    }
  }
  public synchronized void closeAfterRunRecovery() throws DatabaseException {
    try {
      shutdownDaemons();
    }
 catch (    InterruptedException IE) {
    }
    try {
      fileManager.clear();
    }
 catch (    Exception e) {
    }
    try {
      fileManager.close();
    }
 catch (    Exception e) {
    }
    DbEnvPool.getInstance().remove(envHome);
  }
  public synchronized void forceClose() throws DatabaseException {
    referenceCount=1;
    close();
  }
  public synchronized void incReferenceCount(){
    referenceCount++;
  }
  public static int getThreadLocalReferenceCount(){
    return threadLocalReferenceCount;
  }
  public static synchronized void incThreadLocalReferenceCount(){
    threadLocalReferenceCount++;
  }
  public static synchronized void decThreadLocalReferenceCount(){
    threadLocalReferenceCount--;
  }
  public static boolean getNoComparators(){
    return noComparators;
  }
  /** 
 * Invoke a checkpoint programatically. Note that only one checkpoint may
 * run at a time.
 */
  public boolean invokeCheckpoint(  CheckpointConfig config,  boolean flushAll,  String invokingSource) throws DatabaseException {
    if (checkpointer != null) {
      checkpointer.doCheckpoint(config,flushAll,invokingSource);
      return true;
    }
 else {
      return false;
    }
  }
  public int invokeCleaner() throws DatabaseException {
    if (cleaner != null) {
      return cleaner.doClean(true,false);
    }
 else {
      return 0;
    }
  }
  private void requestShutdownDaemons(){
    closing=true;
    this.hook331();
    this.hook334();
    this.hook327();
  }
  /** 
 * Ask all daemon threads to shut down.
 */
  private void shutdownDaemons() throws InterruptedException {
    shutdownCheckpointer();
  }
  void shutdownCheckpointer() throws InterruptedException {
    if (checkpointer != null) {
      this.hook328();
      checkpointer=null;
    }
    return;
  }
  public boolean isNoLocking(){
    return isNoLocking;
  }
  public boolean isTransactional(){
    return isTransactional;
  }
  public boolean isReadOnly(){
    return isReadOnly;
  }
  public DatabaseImpl createDb(  Locker locker,  String databaseName,  DatabaseConfig dbConfig,  Database databaseHandle) throws DatabaseException {
    return dbMapTree.createDb(locker,databaseName,dbConfig,databaseHandle);
  }
  /** 
 * Get a database object given a database name.
 * @param databaseNametarget database.
 * @return null if database doesn't exist.
 */
  public DatabaseImpl getDb(  Locker locker,  String databaseName,  Database databaseHandle) throws DatabaseException {
    return dbMapTree.getDb(locker,databaseName,databaseHandle);
  }
  public List getDbNames() throws DatabaseException {
    return dbMapTree.getDbNames();
  }
  /** 
 * For debugging.
 */
  public void dumpMapTree() throws DatabaseException {
    dbMapTree.dump();
  }
  /** 
 * Transactional services.
 */
  public Txn txnBegin(  Transaction parent,  TransactionConfig txnConfig) throws DatabaseException {
    if (!isTransactional) {
      throw new DatabaseException("beginTransaction called, " + " but Environment was not opened " + "with transactional cpabilities");
    }
    return txnManager.txnBegin(parent,txnConfig);
  }
  public LogManager getLogManager(){
    return logManager;
  }
  public FileManager getFileManager(){
    return fileManager;
  }
  public DbTree getDbMapTree(){
    return dbMapTree;
  }
  /** 
 * Returns the config manager for the current base configuration.
 * <p>
 * The configuration can change, but changes are made by replacing the
 * config manager object with a enw one. To use a consistent set of
 * properties, call this method once and query the returned manager
 * repeatedly for each property, rather than getting the config manager via
 * this method for each property individually.
 * </p>
 */
  public DbConfigManager getConfigManager(){
    return configManager;
  }
  /** 
 * Clones the current configuration.
 */
  public EnvironmentConfig cloneConfig(){
    return DbInternal.cloneConfig(configManager.getEnvironmentConfig());
  }
  /** 
 * Clones the current mutable configuration.
 */
  public EnvironmentMutableConfig cloneMutableConfig(){
    return DbInternal.cloneMutableConfig(configManager.getEnvironmentConfig());
  }
  /** 
 * Throws an exception if an immutable property is changed.
 */
  public void checkImmutablePropsForEquality(  EnvironmentConfig config) throws IllegalArgumentException {
    DbInternal.checkImmutablePropsForEquality(configManager.getEnvironmentConfig(),config);
  }
  /** 
 * Changes the mutable config properties that are present in the given
 * config, and notifies all config observer.
 */
  public synchronized void setMutableConfig(  EnvironmentMutableConfig config) throws DatabaseException {
    EnvironmentConfig newConfig=DbInternal.cloneConfig(configManager.getEnvironmentConfig());
    DbInternal.copyMutablePropsTo(config,newConfig);
    configManager=new DbConfigManager(newConfig);
    for (int i=configObservers.size() - 1; i >= 0; i-=1) {
      EnvConfigObserver o=(EnvConfigObserver)configObservers.get(i);
      o.envConfigUpdate(configManager);
    }
  }
  /** 
 * Adds an observer of mutable config changes.
 */
  public synchronized void addConfigObserver(  EnvConfigObserver o){
    configObservers.add(o);
  }
  /** 
 * Removes an observer of mutable config changes.
 */
  public synchronized void removeConfigObserver(  EnvConfigObserver o){
    configObservers.remove(o);
  }
  public INList getInMemoryINs(){
    return inMemoryINs;
  }
  public TxnManager getTxnManager(){
    return txnManager;
  }
  public Checkpointer getCheckpointer(){
    return checkpointer;
  }
  public Cleaner getCleaner(){
    return cleaner;
  }
  public MemoryBudget getMemoryBudget(){
    return memoryBudget;
  }
  /** 
 * Info about the last recovery
 */
  public RecoveryInfo getLastRecoveryInfo(){
    return lastRecoveryInfo;
  }
  /** 
 * Get the environment home directory.
 */
  public File getEnvironmentHome(){
    return envHome;
  }
  public long getTxnTimeout(){
    return txnTimeout;
  }
  public long getLockTimeout(){
    return lockTimeout;
  }
  /** 
 * For stress testing. Should only ever be called from an assert.
 */
  public static boolean maybeForceYield(){
    if (forcedYield) {
      Thread.yield();
    }
    return true;
  }
@MethodObject static class EnvironmentImpl_createDaemons {
    EnvironmentImpl_createDaemons(    EnvironmentImpl _this){
      this._this=_this;
    }
    void execute() throws DatabaseException {
      checkpointerWakeupTime=0;
      this.hook329();
      _this.checkpointer=new Checkpointer(_this,checkpointerWakeupTime,"Checkpointer");
      this.hook332();
      _this.cleaner=new Cleaner(_this,"Cleaner");
    }
    protected EnvironmentImpl _this;
    protected long checkpointerWakeupTime;
    protected long compressorWakeupInterval;
    protected void hook329() throws DatabaseException {
    }
    protected void hook332() throws DatabaseException {
    }
  }
  protected void hook317(  DbConfigManager mgr) throws DatabaseException {
  }
  protected void hook318() throws DatabaseException {
  }
  protected void hook319() throws DatabaseException {
  }
  protected void hook320() throws DatabaseException {
  }
  protected void hook321() throws DatabaseException {
    logManager=new SyncedLogManager(this,isReadOnly);
  }
  protected void hook322() throws DatabaseException {
  }
  protected void hook323() throws DatabaseException {
  }
  protected void hook324(  long rootLsn) throws DatabaseException {
    mapTreeRootLsn=rootLsn;
  }
  protected void hook325(  StringBuffer errors) throws DatabaseException {
  }
  protected void hook326(  DbConfigManager mgr) throws DatabaseException {
  }
  protected void hook327(){
  }
  protected void hook328() throws InterruptedException {
  }
  protected void hook330(  DbConfigManager mgr) throws DatabaseException {
  }
  protected void hook331(){
  }
  protected void hook333(  DbConfigManager mgr) throws DatabaseException {
  }
  protected void hook334(){
  }
  protected void hook335() throws DatabaseException {
  }
  protected void hook336(  File envHome) throws DatabaseException {
  }
  protected void hook337() throws DatabaseException {
  }
}
\00base/com/sleepycat/je/dbi/SortedLSNTreeWalker.java:package com.sleepycat.je.dbi;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.cleaner.OffsetList;
import com.sleepycat.je.log.LogEntryType;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.DBIN;
import com.sleepycat.je.tree.DIN;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.utilint.DbLsn;
import de.ovgu.cide.jakutil.*;
/** 
 * Class to walk over the tree using sorted LSN fetching for parts of the tree
 * that are not in memory. Returns LSNs for each node in the tree <b>except</b>
 * the root IN, but in an arbitrary order (i.e. not key order). The caller is
 * responsible for getting the root IN's LSN explicitly.
 * <p>
 * A calllback function specified in the constructor is executed for each LSN
 * found.
 * <p>
 * The walker works in two phases. The first phase is to gather and return all
 * the INs from the INList that match the database being iterated over. For each
 * IN, all of the LSNs of the children are passed to the callback method
 * (processLSN). If the child was not in memory, it is added to a list of LSNs
 * to read. When all of the in-memory INs have been processed, the list of LSNs
 * that were harvested are sorted.
 * <p>
 * Then for each of the sorted LSNs, the target is fetched, the type determined,
 * and the LSN and type passed to the callback method for processing. LSNs of
 * the children of those nodes are retrieved and the process repeated until
 * there are no more nodes to be fetched for this database's tree.
 */
public class SortedLSNTreeWalker {
public interface TreeNodeProcessor {
    void processLSN(    long childLSN,    LogEntryType childType);
  }
  protected DatabaseImpl dbImpl;
  private EnvironmentImpl envImpl;
  private long rootLsn;
  private boolean dups;
  private boolean removeINsFromINList;
  private boolean setDbState;
  private long[] currentLSNs;
  private int currentLSNIdx=0;
  private OffsetList accumulatedLSNFileNumbers;
  private OffsetList accumulatedLSNFileOffsets;
  private TreeNodeProcessor callback;
  protected boolean accumulateLNs=false;
  public SortedLSNTreeWalker(  DatabaseImpl dbImpl,  boolean removeINsFromINList,  boolean setDbState,  long rootLsn,  TreeNodeProcessor callback) throws DatabaseException {
    this.dbImpl=dbImpl;
    this.envImpl=dbImpl.getDbEnvironment();
    if (envImpl == null) {
      throw new DatabaseException("environmentImpl is null for target db " + dbImpl.getDebugName());
    }
    this.dups=dbImpl.getSortedDuplicates();
    this.removeINsFromINList=removeINsFromINList;
    this.setDbState=setDbState;
    this.rootLsn=rootLsn;
    this.callback=callback;
    currentLSNs=new long[0];
    currentLSNIdx=0;
  }
  private boolean extractINsForDb(  INList inList) throws DatabaseException {
    return new SortedLSNTreeWalker_extractINsForDb(this,inList).execute();
  }
  /** 
 * Find all non-resident nodes, and execute the callback. The root IN's LSN
 * is not returned to the callback.
 */
  public void walk() throws DatabaseException {
    walkInternal();
  }
  protected void walkInternal() throws DatabaseException {
    INList inList=envImpl.getInMemoryINs();
    IN root=null;
    if (!extractINsForDb(inList)) {
      if (rootLsn == DbLsn.NULL_LSN) {
        return;
      }
      root=getRootIN(rootLsn);
      accumulateLSNs(root);
      releaseRootIN(root);
    }
    this.hook359();
    while (true) {
      maybeGetMoreINs();
      if (currentLSNs != null && currentLSNIdx < currentLSNs.length) {
        fetchAndProcessLSN(currentLSNs[currentLSNIdx++]);
      }
 else {
        break;
      }
    }
  }
  private void maybeGetMoreINs(){
    if ((currentLSNs != null && currentLSNIdx >= currentLSNs.length)) {
      if (accumulatedLSNFileNumbers == null || accumulatedLSNFileNumbers.size() == 0) {
        currentLSNs=null;
        currentLSNIdx=Integer.MAX_VALUE;
        return;
      }
      long[] tempFileNumbers=accumulatedLSNFileNumbers.toArray();
      long[] tempFileOffsets=accumulatedLSNFileOffsets.toArray();
      int nLSNs=tempFileNumbers.length;
      currentLSNIdx=0;
      currentLSNs=new long[nLSNs];
      for (int i=0; i < nLSNs; i++) {
        currentLSNs[i]=DbLsn.makeLsn(tempFileNumbers[i],tempFileOffsets[i]);
      }
      Arrays.sort(currentLSNs);
      accumulatedLSNFileNumbers=null;
      accumulatedLSNFileOffsets=null;
    }
  }
  private void accumulateLSNs(  IN in) throws DatabaseException {
    boolean accumulate=true;
    if (!accumulateLNs) {
      if ((!dups && (in instanceof BIN)) || (in instanceof DBIN)) {
        accumulate=false;
      }
    }
    for (int i=0; i < in.getNEntries(); i++) {
      if (in.isEntryPendingDeleted(i) || in.isEntryKnownDeleted(i)) {
        continue;
      }
      long lsn=in.getLsn(i);
      Node node=in.getTarget(i);
      if (accumulate && (node == null)) {
        if (accumulatedLSNFileNumbers == null) {
          accumulatedLSNFileNumbers=new OffsetList();
          accumulatedLSNFileOffsets=new OffsetList();
        }
        accumulatedLSNFileNumbers.add(DbLsn.getFileNumber(lsn),false);
        accumulatedLSNFileOffsets.add(DbLsn.getFileOffset(lsn),false);
        addToLsnINMap(new Long(lsn),in,i);
      }
 else {
        callback.processLSN(lsn,(node == null) ? LogEntryType.LOG_LN : node.getLogType());
      }
    }
    if (in instanceof DIN) {
      if (in.isRoot()) {
        DIN din=(DIN)in;
        callback.processLSN(din.getDupCountLNRef().getLsn(),LogEntryType.LOG_DUPCOUNTLN);
      }
    }
  }
  private void fetchAndProcessLSN(  long lsn) throws DatabaseException {
    Node node=fetchLSN(lsn);
    if (node != null) {
      callback.processLSN(lsn,node.getLogType());
      if (node instanceof IN) {
        accumulateLSNs((IN)node);
      }
    }
  }
  /** 
 * The default behavior fetches the rootIN from the log, but classes
 * extending this may fetch the root from the tree.
 */
  protected IN getRootIN(  long rootLsn) throws DatabaseException {
    return (IN)envImpl.getLogManager().get(rootLsn);
  }
  protected void releaseRootIN(  IN ignore) throws DatabaseException {
  }
  protected void addToLsnINMap(  Long lsn,  IN in,  int index){
  }
  protected Node fetchLSN(  long lsn) throws DatabaseException {
    return (Node)envImpl.getLogManager().get(lsn);
  }
@MethodObject static class SortedLSNTreeWalker_extractINsForDb {
    SortedLSNTreeWalker_extractINsForDb(    SortedLSNTreeWalker _this,    INList inList){
      this._this=_this;
      this.inList=inList;
    }
    boolean execute() throws DatabaseException {
      foundSome=false;
      foundSet=new HashSet();
      this.hook360();
      this.hook356();
      try {
        this.hook357();
        iter=inList.iterator();
        while (iter.hasNext()) {
          thisIN=(IN)iter.next();
          if (thisIN.getDatabase() == _this.dbImpl) {
            foundSome=true;
            if (_this.removeINsFromINList) {
              iter.remove();
              this.hook361();
            }
            foundSet.add(thisIN);
          }
        }
      }
 catch (      DatabaseException e) {
        this.hook362();
        throw e;
      }
 finally {
        this.hook358();
      }
      if (foundSome) {
        iter1=foundSet.iterator();
        while (iter1.hasNext()) {
          thisIN1=(IN)iter1.next();
          _this.accumulateLSNs(thisIN1);
        }
      }
      foundSet=null;
      return foundSome;
    }
    protected SortedLSNTreeWalker _this;
    protected INList inList;
    protected boolean foundSome;
    protected Set foundSet;
    protected long memoryChange;
    protected MemoryBudget mb;
    protected Iterator iter;
    protected IN thisIN;
    protected Iterator iter1;
    protected IN thisIN1;
    protected void hook356() throws DatabaseException {
    }
    protected void hook357() throws DatabaseException {
    }
    protected void hook358() throws DatabaseException {
    }
    protected void hook360() throws DatabaseException {
    }
    protected void hook361() throws DatabaseException {
    }
    protected void hook362() throws DatabaseException {
    }
  }
  protected void hook359() throws DatabaseException {
  }
}
\00base/com/sleepycat/je/dbi/MemoryBudget.java:package com.sleepycat.je.dbi;
import java.util.Iterator;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.DBIN;
import com.sleepycat.je.tree.DIN;
import com.sleepycat.je.tree.IN;
import de.ovgu.cide.jakutil.*;
/** 
 * MemoryBudget calculates the available memory for JE and how to apportion it
 * between cache and log buffers. It is meant to centralize all memory
 * calculations. Objects that ask for memory budgets should get settings from
 * this class, rather than using the configuration parameter values directly.
 */
public class MemoryBudget implements EnvConfigObserver {
static {
    sinit();
  }
  private static void sinit(){
    new MemoryBudget_sinit().execute();
  }
  public final static long MIN_MAX_MEMORY_SIZE=96 * 1024;
  public final static String MIN_MAX_MEMORY_SIZE_STRING=Long.toString(MIN_MAX_MEMORY_SIZE);
  private final static long N_64MB=(1 << 26);
  private long maxMemory;
  private long logBufferBudget;
  private EnvironmentImpl envImpl;
  MemoryBudget(  EnvironmentImpl envImpl,  DbConfigManager configManager) throws DatabaseException {
    this.envImpl=envImpl;
    envImpl.addConfigObserver(this);
    reset(configManager);
    this.hook351(configManager);
  }
  /** 
 * Respond to config updates.
 */
  public void envConfigUpdate(  DbConfigManager configManager) throws DatabaseException {
    long oldLogBufferBudget=logBufferBudget;
    reset(configManager);
    if (oldLogBufferBudget != logBufferBudget) {
      envImpl.getLogManager().resetPool(configManager);
    }
  }
  /** 
 * Initialize at construction time and when the cache is resized.
 */
  private void reset(  DbConfigManager configManager) throws DatabaseException {
    new MemoryBudget_reset(this,configManager).execute();
  }
  /** 
 * Returns Runtime.maxMemory(), accounting for a MacOS bug. May return
 * Long.MAX_VALUE if there is no inherent limit. Used by unit tests as well
 * as by this class.
 */
  public static long getRuntimeMaxMemory(){
    if ("Mac OS X".equals(System.getProperty("os.name"))) {
      String jvmVersion=System.getProperty("java.version");
      if (jvmVersion != null && jvmVersion.startsWith("1.4.2")) {
        return Long.MAX_VALUE;
      }
    }
    return Runtime.getRuntime().maxMemory();
  }
  public long getLogBufferBudget(){
    return logBufferBudget;
  }
  public long getMaxMemory(){
    return maxMemory;
  }
@MethodObject static class MemoryBudget_sinit {
    void execute(){
      this.hook348();
    }
    protected boolean is64;
    protected boolean isJVM14;
    protected String overrideArch;
    protected String arch;
    protected RuntimeException RE;
    protected void hook348(){
    }
  }
@MethodObject static class MemoryBudget_reset {
    MemoryBudget_reset(    MemoryBudget _this,    DbConfigManager configManager){
      this._this=_this;
      this.configManager=configManager;
    }
    void execute() throws DatabaseException {
      newMaxMemory=configManager.getLong(EnvironmentParams.MAX_MEMORY);
      jvmMemory=_this.getRuntimeMaxMemory();
      if (newMaxMemory != 0) {
        if (jvmMemory < newMaxMemory) {
          throw new IllegalArgumentException(EnvironmentParams.MAX_MEMORY.getName() + " has a value of " + newMaxMemory+ " but the JVM is only configured for "+ jvmMemory+ ". Consider using je.maxMemoryPercent.");
        }
        if (newMaxMemory < _this.MIN_MAX_MEMORY_SIZE) {
          throw new IllegalArgumentException(EnvironmentParams.MAX_MEMORY.getName() + " is " + newMaxMemory+ " which is less than the minimum: "+ _this.MIN_MAX_MEMORY_SIZE);
        }
      }
 else {
        if (jvmMemory == Long.MAX_VALUE) {
          jvmMemory=_this.N_64MB;
        }
        maxMemoryPercent=configManager.getInt(EnvironmentParams.MAX_MEMORY_PERCENT);
        newMaxMemory=(maxMemoryPercent * jvmMemory) / 100;
      }
      newLogBufferBudget=configManager.getLong(EnvironmentParams.LOG_MEM_SIZE);
      if (newLogBufferBudget == 0) {
        newLogBufferBudget=newMaxMemory >> 4;
      }
 else       if (newLogBufferBudget > newMaxMemory / 2) {
        newLogBufferBudget=newMaxMemory / 2;
      }
      numBuffers=configManager.getInt(EnvironmentParams.NUM_LOG_BUFFERS);
      startingBufferSize=newLogBufferBudget / numBuffers;
      logBufferSize=configManager.getInt(EnvironmentParams.LOG_BUFFER_MAX_SIZE);
      if (startingBufferSize > logBufferSize) {
        startingBufferSize=logBufferSize;
        newLogBufferBudget=numBuffers * startingBufferSize;
      }
 else       if (startingBufferSize < EnvironmentParams.MIN_LOG_BUFFER_SIZE) {
        startingBufferSize=EnvironmentParams.MIN_LOG_BUFFER_SIZE;
        newLogBufferBudget=numBuffers * startingBufferSize;
      }
      this.hook350();
      newTrackerBudget=(newMaxMemory * _this.envImpl.getConfigManager().getInt(EnvironmentParams.CLEANER_DETAIL_MAX_MEMORY_PERCENTAGE)) / 100;
      _this.maxMemory=newMaxMemory;
      this.hook349();
      _this.logBufferBudget=newLogBufferBudget;
    }
    protected MemoryBudget _this;
    protected DbConfigManager configManager;
    protected long newMaxMemory;
    protected long jvmMemory;
    protected int maxMemoryPercent;
    protected long newLogBufferBudget;
    protected int numBuffers;
    protected long startingBufferSize;
    protected int logBufferSize;
    protected long newCriticalThreshold;
    protected long newTrackerBudget;
    protected void hook349() throws DatabaseException {
    }
    protected void hook350() throws DatabaseException {
    }
  }
  protected void hook351(  DbConfigManager configManager) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/dbi/PreloadProcessor.java:package com.sleepycat.je.dbi;
import com.sleepycat.je.PreloadStats;
import com.sleepycat.je.dbi.SortedLSNTreeWalker.TreeNodeProcessor;
import com.sleepycat.je.log.LogEntryType;
import com.sleepycat.je.utilint.DbLsn;
import de.ovgu.cide.jakutil.*;
class PreloadProcessor implements TreeNodeProcessor {
  private EnvironmentImpl envImpl;
  private long maxBytes;
  private long targetTime;
  PreloadProcessor(  EnvironmentImpl envImpl,  long maxBytes,  long targetTime,  PreloadStats stats){
    this.envImpl=envImpl;
    this.maxBytes=maxBytes;
    this.targetTime=targetTime;
    this.hook353(stats);
  }
  /** 
 * Called for each LSN that the SortedLSNTreeWalker encounters.
 */
  public void processLSN(  long childLsn,  LogEntryType childType){
    assert childLsn != DbLsn.NULL_LSN;
    if (System.currentTimeMillis() > targetTime) {
      throw DatabaseImpl.timeExceededPreloadException;
    }
    this.hook355();
    this.hook354(childType);
  }
  protected void hook353(  PreloadStats stats){
  }
  protected void hook354(  LogEntryType childType){
  }
  protected void hook355(){
  }
}
\00base/com/sleepycat/je/dbi/CursorImpl.java:package com.sleepycat.je.dbi;
import java.util.Comparator;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.LockStats;
import com.sleepycat.je.OperationStatus;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.log.LogUtils;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.BINBoundary;
import com.sleepycat.je.tree.DBIN;
import com.sleepycat.je.tree.DIN;
import com.sleepycat.je.tree.DupCountLN;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Key;
import com.sleepycat.je.tree.LN;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.TreeWalkerStatsAccumulator;
import com.sleepycat.je.txn.BasicLocker;
import com.sleepycat.je.txn.LockGrantType;
import com.sleepycat.je.txn.LockResult;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.txn.Locker;
import com.sleepycat.je.txn.ThreadLocker;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.TestHook;
import com.sleepycat.je.utilint.TestHookExecute;
import de.ovgu.cide.jakutil.*;
/** 
 * A CursorImpl is the internal implementation of the cursor.
 */
public class CursorImpl implements Cloneable {
  private static final byte CURSOR_NOT_INITIALIZED=1;
  private static final byte CURSOR_INITIALIZED=2;
  private static final byte CURSOR_CLOSED=3;
  private static final String TRACE_DELETE="Delete";
  private static final String TRACE_MOD="Mod:";
  volatile private BIN bin;
  volatile private int index;
  volatile private DBIN dupBin;
  volatile private int dupIndex;
  volatile private BIN binToBeRemoved;
  volatile private DBIN dupBinToBeRemoved;
  private BIN targetBin;
  private int targetIndex;
  private byte[] dupKey;
  private DatabaseImpl database;
  private Locker locker;
  private CursorImpl lockerPrev;
  private CursorImpl lockerNext;
  private boolean retainNonTxnLocks;
  private byte status;
  private TestHook testHook;
  private boolean nonCloning=false;
  private int thisId;
  private static long lastAllocatedId=0;
  private ThreadLocal treeStatsAccumulatorTL=new ThreadLocal();
  private static long getNextCursorId(){
    return ++lastAllocatedId;
  }
  public int hashCode(){
    return thisId;
  }
  private TreeWalkerStatsAccumulator getTreeStatsAccumulator(){
    if (EnvironmentImpl.getThreadLocalReferenceCount() > 0) {
      return (TreeWalkerStatsAccumulator)treeStatsAccumulatorTL.get();
    }
 else {
      return null;
    }
  }
  public void incrementLNCount(){
    TreeWalkerStatsAccumulator treeStatsAccumulator=getTreeStatsAccumulator();
    if (treeStatsAccumulator != null) {
      treeStatsAccumulator.incrementLNCount();
    }
  }
  public void setNonCloning(  boolean nonCloning){
    this.nonCloning=nonCloning;
  }
  /** 
 * public for Cursor et al
 */
public static class SearchMode {
    public static final SearchMode SET=new SearchMode(true,false,"SET");
    public static final SearchMode BOTH=new SearchMode(true,true,"BOTH");
    public static final SearchMode SET_RANGE=new SearchMode(false,false,"SET_RANGE");
    public static final SearchMode BOTH_RANGE=new SearchMode(false,true,"BOTH_RANGE");
    private boolean exactSearch;
    private boolean dataSearch;
    private String name;
    private SearchMode(    boolean exactSearch,    boolean dataSearch,    String name){
      this.exactSearch=exactSearch;
      this.dataSearch=dataSearch;
      this.name="SearchMode." + name;
    }
    /** 
 * Returns true when the key or key/data search is exact, i.e., for SET
 * and BOTH.
 */
    public final boolean isExactSearch(){
      return exactSearch;
    }
    /** 
 * Returns true when the data value is included in the search, i.e., for
 * BOTH and BOTH_RANGE.
 */
    public final boolean isDataSearch(){
      return dataSearch;
    }
    public String toString(){
      return name;
    }
  }
  /** 
 * Holder for an OperationStatus and a keyChange flag. Is used for search
 * and getNextWithKeyChangeStatus operations.
 */
public static class KeyChangeStatus {
    /** 
 * Operation status;
 */
    public OperationStatus status;
    /** 
 * Whether the operation moved to a new key.
 */
    public boolean keyChange;
    public KeyChangeStatus(    OperationStatus status,    boolean keyChange){
      this.status=status;
      this.keyChange=keyChange;
    }
  }
  /** 
 * Creates a cursor with retainNonTxnLocks=true.
 */
  public CursorImpl(  DatabaseImpl database,  Locker locker) throws DatabaseException {
    this(database,locker,true);
  }
  /** 
 * Creates a cursor.
 * @param retainNonTxnLocksis true if non-transactional locks should be retained (not
 * released automatically) when the cursor is closed.
 */
  public CursorImpl(  DatabaseImpl database,  Locker locker,  boolean retainNonTxnLocks) throws DatabaseException {
    thisId=(int)getNextCursorId();
    bin=null;
    index=-1;
    dupBin=null;
    dupIndex=-1;
    assert !(retainNonTxnLocks && (locker instanceof ThreadLocker));
    assert !(!retainNonTxnLocks && locker.getClass() == BasicLocker.class);
    this.retainNonTxnLocks=retainNonTxnLocks;
    this.database=database;
    this.locker=locker;
    this.locker.registerCursor(this);
    status=CURSOR_NOT_INITIALIZED;
  }
  /** 
 * Shallow copy. addCursor() is optionally called.
 */
  public CursorImpl cloneCursor(  boolean addCursor) throws DatabaseException {
    return cloneCursor(addCursor,null);
  }
  /** 
 * Shallow copy. addCursor() is optionally called. Allows inheriting the BIN
 * position from some other cursor.
 */
  public CursorImpl cloneCursor(  boolean addCursor,  CursorImpl usePosition) throws DatabaseException {
    CursorImpl ret=null;
    if (nonCloning) {
      ret=this;
    }
 else {
      try {
        this.hook206();
        ret=(CursorImpl)super.clone();
        if (!retainNonTxnLocks) {
          ret.locker=locker.newNonTxnLocker();
        }
        ret.locker.registerCursor(ret);
        if (usePosition != null && usePosition.status == CURSOR_INITIALIZED) {
          ret.bin=usePosition.bin;
          ret.index=usePosition.index;
          ret.dupBin=usePosition.dupBin;
          ret.dupIndex=usePosition.dupIndex;
        }
        if (addCursor) {
          ret.addCursor();
        }
      }
 catch (      CloneNotSupportedException cannotOccur) {
        return null;
      }
 finally {
        this.hook207();
      }
    }
    return ret;
  }
  public int getIndex(){
    return index;
  }
  public void setIndex(  int idx){
    index=idx;
  }
  public BIN getBIN(){
    return bin;
  }
  public void setBIN(  BIN newBin){
    bin=newBin;
  }
  public BIN getBINToBeRemoved(){
    return binToBeRemoved;
  }
  public int getDupIndex(){
    return dupIndex;
  }
  public void setDupIndex(  int dupIdx){
    dupIndex=dupIdx;
  }
  public DBIN getDupBIN(){
    return dupBin;
  }
  public void setDupBIN(  DBIN newDupBin){
    dupBin=newDupBin;
  }
  public DBIN getDupBINToBeRemoved(){
    return dupBinToBeRemoved;
  }
  public void setTreeStatsAccumulator(  TreeWalkerStatsAccumulator tSA){
    treeStatsAccumulatorTL.set(tSA);
  }
  /** 
 * Figure out which BIN/index set to use.
 */
  private boolean setTargetBin(){
    targetBin=null;
    targetIndex=0;
    boolean isDup=(dupBin != null);
    dupKey=null;
    if (isDup) {
      targetBin=dupBin;
      targetIndex=dupIndex;
      dupKey=dupBin.getDupKey();
    }
 else {
      targetBin=bin;
      targetIndex=index;
    }
    return isDup;
  }
  /** 
 * Advance a cursor. Used so that verify can advance a cursor even in the
 * face of an exception [12932].
 * @param keyon return contains the key if available, or null.
 * @param dataon return contains the data if available, or null.
 */
  public boolean advanceCursor(  DatabaseEntry key,  DatabaseEntry data){
    BIN oldBin=bin;
    BIN oldDupBin=dupBin;
    int oldIndex=index;
    int oldDupIndex=dupIndex;
    key.setData(null);
    data.setData(null);
    try {
      getNext(key,data,LockType.NONE,true,false);
    }
 catch (    DatabaseException ignored) {
    }
    if (bin != oldBin || dupBin != oldDupBin || index != oldIndex || dupIndex != oldDupIndex) {
      if (key.getData() == null && bin != null && index > 0) {
        setDbt(key,bin.getKey(index));
      }
      if (data.getData() == null && dupBin != null && dupIndex > 0) {
        setDbt(data,dupBin.getKey(dupIndex));
      }
      return true;
    }
 else {
      return false;
    }
  }
  public BIN latchBIN() throws DatabaseException {
    return new CursorImpl_latchBIN(this).execute();
  }
  public DBIN latchDBIN() throws DatabaseException {
    return new CursorImpl_latchDBIN(this).execute();
  }
  public Locker getLocker(){
    return locker;
  }
  public void addCursor(  BIN bin){
    if (bin != null) {
      this.hook208(bin);
      bin.addCursor(this);
    }
  }
  /** 
 * Add to the current cursor. (For dups)
 */
  public void addCursor(){
    if (dupBin != null) {
      addCursor(dupBin);
    }
    if (bin != null) {
      addCursor(bin);
    }
  }
  public void updateBin(  BIN bin,  int index) throws DatabaseException {
    removeCursorDBIN();
    setDupIndex(-1);
    setDupBIN(null);
    setIndex(index);
    setBIN(bin);
    addCursor(bin);
  }
  public void updateDBin(  DBIN dupBin,  int dupIndex){
    setDupIndex(dupIndex);
    setDupBIN(dupBin);
    addCursor(dupBin);
  }
  private void removeCursor() throws DatabaseException {
    removeCursorBIN();
    removeCursorDBIN();
  }
  private void removeCursorBIN() throws DatabaseException {
    BIN abin=latchBIN();
    if (abin != null) {
      abin.removeCursor(this);
      this.hook209(abin);
    }
  }
  private void removeCursorDBIN() throws DatabaseException {
    DBIN abin=latchDBIN();
    if (abin != null) {
      abin.removeCursor(this);
      this.hook210(abin);
    }
  }
  /** 
 * Clear the reference to the dup tree, if any.
 */
  public void clearDupBIN(  boolean alreadyLatched) throws DatabaseException {
    if (dupBin != null) {
      if (alreadyLatched) {
        dupBin.removeCursor(this);
        this.hook211();
      }
 else {
        removeCursorDBIN();
      }
      dupBin=null;
      dupIndex=-1;
    }
  }
  public void dumpTree() throws DatabaseException {
    database.getTree().dump();
  }
  /** 
 * @return true if this cursor is closed
 */
  public boolean isClosed(){
    return (status == CURSOR_CLOSED);
  }
  /** 
 * @return true if this cursor is not initialized
 */
  public boolean isNotInitialized(){
    return (status == CURSOR_NOT_INITIALIZED);
  }
  /** 
 * Reset a cursor to an uninitialized state, but unlike close(), allow it to
 * be used further.
 */
  public void reset() throws DatabaseException {
    removeCursor();
    if (!retainNonTxnLocks) {
      locker.releaseNonTxnLocks();
    }
    bin=null;
    index=-1;
    dupBin=null;
    dupIndex=-1;
    status=CURSOR_NOT_INITIALIZED;
  }
  /** 
 * Close a cursor.
 * @throws DatabaseExceptionif the cursor was previously closed.
 */
  public void close() throws DatabaseException {
    assert assertCursorState(false) : dumpToString(true);
    removeCursor();
    locker.unRegisterCursor(this);
    if (!retainNonTxnLocks) {
      locker.releaseNonTxnLocks();
    }
    status=CURSOR_CLOSED;
  }
  public int count(  LockType lockType) throws DatabaseException {
    try {
      assert assertCursorState(true) : dumpToString(true);
      if (!database.getSortedDuplicates()) {
        return 1;
      }
      if (bin == null) {
        return 0;
      }
      this.hook212(lockType);
      throw ReturnHack.returnInt;
    }
 catch (    ReturnInt r) {
      return r.value;
    }
  }
  /** 
 * Delete the item pointed to by the cursor. If cursor is not initialized or
 * item is already deleted, return appropriate codes. Returns with nothing
 * latched. bin and dupBin are latched as appropriate.
 * @return 0 on success, appropriate error code otherwise.
 */
  public OperationStatus delete() throws DatabaseException {
    assert assertCursorState(true) : dumpToString(true);
    boolean isDup=setTargetBin();
    if (targetBin == null) {
      return OperationStatus.KEYEMPTY;
    }
    if (targetBin.isEntryKnownDeleted(targetIndex)) {
      this.hook214();
      return OperationStatus.KEYEMPTY;
    }
    LN ln=(LN)targetBin.fetchTarget(targetIndex);
    if (ln == null) {
      this.hook215();
      return OperationStatus.KEYEMPTY;
    }
    LockResult lockResult=lockLN(ln,LockType.WRITE);
    ln=lockResult.getLN();
    if (ln == null) {
      this.hook216();
      return OperationStatus.KEYEMPTY;
    }
    LockResult dclLockResult=null;
    DIN dupRoot=null;
    this.hook213(isDup,ln,lockResult,dclLockResult,dupRoot);
    return OperationStatus.SUCCESS;
  }
  /** 
 * Return a new copy of the cursor. If position is true, position the
 * returned cursor at the same position.
 */
  public CursorImpl dup(  boolean samePosition) throws DatabaseException {
    assert assertCursorState(false) : dumpToString(true);
    CursorImpl ret=cloneCursor(samePosition);
    if (!samePosition) {
      ret.bin=null;
      ret.index=-1;
      ret.dupBin=null;
      ret.dupIndex=-1;
      ret.status=CURSOR_NOT_INITIALIZED;
    }
    return ret;
  }
  /** 
 * Search for the next key (or duplicate) following the given key (and
 * datum), and acquire a range insert lock on it. If there are no more
 * records following the given key and datum, lock the special EOF node for
 * the database.
 */
  public void lockNextKeyForInsert(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    new CursorImpl_lockNextKeyForInsert(this,key,data).execute();
  }
  /** 
 * Insert the given LN in the tree or return KEYEXIST if the key is already
 * present.
 * <p>
 * This method is called directly internally for putting tree map LNs and
 * file summary LNs. It should not be used otherwise, and in the future we
 * should find a way to remove this special case.
 * </p>
 */
  public OperationStatus putLN(  byte[] key,  LN ln,  boolean allowDuplicates) throws DatabaseException {
    assert assertCursorState(false) : dumpToString(true);
    this.hook217();
    LockResult lockResult=locker.lock(ln.getNodeId(),LockType.WRITE,false,database);
    if (database.getTree().insert(ln,key,allowDuplicates,this,lockResult)) {
      status=CURSOR_INITIALIZED;
      return OperationStatus.SUCCESS;
    }
 else {
      locker.releaseLock(ln.getNodeId());
      return OperationStatus.KEYEXIST;
    }
  }
  /** 
 * Insert or overwrite the key/data pair.
 * @param key
 * @param data
 * @return 0 if successful, failure status value otherwise
 */
  public OperationStatus put(  DatabaseEntry key,  DatabaseEntry data,  DatabaseEntry foundData) throws DatabaseException {
    assert assertCursorState(false) : dumpToString(true);
    OperationStatus result=putLN(Key.makeKey(key),new LN(data),database.getSortedDuplicates());
    if (result == OperationStatus.KEYEXIST) {
      status=CURSOR_INITIALIZED;
      result=putCurrent(data,null,foundData);
    }
    return result;
  }
  /** 
 * Insert the key/data pair in No Overwrite mode.
 * @param key
 * @param data
 * @return 0 if successful, failure status value otherwise
 */
  public OperationStatus putNoOverwrite(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    assert assertCursorState(false) : dumpToString(true);
    return putLN(Key.makeKey(key),new LN(data),false);
  }
  /** 
 * Insert the key/data pair as long as no entry for key/data exists yet.
 */
  public OperationStatus putNoDupData(  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    assert assertCursorState(false) : dumpToString(true);
    if (!database.getSortedDuplicates()) {
      throw new DatabaseException("putNoDupData() called, but database is not configured " + "for duplicate data.");
    }
    return putLN(Key.makeKey(key),new LN(data),true);
  }
  /** 
 * Modify the current record with this data.
 * @param data
 */
  public OperationStatus putCurrent(  DatabaseEntry data,  DatabaseEntry foundKey,  DatabaseEntry foundData) throws DatabaseException {
    try {
      assert assertCursorState(true) : dumpToString(true);
      if (foundKey != null) {
        foundKey.setData(null);
      }
      if (foundData != null) {
        foundData.setData(null);
      }
      if (bin == null) {
        return OperationStatus.KEYEMPTY;
      }
      this.hook219();
      boolean isDup=setTargetBin();
      this.hook218(data,foundKey,foundData,isDup);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (OperationStatus)r.value;
    }
  }
  /** 
 * Retrieve the current record.
 */
  public OperationStatus getCurrent(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType) throws DatabaseException {
    assert assertCursorState(true) : dumpToString(true);
    if (bin == null) {
      return OperationStatus.KEYEMPTY;
    }
    this.hook220();
    return getCurrentAlreadyLatched(foundKey,foundData,lockType,true);
  }
  /** 
 * Retrieve the current record. Assume the bin is already latched. Return
 * with the target bin unlatched.
 */
  public OperationStatus getCurrentAlreadyLatched(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType,  boolean first) throws DatabaseException {
    try {
      assert assertCursorState(true) : dumpToString(true);
      this.hook221(foundKey,foundData,lockType,first);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (OperationStatus)r.value;
    }
  }
  /** 
 * Retrieve the current LN, return with the target bin unlatched.
 */
  public LN getCurrentLN(  LockType lockType) throws DatabaseException {
    assert assertCursorState(true) : dumpToString(true);
    if (bin == null) {
      return null;
    }
 else {
      this.hook222();
      return getCurrentLNAlreadyLatched(lockType);
    }
  }
  /** 
 * Retrieve the current LN, assuming the BIN is already latched. Return with
 * the target BIN unlatched.
 */
  public LN getCurrentLNAlreadyLatched(  LockType lockType) throws DatabaseException {
    try {
      this.hook223(lockType);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (LN)r.value;
    }
  }
  public OperationStatus getNext(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType,  boolean forward,  boolean alreadyLatched) throws DatabaseException {
    return getNextWithKeyChangeStatus(foundKey,foundData,lockType,forward,alreadyLatched).status;
  }
  /** 
 * Move the cursor forward and return the next record. This will cross BIN
 * boundaries and dip into duplicate sets.
 * @param foundKeyDatabaseEntry to use for returning key
 * @param foundDataDatabaseEntry to use for returning data
 * @param forwardif true, move forward, else move backwards
 * @param alreadyLatchedif true, the bin that we're on is already latched.
 * @return the status and an indication of whether we advanced to a new key
 * during the operation.
 */
  public KeyChangeStatus getNextWithKeyChangeStatus(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType,  boolean forward,  boolean alreadyLatched) throws DatabaseException {
    assert assertCursorState(true) : dumpToString(true);
    this.hook224(alreadyLatched);
    KeyChangeStatus result=new KeyChangeStatus(OperationStatus.NOTFOUND,true);
    try {
      while (bin != null) {
        if (dupBin != null) {
          this.hook277();
          if (getNextDuplicate(foundKey,foundData,lockType,forward,alreadyLatched) == OperationStatus.SUCCESS) {
            result.status=OperationStatus.SUCCESS;
            result.keyChange=false;
            break;
          }
 else {
            removeCursorDBIN();
            alreadyLatched=this.hook226(alreadyLatched);
            dupBin=null;
            dupIndex=-1;
            continue;
          }
        }
        alreadyLatched=this.hook225(alreadyLatched);
        this.hook276();
        if ((forward && ++index < bin.getNEntries()) || (!forward && --index > -1)) {
          OperationStatus ret=getCurrentAlreadyLatched(foundKey,foundData,lockType,forward);
          if (ret == OperationStatus.SUCCESS) {
            incrementLNCount();
            result.status=OperationStatus.SUCCESS;
            break;
          }
 else {
            this.hook227();
            if (binToBeRemoved != null) {
              flushBINToBeRemoved();
            }
            continue;
          }
        }
 else {
          if (binToBeRemoved != null) {
            this.hook229();
            flushBINToBeRemoved();
            this.hook228();
          }
          binToBeRemoved=bin;
          bin=null;
          BIN newBin;
          assert TestHookExecute.doHookIfSet(testHook);
          if (forward) {
            newBin=database.getTree().getNextBin(binToBeRemoved,false);
          }
 else {
            newBin=database.getTree().getPrevBin(binToBeRemoved,false);
          }
          if (newBin == null) {
            result.status=OperationStatus.NOTFOUND;
            break;
          }
 else {
            if (forward) {
              index=-1;
            }
 else {
              index=newBin.getNEntries();
            }
            addCursor(newBin);
            bin=newBin;
            this.hook230(alreadyLatched);
          }
        }
      }
    }
  finally {
      this.hook231();
      if (binToBeRemoved != null) {
        flushBINToBeRemoved();
      }
    }
    return result;
  }
  private void flushBINToBeRemoved() throws DatabaseException {
    binToBeRemoved.removeCursor(this);
    this.hook232();
    binToBeRemoved=null;
  }
  public OperationStatus getNextNoDup(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType,  boolean forward,  boolean alreadyLatched) throws DatabaseException {
    assert assertCursorState(true) : dumpToString(true);
    if (dupBin != null) {
      clearDupBIN(alreadyLatched);
      alreadyLatched=false;
    }
    return getNext(foundKey,foundData,lockType,forward,alreadyLatched);
  }
  /** 
 * Retrieve the first duplicate at the current cursor position.
 */
  public OperationStatus getFirstDuplicate(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType) throws DatabaseException {
    assert assertCursorState(true) : dumpToString(true);
    if (dupBin != null) {
      removeCursorDBIN();
      dupBin=null;
      dupIndex=-1;
    }
    return getCurrent(foundKey,foundData,lockType);
  }
  /** 
 * Enter with dupBin unlatched. Pass foundKey == null to just advance cursor
 * to next duplicate without fetching data.
 */
  public OperationStatus getNextDuplicate(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType,  boolean forward,  boolean alreadyLatched) throws DatabaseException {
    return new CursorImpl_getNextDuplicate(this,foundKey,foundData,lockType,forward,alreadyLatched).execute();
  }
  private void flushDBINToBeRemoved() throws DatabaseException {
    dupBinToBeRemoved.removeCursor(this);
    this.hook233();
    dupBinToBeRemoved=null;
  }
  /** 
 * Position the cursor at the first or last record of the database. It's
 * okay if this record is deleted. Returns with the target BIN latched.
 * @return true if a first or last position is found, false if the tree
 * being searched is empty.
 */
  public boolean positionFirstOrLast(  boolean first,  DIN duplicateRoot) throws DatabaseException {
    try {
      assert assertCursorState(false) : dumpToString(true);
      IN in=null;
      boolean found=false;
      this.hook234(first,duplicateRoot,in,found);
      throw ReturnHack.returnBoolean;
    }
 catch (    ReturnBoolean r) {
      return r.value;
    }
  }
  public static final int FOUND=0x1;
  public static final int EXACT_KEY=0x2;
  public static final int EXACT_DATA=0x4;
  public static final int FOUND_LAST=0x8;
  /** 
 * Position the cursor at the key. This returns a three part value that's
 * bitwise or'ed into the int. We find out if there was any kind of match
 * and if the match was exact. Note that this match focuses on whether the
 * searching criteria (key, or key and data, depending on the search type)
 * is met.
 * <p>
 * Note this returns with the BIN latched!
 * </p>
 * <p>
 * If this method returns without the FOUND bit set, the caller can assume
 * that no match is possible. Otherwise, if the FOUND bit is set, the caller
 * should check the EXACT_KEY and EXACT_DATA bits. If EXACT_KEY is not set
 * (or for BOTH and BOTH_RANGE, if EXACT_DATA is not set), an approximate
 * match was found. In an approximate match, the cursor is always positioned
 * before the target key/data. This allows the caller to perform a 'next'
 * operation to advance to the value that is equal or higher than the target
 * key/data.
 * </p>
 * <p>
 * Even if the search returns an exact result, the record may be deleted.
 * The caller must therefore check for both an approximate match and for
 * whether the cursor is positioned on a deleted record.
 * </p>
 * <p>
 * If SET or BOTH is specified, the FOUND bit will only be returned if an
 * exact match is found. However, the record found may be deleted.
 * </p>
 * <p>
 * There is one special case where this method may be called without
 * checking the EXACT_KEY (and EXACT_DATA) bits and without checking for a
 * deleted record: If SearchMode.SET is specified then only the FOUND bit
 * need be checked. When SET is specified and FOUND is returned, it is
 * guaranteed to be an exact match on a non-deleted record. It is for this
 * case only that this method is public.
 * </p>
 * <p>
 * If FOUND is set, FOUND_LAST may also be set if the cursor is positioned
 * on the last record in the database. Note that this state can only be
 * counted on as long as the BIN is latched, so it is not set if this method
 * must release the latch to lock the record. Therefore, it should only be
 * used for optimizations. If FOUND_LAST is set, the cursor is positioned on
 * the last record and the BIN is latched. If FOUND_LAST is not set, the
 * cursor may or may not be positioned on the last record. Note that exact
 * searches always perform an unlatch and a lock, so FOUND_LAST will only be
 * set for inexact (range) searches.
 * </p>
 * <p>
 * Be aware that when an approximate match is returned, the index or
 * dupIndex may be set to -1. This is done intentionally so that a 'next'
 * operation will increment it.
 * </p>
 */
  public int searchAndPosition(  DatabaseEntry matchKey,  DatabaseEntry matchData,  SearchMode searchMode,  LockType lockType) throws DatabaseException {
    try {
      assert assertCursorState(false) : dumpToString(true);
      removeCursor();
      bin=null;
      boolean foundSomething=false;
      boolean foundExactKey=false;
      boolean foundExactData=false;
      boolean foundLast=false;
      boolean exactSearch=searchMode.isExactSearch();
      BINBoundary binBoundary=new BINBoundary();
      this.hook235(matchKey,matchData,searchMode,lockType,foundSomething,foundExactKey,foundExactData,foundLast,exactSearch,binBoundary);
      throw ReturnHack.returnInt;
    }
 catch (    ReturnInt r) {
      return r.value;
    }
  }
  /** 
 * For this type of search, we need to match both key and data. This method
 * is called after the key is matched to perform the data portion of the
 * match. We may be matching just against an LN, or doing further searching
 * into the dup tree. See searchAndPosition for more details.
 */
  private int searchAndPositionBoth(  boolean containsDuplicates,  Node n,  DatabaseEntry matchData,  boolean exactSearch,  LockType lockType,  long oldLsn) throws DatabaseException {
    assert assertCursorState(false) : dumpToString(true);
    boolean found=false;
    boolean exact=false;
    assert (matchData != null);
    byte[] data=Key.makeKey(matchData);
    if (containsDuplicates) {
      DIN duplicateRoot=(DIN)n;
      this.hook236(duplicateRoot);
      dupBin=(DBIN)database.getTree().searchSubTree(duplicateRoot,data,Tree.SearchType.NORMAL,-1,null,true);
      if (dupBin != null) {
        addCursor(dupBin);
        dupIndex=dupBin.findEntry(data,true,exactSearch);
        if (dupIndex >= 0) {
          if ((dupIndex & IN.EXACT_MATCH) != 0) {
            exact=true;
          }
          dupIndex&=~IN.EXACT_MATCH;
          found=true;
        }
 else {
          dupIndex=-1;
          found=!exactSearch;
        }
      }
    }
 else {
      LN ln=(LN)n;
      LockResult lockResult=lockLN(ln,lockType);
      ln=lockResult.getLN();
      if (ln == null) {
        found=!exactSearch;
      }
 else {
        dupBin=null;
        dupIndex=-1;
        int cmp=Key.compareKeys(ln.getData(),data,database.getDuplicateComparator());
        if (cmp == 0 || (cmp <= 0 && !exactSearch)) {
          if (cmp == 0) {
            exact=true;
          }
          found=true;
        }
 else {
          index--;
          found=!exactSearch;
        }
      }
    }
    return (found ? FOUND : 0) | (exact ? EXACT_DATA : 0);
  }
  private OperationStatus fetchCurrent(  DatabaseEntry foundKey,  DatabaseEntry foundData,  LockType lockType,  boolean first) throws DatabaseException {
    return new CursorImpl_fetchCurrent(this,foundKey,foundData,lockType,first).execute();
  }
  /** 
 * Locks the given LN's node ID; a deleted LN will not be locked or
 * returned. Attempts to use a non-blocking lock to avoid
 * unlatching/relatching. Retries if necessary, to handle the case where the
 * LN is changed while the BIN is unlatched.
 * Preconditions: The target BIN must be latched. When positioned in a dup
 * tree, the BIN may be latched on entry also and if so it will be latched
 * on exit.
 * Postconditions: The target BIN is latched. When positioned in a dup tree,
 * the BIN will be latched if it was latched on entry or a blocking lock was
 * needed. Therefore, when positioned in a dup tree, releaseDBIN should be
 * called.
 * @param lnthe LN to be locked.
 * @param lockTypethe type of lock requested.
 * @return the LockResult containing the LN that was locked, or containing a
 * null LN if the LN was deleted or cleaned. If the LN is deleted, a
 * lock will not be held.
 */
  private LockResult lockLN(  LN ln,  LockType lockType) throws DatabaseException {
    LockResult lockResult=lockLNDeletedAllowed(ln,lockType);
    ln=lockResult.getLN();
    if (ln != null) {
      setTargetBin();
      if (targetBin.isEntryKnownDeleted(targetIndex) || ln.isDeleted()) {
        revertLock(ln.getNodeId(),lockResult.getLockGrant());
        lockResult.setLN(null);
      }
    }
    return lockResult;
  }
  /** 
 * Locks the given LN's node ID; a deleted LN will be locked and returned.
 * Attempts to use a non-blocking lock to avoid unlatching/relatching.
 * Retries if necessary, to handle the case where the LN is changed while
 * the BIN is unlatched.
 * Preconditions: The target BIN must be latched. When positioned in a dup
 * tree, the BIN may be latched on entry also and if so it will be latched
 * on exit.
 * Postconditions: The target BIN is latched. When positioned in a dup tree,
 * the BIN will be latched if it was latched on entry or a blocking lock was
 * needed. Therefore, when positioned in a dup tree, releaseDBIN should be
 * called.
 * @param lnthe LN to be locked.
 * @param lockTypethe type of lock requested.
 * @return the LockResult containing the LN that was locked, or containing a
 * null LN if the LN was cleaned.
 */
  public LockResult lockLNDeletedAllowed(  LN ln,  LockType lockType) throws DatabaseException {
    LockResult lockResult;
    if (lockType == LockType.NONE) {
      lockResult=new LockResult(LockGrantType.NONE_NEEDED,null);
      lockResult.setLN(ln);
      return lockResult;
    }
    if (locker.getDefaultNoWait()) {
      lockResult=locker.lock(ln.getNodeId(),lockType,true,database);
    }
 else {
      lockResult=locker.nonBlockingLock(ln.getNodeId(),lockType,database);
    }
    if (lockResult.getLockGrant() != LockGrantType.DENIED) {
      lockResult.setLN(ln);
      return lockResult;
    }
    while (true) {
      long nodeId=ln.getNodeId();
      this.hook238();
      lockResult=locker.lock(nodeId,lockType,false,database);
      this.hook237();
      setTargetBin();
      ln=(LN)targetBin.fetchTarget(targetIndex);
      if (ln != null && nodeId != ln.getNodeId()) {
        revertLock(nodeId,lockResult.getLockGrant());
        continue;
      }
 else {
        lockResult.setLN(ln);
        return lockResult;
      }
    }
  }
  /** 
 * Locks the DupCountLN for the given duplicate root. Attempts to use a
 * non-blocking lock to avoid unlatching/relatching.
 * Preconditions: The dupRoot, BIN and DBIN are latched. Postconditions: The
 * dupRoot, BIN and DBIN are latched.
 * Note that the dupRoot may change during locking and should be refetched
 * if needed.
 * @param dupRootthe duplicate root containing the DupCountLN to be locked.
 * @param lockTypethe type of lock requested.
 * @return the LockResult containing the LN that was locked.
 */
  public LockResult lockDupCountLN(  DIN dupRoot,  LockType lockType) throws DatabaseException {
    DupCountLN ln=dupRoot.getDupCountLN();
    LockResult lockResult;
    if (locker.getDefaultNoWait()) {
      lockResult=locker.lock(ln.getNodeId(),lockType,true,database);
    }
 else {
      lockResult=locker.nonBlockingLock(ln.getNodeId(),lockType,database);
    }
    if (lockResult.getLockGrant() == LockGrantType.DENIED) {
      this.hook241(dupRoot);
      lockResult=locker.lock(ln.getNodeId(),lockType,false,database);
      this.hook240();
      dupRoot=(DIN)bin.fetchTarget(index);
      this.hook239(dupRoot);
      ln=dupRoot.getDupCountLN();
    }
    lockResult.setLN(ln);
    return lockResult;
  }
  /** 
 * Fetch, latch and return the DIN root of the duplicate tree at the cursor
 * position.
 * Preconditions: The BIN must be latched and the current BIN entry must
 * contain a DIN.
 * Postconditions: The BIN and DIN will be latched. The DBIN will remain
 * latched if isDBINLatched is true.
 * @param isDBINLatchedis true if the DBIN is currently latched.
 */
  public DIN getLatchedDupRoot(  boolean isDBINLatched) throws DatabaseException {
    assert bin != null;
    this.hook243();
    assert index >= 0;
    DIN dupRoot=(DIN)bin.fetchTarget(index);
    this.hook242(isDBINLatched,dupRoot);
    return dupRoot;
  }
  /** 
 * Helper to return a Data DBT from a BIN.
 */
  private void setDbt(  DatabaseEntry data,  byte[] bytes){
    if (bytes != null) {
      boolean partial=data.getPartial();
      int off=partial ? data.getPartialOffset() : 0;
      int len=partial ? data.getPartialLength() : bytes.length;
      if (off + len > bytes.length) {
        len=(off > bytes.length) ? 0 : bytes.length - off;
      }
      byte[] newdata=null;
      if (len == 0) {
        newdata=LogUtils.ZERO_LENGTH_BYTE_ARRAY;
      }
 else {
        newdata=new byte[len];
        System.arraycopy(bytes,off,newdata,0,len);
      }
      data.setData(newdata);
      data.setOffset(0);
      data.setSize(len);
    }
 else {
      data.setData(null);
      data.setOffset(0);
      data.setSize(0);
    }
  }
  /** 
 * Calls checkCursorState and returns false is an exception is thrown.
 */
  private boolean assertCursorState(  boolean mustBeInitialized){
    try {
      checkCursorState(mustBeInitialized);
      return true;
    }
 catch (    DatabaseException e) {
      return false;
    }
  }
  /** 
 * Check that the cursor is open and optionally if it is initialized.
 */
  public void checkCursorState(  boolean mustBeInitialized) throws DatabaseException {
    if (status == CURSOR_INITIALIZED) {
      this.hook278();
      return;
    }
 else     if (status == CURSOR_NOT_INITIALIZED) {
      if (mustBeInitialized) {
        throw new DatabaseException("Cursor Not Initialized.");
      }
    }
 else     if (status == CURSOR_CLOSED) {
      throw new DatabaseException("Cursor has been closed.");
    }
 else {
      throw new DatabaseException("Unknown cursor status: " + status);
    }
  }
  /** 
 * Return this lock to its prior status. If the lock was just obtained,
 * release it. If it was promoted, demote it.
 */
  private void revertLock(  LN ln,  LockResult lockResult) throws DatabaseException {
    revertLock(ln.getNodeId(),lockResult.getLockGrant());
  }
  /** 
 * Return this lock to its prior status. If the lock was just obtained,
 * release it. If it was promoted, demote it.
 */
  private void revertLock(  long nodeId,  LockGrantType lockStatus) throws DatabaseException {
    if ((lockStatus == LockGrantType.NEW) || (lockStatus == LockGrantType.WAIT_NEW)) {
      locker.releaseLock(nodeId);
    }
 else     if ((lockStatus == LockGrantType.PROMOTION) || (lockStatus == LockGrantType.WAIT_PROMOTION)) {
      locker.demoteLock(nodeId);
    }
  }
  /** 
 * Locks the logical EOF node for the database.
 */
  public void lockEofNode(  LockType lockType) throws DatabaseException {
    locker.lock(database.getEofNodeId(),lockType,false,database);
  }
  /** 
 * @throws RunRecoveryExceptionif the underlying environment is invalid.
 */
  public void checkEnv() throws RunRecoveryException {
    database.getDbEnvironment().checkIfInvalid();
  }
  public CursorImpl getLockerPrev(){
    return lockerPrev;
  }
  public CursorImpl getLockerNext(){
    return lockerNext;
  }
  public void setLockerPrev(  CursorImpl p){
    lockerPrev=p;
  }
  public void setLockerNext(  CursorImpl n){
    lockerNext=n;
  }
  /** 
 * Dump the cursor for debugging purposes. Dump the bin and dbin that the
 * cursor refers to if verbose is true.
 */
  public void dump(  boolean verbose){
    System.out.println(dumpToString(verbose));
  }
  /** 
 * dump the cursor for debugging purposes.
 */
  public void dump(){
    System.out.println(dumpToString(true));
  }
  private String statusToString(  byte status){
switch (status) {
case CURSOR_NOT_INITIALIZED:
      return "CURSOR_NOT_INITIALIZED";
case CURSOR_INITIALIZED:
    return "CURSOR_INITIALIZED";
case CURSOR_CLOSED:
  return "CURSOR_CLOSED";
default :
return "UNKNOWN (" + Byte.toString(status) + ")";
}
}
public String dumpToString(boolean verbose){
StringBuffer sb=new StringBuffer();
sb.append("<Cursor idx=\"").append(index).append("\"");
if (dupBin != null) {
sb.append(" dupIdx=\"").append(dupIndex).append("\"");
}
sb.append(" status=\"").append(statusToString(status)).append("\"");
sb.append(">\n");
if (verbose) {
sb.append((bin == null) ? "" : bin.dumpString(2,true));
sb.append((dupBin == null) ? "" : dupBin.dumpString(2,true));
}
sb.append("\n</Cursor>");
return sb.toString();
}
public void setTestHook(TestHook hook){
testHook=hook;
}
@MethodObject static class CursorImpl_latchBIN {
CursorImpl_latchBIN(CursorImpl _this){
this._this=_this;
}
BIN execute() throws DatabaseException {
try {
this.hook244();
throw ReturnHack.returnObject;
}
 catch (ReturnObject r) {
return (BIN)r.value;
}
}
protected CursorImpl _this;
protected BIN waitingOn;
protected void hook244() throws DatabaseException {
this.hook245();
}
protected void hook245() throws DatabaseException {
throw new ReturnObject(_this.bin);
}
}
@MethodObject static class CursorImpl_latchDBIN {
CursorImpl_latchDBIN(CursorImpl _this){
this._this=_this;
}
DBIN execute() throws DatabaseException {
try {
this.hook246();
throw ReturnHack.returnObject;
}
 catch (ReturnObject r) {
return (DBIN)r.value;
}
}
protected CursorImpl _this;
protected BIN waitingOn;
protected void hook246() throws DatabaseException {
this.hook247();
}
protected void hook247() throws DatabaseException {
throw new ReturnObject(_this.dupBin);
}
}
@MethodObject static class CursorImpl_lockNextKeyForInsert {
CursorImpl_lockNextKeyForInsert(CursorImpl _this,DatabaseEntry key,DatabaseEntry data){
this._this=_this;
this.key=key;
this.data=data;
}
void execute() throws DatabaseException {
tempKey=new DatabaseEntry(key.getData(),key.getOffset(),key.getSize());
tempData=new DatabaseEntry(data.getData(),data.getOffset(),data.getSize());
tempKey.setPartial(0,0,true);
tempData.setPartial(0,0,true);
lockedNextKey=false;
searchMode=_this.database.getSortedDuplicates() ? SearchMode.BOTH_RANGE : SearchMode.SET_RANGE;
this.hook248();
if (!lockedNextKey) {
_this.lockEofNode(LockType.RANGE_INSERT);
}
}
protected CursorImpl _this;
protected DatabaseEntry key;
protected DatabaseEntry data;
protected DatabaseEntry tempKey;
protected DatabaseEntry tempData;
protected boolean lockedNextKey;
protected SearchMode searchMode;
protected boolean latched;
protected int searchResult;
protected OperationStatus status;
protected void hook248() throws DatabaseException {
searchResult=_this.searchAndPosition(tempKey,tempData,searchMode,LockType.RANGE_INSERT);
if ((searchResult & _this.FOUND) != 0 && (searchResult & _this.FOUND_LAST) == 0) {
{
}
if ((searchResult & _this.EXACT_KEY) != 0) {
  status=_this.getNext(tempKey,tempData,LockType.RANGE_INSERT,true,true);
}
 else {
  status=_this.getNextNoDup(tempKey,tempData,LockType.RANGE_INSERT,true,true);
}
if (status == OperationStatus.SUCCESS) {
  lockedNextKey=true;
}
this.hook249();
}
}
protected void hook249() throws DatabaseException {
}
}
@MethodObject static class CursorImpl_getNextDuplicate {
CursorImpl_getNextDuplicate(CursorImpl _this,DatabaseEntry foundKey,DatabaseEntry foundData,LockType lockType,boolean forward,boolean alreadyLatched){
this._this=_this;
this.foundKey=foundKey;
this.foundData=foundData;
this.lockType=lockType;
this.forward=forward;
this.alreadyLatched=alreadyLatched;
}
OperationStatus execute() throws DatabaseException {
try {
assert _this.assertCursorState(true) : _this.dumpToString(true);
this.hook250();
try {
  while (_this.dupBin != null) {
    this.hook251();
    this.hook279();
    if ((forward && ++_this.dupIndex < _this.dupBin.getNEntries()) || (!forward && --_this.dupIndex > -1)) {
      ret=OperationStatus.SUCCESS;
      if (foundKey != null) {
        ret=_this.getCurrentAlreadyLatched(foundKey,foundData,lockType,forward);
      }
 else {
        this.hook252();
      }
      if (ret == OperationStatus.SUCCESS) {
        _this.incrementLNCount();
        return ret;
      }
 else {
        this.hook253();
        if (_this.dupBinToBeRemoved != null) {
          _this.flushDBINToBeRemoved();
        }
        continue;
      }
    }
 else {
      if (_this.dupBinToBeRemoved != null) {
        _this.flushDBINToBeRemoved();
      }
      _this.dupBinToBeRemoved=_this.dupBin;
      _this.dupBin=null;
      this.hook255();
      this.hook275();
      this.hook254();
{
      }
      if (forward) {
        newDupBin=(DBIN)_this.database.getTree().getNextBin(_this.dupBinToBeRemoved,true);
      }
 else {
        newDupBin=(DBIN)_this.database.getTree().getPrevBin(_this.dupBinToBeRemoved,true);
      }
      if (newDupBin == null) {
        return OperationStatus.NOTFOUND;
      }
 else {
        if (forward) {
          _this.dupIndex=-1;
        }
 else {
          _this.dupIndex=newDupBin.getNEntries();
        }
        _this.addCursor(newDupBin);
        _this.dupBin=newDupBin;
        this.hook256();
      }
    }
  }
}
  finally {
  this.hook257();
  if (_this.dupBinToBeRemoved != null) {
    _this.flushDBINToBeRemoved();
  }
}
return OperationStatus.NOTFOUND;
}
 catch (ReturnObject r) {
return (OperationStatus)r.value;
}
}
protected CursorImpl _this;
protected DatabaseEntry foundKey;
protected DatabaseEntry foundData;
protected LockType lockType;
protected boolean forward;
protected boolean alreadyLatched;
protected OperationStatus ret;
protected TreeWalkerStatsAccumulator treeStatsAccumulator;
protected DIN duplicateRoot;
protected DupCountLN dcl;
protected DBIN newDupBin;
protected void hook250() throws DatabaseException {
}
protected void hook251() throws DatabaseException {
}
protected void hook252() throws DatabaseException {
}
protected void hook253() throws DatabaseException {
}
protected void hook254() throws DatabaseException {
}
protected void hook255() throws DatabaseException {
}
protected void hook256() throws DatabaseException {
}
protected void hook257() throws DatabaseException {
}
protected void hook275() throws DatabaseException {
}
protected void hook279() throws DatabaseException {
}
}
@MethodObject static class CursorImpl_fetchCurrent {
CursorImpl_fetchCurrent(CursorImpl _this,DatabaseEntry foundKey,DatabaseEntry foundData,LockType lockType,boolean first){
this._this=_this;
this.foundKey=foundKey;
this.foundData=foundData;
this.lockType=lockType;
this.first=first;
}
OperationStatus execute() throws DatabaseException {
try {
treeStatsAccumulator=_this.getTreeStatsAccumulator();
duplicateFetch=_this.setTargetBin();
if (_this.targetBin == null) {
  return OperationStatus.NOTFOUND;
}
this.hook259();
n=null;
if (_this.targetIndex < 0 || _this.targetIndex >= _this.targetBin.getNEntries() || _this.targetBin.isEntryKnownDeleted(_this.targetIndex)) {
}
 else {
  if (_this.targetBin.isEntryPendingDeleted(_this.targetIndex)) {
    this.hook280();
  }
  this.hook260();
}
if (n == null) {
  if (treeStatsAccumulator != null) {
    treeStatsAccumulator.incrementDeletedLNCount();
  }
  this.hook261();
  return OperationStatus.KEYEMPTY;
}
_this.addCursor(_this.targetBin);
if (n.containsDuplicates()) {
  assert !duplicateFetch;
  duplicateRoot=(DIN)n;
  this.hook262();
  if (_this.positionFirstOrLast(first,duplicateRoot)) {
    this.hook263();
  }
 else {
    return OperationStatus.NOTFOUND;
  }
}
ln=(LN)n;
assert TestHookExecute.doHookIfSet(_this.testHook);
lockResult=_this.lockLN(ln,lockType);
this.hook258();
throw ReturnHack.returnObject;
}
 catch (ReturnObject r) {
return (OperationStatus)r.value;
}
}
protected CursorImpl _this;
protected DatabaseEntry foundKey;
protected DatabaseEntry foundData;
protected LockType lockType;
protected boolean first;
protected TreeWalkerStatsAccumulator treeStatsAccumulator;
protected boolean duplicateFetch;
protected Node n;
protected EnvironmentImpl envImpl;
protected DIN duplicateRoot;
protected LN ln;
protected LockResult lockResult;
protected byte[] lnData;
protected void hook258() throws DatabaseException {
ln=lockResult.getLN();
lnData=(ln != null) ? ln.getData() : null;
if (ln == null || lnData == null) {
if (treeStatsAccumulator != null) {
  treeStatsAccumulator.incrementDeletedLNCount();
}
throw new ReturnObject(OperationStatus.KEYEMPTY);
}
duplicateFetch=_this.setTargetBin();
if (duplicateFetch) {
if (foundData != null) {
  _this.setDbt(foundData,_this.targetBin.getKey(_this.targetIndex));
}
if (foundKey != null) {
  _this.setDbt(foundKey,_this.targetBin.getDupKey());
}
}
 else {
if (foundData != null) {
  _this.setDbt(foundData,lnData);
}
if (foundKey != null) {
  _this.setDbt(foundKey,_this.targetBin.getKey(_this.targetIndex));
}
}
throw new ReturnObject(OperationStatus.SUCCESS);
}
protected void hook259() throws DatabaseException {
}
protected void hook260() throws DatabaseException {
n=_this.targetBin.fetchTarget(_this.targetIndex);
}
protected void hook261() throws DatabaseException {
}
protected void hook262() throws DatabaseException {
}
protected void hook263() throws DatabaseException {
throw new ReturnObject(_this.fetchCurrent(foundKey,foundData,lockType,first));
}
protected void hook280() throws DatabaseException {
}
}
protected void hook204(LN ln,long oldLsn,long newLsn) throws DatabaseException {
}
protected void hook205(LN ln,long oldLsn,long newLsn) throws DatabaseException {
}
protected void hook206() throws DatabaseException, CloneNotSupportedException {
}
protected void hook207() throws DatabaseException {
}
protected void hook208(BIN bin){
}
protected void hook209(BIN abin) throws DatabaseException {
}
protected void hook210(DBIN abin) throws DatabaseException {
}
protected void hook211() throws DatabaseException {
}
protected void hook212(LockType lockType) throws DatabaseException {
if (bin.getNEntries() <= index) {
throw new ReturnInt(0);
}
Node n=bin.fetchTarget(index);
if (n != null && n.containsDuplicates()) {
DIN dupRoot=(DIN)n;
this.hook265(dupRoot);
DupCountLN dupCountLN=(DupCountLN)dupRoot.getDupCountLNRef().fetchTarget(database,dupRoot);
this.hook264(dupRoot);
if (lockType != LockType.NONE) {
locker.lock(dupCountLN.getNodeId(),lockType,false,database);
}
throw new ReturnInt(dupCountLN.getDupCount());
}
 else {
throw new ReturnInt(1);
}
}
protected void hook213(boolean isDup,LN ln,LockResult lockResult,LockResult dclLockResult,DIN dupRoot) throws DatabaseException {
isDup=(dupBin != null);
if (isDup) {
dupRoot=getLatchedDupRoot(true);
dclLockResult=lockDupCountLN(dupRoot,LockType.WRITE);
dupRoot=(DIN)bin.getTarget(index);
this.hook267();
}
setTargetBin();
long oldLsn=targetBin.getLsn(targetIndex);
byte[] lnKey=targetBin.getKey(targetIndex);
lockResult.setAbortLsn(oldLsn,targetBin.isEntryKnownDeleted(targetIndex));
long oldLNSize=0;
oldLNSize=this.hook284(ln,oldLNSize);
long newLsn=ln.delete(database,lnKey,dupKey,oldLsn,locker);
long newLNSize=0;
newLNSize=this.hook283(ln,newLNSize);
targetBin.updateEntry(targetIndex,newLsn,oldLNSize,newLNSize);
targetBin.setPendingDeleted(targetIndex);
this.hook266();
if (isDup) {
dupRoot.incrementDuplicateCount(dclLockResult,dupKey,locker,false);
this.hook268(dupRoot);
dupRoot=null;
this.hook281(lnKey);
}
 else {
this.hook282(lnKey);
}
this.hook204(ln,oldLsn,newLsn);
}
protected void hook214() throws DatabaseException {
}
protected void hook215() throws DatabaseException {
}
protected void hook216() throws DatabaseException {
}
protected void hook217() throws DatabaseException {
}
protected void hook218(DatabaseEntry data,DatabaseEntry foundKey,DatabaseEntry foundData,boolean isDup) throws DatabaseException {
LN ln=(LN)targetBin.fetchTarget(targetIndex);
byte[] lnKey=targetBin.getKey(targetIndex);
Comparator userComparisonFcn=targetBin.getKeyComparator();
if (targetBin.isEntryKnownDeleted(targetIndex) || ln == null) {
this.hook270();
throw new ReturnObject(OperationStatus.NOTFOUND);
}
LockResult lockResult=lockLN(ln,LockType.WRITE);
ln=lockResult.getLN();
if (ln == null) {
this.hook271();
throw new ReturnObject(OperationStatus.NOTFOUND);
}
byte[] foundDataBytes;
byte[] foundKeyBytes;
isDup=setTargetBin();
if (isDup) {
foundDataBytes=lnKey;
foundKeyBytes=targetBin.getDupKey();
}
 else {
foundDataBytes=ln.getData();
foundKeyBytes=lnKey;
}
byte[] newData;
if (data.getPartial()) {
int dlen=data.getPartialLength();
int doff=data.getPartialOffset();
int origlen=(foundDataBytes != null) ? foundDataBytes.length : 0;
int oldlen=(doff + dlen > origlen) ? doff + dlen : origlen;
int len=oldlen - dlen + data.getSize();
if (len == 0) {
newData=LogUtils.ZERO_LENGTH_BYTE_ARRAY;
}
 else {
newData=new byte[len];
}
int pos=0;
int slicelen=(doff < origlen) ? doff : origlen;
if (slicelen > 0) System.arraycopy(foundDataBytes,0,newData,pos,slicelen);
pos+=doff;
slicelen=data.getSize();
System.arraycopy(data.getData(),data.getOffset(),newData,pos,slicelen);
pos+=slicelen;
slicelen=origlen - (doff + dlen);
if (slicelen > 0) System.arraycopy(foundDataBytes,doff + dlen,newData,pos,slicelen);
}
 else {
int len=data.getSize();
if (len == 0) {
newData=LogUtils.ZERO_LENGTH_BYTE_ARRAY;
}
 else {
newData=new byte[len];
}
System.arraycopy(data.getData(),data.getOffset(),newData,0,len);
}
if (database.getSortedDuplicates()) {
boolean keysEqual=false;
if (foundDataBytes != null) {
keysEqual=Key.compareKeys(foundDataBytes,newData,userComparisonFcn) == 0;
}
if (!keysEqual) {
revertLock(ln,lockResult);
throw new DatabaseException("Can't replace a duplicate with different data.");
}
}
if (foundData != null) {
setDbt(foundData,foundDataBytes);
}
if (foundKey != null) {
setDbt(foundKey,foundKeyBytes);
}
long oldLsn=targetBin.getLsn(targetIndex);
lockResult.setAbortLsn(oldLsn,targetBin.isEntryKnownDeleted(targetIndex));
long oldLNSize=0;
oldLNSize=this.hook286(ln,oldLNSize);
byte[] newKey=(isDup ? targetBin.getDupKey() : lnKey);
long newLsn=ln.modify(newData,database,newKey,oldLsn,locker);
long newLNSize=0;
newLNSize=this.hook285(ln,newLNSize);
targetBin.updateEntry(targetIndex,newLsn,oldLNSize,newLNSize);
this.hook269();
this.hook205(ln,oldLsn,newLsn);
status=CURSOR_INITIALIZED;
throw new ReturnObject(OperationStatus.SUCCESS);
}
protected void hook219() throws DatabaseException {
}
protected void hook220() throws DatabaseException {
}
protected void hook221(DatabaseEntry foundKey,DatabaseEntry foundData,LockType lockType,boolean first) throws DatabaseException {
throw new ReturnObject(fetchCurrent(foundKey,foundData,lockType,first));
}
protected void hook222() throws DatabaseException {
}
protected void hook223(LockType lockType) throws DatabaseException {
assert assertCursorState(true) : dumpToString(true);
this.hook272();
if (bin == null) {
throw new ReturnObject(null);
}
LN ln=null;
if (!bin.isEntryKnownDeleted(index)) {
ln=(LN)bin.fetchTarget(index);
}
if (ln == null) {
this.hook273();
throw new ReturnObject(null);
}
addCursor(bin);
LockResult lockResult=lockLN(ln,lockType);
ln=lockResult.getLN();
throw new ReturnObject(ln);
}
protected void hook224(boolean alreadyLatched) throws DatabaseException {
}
protected boolean hook225(boolean alreadyLatched) throws DatabaseException {
return alreadyLatched;
}
protected boolean hook226(boolean alreadyLatched) throws DatabaseException {
return alreadyLatched;
}
protected void hook227() throws DatabaseException {
}
protected void hook228() throws DatabaseException {
}
protected void hook229() throws DatabaseException {
}
protected void hook230(boolean alreadyLatched) throws DatabaseException {
}
protected void hook231() throws DatabaseException {
}
protected void hook232() throws DatabaseException {
}
protected void hook233() throws DatabaseException {
}
protected void hook234(boolean first,DIN duplicateRoot,IN in,boolean found) throws DatabaseException {
if (duplicateRoot == null) {
removeCursorBIN();
if (first) {
in=database.getTree().getFirstNode();
}
 else {
in=database.getTree().getLastNode();
}
if (in != null) {
assert (in instanceof BIN);
dupBin=null;
dupIndex=-1;
bin=(BIN)in;
index=(first ? 0 : (bin.getNEntries() - 1));
addCursor(bin);
TreeWalkerStatsAccumulator treeStatsAccumulator=getTreeStatsAccumulator();
if (bin.getNEntries() == 0) {
  found=true;
}
 else {
  Node n=null;
  if (!in.isEntryKnownDeleted(index)) {
    n=in.fetchTarget(index);
  }
  if (n != null && n.containsDuplicates()) {
    DIN dupRoot=(DIN)n;
    this.hook274(in,dupRoot);
    in=null;
    found=positionFirstOrLast(first,dupRoot);
  }
 else {
    if (treeStatsAccumulator != null) {
      if (n == null || ((LN)n).isDeleted()) {
        treeStatsAccumulator.incrementDeletedLNCount();
      }
 else {
        treeStatsAccumulator.incrementLNCount();
      }
    }
    found=true;
  }
}
}
}
 else {
removeCursorDBIN();
if (first) {
in=database.getTree().getFirstNode(duplicateRoot);
}
 else {
in=database.getTree().getLastNode(duplicateRoot);
}
if (in != null) {
assert (in instanceof DBIN);
dupBin=(DBIN)in;
dupIndex=(first ? 0 : (dupBin.getNEntries() - 1));
addCursor(dupBin);
found=true;
}
}
status=CURSOR_INITIALIZED;
throw new ReturnBoolean(found);
}
protected void hook235(DatabaseEntry matchKey,DatabaseEntry matchData,SearchMode searchMode,LockType lockType,boolean foundSomething,boolean foundExactKey,boolean foundExactData,boolean foundLast,boolean exactSearch,BINBoundary binBoundary) throws DatabaseException {
byte[] key=Key.makeKey(matchKey);
bin=(BIN)database.getTree().search(key,Tree.SearchType.NORMAL,-1,binBoundary,true);
if (bin != null) {
addCursor(bin);
index=bin.findEntry(key,true,exactSearch);
foundSomething=!exactSearch;
dupBin=null;
dupIndex=-1;
boolean containsDuplicates=false;
if (index >= 0) {
if ((index & IN.EXACT_MATCH) != 0) {
  foundExactKey=true;
  index&=~IN.EXACT_MATCH;
}
Node n=null;
if (!bin.isEntryKnownDeleted(index)) {
  n=bin.fetchTarget(index);
}
if (n != null) {
  containsDuplicates=n.containsDuplicates();
  if (searchMode.isDataSearch()) {
    if (foundExactKey) {
      int searchResult=searchAndPositionBoth(containsDuplicates,n,matchData,exactSearch,lockType,bin.getLsn(index));
      foundSomething=(searchResult & FOUND) != 0;
      foundExactData=(searchResult & EXACT_DATA) != 0;
    }
  }
 else {
    foundSomething=true;
    if (!containsDuplicates && exactSearch) {
      LN ln=(LN)n;
      LockResult lockResult=lockLN(ln,lockType);
      ln=lockResult.getLN();
      if (ln == null) {
        foundSomething=false;
      }
    }
  }
}
foundLast=(searchMode == SearchMode.SET_RANGE && foundSomething && !containsDuplicates && binBoundary.isLastBin && index == bin.getNEntries() - 1);
}
}
status=CURSOR_INITIALIZED;
throw new ReturnInt((foundSomething ? FOUND : 0) | (foundExactKey ? EXACT_KEY : 0) | (foundExactData ? EXACT_DATA : 0)| (foundLast ? FOUND_LAST : 0));
}
protected void hook236(DIN duplicateRoot) throws DatabaseException {
}
protected void hook237() throws DatabaseException {
}
protected void hook238() throws DatabaseException {
}
protected void hook239(DIN dupRoot) throws DatabaseException {
}
protected void hook240() throws DatabaseException {
}
protected void hook241(DIN dupRoot) throws DatabaseException {
}
protected void hook242(boolean isDBINLatched,DIN dupRoot) throws DatabaseException {
}
protected void hook243() throws DatabaseException {
}
protected void hook264(DIN dupRoot) throws DatabaseException {
}
protected void hook265(DIN dupRoot) throws DatabaseException {
}
protected void hook266() throws DatabaseException {
}
protected void hook267() throws DatabaseException {
}
protected void hook268(DIN dupRoot) throws DatabaseException {
}
protected void hook269() throws DatabaseException {
}
protected void hook270() throws DatabaseException {
}
protected void hook271() throws DatabaseException {
}
protected void hook272() throws DatabaseException {
}
protected void hook273() throws DatabaseException {
}
protected void hook274(IN in,DIN dupRoot) throws DatabaseException {
}
protected void hook276() throws DatabaseException {
}
protected void hook277() throws DatabaseException {
}
protected void hook278() throws DatabaseException {
}
protected void hook281(byte[] lnKey) throws DatabaseException {
}
protected void hook282(byte[] lnKey) throws DatabaseException {
}
protected long hook283(LN ln,long newLNSize) throws DatabaseException {
return newLNSize;
}
protected long hook284(LN ln,long oldLNSize) throws DatabaseException {
return oldLNSize;
}
protected long hook285(LN ln,long newLNSize) throws DatabaseException {
return newLNSize;
}
protected long hook286(LN ln,long oldLNSize) throws DatabaseException {
return oldLNSize;
}
}
\00base/com/sleepycat/je/dbi/INList.java:package com.sleepycat.je.dbi;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;
import java.util.SortedSet;
import java.util.TreeSet;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.tree.IN;
import de.ovgu.cide.jakutil.*;
/** 
 * The INList is a list of in-memory INs for a given environment. 
 */
public class INList {
  private static final String DEBUG_NAME=INList.class.getName();
  private SortedSet ins=null;
  private EnvironmentImpl envImpl;
  INList(  EnvironmentImpl envImpl){
    this.envImpl=envImpl;
    ins=new TreeSet();
    this.hook338(envImpl);
  }
  /** 
 * Used only by tree verifier when validating INList. Must be called with
 * orig.majorLatch acquired.
 */
  public INList(  INList orig,  EnvironmentImpl envImpl) throws DatabaseException {
    ins=new TreeSet(orig.getINs());
    this.hook340();
    this.envImpl=envImpl;
    this.hook339(envImpl);
  }
  public SortedSet getINs(){
    return ins;
  }
  public int getSize(){
    return ins.size();
  }
  /** 
 * An IN has just come into memory, add it to the list.
 */
  public void add(  IN in) throws DatabaseException {
    new INList_add(this,in).execute();
  }
  private void addAndSetMemory(  Set set,  IN in){
    new INList_addAndSetMemory(this,set,in).execute();
  }
  /** 
 * An IN is getting evicted or is displaced by recovery.  Caller is
 * responsible for acquiring the major latch before calling this and
 * releasing it when they're done.
 */
  public void removeLatchAlreadyHeld(  IN in) throws DatabaseException {
    boolean removeDone=ins.remove(in);
    removeDone=this.hook341(in,removeDone);
    assert removeDone;
    this.hook346(in);
  }
  /** 
 * An IN is getting swept or is displaced by recovery.
 */
  public void remove(  IN in) throws DatabaseException {
    removeLatchAlreadyHeld(in);
  }
  public SortedSet tailSet(  IN in) throws DatabaseException {
    return ins.tailSet(in);
  }
  public IN first() throws DatabaseException {
    return (IN)ins.first();
  }
  /** 
 * Return an iterator over the main 'ins' set.  Returned iterator will not
 * show the elements in addedINs.
 * The major latch should be held before entering.  The caller is
 * responsible for releasing the major latch when they're finished with the
 * iterator.
 * @return an iterator over the main 'ins' set.
 */
  public Iterator iterator(){
    return ins.iterator();
  }
  /** 
 * Clear the entire list during recovery and at shutdown.
 */
  public void clear() throws DatabaseException {
    ins.clear();
    this.hook342();
  }
  public void dump(){
    System.out.println("size=" + getSize());
    Iterator iter=ins.iterator();
    while (iter.hasNext()) {
      IN theIN=(IN)iter.next();
      System.out.println("db=" + theIN.getDatabase().getId() + " nid=: "+ theIN.getNodeId()+ "/"+ theIN.getLevel());
    }
  }
@MethodObject static class INList_add {
    INList_add(    INList _this,    IN in){
      this._this=_this;
      this.in=in;
    }
    void execute() throws DatabaseException {
      this.hook344();
      addToMajor=true;
      this.hook343();
    }
    protected INList _this;
    protected IN in;
    protected boolean enteredWithLatchHeld;
    protected boolean addToMajor;
    protected void hook343() throws DatabaseException {
      this.hook345();
    }
    protected void hook344() throws DatabaseException {
    }
    protected void hook345() throws DatabaseException {
      _this.addAndSetMemory(_this.ins,in);
    }
  }
@MethodObject static class INList_addAndSetMemory {
    INList_addAndSetMemory(    INList _this,    Set set,    IN in){
      this._this=_this;
      this.set=set;
      this.in=in;
    }
    void execute(){
      addOk=set.add(in);
      assert addOk : "failed adding in " + in.getNodeId();
    }
    protected INList _this;
    protected Set set;
    protected IN in;
    protected boolean addOk;
    protected MemoryBudget mb;
  }
  protected void hook338(  EnvironmentImpl envImpl){
  }
  protected void hook339(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected void hook340() throws DatabaseException {
  }
  protected boolean hook341(  IN in,  boolean removeDone) throws DatabaseException {
    return removeDone;
  }
  protected void hook342() throws DatabaseException {
  }
  protected void hook346(  IN in) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/dbi/DatabaseImpl.java:package com.sleepycat.je.dbi;
import java.io.PrintStream;
import java.nio.ByteBuffer;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import com.sleepycat.je.Cursor;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.LockMode;
import com.sleepycat.je.OperationStatus;
import com.sleepycat.je.PreloadConfig;
import com.sleepycat.je.PreloadStats;
import com.sleepycat.je.PreloadStatus;
import com.sleepycat.je.SecondaryDatabase;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.SortedLSNTreeWalker.TreeNodeProcessor;
import com.sleepycat.je.log.LogEntryType;
import com.sleepycat.je.log.LogException;
import com.sleepycat.je.log.LogReadable;
import com.sleepycat.je.log.LogUtils;
import com.sleepycat.je.log.LogWritable;
import com.sleepycat.je.tree.ChildReference;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.TreeUtils;
import com.sleepycat.je.tree.TreeWalkerStatsAccumulator;
import com.sleepycat.je.tree.WithRootLatched;
import com.sleepycat.je.txn.Locker;
import com.sleepycat.je.txn.ThreadLocker;
import com.sleepycat.je.utilint.CmdUtil;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.TestHook;
import de.ovgu.cide.jakutil.*;
/** 
 * The underlying object for a given database.
 */
public class DatabaseImpl implements LogWritable, LogReadable, Cloneable {
  private DatabaseId id;
  Tree tree;
  private EnvironmentImpl envImpl;
  private boolean duplicatesAllowed;
  private boolean transactional;
  private Set referringHandles;
  private long eofNodeId;
  private Comparator btreeComparator=null;
  private Comparator duplicateComparator=null;
  private String btreeComparatorName="";
  private String duplicateComparatorName="";
  private int binDeltaPercent;
  private int binMaxDeltas;
  private int maxMainTreeEntriesPerNode;
  private int maxDupTreeEntriesPerNode;
  private String debugDatabaseName;
  private TestHook pendingDeletedHook;
  /** 
 * Create a database object for a new database.
 */
  public DatabaseImpl(  String dbName,  DatabaseId id,  EnvironmentImpl envImpl,  DatabaseConfig dbConfig) throws DatabaseException {
    this.id=id;
    this.envImpl=envImpl;
    this.btreeComparator=dbConfig.getBtreeComparator();
    this.duplicateComparator=dbConfig.getDuplicateComparator();
    duplicatesAllowed=dbConfig.getSortedDuplicates();
    transactional=dbConfig.getTransactional();
    maxMainTreeEntriesPerNode=dbConfig.getNodeMaxEntries();
    maxDupTreeEntriesPerNode=dbConfig.getNodeMaxDupTreeEntries();
    initDefaultSettings();
    this.hook288();
    tree=new Tree(this);
    referringHandles=Collections.synchronizedSet(new HashSet());
    eofNodeId=Node.getNextNodeId();
    debugDatabaseName=dbName;
  }
  /** 
 * Create an empty database object for initialization from the log. Note
 * that the rest of the initialization comes from readFromLog(), except for
 * the debugDatabaseName, which is set by the caller.
 */
  public DatabaseImpl() throws DatabaseException {
    id=new DatabaseId();
    envImpl=null;
    this.hook289();
    tree=new Tree();
    referringHandles=Collections.synchronizedSet(new HashSet());
    eofNodeId=Node.getNextNodeId();
  }
  public void setDebugDatabaseName(  String debugName){
    debugDatabaseName=debugName;
  }
  public String getDebugName(){
    return debugDatabaseName;
  }
  public void setPendingDeletedHook(  TestHook hook){
    pendingDeletedHook=hook;
  }
  /** 
 * Initialize configuration settings when creating a new instance or after
 * reading an instance from the log. The envImpl field must be set before
 * calling this method.
 */
  private void initDefaultSettings() throws DatabaseException {
    DbConfigManager configMgr=envImpl.getConfigManager();
    binDeltaPercent=configMgr.getInt(EnvironmentParams.BIN_DELTA_PERCENT);
    binMaxDeltas=configMgr.getInt(EnvironmentParams.BIN_MAX_DELTAS);
    if (maxMainTreeEntriesPerNode == 0) {
      maxMainTreeEntriesPerNode=configMgr.getInt(EnvironmentParams.NODE_MAX);
    }
    if (maxDupTreeEntriesPerNode == 0) {
      maxDupTreeEntriesPerNode=configMgr.getInt(EnvironmentParams.NODE_MAX_DUPTREE);
    }
  }
  /** 
 * Clone. For now just pass off to the super class for a field-by-field
 * copy.
 */
  public Object clone() throws CloneNotSupportedException {
    return super.clone();
  }
  /** 
 * @return the database tree.
 */
  public Tree getTree(){
    return tree;
  }
  void setTree(  Tree tree){
    this.tree=tree;
  }
  /** 
 * @return the database id.
 */
  public DatabaseId getId(){
    return id;
  }
  void setId(  DatabaseId id){
    this.id=id;
  }
  public long getEofNodeId(){
    return eofNodeId;
  }
  /** 
 * @return true if this database is transactional.
 */
  public boolean isTransactional(){
    return transactional;
  }
  /** 
 * Sets the transactional property for the first opened handle.
 */
  public void setTransactional(  boolean transactional){
    this.transactional=transactional;
  }
  /** 
 * @return true if duplicates are allowed in this database.
 */
  public boolean getSortedDuplicates(){
    return duplicatesAllowed;
  }
  public int getNodeMaxEntries(){
    return maxMainTreeEntriesPerNode;
  }
  public int getNodeMaxDupTreeEntries(){
    return maxDupTreeEntriesPerNode;
  }
  /** 
 * Set the duplicate comparison function for this database.
 * @param duplicateComparator -
 * The Duplicate Comparison function.
 */
  public void setDuplicateComparator(  Comparator duplicateComparator){
    this.duplicateComparator=duplicateComparator;
  }
  /** 
 * Set the btree comparison function for this database.
 * @param btreeComparator -
 * The btree Comparison function.
 */
  public void setBtreeComparator(  Comparator btreeComparator){
    this.btreeComparator=btreeComparator;
  }
  /** 
 * @return the btree Comparator object.
 */
  public Comparator getBtreeComparator(){
    return btreeComparator;
  }
  /** 
 * @return the duplicate Comparator object.
 */
  public Comparator getDuplicateComparator(){
    return duplicateComparator;
  }
  /** 
 * Set the db environment during recovery, after instantiating the database
 * from the log
 */
  public void setEnvironmentImpl(  EnvironmentImpl envImpl) throws DatabaseException {
    this.envImpl=envImpl;
    initDefaultSettings();
    tree.setDatabase(this);
  }
  /** 
 * @return the database environment.
 */
  public EnvironmentImpl getDbEnvironment(){
    return envImpl;
  }
  /** 
 * Returns whether one or more handles are open.
 */
  public boolean hasOpenHandles(){
    return referringHandles.size() > 0;
  }
  /** 
 * Add a referring handle
 */
  public void addReferringHandle(  Database db){
    referringHandles.add(db);
  }
  /** 
 * Decrement the reference count.
 */
  public void removeReferringHandle(  Database db){
    referringHandles.remove(db);
  }
  /** 
 * @return the referring handle count.
 */
  synchronized int getReferringHandleCount(){
    return referringHandles.size();
  }
  /** 
 * For this secondary database return the primary that it is associated
 * with, or null if not associated with any primary. Note that not all
 * handles need be associated with a primary.
 */
  public Database findPrimaryDatabase() throws DatabaseException {
    for (Iterator i=referringHandles.iterator(); i.hasNext(); ) {
      Object obj=i.next();
      if (obj instanceof SecondaryDatabase) {
        return ((SecondaryDatabase)obj).getPrimaryDatabase();
      }
    }
    return null;
  }
  public String getName() throws DatabaseException {
    return envImpl.getDbMapTree().getDbName(id);
  }
private static class ObsoleteProcessor implements TreeNodeProcessor {
    private UtilizationTracker tracker;
    ObsoleteProcessor(    UtilizationTracker tracker){
      this.tracker=tracker;
    }
    public void processLSN(    long childLsn,    LogEntryType childType){
      assert childLsn != DbLsn.NULL_LSN;
      tracker.countObsoleteNodeInexact(childLsn,childType);
    }
  }
  /** 
 * Return the count of nodes in the database. Used for truncate, perhaps
 * should be made available through other means? Database should be
 * quiescent.
 */
  long countRecords() throws DatabaseException {
    LNCounter lnCounter=new LNCounter();
    SortedLSNTreeWalker walker=new SortedLSNTreeWalker(this,false,false,tree.getRootLsn(),lnCounter);
    walker.walk();
    return lnCounter.getCount();
  }
private static class LNCounter implements TreeNodeProcessor {
    private long counter;
    public void processLSN(    long childLsn,    LogEntryType childType){
      assert childLsn != DbLsn.NULL_LSN;
      if (childType.equals(LogEntryType.LOG_LN_TRANSACTIONAL) || childType.equals(LogEntryType.LOG_LN)) {
        counter++;
      }
    }
    long getCount(){
      return counter;
    }
  }
  private boolean walkDatabaseTree(  TreeWalkerStatsAccumulator statsAcc,  PrintStream out,  boolean verbose) throws DatabaseException {
    boolean ok=true;
    Locker locker=new ThreadLocker(envImpl);
    Cursor cursor=null;
    CursorImpl impl=null;
    try {
      EnvironmentImpl.incThreadLocalReferenceCount();
      cursor=DbInternal.newCursor(this,locker,null);
      impl=DbInternal.getCursorImpl(cursor);
      tree.setTreeStatsAccumulator(statsAcc);
      impl.setTreeStatsAccumulator(statsAcc);
      DatabaseEntry foundData=new DatabaseEntry();
      DatabaseEntry key=new DatabaseEntry();
      OperationStatus status=DbInternal.position(cursor,key,foundData,LockMode.READ_UNCOMMITTED,true);
      while (status == OperationStatus.SUCCESS) {
        try {
          status=DbInternal.retrieveNext(cursor,key,foundData,LockMode.READ_UNCOMMITTED,GetMode.NEXT);
        }
 catch (        DatabaseException DBE) {
          ok=false;
          if (DbInternal.advanceCursor(cursor,key,foundData)) {
            if (verbose) {
              out.println("Error encountered (continuing):");
              out.println(DBE);
              printErrorRecord(out,key,foundData);
            }
          }
 else {
            throw DBE;
          }
        }
      }
    }
  finally {
      if (impl != null) {
        impl.setTreeStatsAccumulator(null);
      }
      tree.setTreeStatsAccumulator(null);
      EnvironmentImpl.decThreadLocalReferenceCount();
      if (cursor != null) {
        cursor.close();
      }
    }
    return ok;
  }
  /** 
 * Prints the key and data, if available, for a BIN entry that could not be
 * read/verified. Uses the same format as DbDump and prints both the hex and
 * printable versions of the entries.
 */
  private void printErrorRecord(  PrintStream out,  DatabaseEntry key,  DatabaseEntry data){
    byte[] bytes=key.getData();
    StringBuffer sb=new StringBuffer("Error Key ");
    if (bytes == null) {
      sb.append("UNKNOWN");
    }
 else {
      CmdUtil.formatEntry(sb,bytes,false);
      sb.append(' ');
      CmdUtil.formatEntry(sb,bytes,true);
    }
    out.println(sb);
    bytes=data.getData();
    sb=new StringBuffer("Error Data ");
    if (bytes == null) {
      sb.append("UNKNOWN");
    }
 else {
      CmdUtil.formatEntry(sb,bytes,false);
      sb.append(' ');
      CmdUtil.formatEntry(sb,bytes,true);
    }
    out.println(sb);
  }
  /** 
 * Undeclared exception used to throw through SortedLSNTreeWalker code when
 * preload has either filled the user's max byte or time request.
 */
private static class HaltPreloadException extends RuntimeException {
    private PreloadStatus status;
    HaltPreloadException(    PreloadStatus status){
      super(status.toString());
      this.status=status;
    }
    PreloadStatus getStatus(){
      return status;
    }
  }
  static final HaltPreloadException timeExceededPreloadException=new HaltPreloadException(PreloadStatus.EXCEEDED_TIME);
  static final HaltPreloadException memoryExceededPreloadException=new HaltPreloadException(PreloadStatus.FILLED_CACHE);
  /** 
 * Preload the cache, using up to maxBytes bytes or maxMillsecs msec.
 */
  public PreloadStats preload(  PreloadConfig config) throws DatabaseException {
    return new DatabaseImpl_preload(this,config).execute();
  }
  public String dumpString(  int nSpaces){
    StringBuffer sb=new StringBuffer();
    sb.append(TreeUtils.indent(nSpaces));
    sb.append("<database id=\"");
    sb.append(id.toString());
    sb.append("\"");
    if (btreeComparator != null) {
      sb.append(" btc=\"");
      sb.append(serializeComparator(btreeComparator));
      sb.append("\"");
    }
    if (duplicateComparator != null) {
      sb.append(" dupc=\"");
      sb.append(serializeComparator(duplicateComparator));
      sb.append("\"");
    }
    sb.append("/>");
    return sb.toString();
  }
  /** 
 * @see LogWritable#getLogSize
 */
  public int getLogSize(){
    return id.getLogSize() + tree.getLogSize() + LogUtils.getBooleanLogSize()+ LogUtils.getStringLogSize(serializeComparator(btreeComparator))+ LogUtils.getStringLogSize(serializeComparator(duplicateComparator))+ (LogUtils.getIntLogSize() * 2);
  }
  /** 
 * @see LogWritable#writeToLog
 */
  public void writeToLog(  ByteBuffer logBuffer){
    id.writeToLog(logBuffer);
    tree.writeToLog(logBuffer);
    LogUtils.writeBoolean(logBuffer,duplicatesAllowed);
    LogUtils.writeString(logBuffer,serializeComparator(btreeComparator));
    LogUtils.writeString(logBuffer,serializeComparator(duplicateComparator));
    LogUtils.writeInt(logBuffer,maxMainTreeEntriesPerNode);
    LogUtils.writeInt(logBuffer,maxDupTreeEntriesPerNode);
  }
  /** 
 * @see LogReadable#readFromLog
 */
  public void readFromLog(  ByteBuffer itemBuffer,  byte entryTypeVersion) throws LogException {
    id.readFromLog(itemBuffer,entryTypeVersion);
    tree.readFromLog(itemBuffer,entryTypeVersion);
    duplicatesAllowed=LogUtils.readBoolean(itemBuffer);
    btreeComparatorName=LogUtils.readString(itemBuffer);
    duplicateComparatorName=LogUtils.readString(itemBuffer);
    try {
      if (!EnvironmentImpl.getNoComparators()) {
        if (btreeComparatorName.length() != 0) {
          Class btreeComparatorClass=Class.forName(btreeComparatorName);
          btreeComparator=instantiateComparator(btreeComparatorClass,"Btree");
        }
        if (duplicateComparatorName.length() != 0) {
          Class duplicateComparatorClass=Class.forName(duplicateComparatorName);
          duplicateComparator=instantiateComparator(duplicateComparatorClass,"Duplicate");
        }
      }
    }
 catch (    ClassNotFoundException CNFE) {
      throw new LogException("couldn't instantiate class comparator",CNFE);
    }
    if (entryTypeVersion >= 1) {
      maxMainTreeEntriesPerNode=LogUtils.readInt(itemBuffer);
      maxDupTreeEntriesPerNode=LogUtils.readInt(itemBuffer);
    }
  }
  /** 
 * @see LogReadable#dumpLog
 */
  public void dumpLog(  StringBuffer sb,  boolean verbose){
    sb.append("<database>");
    id.dumpLog(sb,verbose);
    tree.dumpLog(sb,verbose);
    sb.append("<dupsort v=\"").append(duplicatesAllowed);
    sb.append("\"/>");
    sb.append("<btcf name=\"");
    sb.append(btreeComparatorName);
    sb.append("\"/>");
    sb.append("<dupcf name=\"");
    sb.append(duplicateComparatorName);
    sb.append("\"/>");
    sb.append("</database>");
  }
  /** 
 * @see LogReadable#logEntryIsTransactional
 */
  public boolean logEntryIsTransactional(){
    return false;
  }
  /** 
 * @see LogReadable#getTransactionId
 */
  public long getTransactionId(){
    return 0;
  }
  /** 
 * Used both to write to the log and to validate a comparator when set in
 * DatabaseConfig.
 */
  public static String serializeComparator(  Comparator comparator){
    if (comparator != null) {
      return comparator.getClass().getName();
    }
 else {
      return "";
    }
  }
  /** 
 * Used both to read from the log and to validate a comparator when set in
 * DatabaseConfig.
 */
  public static Comparator instantiateComparator(  Class comparator,  String comparatorType) throws LogException {
    if (comparator == null) {
      return null;
    }
    try {
      return (Comparator)comparator.newInstance();
    }
 catch (    InstantiationException IE) {
      throw new LogException("Exception while trying to load " + comparatorType + " Comparator class: "+ IE);
    }
catch (    IllegalAccessException IAE) {
      throw new LogException("Exception while trying to load " + comparatorType + " Comparator class: "+ IAE);
    }
  }
  public int getBinDeltaPercent(){
    return binDeltaPercent;
  }
  public int getBinMaxDeltas(){
    return binMaxDeltas;
  }
@MethodObject static class DatabaseImpl_preload {
    DatabaseImpl_preload(    DatabaseImpl _this,    PreloadConfig config){
      this._this=_this;
      this.config=config;
    }
    PreloadStats execute() throws DatabaseException {
      maxBytes=config.getMaxBytes();
      maxMillisecs=config.getMaxMillisecs();
      targetTime=Long.MAX_VALUE;
      if (maxMillisecs > 0) {
        targetTime=System.currentTimeMillis() + maxMillisecs;
      }
      this.hook290();
      ret=new PreloadStats();
      callback=new PreloadProcessor(_this.envImpl,maxBytes,targetTime,ret);
      walker=new PreloadLSNTreeWalker(_this,callback,config);
      this.hook287();
      return ret;
    }
    protected DatabaseImpl _this;
    protected PreloadConfig config;
    protected long maxBytes;
    protected long maxMillisecs;
    protected long targetTime;
    protected long cacheBudget;
    protected PreloadStats ret;
    protected PreloadProcessor callback;
    protected SortedLSNTreeWalker walker;
    protected void hook287() throws DatabaseException {
      walker.walk();
    }
    protected void hook290() throws DatabaseException {
    }
  }
  protected void hook288() throws DatabaseException {
  }
  protected void hook289() throws DatabaseException {
  }
}
\00base/com/sleepycat/je/txn/TxnManager.java:package com.sleepycat.je.txn;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import javax.transaction.xa.Xid;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.LockStats;
import com.sleepycat.je.Transaction;
import com.sleepycat.je.TransactionConfig;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.utilint.DbLsn;
import de.ovgu.cide.jakutil.*;
/** 
 * Class to manage transactions. Basically a Set of all transactions with add
 * and remove methods and a latch around the set.
 */
public class TxnManager {
  static final long NULL_TXN_ID=-1;
  private static final String DEBUG_NAME=TxnManager.class.getName();
  private LockManager lockManager;
  private EnvironmentImpl env;
  private Set allTxns;
  private Map allXATxns;
  private Map thread2Txn;
  private long lastUsedTxnId;
  private int nActiveSerializable;
  public TxnManager(  EnvironmentImpl env) throws DatabaseException {
    this.hook822(env);
    this.env=env;
    allTxns=new HashSet();
    this.hook821(env);
    allXATxns=Collections.synchronizedMap(new HashMap());
    thread2Txn=Collections.synchronizedMap(new HashMap());
    this.hook824();
    lastUsedTxnId=0;
  }
  /** 
 * Set the txn id sequence.
 */
  synchronized public void setLastTxnId(  long lastId){
    this.lastUsedTxnId=lastId;
  }
  /** 
 * Get the last used id, for checkpoint info.
 */
  public synchronized long getLastTxnId(){
    return lastUsedTxnId;
  }
  /** 
 * Get the next transaction id to use.
 */
  synchronized long incTxnId(){
    return ++lastUsedTxnId;
  }
  /** 
 * Create a new transaction.
 * @param parentfor nested transactions, not yet supported
 * @param txnConfigspecifies txn attributes
 * @return the new txn
 */
  public Txn txnBegin(  Transaction parent,  TransactionConfig txnConfig) throws DatabaseException {
    if (parent != null) {
      throw new DatabaseException("Nested transactions are not supported yet.");
    }
    return new Txn(env,txnConfig);
  }
  /** 
 * Give transactions and environment access to lock manager.
 */
  public LockManager getLockManager(){
    return lockManager;
  }
  /** 
 * Called when txn is created.
 */
  void registerTxn(  Txn txn) throws DatabaseException {
    allTxns.add(txn);
    if (txn.isSerializableIsolation()) {
      nActiveSerializable++;
    }
  }
  /** 
 * Called when txn ends.
 */
  void unRegisterTxn(  Txn txn,  boolean isCommit) throws DatabaseException {
    allTxns.remove(txn);
    this.hook828(txn);
    this.hook825(isCommit);
    if (txn.isSerializableIsolation()) {
      nActiveSerializable--;
    }
  }
  /** 
 * Called when txn is created.
 */
  public void registerXATxn(  Xid xid,  Txn txn,  boolean isPrepare) throws DatabaseException {
    if (!allXATxns.containsKey(xid)) {
      allXATxns.put(xid,txn);
      this.hook829();
    }
    this.hook826(isPrepare);
  }
  /** 
 * Called when txn ends.
 */
  void unRegisterXATxn(  Xid xid,  boolean isCommit) throws DatabaseException {
    if (allXATxns.remove(xid) == null) {
      throw new DatabaseException("XA Transaction " + xid + " can not be unregistered.");
    }
    this.hook830();
    this.hook827(isCommit);
  }
  /** 
 * Retrieve a Txn object from an Xid.
 */
  public Txn getTxnFromXid(  Xid xid) throws DatabaseException {
    return (Txn)allXATxns.get(xid);
  }
  /** 
 * Called when txn is assoc'd with this thread.
 */
  public void setTxnForThread(  Transaction txn){
    Thread curThread=Thread.currentThread();
    thread2Txn.put(curThread,txn);
  }
  /** 
 * Called when txn is assoc'd with this thread.
 */
  public Transaction unsetTxnForThread() throws DatabaseException {
    Thread curThread=Thread.currentThread();
    return (Transaction)thread2Txn.remove(curThread);
  }
  /** 
 * Retrieve a Txn object for this Thread.
 */
  public Transaction getTxnForThread() throws DatabaseException {
    return (Transaction)thread2Txn.get(Thread.currentThread());
  }
  public Xid[] XARecover() throws DatabaseException {
    Set xidSet=allXATxns.keySet();
    Xid[] ret=new Xid[xidSet.size()];
    ret=(Xid[])xidSet.toArray(ret);
    return ret;
  }
  /** 
 * Returns whether there are any active serializable transactions, excluding
 * the transaction given (if non-null). This is intentionally returned
 * without latching, since latching would not make the act of reading an
 * integer more atomic than it already is.
 */
  public boolean areOtherSerializableTransactionsActive(  Locker excludeLocker){
    int exclude=(excludeLocker != null && excludeLocker.isSerializableIsolation()) ? 1 : 0;
    return (nActiveSerializable - exclude > 0);
  }
  /** 
 * Get the earliest LSN of all the active transactions, for checkpoint.
 */
  public long getFirstActiveLsn() throws DatabaseException {
    long firstActive=DbLsn.NULL_LSN;
    firstActive=this.hook823(firstActive);
    return firstActive;
  }
  protected void hook821(  EnvironmentImpl env) throws DatabaseException {
  }
  protected void hook822(  EnvironmentImpl env) throws DatabaseException {
    if (env.isNoLocking()) {
      lockManager=new DummyLockManager(env);
    }
 else {
      lockManager=new SyncedLockManager(env);
    }
  }
  protected long hook823(  long firstActive) throws DatabaseException {
    Iterator iter=allTxns.iterator();
    while (iter.hasNext()) {
      long txnFirstActive=((Txn)iter.next()).getFirstActiveLsn();
      if (firstActive == DbLsn.NULL_LSN) {
        firstActive=txnFirstActive;
      }
 else       if (txnFirstActive != DbLsn.NULL_LSN) {
        if (DbLsn.compareTo(txnFirstActive,firstActive) < 0) {
          firstActive=txnFirstActive;
        }
      }
    }
    return firstActive;
  }
  protected void hook824() throws DatabaseException {
  }
  protected void hook825(  boolean isCommit) throws DatabaseException {
  }
  protected void hook826(  boolean isPrepare) throws DatabaseException {
  }
  protected void hook827(  boolean isCommit) throws DatabaseException {
  }
  protected void hook828(  Txn txn) throws DatabaseException {
  }
  protected void hook829() throws DatabaseException {
  }
  protected void hook830() throws DatabaseException {
  }
}
\00base/com/sleepycat/je/txn/LockManager.java:package com.sleepycat.je.txn;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DeadlockException;
import com.sleepycat.je.LockStats;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.EnvConfigObserver;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.dbi.RangeRestartException;
import de.ovgu.cide.jakutil.*;
/** 
 * LockManager manages locks.
 * Note that locks are counted as taking up part of the JE cache;
 */
public abstract class LockManager implements EnvConfigObserver {
  protected int nLockTables=1;
  private Map[] lockTables;
  private EnvironmentImpl envImpl;
  private MemoryBudget memoryBudget;
  private static RangeRestartException rangeRestartException=new RangeRestartException();
  private static boolean lockTableDump=false;
  public LockManager(  EnvironmentImpl envImpl) throws DatabaseException {
    DbConfigManager configMgr=envImpl.getConfigManager();
    this.hook779(configMgr);
    lockTables=new Map[nLockTables];
    this.hook770();
    for (int i=0; i < nLockTables; i++) {
      lockTables[i]=new HashMap();
      this.hook771(envImpl,i);
    }
    this.envImpl=envImpl;
    memoryBudget=envImpl.getMemoryBudget();
    this.hook774();
    envConfigUpdate(configMgr);
    envImpl.addConfigObserver(this);
  }
  /** 
 * Process notifications of mutable property changes.
 */
  public void envConfigUpdate(  DbConfigManager configMgr) throws DatabaseException {
    LockInfo.setDeadlockStackTrace(configMgr.getBoolean(EnvironmentParams.TXN_DEADLOCK_STACK_TRACE));
    setLockTableDump(configMgr.getBoolean(EnvironmentParams.TXN_DUMPLOCKS));
  }
  /** 
 * Called when the je.txn.dumpLocks property is changed.
 */
  static void setLockTableDump(  boolean enable){
    lockTableDump=enable;
  }
  protected int getLockTableIndex(  Long nodeId){
    return ((int)nodeId.longValue()) % nLockTables;
  }
  protected int getLockTableIndex(  long nodeId){
    return ((int)nodeId) % nLockTables;
  }
  /** 
 * Attempt to acquire a lock of <i>type</i> on <i>nodeId</i>. If the lock
 * acquisition would result in a deadlock, throw an exception.<br>
 * If the requested lock is not currently available, block until it is or
 * until timeout milliseconds have elapsed.<br>
 * If a lock of <i>type</i> is already held, return EXISTING.<br>
 * If a WRITE lock is held and a READ lock is requested, return PROMOTION.<br>
 * If a lock request is for a lock that is not currently held, return either
 * NEW or DENIED depending on whether the lock is granted or not.<br>
 * @param nodeIdThe NodeId to lock.
 * @param lockerThe Locker to lock this on behalf of.
 * @param typeThe lock type requested.
 * @param timeoutmilliseconds to time out after if lock couldn't be obtained. 0
 * means block indefinitely. Not used if nonBlockingRequest is
 * true.
 * @param nonBlockingRequestif true, means don't block if lock can't be acquired, and
 * ignore the timeout parameter.
 * @return a LockGrantType indicating whether the request was fulfilled or
 * not. LockGrantType.NEW means the lock grant was fulfilled and the
 * caller did not previously hold the lock. PROMOTION means the lock
 * was granted and it was a promotion from READ to WRITE. EXISTING
 * means the lock was already granted (not a promotion). DENIED
 * means the lock was not granted either because the timeout passed
 * without acquiring the lock or timeout was -1 and the lock was not
 * immediately available.
 * @throws DeadlockExceptionif acquiring the lock would result in a deadlock.
 */
  public LockGrantType lock(  long nodeId,  Locker locker,  LockType type,  long timeout,  boolean nonBlockingRequest,  DatabaseImpl database) throws DeadlockException, DatabaseException {
    assert timeout >= 0;
synchronized (locker) {
      Long nid=new Long(nodeId);
      LockAttemptResult result=attemptLock(nid,locker,type,nonBlockingRequest);
      if (result.success || result.lockGrant == LockGrantType.DENIED) {
        return result.lockGrant;
      }
      this.hook772(nonBlockingRequest);
      assert !nonBlockingRequest;
      try {
        boolean doWait=true;
        if (locker.isTimedOut()) {
          if (validateOwnership(nid,locker,type,true,memoryBudget)) {
            doWait=false;
          }
 else {
            String errMsg=makeTimeoutMsg("Transaction",locker,nodeId,type,result.lockGrant,result.useLock,locker.getTxnTimeOut(),locker.getTxnStartMillis(),System.currentTimeMillis(),database);
            throw new DeadlockException(errMsg);
          }
        }
        boolean keepTime=(timeout > 0);
        long startTime=(keepTime ? System.currentTimeMillis() : 0);
        while (doWait) {
          locker.setWaitingFor(result.useLock);
          try {
            locker.wait(timeout);
          }
 catch (          InterruptedException IE) {
            throw new RunRecoveryException(envImpl,IE);
          }
          boolean lockerTimedOut=locker.isTimedOut();
          long now=System.currentTimeMillis();
          boolean thisLockTimedOut=(keepTime && (now - startTime > timeout));
          boolean isRestart=(result.lockGrant == LockGrantType.WAIT_RESTART);
          if (validateOwnership(nid,locker,type,lockerTimedOut || thisLockTimedOut || isRestart,memoryBudget)) {
            break;
          }
 else {
            if (isRestart) {
              throw rangeRestartException;
            }
            if (thisLockTimedOut) {
              locker.setOnlyAbortable();
              String errMsg=makeTimeoutMsg("Lock",locker,nodeId,type,result.lockGrant,result.useLock,timeout,startTime,now,database);
              throw new DeadlockException(errMsg);
            }
            if (lockerTimedOut) {
              locker.setOnlyAbortable();
              String errMsg=makeTimeoutMsg("Transaction",locker,nodeId,type,result.lockGrant,result.useLock,locker.getTxnTimeOut(),locker.getTxnStartMillis(),now,database);
              throw new DeadlockException(errMsg);
            }
          }
        }
      }
  finally {
        locker.setWaitingFor(null);
        assert EnvironmentImpl.maybeForceYield();
      }
      locker.addLock(nid,result.useLock,type,result.lockGrant);
      return result.lockGrant;
    }
  }
  abstract protected LockAttemptResult attemptLock(  Long nodeId,  Locker locker,  LockType type,  boolean nonBlockingRequest) throws DatabaseException ;
  protected LockAttemptResult attemptLockInternal(  Long nodeId,  Locker locker,  LockType type,  boolean nonBlockingRequest,  int lockTableIndex) throws DatabaseException {
    Map lockTable=lockTables[lockTableIndex];
    Lock useLock=(Lock)lockTable.get(nodeId);
    if (useLock == null) {
      useLock=new Lock(nodeId);
      lockTable.put(nodeId,useLock);
      this.hook780(lockTableIndex);
    }
    LockGrantType lockGrant=useLock.lock(type,locker,nonBlockingRequest,memoryBudget,lockTableIndex);
    boolean success=false;
    if ((lockGrant == LockGrantType.NEW) || (lockGrant == LockGrantType.PROMOTION)) {
      locker.addLock(nodeId,useLock,type,lockGrant);
      success=true;
    }
 else     if (lockGrant == LockGrantType.EXISTING) {
      success=true;
    }
 else     if (lockGrant == LockGrantType.DENIED) {
    }
 else {
      this.hook775();
    }
    return new LockAttemptResult(useLock,lockGrant,success);
  }
  /** 
 * Create a informative lock or txn timeout message.
 */
  protected abstract String makeTimeoutMsg(  String lockOrTxn,  Locker locker,  long nodeId,  LockType type,  LockGrantType grantType,  Lock useLock,  long timeout,  long start,  long now,  DatabaseImpl database) throws DatabaseException ;
  /** 
 * Do the real work of creating an lock or txn timeout message.
 */
  protected String makeTimeoutMsgInternal(  String lockOrTxn,  Locker locker,  long nodeId,  LockType type,  LockGrantType grantType,  Lock useLock,  long timeout,  long start,  long now,  DatabaseImpl database){
    if (lockTableDump) {
      System.out.println("++++++++++ begin lock table dump ++++++++++");
      for (int i=0; i < nLockTables; i++) {
        StringBuffer sb=new StringBuffer();
        dumpToStringNoLatch(sb,i);
        System.out.println(sb.toString());
      }
      System.out.println("++++++++++ end lock table dump ++++++++++");
    }
    StringBuffer sb=new StringBuffer();
    sb.append(lockOrTxn);
    sb.append(" expired. Locker ").append(locker);
    sb.append(": waited for lock");
    if (database != null) {
      sb.append(" on database=").append(database.getDebugName());
    }
    sb.append(" node=").append(nodeId);
    sb.append(" type=").append(type);
    sb.append(" grant=").append(grantType);
    sb.append(" timeoutMillis=").append(timeout);
    sb.append(" startTime=").append(start);
    sb.append(" endTime=").append(now);
    sb.append("\nOwners: ").append(useLock.getOwnersClone());
    sb.append("\nWaiters: ").append(useLock.getWaitersListClone()).append("\n");
    StringBuffer deadlockInfo=findDeadlock(useLock,locker);
    if (deadlockInfo != null) {
      sb.append(deadlockInfo);
    }
    return sb.toString();
  }
  /** 
 * Release a lock and possibly notify any waiters that they have been
 * granted the lock.
 * @param nodeIdThe node ID of the lock to release.
 * @return true if the lock is released successfully, false if the lock is
 * not currently being held.
 */
  boolean release(  long nodeId,  Locker locker) throws DatabaseException {
    return release(nodeId,null,locker,true);
  }
  /** 
 * Release a lock and possibly notify any waiters that they have been
 * granted the lock.
 * @param lockThe lock to release
 * @return true if the lock is released successfully, false if the lock is
 * not currently being held.
 */
  boolean release(  Lock lock,  Locker locker) throws DatabaseException {
    return release(-1,lock,locker,false);
  }
  /** 
 * Do the work of releasing a lock and notifying any waiters that they have
 * been granted the lock.
 * @param lockThe lock to release. If null, use nodeId to find lock
 * @param nodeIdThe node ID of the lock to release, if lock is null. May not
 * be valid if lock is not null. MUST be valid if
 * removeFromLocker is true
 * @param locker
 * @param removeFromLockertrue if we're responsible for
 * @return true if the lock is released successfully, false if the lock is
 * not currently being held.
 */
  private boolean release(  long nodeId,  Lock lock,  Locker locker,  boolean removeFromLocker) throws DatabaseException {
synchronized (locker) {
      Set newOwners=releaseAndFindNotifyTargets(nodeId,lock,locker,removeFromLocker);
      if (newOwners == null) {
        return false;
      }
      if (newOwners.size() > 0) {
        Iterator iter=newOwners.iterator();
        while (iter.hasNext()) {
          Locker lockerToNotify=(Locker)iter.next();
synchronized (lockerToNotify) {
            lockerToNotify.notifyAll();
          }
          assert EnvironmentImpl.maybeForceYield();
        }
      }
      return true;
    }
  }
  /** 
 * Release the lock, and return the set of new owners to notify, if any.
 * @return null if the lock does not exist or the given locker was not the
 * owner, a non-empty set if owners should be notified after
 * releasing, an empty set if no notification is required.
 */
  protected abstract Set releaseAndFindNotifyTargets(  long nodeId,  Lock lock,  Locker locker,  boolean removeFromLocker) throws DatabaseException ;
  /** 
 * Do the real work of releaseAndFindNotifyTargets
 */
  protected Set releaseAndFindNotifyTargetsInternal(  long nodeId,  Lock lock,  Locker locker,  boolean removeFromLocker,  int lockTableIndex) throws DatabaseException {
    Lock useLock=lock;
    Map lockTable=lockTables[lockTableIndex];
    if (useLock == null) {
      useLock=(Lock)lockTable.get(new Long(nodeId));
    }
    if (useLock == null) {
      return null;
    }
    Set lockersToNotify=useLock.release(locker,memoryBudget,lockTableIndex);
    if (lockersToNotify == null) {
      return null;
    }
    if (removeFromLocker) {
      assert nodeId != -1;
      locker.removeLock(nodeId,useLock);
    }
    if ((useLock.nWaiters() == 0) && (useLock.nOwners() == 0)) {
      lockTables[lockTableIndex].remove(useLock.getNodeId());
      this.hook781(lockTableIndex);
    }
    return lockersToNotify;
  }
  /** 
 * Transfer ownership a lock from one locker to another locker. We're not
 * sending any notification to the waiters on the lock table, and the past
 * and present owner should be ready for the transfer.
 */
  abstract void transfer(  long nodeId,  Locker owningLocker,  Locker destLocker,  boolean demoteToRead) throws DatabaseException ;
  /** 
 * Do the real work of transfer
 */
  protected void transferInternal(  long nodeId,  Locker owningLocker,  Locker destLocker,  boolean demoteToRead,  int lockTableIndex) throws DatabaseException {
    Map lockTable=lockTables[lockTableIndex];
    Lock useLock=(Lock)lockTable.get(new Long(nodeId));
    assert useLock != null : "Transfer, lock " + nodeId + " was null";
    if (demoteToRead) {
      useLock.demote(owningLocker);
    }
    useLock.transfer(owningLocker,destLocker,memoryBudget,lockTableIndex);
    owningLocker.removeLock(nodeId,useLock);
  }
  /** 
 * Transfer ownership a lock from one locker to a set of other txns, cloning
 * the lock as necessary. This will always be demoted to read, as we can't
 * have multiple locker owners any other way. We're not sending any
 * notification to the waiters on the lock table, and the past and present
 * owners should be ready for the transfer.
 */
  abstract void transferMultiple(  long nodeId,  Locker owningLocker,  Locker[] destLockers) throws DatabaseException ;
  /** 
 * Do the real work of transferMultiple
 */
  protected void transferMultipleInternal(  long nodeId,  Locker owningLocker,  Locker[] destLockers,  int lockTableIndex) throws DatabaseException {
    Map lockTable=lockTables[lockTableIndex];
    Lock useLock=(Lock)lockTable.get(new Long(nodeId));
    assert useLock != null : "Transfer, lock " + nodeId + " was null";
    useLock.demote(owningLocker);
    useLock.transferMultiple(owningLocker,destLockers,memoryBudget,lockTableIndex);
    owningLocker.removeLock(nodeId,useLock);
  }
  /** 
 * Demote a lock from write to read. Call back to the owning locker to move
 * this to its read collection.
 * @param lockThe lock to release. If null, use nodeId to find lock
 * @param locker
 */
  abstract void demote(  long nodeId,  Locker locker) throws DatabaseException ;
  /** 
 * Do the real work of demote.
 */
  protected void demoteInternal(  long nodeId,  Locker locker,  int lockTableIndex) throws DatabaseException {
    Map lockTable=lockTables[lockTableIndex];
    Lock useLock=(Lock)lockTable.get(new Long(nodeId));
    useLock.demote(locker);
    locker.moveWriteToReadLock(nodeId,useLock);
  }
  /** 
 * Test the status of the lock on nodeId. If any transaction holds any lock
 * on it, true is returned. If no transaction holds a lock on it, false is
 * returned.
 * This method is only used by unit tests.
 * @param nodeIdThe NodeId to check.
 * @return true if any transaction holds any lock on the nodeid. false if no
 * lock is held by any transaction.
 */
  abstract boolean isLocked(  Long nodeId) throws DatabaseException ;
  /** 
 * Do the real work of isLocked.
 */
  protected boolean isLockedInternal(  Long nodeId,  int lockTableIndex){
    Map lockTable=lockTables[lockTableIndex];
    Lock entry=(Lock)lockTable.get(nodeId);
    if (entry == null) {
      return false;
    }
    return entry.nOwners() != 0;
  }
  /** 
 * Return true if this locker owns this a lock of this type on given node.
 * This method is only used by unit tests.
 */
  abstract boolean isOwner(  Long nodeId,  Locker locker,  LockType type) throws DatabaseException ;
  /** 
 * Do the real work of isOwner.
 */
  protected boolean isOwnerInternal(  Long nodeId,  Locker locker,  LockType type,  int lockTableIndex){
    Map lockTable=lockTables[lockTableIndex];
    Lock entry=(Lock)lockTable.get(nodeId);
    if (entry == null) {
      return false;
    }
    return entry.isOwner(locker,type);
  }
  /** 
 * Return true if this locker is waiting on this lock.
 * This method is only used by unit tests.
 */
  abstract boolean isWaiter(  Long nodeId,  Locker locker) throws DatabaseException ;
  /** 
 * Do the real work of isWaiter.
 */
  protected boolean isWaiterInternal(  Long nodeId,  Locker locker,  int lockTableIndex){
    Map lockTable=lockTables[lockTableIndex];
    Lock entry=(Lock)lockTable.get(nodeId);
    if (entry == null) {
      return false;
    }
    return entry.isWaiter(locker);
  }
  /** 
 * Return the number of waiters for this lock.
 */
  abstract int nWaiters(  Long nodeId) throws DatabaseException ;
  /** 
 * Do the real work of nWaiters.
 */
  protected int nWaitersInternal(  Long nodeId,  int lockTableIndex){
    Map lockTable=lockTables[lockTableIndex];
    Lock entry=(Lock)lockTable.get(nodeId);
    if (entry == null) {
      return -1;
    }
    return entry.nWaiters();
  }
  /** 
 * Return the number of owners of this lock.
 */
  abstract int nOwners(  Long nodeId) throws DatabaseException ;
  /** 
 * Do the real work of nWaiters.
 */
  protected int nOwnersInternal(  Long nodeId,  int lockTableIndex){
    Map lockTable=lockTables[lockTableIndex];
    Lock entry=(Lock)lockTable.get(nodeId);
    if (entry == null) {
      return -1;
    }
    return entry.nOwners();
  }
  /** 
 * @return the transaction that owns the write lock for this
 */
  abstract Locker getWriteOwnerLocker(  Long nodeId) throws DatabaseException ;
  /** 
 * Do the real work of getWriteOwnerLocker.
 */
  protected Locker getWriteOwnerLockerInternal(  Long nodeId,  int lockTableIndex) throws DatabaseException {
    Map lockTable=lockTables[lockTableIndex];
    Lock lock=(Lock)lockTable.get(nodeId);
    if (lock == null) {
      return null;
    }
 else     if (lock.nOwners() > 1) {
      return null;
    }
 else {
      return lock.getWriteOwnerLocker();
    }
  }
  abstract protected boolean validateOwnership(  Long nodeId,  Locker locker,  LockType type,  boolean flushFromWaiters,  MemoryBudget mb) throws DatabaseException ;
  protected boolean validateOwnershipInternal(  Long nodeId,  Locker locker,  LockType type,  boolean flushFromWaiters,  MemoryBudget mb,  int lockTableIndex) throws DatabaseException {
    if (isOwnerInternal(nodeId,locker,type,lockTableIndex)) {
      return true;
    }
    if (flushFromWaiters) {
      Lock entry=(Lock)lockTables[lockTableIndex].get(nodeId);
      if (entry != null) {
        entry.flushWaiter(locker,mb,lockTableIndex);
      }
    }
    return false;
  }
  /** 
 * Dump the lock table to the lock stats.
 */
  abstract protected void dumpLockTable(  LockStats stats) throws DatabaseException ;
  /** 
 * Do the real work of dumpLockTableInternal.
 */
  protected void dumpLockTableInternal(  LockStats stats,  int i){
    Map lockTable=lockTables[i];
    this.hook776(stats,lockTable);
    Iterator iter=lockTable.values().iterator();
    while (iter.hasNext()) {
      Lock lock=(Lock)iter.next();
      this.hook777(stats,lock);
      Iterator ownerIter=lock.getOwnersClone().iterator();
      while (ownerIter.hasNext()) {
        LockInfo info=(LockInfo)ownerIter.next();
        this.hook778(stats,info);
      }
    }
  }
  /** 
 * Debugging
 */
  public void dump() throws DatabaseException {
    System.out.println(dumpToString());
  }
  public String dumpToString() throws DatabaseException {
    StringBuffer sb=new StringBuffer();
    for (int i=0; i < nLockTables; i++) {
      this.hook773(sb,i);
    }
    return sb.toString();
  }
  private void dumpToStringNoLatch(  StringBuffer sb,  int whichTable){
    Map lockTable=lockTables[whichTable];
    Iterator entries=lockTable.entrySet().iterator();
    while (entries.hasNext()) {
      Map.Entry entry=(Map.Entry)entries.next();
      Long nid=(Long)entry.getKey();
      Lock lock=(Lock)entry.getValue();
      sb.append("---- Node Id: ").append(nid).append("----\n");
      sb.append(lock);
      sb.append('\n');
    }
  }
  private StringBuffer findDeadlock(  Lock lock,  Locker rootLocker){
    Set ownerSet=new HashSet();
    ownerSet.add(rootLocker);
    StringBuffer ret=findDeadlock1(ownerSet,lock,rootLocker);
    if (ret != null) {
      return ret;
    }
 else {
      return null;
    }
  }
  private StringBuffer findDeadlock1(  Set ownerSet,  Lock lock,  Locker rootLocker){
    Iterator ownerIter=lock.getOwnersClone().iterator();
    while (ownerIter.hasNext()) {
      LockInfo info=(LockInfo)ownerIter.next();
      Locker locker=info.getLocker();
      Lock waitsFor=locker.getWaitingFor();
      if (ownerSet.contains(locker) || locker == rootLocker) {
        StringBuffer ret=new StringBuffer();
        ret.append("Transaction ").append(locker.toString());
        ret.append(" owns ").append(lock.getNodeId());
        ret.append(" ").append(info).append("\n");
        ret.append("Transaction ").append(locker.toString());
        ret.append(" waits for ");
        if (waitsFor == null) {
          ret.append(" nothing");
        }
 else {
          ret.append(" node ");
          ret.append(waitsFor.getNodeId());
        }
        ret.append("\n");
        return ret;
      }
      if (waitsFor != null) {
        ownerSet.add(locker);
        StringBuffer sb=findDeadlock1(ownerSet,waitsFor,rootLocker);
        if (sb != null) {
          String waitInfo="Transaction " + locker + " waits for node "+ waitsFor.getNodeId()+ "\n";
          sb.insert(0,waitInfo);
          return sb;
        }
        ownerSet.remove(locker);
      }
    }
    return null;
  }
  /** 
 * This is just a struct to hold a multi-value return.
 */
static class LockAttemptResult {
    boolean success;
    Lock useLock;
    LockGrantType lockGrant;
    LockAttemptResult(    Lock useLock,    LockGrantType lockGrant,    boolean success){
      this.useLock=useLock;
      this.lockGrant=lockGrant;
      this.success=success;
    }
  }
  protected void hook770() throws DatabaseException {
  }
  protected void hook771(  EnvironmentImpl envImpl,  int i) throws DatabaseException {
  }
  protected void hook772(  boolean nonBlockingRequest) throws DeadlockException, DatabaseException {
  }
  protected void hook773(  StringBuffer sb,  int i) throws DatabaseException {
    dumpToStringNoLatch(sb,i);
  }
  protected void hook774() throws DatabaseException {
  }
  protected void hook775() throws DatabaseException {
  }
  protected void hook776(  LockStats stats,  Map lockTable){
  }
  protected void hook777(  LockStats stats,  Lock lock){
  }
  protected void hook778(  LockStats stats,  LockInfo info){
  }
  protected void hook779(  DbConfigManager configMgr) throws DatabaseException {
  }
  protected void hook780(  int lockTableIndex) throws DatabaseException {
  }
  protected void hook781(  int lockTableIndex) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/txn/Txn.java:package com.sleepycat.je.txn;
import java.nio.ByteBuffer;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import java.util.logging.Level;
import java.util.logging.Logger;
import javax.transaction.xa.XAResource;
import javax.transaction.xa.Xid;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.LockStats;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.TransactionConfig;
import com.sleepycat.je.dbi.CursorImpl;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.log.LogReadable;
import com.sleepycat.je.log.LogUtils;
import com.sleepycat.je.log.LogWritable;
import com.sleepycat.je.log.entry.LNLogEntry;
import com.sleepycat.je.recovery.RecoveryManager;
import com.sleepycat.je.tree.LN;
import com.sleepycat.je.tree.TreeLocation;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * A Txn is one that's created by a call to Environment.txnBegin.  This class
 * must support multithreaded use.
 */
public class Txn extends Locker implements LogWritable, LogReadable {
  public static final byte TXN_NOSYNC=0;
  public static final byte TXN_WRITE_NOSYNC=1;
  public static final byte TXN_SYNC=2;
  private static final String DEBUG_NAME=Txn.class.getName();
  private byte txnState;
  private CursorImpl cursorSet;
  private static final byte USABLE=0;
  private static final byte CLOSED=1;
  private static final byte ONLY_ABORTABLE=2;
  private static final byte STATE_BITS=3;
  private static final byte IS_PREPARED=4;
  private static final byte XA_SUSPENDED=8;
  private Set readLocks;
  private Map writeInfo;
  private Map undoDatabases;
  private long lastLoggedLsn=DbLsn.NULL_LSN;
  private long firstLoggedLsn=DbLsn.NULL_LSN;
  private byte defaultFlushSyncBehavior;
  private boolean serializableIsolation;
  private boolean readCommittedIsolation;
  private int inMemorySize;
  public static int ACCUMULATED_LIMIT=10000;
  /** 
 * Create a transaction from Environment.txnBegin.
 */
  public Txn(  EnvironmentImpl envImpl,  TransactionConfig config) throws DatabaseException {
    super(envImpl,config.getReadUncommitted(),config.getNoWait());
    init(envImpl,config);
  }
  public Txn(  EnvironmentImpl envImpl,  TransactionConfig config,  long id) throws DatabaseException {
    super(envImpl,config.getReadUncommitted(),config.getNoWait());
    init(envImpl,config);
    this.id=id;
  }
  private void init(  EnvironmentImpl envImpl,  TransactionConfig config) throws DatabaseException {
    serializableIsolation=config.getSerializableIsolation();
    readCommittedIsolation=config.getReadCommitted();
    if (config.getSync()) {
      defaultFlushSyncBehavior=TXN_SYNC;
    }
 else     if (config.getWriteNoSync()) {
      defaultFlushSyncBehavior=TXN_WRITE_NOSYNC;
    }
 else     if (config.getNoSync()) {
      defaultFlushSyncBehavior=TXN_NOSYNC;
    }
 else {
      defaultFlushSyncBehavior=TXN_SYNC;
    }
    lastLoggedLsn=DbLsn.NULL_LSN;
    firstLoggedLsn=DbLsn.NULL_LSN;
    txnState=USABLE;
    this.hook809();
    this.envImpl.getTxnManager().registerTxn(this);
  }
  /** 
 * Constructor for reading from log.
 */
  public Txn(){
    lastLoggedLsn=DbLsn.NULL_LSN;
  }
  /** 
 * UserTxns get a new unique id for each instance.
 */
  protected long generateId(  TxnManager txnManager){
    return txnManager.incTxnId();
  }
  /** 
 * Access to last LSN.
 */
  long getLastLsn(){
    return lastLoggedLsn;
  }
  public void setPrepared(  boolean prepared){
    if (prepared) {
      txnState|=IS_PREPARED;
    }
 else {
      txnState&=~IS_PREPARED;
    }
  }
  public void setSuspended(  boolean suspended){
    if (suspended) {
      txnState|=XA_SUSPENDED;
    }
 else {
      txnState&=~XA_SUSPENDED;
    }
  }
  public boolean isSuspended(){
    return (txnState & XA_SUSPENDED) != 0;
  }
  /** 
 * Gets a lock on this nodeId and, if it is a write lock, saves an abort
 * LSN.  Caller will set the abortLsn later, after the write lock has been
 * obtained.
 * @see Locker#lockInternal
 * @Override
 */
  LockResult lockInternal(  long nodeId,  LockType lockType,  boolean noWait,  DatabaseImpl database) throws DatabaseException {
    long timeout=0;
    boolean useNoWait=noWait || defaultNoWait;
synchronized (this) {
      checkState(false);
      if (!useNoWait) {
        timeout=lockTimeOutMillis;
      }
    }
    LockGrantType grant=lockManager.lock(nodeId,this,lockType,timeout,useNoWait,database);
    WriteLockInfo info=null;
    if (writeInfo != null) {
      if (grant != LockGrantType.DENIED && lockType.isWriteLock()) {
synchronized (this) {
          info=(WriteLockInfo)writeInfo.get(new Long(nodeId));
          undoDatabases.put(database.getId(),database);
        }
      }
    }
    return new LockResult(grant,info);
  }
  public int prepare(  Xid xid) throws DatabaseException {
    if ((txnState & IS_PREPARED) != 0) {
      throw new DatabaseException("prepare() has already been called for Transaction " + id + ".");
    }
synchronized (this) {
      checkState(false);
      if (checkCursorsForClose()) {
        throw new DatabaseException("Transaction " + id + " prepare failed because there were open cursors.");
      }
      TxnPrepare prepareRecord=new TxnPrepare(id,xid);
      LogManager logManager=envImpl.getLogManager();
      logManager.logForceFlush(prepareRecord,true);
    }
    setPrepared(true);
    return XAResource.XA_OK;
  }
  public void commit(  Xid xid) throws DatabaseException {
    commit(TXN_SYNC);
    envImpl.getTxnManager().unRegisterXATxn(xid,true);
    return;
  }
  public void abort(  Xid xid) throws DatabaseException {
    abort(true);
    envImpl.getTxnManager().unRegisterXATxn(xid,false);
    return;
  }
  /** 
 * Call commit() with the default sync configuration property.
 */
  public long commit() throws DatabaseException {
    return commit(defaultFlushSyncBehavior);
  }
  /** 
 * Commit this transaction
 * 1. Releases read locks
 * 2. Writes a txn commit record into the log
 * 3. Flushes the log to disk.
 * 4. Add deleted LN info to IN compressor queue
 * 5. Release all write locks 
 * If any step of this fails, we must convert this transaction to an abort.
 */
  public long commit(  byte flushSyncBehavior) throws DatabaseException {
    try {
      long commitLsn=DbLsn.NULL_LSN;
synchronized (this) {
        checkState(false);
        if (checkCursorsForClose()) {
          throw new DatabaseException("Transaction " + id + " commit failed because there were open cursors.");
        }
        if (handleLockToHandleMap != null) {
          Iterator handleLockIter=handleLockToHandleMap.entrySet().iterator();
          while (handleLockIter.hasNext()) {
            Map.Entry entry=(Map.Entry)handleLockIter.next();
            transferHandleLockToHandleSet((Long)entry.getKey(),(Set)entry.getValue());
          }
        }
        LogManager logManager=envImpl.getLogManager();
        int numReadLocks=clearReadLocks();
        int numWriteLocks=0;
        if (writeInfo != null) {
          numWriteLocks=writeInfo.size();
          TxnCommit commitRecord=new TxnCommit(id,lastLoggedLsn);
          if (flushSyncBehavior == TXN_SYNC) {
            commitLsn=logManager.logForceFlush(commitRecord,true);
          }
 else           if (flushSyncBehavior == TXN_WRITE_NOSYNC) {
            commitLsn=logManager.logForceFlush(commitRecord,false);
          }
 else {
            commitLsn=logManager.log(commitRecord);
          }
          this.hook806();
          Set alreadyCountedLsnSet=new HashSet();
          Iterator iter=writeInfo.values().iterator();
          while (iter.hasNext()) {
            WriteLockInfo info=(WriteLockInfo)iter.next();
            lockManager.release(info.lock,this);
            if (info.abortLsn != DbLsn.NULL_LSN && !info.abortKnownDeleted) {
              Long longLsn=new Long(info.abortLsn);
              if (!alreadyCountedLsnSet.contains(longLsn)) {
                logManager.countObsoleteNode(info.abortLsn,null);
                alreadyCountedLsnSet.add(longLsn);
              }
            }
          }
          writeInfo=null;
          this.hook803();
        }
        traceCommit(numWriteLocks,numReadLocks);
      }
      this.hook805();
      close(true);
      return commitLsn;
    }
 catch (    RunRecoveryException e) {
      throw e;
    }
catch (    Throwable t) {
      try {
        abortInternal(flushSyncBehavior == TXN_SYNC,!(t instanceof DatabaseException));
        this.hook800(t);
      }
 catch (      Throwable abortT2) {
        throw new DatabaseException("Failed while attempting to commit transaction " + id + ". The attempt to abort and clean up also failed. "+ "The original exception seen from commit = "+ t.getMessage()+ " The exception from the cleanup = "+ abortT2.getMessage(),t);
      }
      throw new DatabaseException("Failed while attempting to commit transaction " + id + ", aborted instead. Original exception = "+ t.getMessage(),t);
    }
  }
  /** 
 * Abort this transaction. Steps are:
 * 1. Release LN read locks.
 * 2. Write a txn abort entry to the log. This is only for log
 * file cleaning optimization and there's no need to guarantee a
 * flush to disk.  
 * 3. Find the last LN log entry written for this txn, and use that
 * to traverse the log looking for nodes to undo. For each node,
 * use the same undo logic as recovery to rollback the transaction. Note
 * that we walk the log in order to undo in reverse order of the
 * actual operations. For example, suppose the txn did this:
 * delete K1/D1 (in LN 10)
 * create K1/D1 (in LN 20)
 * If we process LN10 before LN 20, we'd inadvertently create a 
 * duplicate tree of "K1", which would be fatal for the mapping tree.
 * 4. Release the write lock for this LN.
 */
  public long abort(  boolean forceFlush) throws DatabaseException {
    return abortInternal(forceFlush,true);
  }
  private long abortInternal(  boolean forceFlush,  boolean writeAbortRecord) throws DatabaseException {
    try {
      int numReadLocks;
      int numWriteLocks;
      long abortLsn;
synchronized (this) {
        checkState(true);
        TxnAbort abortRecord=new TxnAbort(id,lastLoggedLsn);
        abortLsn=DbLsn.NULL_LSN;
        if (writeInfo != null) {
          if (writeAbortRecord) {
            if (forceFlush) {
              abortLsn=envImpl.getLogManager().logForceFlush(abortRecord,true);
            }
 else {
              abortLsn=envImpl.getLogManager().log(abortRecord);
            }
          }
        }
        undo();
        numReadLocks=(readLocks == null) ? 0 : clearReadLocks();
        this.hook808();
        numWriteLocks=(writeInfo == null) ? 0 : clearWriteLocks();
        this.hook804();
      }
      this.hook807();
synchronized (this) {
        boolean openCursors=checkCursorsForClose();
        this.hook799(numReadLocks,numWriteLocks,openCursors);
        if (openCursors) {
          throw new DatabaseException("Transaction " + id + " detected open cursors while aborting");
        }
        if (handleToHandleLockMap != null) {
          Iterator handleIter=handleToHandleLockMap.keySet().iterator();
          while (handleIter.hasNext()) {
            Database handle=(Database)handleIter.next();
            DbInternal.dbInvalidate(handle);
          }
        }
        return abortLsn;
      }
    }
  finally {
      close(false);
    }
  }
  /** 
 * Rollback the changes to this txn's write locked nodes.
 */
  private void undo() throws DatabaseException {
    Long nodeId=null;
    long undoLsn=lastLoggedLsn;
    LogManager logManager=envImpl.getLogManager();
    try {
      Set alreadyUndone=new HashSet();
      TreeLocation location=new TreeLocation();
      while (undoLsn != DbLsn.NULL_LSN) {
        LNLogEntry undoEntry=(LNLogEntry)logManager.getLogEntry(undoLsn);
        LN undoLN=undoEntry.getLN();
        nodeId=new Long(undoLN.getNodeId());
        if (!alreadyUndone.contains(nodeId)) {
          alreadyUndone.add(nodeId);
          DatabaseId dbId=undoEntry.getDbId();
          DatabaseImpl db=(DatabaseImpl)undoDatabases.get(dbId);
          undoLN.postFetchInit(db,undoLsn);
          long abortLsn=undoEntry.getAbortLsn();
          boolean abortKnownDeleted=undoEntry.getAbortKnownDeleted();
          this.hook802(undoLsn,location,undoEntry,undoLN,db,abortLsn,abortKnownDeleted);
          if (!undoLN.isDeleted()) {
            logManager.countObsoleteNode(undoLsn,null);
          }
        }
        undoLsn=undoEntry.getUserTxn().getLastLsn();
      }
    }
 catch (    RuntimeException e) {
      throw new DatabaseException("Txn undo for node=" + nodeId + " LSN="+ DbLsn.getNoFormatString(undoLsn),e);
    }
catch (    DatabaseException e) {
      this.hook801(nodeId,undoLsn,e);
      throw e;
    }
  }
  private int clearWriteLocks() throws DatabaseException {
    int numWriteLocks=writeInfo.size();
    Iterator iter=writeInfo.values().iterator();
    while (iter.hasNext()) {
      WriteLockInfo info=(WriteLockInfo)iter.next();
      lockManager.release(info.lock,this);
    }
    writeInfo=null;
    return numWriteLocks;
  }
  private int clearReadLocks() throws DatabaseException {
    int numReadLocks=0;
    if (readLocks != null) {
      numReadLocks=readLocks.size();
      Iterator iter=readLocks.iterator();
      while (iter.hasNext()) {
        Lock rLock=(Lock)iter.next();
        lockManager.release(rLock,this);
      }
      readLocks=null;
    }
    return numReadLocks;
  }
  /** 
 * Called by the recovery manager when logging a transaction aware object.
 * This method is synchronized by the caller, by being called within the
 * log latch. Record the last LSN for this transaction, to create the
 * transaction chain, and also record the LSN in the write info for abort
 * logic.
 */
  public void addLogInfo(  long lastLsn) throws DatabaseException {
    lastLoggedLsn=lastLsn;
synchronized (this) {
      if (firstLoggedLsn == DbLsn.NULL_LSN) {
        firstLoggedLsn=lastLsn;
      }
    }
  }
  /** 
 * @return first logged LSN, to aid recovery rollback.
 */
  long getFirstActiveLsn() throws DatabaseException {
synchronized (this) {
      return firstLoggedLsn;
    }
  }
  /** 
 * Add lock to the appropriate queue.
 */
  void addLock(  Long nodeId,  Lock lock,  LockType type,  LockGrantType grantStatus) throws DatabaseException {
    new Txn_addLock(this,nodeId,lock,type,grantStatus).execute();
  }
  private void addReadLock(  Lock lock){
    int delta=0;
    if (readLocks == null) {
      readLocks=new HashSet();
      delta=this.hook811(delta);
    }
    readLocks.add(lock);
    this.hook810(delta);
  }
  /** 
 * Remove the lock from the set owned by this transaction. If specified to
 * LockManager.release, the lock manager will call this when its releasing
 * a lock. Usually done because the transaction doesn't need to really keep
 * the lock, i.e for a deleted record.
 */
  void removeLock(  long nodeId,  Lock lock) throws DatabaseException {
synchronized (this) {
      if ((readLocks != null) && readLocks.remove(lock)) {
        this.hook812();
      }
 else       if ((writeInfo != null) && (writeInfo.remove(new Long(nodeId)) != null)) {
        this.hook813();
      }
    }
  }
  /** 
 * A lock is being demoted. Move it from the write collection into the read
 * collection.
 */
  void moveWriteToReadLock(  long nodeId,  Lock lock){
    boolean found=false;
synchronized (this) {
      if ((writeInfo != null) && (writeInfo.remove(new Long(nodeId)) != null)) {
        found=true;
        this.hook814();
      }
      assert found : "Couldn't find lock for Node " + nodeId + " in writeInfo Map.";
      addReadLock(lock);
    }
  }
  /** 
 * @return true if this transaction created this node. We know that this
 * is true if the node is write locked and has a null abort LSN.
 */
  public boolean createdNode(  long nodeId) throws DatabaseException {
    boolean created=false;
synchronized (this) {
      if (writeInfo != null) {
        WriteLockInfo info=(WriteLockInfo)writeInfo.get(new Long(nodeId));
        if (info != null) {
          created=info.createdThisTxn;
        }
      }
    }
    return created;
  }
  /** 
 * @return the abortLsn for this node.
 */
  public long getAbortLsn(  long nodeId) throws DatabaseException {
    WriteLockInfo info=null;
synchronized (this) {
      if (writeInfo != null) {
        info=(WriteLockInfo)writeInfo.get(new Long(nodeId));
      }
    }
    if (info == null) {
      return DbLsn.NULL_LSN;
    }
 else {
      return info.abortLsn;
    }
  }
  /** 
 * @return the WriteLockInfo for this node.
 */
  public WriteLockInfo getWriteLockInfo(  long nodeId) throws DatabaseException {
    WriteLockInfo info=WriteLockInfo.basicWriteLockInfo;
synchronized (this) {
      if (writeInfo != null) {
        info=(WriteLockInfo)writeInfo.get(new Long(nodeId));
      }
    }
    return info;
  }
  /** 
 * Is always transactional.
 */
  public boolean isTransactional(){
    return true;
  }
  /** 
 * Is serializable isolation if so configured.
 */
  public boolean isSerializableIsolation(){
    return serializableIsolation;
  }
  /** 
 * Is read-committed isolation if so configured.
 */
  public boolean isReadCommittedIsolation(){
    return readCommittedIsolation;
  }
  /** 
 * This is a transactional locker.
 */
  public Txn getTxnLocker(){
    return this;
  }
  /** 
 * Returns 'this', since this locker holds no non-transactional locks.
 */
  public Locker newNonTxnLocker() throws DatabaseException {
    return this;
  }
  /** 
 * This locker holds no non-transactional locks.
 */
  public void releaseNonTxnLocks() throws DatabaseException {
  }
  /** 
 * Created transactions do nothing at the end of the operation.
 */
  public void operationEnd() throws DatabaseException {
  }
  /** 
 * Created transactions do nothing at the end of the operation.
 */
  public void operationEnd(  boolean operationOK) throws DatabaseException {
  }
  /** 
 * Created transactions don't transfer locks until commit.
 */
  public void setHandleLockOwner(  boolean ignore,  Database dbHandle,  boolean dbIsClosing) throws DatabaseException {
    if (dbIsClosing) {
      Long handleLockId=(Long)handleToHandleLockMap.get(dbHandle);
      if (handleLockId != null) {
        Set dbHandleSet=(Set)handleLockToHandleMap.get(handleLockId);
        boolean removed=dbHandleSet.remove(dbHandle);
        assert removed : "Can't find " + dbHandle + " from dbHandleSet";
        if (dbHandleSet.size() == 0) {
          Object foo=handleLockToHandleMap.remove(handleLockId);
          assert (foo != null) : "Can't find " + handleLockId + " from handleLockIdtoHandleMap.";
        }
      }
      unregisterHandle(dbHandle);
    }
 else {
      if (dbHandle != null) {
        DbInternal.dbSetHandleLocker(dbHandle,this);
      }
    }
  }
  /** 
 * Cursors operating under this transaction are added to the collection.
 */
  public void registerCursor(  CursorImpl cursor) throws DatabaseException {
synchronized (this) {
      cursor.setLockerNext(cursorSet);
      if (cursorSet != null) {
        cursorSet.setLockerPrev(cursor);
      }
      cursorSet=cursor;
    }
  }
  /** 
 * Remove a cursor from the collection.
 */
  public void unRegisterCursor(  CursorImpl cursor) throws DatabaseException {
synchronized (this) {
      CursorImpl prev=cursor.getLockerPrev();
      CursorImpl next=cursor.getLockerNext();
      if (prev == null) {
        cursorSet=next;
      }
 else {
        prev.setLockerNext(next);
      }
      if (next != null) {
        next.setLockerPrev(prev);
      }
      cursor.setLockerPrev(null);
      cursor.setLockerNext(null);
    }
  }
  /** 
 * @return true if this txn is willing to give up the handle lock to
 * another txn before this txn ends.
 */
  public boolean isHandleLockTransferrable(){
    return false;
  }
  /** 
 * Check if all cursors associated with the txn are closed. If not, those
 * open cursors will be forcibly closed.
 * @return true if open cursors exist
 */
  private boolean checkCursorsForClose() throws DatabaseException {
    CursorImpl c=cursorSet;
    while (c != null) {
      if (!c.isClosed()) {
        return true;
      }
      c=c.getLockerNext();
    }
    return false;
  }
  /** 
 * Set the state of a transaction to ONLY_ABORTABLE.
 */
  public void setOnlyAbortable(){
    txnState&=~STATE_BITS;
    txnState|=ONLY_ABORTABLE;
  }
  /** 
 * Get the state of a transaction's ONLY_ABORTABLE.
 */
  public boolean getOnlyAbortable(){
    return (txnState & ONLY_ABORTABLE) != 0;
  }
  /** 
 * Throw an exception if the transaction is not open.
 * If calledByAbort is true, it means we're being called
 * from abort().
 * Caller must invoke with "this" synchronized.
 */
  protected void checkState(  boolean calledByAbort) throws DatabaseException {
    boolean ok=false;
    boolean onlyAbortable=false;
    byte state=(byte)(txnState & STATE_BITS);
    ok=(state == USABLE);
    onlyAbortable=(state == ONLY_ABORTABLE);
    if (!calledByAbort && onlyAbortable) {
      throw new DatabaseException("Transaction " + id + " must be aborted.");
    }
    if (ok || (calledByAbort && onlyAbortable)) {
      return;
    }
    throw new DatabaseException("Transaction " + id + " has been closed.");
  }
  /** 
 */
  private void close(  boolean isCommit) throws DatabaseException {
synchronized (this) {
      txnState&=~STATE_BITS;
      txnState|=CLOSED;
    }
    envImpl.getTxnManager().unRegisterTxn(this,isCommit);
  }
  /** 
 * @see LogWritable#getLogSize
 */
  public int getLogSize(){
    return LogUtils.LONG_BYTES + LogUtils.LONG_BYTES;
  }
  /** 
 * @see LogWritable#writeToLog
 */
  public void writeToLog(  ByteBuffer logBuffer){
    LogUtils.writeLong(logBuffer,id);
    LogUtils.writeLong(logBuffer,lastLoggedLsn);
  }
  /** 
 * @see LogReadable#readFromLogIt's ok for FindBugs to whine about id not being synchronized.
 */
  public void readFromLog(  ByteBuffer logBuffer,  byte entryTypeVersion){
    id=LogUtils.readLong(logBuffer);
    lastLoggedLsn=LogUtils.readLong(logBuffer);
  }
  /** 
 * @see LogReadable#dumpLog
 */
  public void dumpLog(  StringBuffer sb,  boolean verbose){
    sb.append("<txn id=\"");
    sb.append(super.toString());
    sb.append("\">");
    sb.append(DbLsn.toString(lastLoggedLsn));
    sb.append("</txn>");
  }
  /** 
 * @see LogReadable#getTransactionId
 */
  public long getTransactionId(){
    return getId();
  }
  /** 
 * @see LogReadable#logEntryIsTransactional
 */
  public boolean logEntryIsTransactional(){
    return true;
  }
  /** 
 * Transfer a single handle lock to the set of corresponding handles at
 * commit time.
 */
  private void transferHandleLockToHandleSet(  Long handleLockId,  Set dbHandleSet) throws DatabaseException {
    int numHandles=dbHandleSet.size();
    Database[] dbHandles=new Database[numHandles];
    dbHandles=(Database[])dbHandleSet.toArray(dbHandles);
    Locker[] destTxns=new Locker[numHandles];
    for (int i=0; i < numHandles; i++) {
      destTxns[i]=new BasicLocker(envImpl);
    }
    long nodeId=handleLockId.longValue();
    lockManager.transferMultiple(nodeId,this,destTxns);
    for (int i=0; i < numHandles; i++) {
      destTxns[i].addToHandleMaps(handleLockId,dbHandles[i]);
      DbInternal.dbSetHandleLocker(dbHandles[i],destTxns[i]);
    }
  }
  /** 
 * Send trace messages to the java.util.logger. Don't rely on the logger
 * alone to conditionalize whether we send this message, we don't even want
 * to construct the message if the level is not enabled.  The string
 * construction can be numerous enough to show up on a performance profile.
 */
  private void traceCommit(  int numWriteLocks,  int numReadLocks){
    new Txn_traceCommit(this,numWriteLocks,numReadLocks).execute();
  }
  int getInMemorySize(){
    return inMemorySize;
  }
  /** 
 * Store information about a DatabaseImpl that will have to be
 * purged at transaction commit or abort. This handles cleanup after
 * operations like Environment.truncateDatabase, 
 * Environment.removeDatabase. Cleanup like this is done outside the
 * usual transaction commit or node undo processing, because
 * the mapping tree is always AutoTxn'ed to avoid deadlock and is 
 * essentially  non-transactional
 */
private static class DatabaseCleanupInfo {
    DatabaseImpl dbImpl;
    boolean deleteAtCommit;
    DatabaseCleanupInfo(    DatabaseImpl dbImpl,    boolean deleteAtCommit){
      this.dbImpl=dbImpl;
      this.deleteAtCommit=deleteAtCommit;
    }
  }
@MethodObject static class Txn_addLock {
    Txn_addLock(    Txn _this,    Long nodeId,    Lock lock,    LockType type,    LockGrantType grantStatus){
      this._this=_this;
      this.nodeId=nodeId;
      this.lock=lock;
      this.type=type;
      this.grantStatus=grantStatus;
    }
    void execute() throws DatabaseException {
synchronized (_this) {
        this.hook815();
        if (type.isWriteLock()) {
          if (_this.writeInfo == null) {
            _this.writeInfo=new HashMap();
            _this.undoDatabases=new HashMap();
            this.hook818();
          }
          _this.writeInfo.put(nodeId,new WriteLockInfo(lock));
          this.hook817();
          if ((grantStatus == LockGrantType.PROMOTION) || (grantStatus == LockGrantType.WAIT_PROMOTION)) {
            _this.readLocks.remove(lock);
            this.hook819();
          }
          this.hook816();
        }
 else {
          _this.addReadLock(lock);
        }
      }
    }
    protected Txn _this;
    protected Long nodeId;
    protected Lock lock;
    protected LockType type;
    protected LockGrantType grantStatus;
    protected int delta;
    protected void hook815() throws DatabaseException {
    }
    protected void hook816() throws DatabaseException {
    }
    protected void hook817() throws DatabaseException {
    }
    protected void hook818() throws DatabaseException {
    }
    protected void hook819() throws DatabaseException {
    }
  }
@MethodObject static class Txn_traceCommit {
    Txn_traceCommit(    Txn _this,    int numWriteLocks,    int numReadLocks){
      this._this=_this;
      this.numWriteLocks=numWriteLocks;
      this.numReadLocks=numReadLocks;
    }
    void execute(){
    }
    protected Txn _this;
    protected int numWriteLocks;
    protected int numReadLocks;
    protected Logger logger;
    protected StringBuffer sb;
  }
  protected void hook799(  int numReadLocks,  int numWriteLocks,  boolean openCursors) throws DatabaseException {
  }
  protected void hook800(  Throwable t) throws DatabaseException, Throwable {
  }
  protected void hook801(  Long nodeId,  long undoLsn,  DatabaseException e) throws DatabaseException {
  }
  protected void hook802(  long undoLsn,  TreeLocation location,  LNLogEntry undoEntry,  LN undoLN,  DatabaseImpl db,  long abortLsn,  boolean abortKnownDeleted) throws DatabaseException, RuntimeException {
    RecoveryManager.undo(Level.FINER,db,location,undoLN,undoEntry.getKey(),undoEntry.getDupKey(),undoLsn,abortLsn,abortKnownDeleted,null,false);
  }
  protected void hook803() throws DatabaseException, RunRecoveryException, Throwable {
  }
  protected void hook804() throws DatabaseException {
  }
  protected void hook805() throws DatabaseException, RunRecoveryException, Throwable {
  }
  protected void hook806() throws DatabaseException, RunRecoveryException, Throwable {
  }
  protected void hook807() throws DatabaseException {
  }
  protected void hook808() throws DatabaseException {
  }
  protected void hook809() throws DatabaseException {
  }
  protected void hook810(  int delta){
  }
  protected int hook811(  int delta){
    return delta;
  }
  protected void hook812() throws DatabaseException {
  }
  protected void hook813() throws DatabaseException {
  }
  protected void hook814(){
  }
}
\00base/com/sleepycat/je/utilint/DaemonThread.java:package com.sleepycat.je.utilint;
import java.util.Collection;
import java.util.HashSet;
import java.util.Set;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DeadlockException;
import com.sleepycat.je.dbi.EnvironmentImpl;
import de.ovgu.cide.jakutil.*;
/** 
 * A daemon thread.
 */
public abstract class DaemonThread implements DaemonRunner, Runnable {
  private static final int JOIN_MILLIS=10;
  private long waitTime;
  private Object synchronizer=new Object();
  private Thread thread;
  private EnvironmentImpl env;
  protected String name;
  protected Set workQueue;
  protected int nWakeupRequests;
  private volatile boolean shutdownRequest=false;
  private volatile boolean paused=false;
  private boolean running=false;
  protected DaemonThread(){
  }
  public DaemonThread(  long waitTime,  String name,  EnvironmentImpl env){
    init(waitTime,name,env);
  }
  protected void init(  long waitTime,  String name,  EnvironmentImpl env){
    this.waitTime=waitTime;
    this.name=name;
    this.env=env;
    workQueue=new HashSet();
    this.hook856(name,env);
  }
  /** 
 * For testing.
 */
  public Thread getThread(){
    return thread;
  }
  /** 
 * If run is true, starts the thread if not started or unpauses it if
 * already started; if run is false, pauses the thread if started or does
 * nothing if not started.
 */
  public void runOrPause(  boolean run){
    if (run) {
      paused=false;
      if (thread != null) {
        wakeup();
      }
 else {
        thread=new Thread(this,name);
        thread.setDaemon(true);
        thread.start();
      }
    }
 else {
      paused=true;
    }
  }
  public void requestShutdown(){
    shutdownRequest=true;
  }
  /** 
 * Requests shutdown and calls join() to wait for the thread to stop.
 */
  public void shutdown(){
    if (thread != null) {
      shutdownRequest=true;
      while (thread.isAlive()) {
synchronized (synchronizer) {
          synchronizer.notifyAll();
        }
        try {
          thread.join(JOIN_MILLIS);
        }
 catch (        InterruptedException e) {
        }
      }
      thread=null;
    }
  }
  public String toString(){
    StringBuffer sb=new StringBuffer();
    sb.append("<DaemonThread name=\"").append(name).append("\"/>");
    return sb.toString();
  }
  public void addToQueue(  Object o) throws DatabaseException {
    workQueue.add(o);
    wakeup();
  }
  public int getQueueSize() throws DatabaseException {
    int count=workQueue.size();
    return count;
  }
  public void addToQueueAlreadyLatched(  Collection c) throws DatabaseException {
    workQueue.addAll(c);
  }
  public void wakeup(){
    if (!paused) {
synchronized (synchronizer) {
        synchronizer.notifyAll();
      }
    }
  }
  public void run(){
    while (true) {
      if (shutdownRequest) {
        break;
      }
      try {
        this.hook858();
        boolean nothingToDo=workQueue.size() == 0;
        this.hook857();
        if (nothingToDo) {
synchronized (synchronizer) {
            if (waitTime == 0) {
              synchronizer.wait();
            }
 else {
              synchronizer.wait(waitTime);
            }
          }
        }
        if (shutdownRequest) {
          break;
        }
        if (paused) {
synchronized (synchronizer) {
            synchronizer.wait();
          }
          continue;
        }
        int numTries=0;
        int maxRetries=nDeadlockRetries();
        do {
          try {
            nWakeupRequests++;
            running=true;
            onWakeup();
            break;
          }
 catch (          DeadlockException e) {
          }
 finally {
            running=false;
          }
          numTries++;
          if (shutdownRequest) {
            break;
          }
        }
 while (numTries <= maxRetries);
        if (shutdownRequest) {
          break;
        }
      }
 catch (      InterruptedException IE) {
        System.err.println("Shutting down " + this + " due to exception: "+ IE);
        shutdownRequest=true;
      }
catch (      Exception E) {
        System.err.println(this + " caught exception: " + E);
        E.printStackTrace(System.err);
        if (env.mayNotWrite()) {
          System.err.println("Exiting");
          shutdownRequest=true;
        }
 else {
          System.err.println("Continuing");
        }
      }
    }
  }
  /** 
 * Returns the number of retries to perform when Deadlock Exceptions occur.
 */
  protected int nDeadlockRetries() throws DatabaseException {
    return 0;
  }
  /** 
 * onWakeup is synchronized to ensure that multiple invocations of the
 * DaemonThread aren't made. isRunnable must be called from within onWakeup
 * to avoid having the following sequence: Thread A: isRunnable() => true,
 * Thread B: isRunnable() => true, Thread A: onWakeup() starts Thread B:
 * waits for monitor on thread to call onWakeup() Thread A: onWakeup()
 * completes rendering isRunnable() predicate false Thread B: onWakeup()
 * starts, but isRunnable predicate is now false
 */
  abstract protected void onWakeup() throws DatabaseException ;
  /** 
 * Returns whether shutdown has been requested. This method should be used
 * to to terminate daemon loops.
 */
  protected boolean isShutdownRequested(){
    return shutdownRequest;
  }
  /** 
 * Returns whether the onWakeup method is currently executing. This is only
 * an approximation and is used to avoid unnecessary wakeups.
 */
  public boolean isRunning(){
    return running;
  }
  /** 
 * For unit testing.
 */
  public int getNWakeupRequests(){
    return nWakeupRequests;
  }
  protected void hook856(  String name,  EnvironmentImpl env){
  }
  protected void hook857() throws InterruptedException, Exception {
  }
  protected void hook858() throws InterruptedException, Exception {
  }
}
\00base/com/sleepycat/je/log/LogBufferPool.java:package com.sleepycat.je.log;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Iterator;
import java.util.LinkedList;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.EnvironmentImpl;
import de.ovgu.cide.jakutil.*;
/** 
 * LogBufferPool keeps a set of log buffers.
 */
class LogBufferPool {
  private static final String DEBUG_NAME=LogBufferPool.class.getName();
  private EnvironmentImpl envImpl=null;
  private int logBufferSize;
  private LinkedList bufferPool;
  private LogBuffer currentWriteBuffer;
  private FileManager fileManager;
  private boolean runInMemory;
  LogBufferPool(  FileManager fileManager,  EnvironmentImpl envImpl) throws DatabaseException {
    this.fileManager=fileManager;
    this.envImpl=envImpl;
    this.hook485(envImpl);
    DbConfigManager configManager=envImpl.getConfigManager();
    runInMemory=configManager.getBoolean(EnvironmentParams.LOG_MEMORY_ONLY);
    reset(configManager);
    currentWriteBuffer=(LogBuffer)bufferPool.getFirst();
  }
  /** 
 * Initialize the pool at construction time and when the cache is resized.
 * This method is called after the memory budget has been calculated.
 */
  void reset(  DbConfigManager configManager) throws DatabaseException {
    if (runInMemory && bufferPool != null) {
      return;
    }
    int numBuffers=configManager.getInt(EnvironmentParams.NUM_LOG_BUFFERS);
    long logBufferBudget=envImpl.getMemoryBudget().getLogBufferBudget();
    int newBufferSize=(int)logBufferBudget / numBuffers;
    LinkedList newPool=new LinkedList();
    if (runInMemory) {
      numBuffers=1;
    }
    for (int i=0; i < numBuffers; i++) {
      newPool.add(new LogBuffer(newBufferSize,envImpl));
    }
    this.hook486();
    bufferPool=newPool;
    logBufferSize=newBufferSize;
  }
  /** 
 * Get a log buffer for writing sizeNeeded bytes. If currentWriteBuffer is
 * too small or too full, flush currentWriteBuffer and get a new one.
 * Called within the log write latch.
 * @return a buffer that can hold sizeNeeded bytes.
 */
  LogBuffer getWriteBuffer(  int sizeNeeded,  boolean flippedFile) throws IOException, DatabaseException {
    if ((!currentWriteBuffer.hasRoom(sizeNeeded)) || flippedFile) {
      writeBufferToFile(sizeNeeded);
    }
    if (flippedFile) {
      if (!runInMemory) {
        fileManager.syncLogEndAndFinishFile();
      }
    }
    return currentWriteBuffer;
  }
  /** 
 * Write the contents of the currentWriteBuffer to disk.  Leave this buffer
 * in memory to be available to would be readers.  Set up a new
 * currentWriteBuffer. Assumes the log write latch is held.
 * @param sizeNeeded is the size of the next object we need to write to
 * the log. May be 0 if this is called on behalf of LogManager.flush().
 */
  void writeBufferToFile(  int sizeNeeded) throws IOException, DatabaseException {
    int bufferSize=((logBufferSize > sizeNeeded) ? logBufferSize : sizeNeeded);
    this.hook488();
    LogBuffer latchedBuffer=currentWriteBuffer;
    this.hook487(bufferSize,latchedBuffer);
  }
  /** 
 * A loggable object has been freshly marshalled into the write log buffer.
 * 1. Update buffer so it knows what LSNs it contains.
 * 2. If this object requires a flush, write this buffer out to the 
 * backing file.
 * Assumes log write latch is held.
 */
  void writeCompleted(  long lsn,  boolean flushRequired) throws DatabaseException, IOException {
    currentWriteBuffer.registerLsn(lsn);
    if (flushRequired) {
      writeBufferToFile(0);
    }
  }
  /** 
 * Find a buffer that holds this LSN.
 * @return the buffer that contains this LSN, latched and ready to
 * read, or return null.
 */
  LogBuffer getReadBuffer(  long lsn) throws DatabaseException {
    LogBuffer foundBuffer=null;
    foundBuffer=this.hook489(lsn,foundBuffer);
    if (foundBuffer == null) {
      return null;
    }
 else {
      return foundBuffer;
    }
  }
  protected void hook485(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected void hook486() throws DatabaseException {
  }
  protected void hook487(  int bufferSize,  LogBuffer latchedBuffer) throws IOException, DatabaseException {
    ByteBuffer currentByteBuffer=currentWriteBuffer.getDataBuffer();
    int savePosition=currentByteBuffer.position();
    int saveLimit=currentByteBuffer.limit();
    currentByteBuffer.flip();
    if (runInMemory) {
      this.hook492(latchedBuffer);
      latchedBuffer=null;
      this.hook491();
      currentWriteBuffer=new LogBuffer(bufferSize,envImpl);
      bufferPool.add(currentWriteBuffer);
      this.hook490();
    }
 else {
      try {
        fileManager.writeLogBuffer(currentWriteBuffer);
        currentWriteBuffer.getDataBuffer().rewind();
        this.hook494(latchedBuffer);
        latchedBuffer=null;
        LogBuffer nextToUse=null;
        this.hook493(nextToUse);
      }
 catch (      DatabaseException DE) {
        currentByteBuffer.position(savePosition);
        currentByteBuffer.limit(saveLimit);
        throw DE;
      }
    }
  }
  protected void hook488() throws IOException, DatabaseException {
  }
  protected LogBuffer hook489(  long lsn,  LogBuffer foundBuffer) throws DatabaseException {
    Iterator iter=bufferPool.iterator();
    while (iter.hasNext()) {
      LogBuffer l=(LogBuffer)iter.next();
      if (l.containsLsn(lsn)) {
        foundBuffer=l;
        break;
      }
    }
    if (foundBuffer == null && currentWriteBuffer.containsLsn(lsn)) {
      foundBuffer=currentWriteBuffer;
    }
    if (foundBuffer == null) {
      this.hook496();
    }
    return foundBuffer;
  }
  protected void hook490() throws IOException, DatabaseException {
  }
  protected void hook491() throws IOException, DatabaseException {
  }
  protected void hook492(  LogBuffer latchedBuffer) throws IOException, DatabaseException {
  }
  protected void hook493(  LogBuffer nextToUse) throws IOException, DatabaseException {
    this.hook495();
    Iterator iter=bufferPool.iterator();
    nextToUse=(LogBuffer)iter.next();
    boolean done=bufferPool.remove(nextToUse);
    assert done;
    nextToUse.reinit();
    bufferPool.add(nextToUse);
    currentWriteBuffer=nextToUse;
  }
  protected void hook494(  LogBuffer latchedBuffer) throws IOException, DatabaseException {
  }
  protected void hook495() throws IOException, DatabaseException {
  }
  protected void hook496() throws DatabaseException {
  }
}
\00base/com/sleepycat/je/log/FileReader.java:package com.sleepycat.je.log;
import java.io.IOException;
import java.nio.Buffer;
import java.nio.ByteBuffer;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * A FileReader is an abstract class that traverses the log files, reading in
 * chunks of the file at a time. Concrete subclasses perform a particular action
 * to each entry.
 */
public abstract class FileReader {
  protected EnvironmentImpl env;
  protected FileManager fileManager;
  private ByteBuffer readBuffer;
  private ByteBuffer saveBuffer;
  private int maxReadBufferSize;
  private boolean singleFile;
  protected boolean eof;
  private boolean forward;
  protected long readBufferFileNum;
  protected long readBufferFileStart;
  protected long readBufferFileEnd;
  private int nRead;
  private long nRepeatIteratorReads;
  protected byte currentEntryTypeNum;
  protected byte currentEntryTypeVersion;
  protected long currentEntryPrevOffset;
  protected int currentEntrySize;
  protected long currentEntryChecksum;
  protected long currentEntryOffset;
  protected long nextEntryOffset;
  protected long startLsn;
  private long finishLsn;
  protected boolean anticipateChecksumErrors;
  /** 
 * A FileReader just needs to know what size chunks to read in.
 * @param endOfFileLsnindicates the end of the log file
 */
  public FileReader(  EnvironmentImpl env,  int readBufferSize,  boolean forward,  long startLsn,  Long singleFileNumber,  long endOfFileLsn,  long finishLsn) throws IOException, DatabaseException {
    this.env=env;
    this.fileManager=env.getFileManager();
    this.hook473(env);
    this.singleFile=(singleFileNumber != null);
    this.forward=forward;
    readBuffer=ByteBuffer.allocate(readBufferSize);
    threadSafeBufferFlip(readBuffer);
    saveBuffer=ByteBuffer.allocate(readBufferSize);
    DbConfigManager configManager=env.getConfigManager();
    maxReadBufferSize=configManager.getInt(EnvironmentParams.LOG_ITERATOR_MAX_SIZE);
    this.startLsn=startLsn;
    this.finishLsn=finishLsn;
    initStartingPosition(endOfFileLsn,singleFileNumber);
    nRead=0;
    this.hook472();
    anticipateChecksumErrors=false;
  }
  /** 
 * Helper for determining the starting position and opening up a file at the
 * desired location.
 */
  protected void initStartingPosition(  long endOfFileLsn,  Long ignoreSingleFileNumber) throws IOException, DatabaseException {
    eof=false;
    if (forward) {
      if (startLsn != DbLsn.NULL_LSN) {
        readBufferFileNum=DbLsn.getFileNumber(startLsn);
        readBufferFileEnd=DbLsn.getFileOffset(startLsn);
      }
 else {
        Long firstNum=fileManager.getFirstFileNum();
        if (firstNum == null) {
          eof=true;
        }
 else {
          readBufferFileNum=firstNum.longValue();
          readBufferFileEnd=0;
        }
      }
      nextEntryOffset=readBufferFileEnd;
    }
 else {
      assert startLsn != DbLsn.NULL_LSN;
      readBufferFileNum=DbLsn.getFileNumber(endOfFileLsn);
      readBufferFileStart=DbLsn.getFileOffset(endOfFileLsn);
      readBufferFileEnd=readBufferFileStart;
      if (DbLsn.getFileNumber(startLsn) == DbLsn.getFileNumber(endOfFileLsn)) {
        currentEntryPrevOffset=DbLsn.getFileOffset(startLsn);
      }
 else {
        currentEntryPrevOffset=0;
      }
      currentEntryOffset=DbLsn.getFileOffset(endOfFileLsn);
    }
  }
  /** 
 * @return the number of entries processed by this reader.
 */
  public int getNumRead(){
    return nRead;
  }
  public long getNRepeatIteratorReads(){
    return nRepeatIteratorReads;
  }
  /** 
 * Get LSN of the last entry read.
 */
  public long getLastLsn(){
    return DbLsn.makeLsn(readBufferFileNum,currentEntryOffset);
  }
  /** 
 * readNextEntry scans the log files until either it's reached the end of
 * the log or has hit an invalid portion. It then returns false.
 * @return true if an element has been read
 */
  public boolean readNextEntry() throws DatabaseException, IOException {
    return new FileReader_readNextEntry(this).execute();
  }
  protected boolean resyncReader(  long nextGoodRecordPostCorruption,  boolean dumpCorruptedBounds) throws DatabaseException, IOException {
    return false;
  }
  /** 
 * Make sure that the start of the target log entry is in the header. This
 * is a no-op if we're reading forwards
 */
  private void getLogEntryInReadBuffer() throws IOException, DatabaseException, EOFException {
    if (!forward) {
      if ((currentEntryPrevOffset != 0) && (currentEntryPrevOffset >= readBufferFileStart)) {
        long nextLsn=DbLsn.makeLsn(readBufferFileNum,currentEntryPrevOffset);
        if (finishLsn != DbLsn.NULL_LSN) {
          if (DbLsn.compareTo(nextLsn,finishLsn) == -1) {
            throw new EOFException();
          }
        }
        threadSafeBufferPosition(readBuffer,(int)(currentEntryPrevOffset - readBufferFileStart));
      }
 else {
        if (currentEntryPrevOffset == 0) {
          currentEntryPrevOffset=fileManager.getFileHeaderPrevOffset(readBufferFileNum);
          Long prevFileNum=fileManager.getFollowingFileNum(readBufferFileNum,false);
          if (prevFileNum == null) {
            throw new EOFException();
          }
          if (readBufferFileNum - prevFileNum.longValue() != 1) {
            if (!resyncReader(DbLsn.makeLsn(prevFileNum.longValue(),DbLsn.MAX_FILE_OFFSET),false)) {
              throw new DatabaseException("Cannot read backward over cleaned file" + " from " + readBufferFileNum + " to "+ prevFileNum);
            }
          }
          readBufferFileNum=prevFileNum.longValue();
          readBufferFileStart=currentEntryPrevOffset;
        }
 else         if ((currentEntryOffset - currentEntryPrevOffset) > readBuffer.capacity()) {
          readBufferFileStart=currentEntryPrevOffset;
        }
 else {
          long newPosition=currentEntryOffset - readBuffer.capacity();
          readBufferFileStart=(newPosition < 0) ? 0 : newPosition;
        }
        long nextLsn=DbLsn.makeLsn(readBufferFileNum,currentEntryPrevOffset);
        if (finishLsn != DbLsn.NULL_LSN) {
          if (DbLsn.compareTo(nextLsn,finishLsn) == -1) {
            throw new EOFException();
          }
        }
        FileHandle fileHandle=fileManager.getFileHandle(readBufferFileNum);
        this.hook469(fileHandle);
        readBufferFileEnd=readBufferFileStart + threadSafeBufferPosition(readBuffer);
        threadSafeBufferFlip(readBuffer);
        threadSafeBufferPosition(readBuffer,(int)(currentEntryPrevOffset - readBufferFileStart));
      }
      currentEntryOffset=currentEntryPrevOffset;
    }
 else {
      if (finishLsn != DbLsn.NULL_LSN) {
        long nextLsn=DbLsn.makeLsn(readBufferFileNum,nextEntryOffset);
        if (DbLsn.compareTo(nextLsn,finishLsn) >= 0) {
          throw new EOFException();
        }
      }
    }
  }
  /** 
 * Read the log entry header, leaving the buffer mark at the beginning of
 * the checksummed header data.
 */
  private void readHeader(  ByteBuffer dataBuffer) throws DatabaseException {
    currentEntryChecksum=LogUtils.getUnsignedInt(dataBuffer);
    dataBuffer.mark();
    currentEntryTypeNum=dataBuffer.get();
    if (!LogEntryType.isValidType(currentEntryTypeNum))     throw new DbChecksumException((anticipateChecksumErrors ? null : env),"FileReader read invalid log entry type: " + currentEntryTypeNum);
    currentEntryTypeVersion=dataBuffer.get();
    currentEntryPrevOffset=LogUtils.getUnsignedInt(dataBuffer);
    currentEntrySize=LogUtils.readInt(dataBuffer);
  }
  /** 
 * Try to read a specified number of bytes.
 * @param amountToReadis the number of bytes we need
 * @param collectDatais true if we need to actually look at the data. If false, we
 * know we're skipping this entry, and all we need to do is to
 * count until we get to the right spot.
 * @return a byte buffer positioned at the head of the desired portion, or
 * null if we reached eof.
 */
  private ByteBuffer readData(  int amountToRead,  boolean collectData) throws IOException, DatabaseException, EOFException {
    int alreadyRead=0;
    ByteBuffer completeBuffer=null;
    saveBuffer.clear();
    while ((alreadyRead < amountToRead) && !eof) {
      int bytesNeeded=amountToRead - alreadyRead;
      if (readBuffer.hasRemaining()) {
        if (collectData) {
          if ((alreadyRead > 0) || (readBuffer.remaining() < bytesNeeded)) {
            copyToSaveBuffer(bytesNeeded);
            alreadyRead=threadSafeBufferPosition(saveBuffer);
            completeBuffer=saveBuffer;
          }
 else {
            completeBuffer=readBuffer;
            alreadyRead=amountToRead;
          }
        }
 else {
          int positionIncrement=(readBuffer.remaining() > bytesNeeded) ? bytesNeeded : readBuffer.remaining();
          alreadyRead+=positionIncrement;
          threadSafeBufferPosition(readBuffer,threadSafeBufferPosition(readBuffer) + positionIncrement);
          completeBuffer=readBuffer;
        }
      }
 else {
        fillReadBuffer(bytesNeeded);
      }
    }
    threadSafeBufferFlip(saveBuffer);
    return completeBuffer;
  }
  /** 
 * Change the read buffer size if we start hitting large log entries so we
 * don't get into an expensive cycle of multiple reads and piecing together
 * of log entries.
 */
  private void adjustReadBufferSize(  int amountToRead){
    int readBufferSize=readBuffer.capacity();
    if (amountToRead > readBufferSize) {
      if (readBufferSize < maxReadBufferSize) {
        if (amountToRead < maxReadBufferSize) {
          readBufferSize=amountToRead;
          int remainder=readBufferSize % 1024;
          readBufferSize+=1024 - remainder;
          readBufferSize=Math.min(readBufferSize,maxReadBufferSize);
        }
 else {
          readBufferSize=maxReadBufferSize;
        }
        readBuffer=ByteBuffer.allocate(readBufferSize);
      }
      if (amountToRead > readBuffer.capacity()) {
        nRepeatIteratorReads++;
      }
    }
  }
  /** 
 * Copy the required number of bytes into the save buffer.
 */
  private void copyToSaveBuffer(  int bytesNeeded){
    int bytesFromThisBuffer;
    if (bytesNeeded <= readBuffer.remaining()) {
      bytesFromThisBuffer=bytesNeeded;
    }
 else {
      bytesFromThisBuffer=readBuffer.remaining();
    }
    ByteBuffer temp;
    if (saveBuffer.capacity() - threadSafeBufferPosition(saveBuffer) < bytesFromThisBuffer) {
      temp=ByteBuffer.allocate(saveBuffer.capacity() + bytesFromThisBuffer);
      threadSafeBufferFlip(saveBuffer);
      temp.put(saveBuffer);
      saveBuffer=temp;
    }
    temp=readBuffer.slice();
    temp.limit(bytesFromThisBuffer);
    saveBuffer.put(temp);
    threadSafeBufferPosition(readBuffer,threadSafeBufferPosition(readBuffer) + bytesFromThisBuffer);
  }
  /** 
 * Fill up the read buffer with more data.
 */
  private void fillReadBuffer(  int bytesNeeded) throws DatabaseException, EOFException {
    FileHandle fileHandle=null;
    try {
      adjustReadBufferSize(bytesNeeded);
      fileHandle=fileManager.getFileHandle(readBufferFileNum);
      boolean fileOk=false;
      if (readBufferFileEnd < fileHandle.getFile().length()) {
        fileOk=true;
      }
 else {
        if (!singleFile) {
          Long nextFile=fileManager.getFollowingFileNum(readBufferFileNum,forward);
          if (nextFile != null) {
            readBufferFileNum=nextFile.longValue();
            this.hook470(fileHandle);
            fileHandle=fileManager.getFileHandle(readBufferFileNum);
            fileOk=true;
            readBufferFileEnd=0;
            nextEntryOffset=0;
          }
        }
      }
      if (fileOk) {
        readBuffer.clear();
        fileManager.readFromFile(fileHandle.getFile(),readBuffer,readBufferFileEnd);
        assert EnvironmentImpl.maybeForceYield();
        readBufferFileStart=readBufferFileEnd;
        readBufferFileEnd=readBufferFileStart + threadSafeBufferPosition(readBuffer);
        threadSafeBufferFlip(readBuffer);
      }
 else {
        throw new EOFException();
      }
    }
 catch (    IOException e) {
      e.printStackTrace();
      throw new DatabaseException("Problem in fillReadBuffer, readBufferFileNum = " + readBufferFileNum + ": "+ e.getMessage());
    }
 finally {
      this.hook471(fileHandle);
    }
  }
  /** 
 * @return true if this reader should process this entry, or just skip over
 * it.
 */
  protected boolean isTargetEntry(  byte logEntryTypeNumber,  byte logEntryTypeVersion) throws DatabaseException {
    return true;
  }
  /** 
 * Each file reader implements this method to process the entry data.
 * @param enteryBuffercontains the entry data and is positioned at the data
 * @return true if this entry should be returned
 */
  protected abstract boolean processEntry(  ByteBuffer entryBuffer) throws DatabaseException ;
private static class EOFException extends Exception {
  }
  /** 
 * Note that we catch Exception here because it is possible that another
 * thread is modifying the state of buffer simultaneously. Specifically,
 * this can happen if another thread is writing this log buffer out and it
 * does (e.g.) a flip operation on it. The actual mark/pos of the buffer may
 * be caught in an unpredictable state. We could add another latch to
 * protect this buffer, but that's heavier weight than we need. So the
 * easiest thing to do is to just retry the duplicate operation. See
 * [#9822].
 */
  private Buffer threadSafeBufferFlip(  ByteBuffer buffer){
    while (true) {
      try {
        return buffer.flip();
      }
 catch (      IllegalArgumentException IAE) {
        continue;
      }
    }
  }
  private int threadSafeBufferPosition(  ByteBuffer buffer){
    while (true) {
      try {
        return buffer.position();
      }
 catch (      IllegalArgumentException IAE) {
        continue;
      }
    }
  }
  private Buffer threadSafeBufferPosition(  ByteBuffer buffer,  int newPosition){
    while (true) {
      try {
        return buffer.position(newPosition);
      }
 catch (      IllegalArgumentException IAE) {
        continue;
      }
    }
  }
@MethodObject static class FileReader_readNextEntry {
    FileReader_readNextEntry(    FileReader _this){
      this._this=_this;
    }
    boolean execute() throws DatabaseException, IOException {
      foundEntry=false;
      try {
        while ((!_this.eof) && (!foundEntry)) {
          _this.getLogEntryInReadBuffer();
          dataBuffer=_this.readData(LogManager.HEADER_BYTES,true);
          _this.readHeader(dataBuffer);
          isTargetEntry=_this.isTargetEntry(_this.currentEntryTypeNum,_this.currentEntryTypeVersion);
          this.hook476();
          collectData=isTargetEntry;
          this.hook475();
          dataBuffer=_this.readData(_this.currentEntrySize,collectData);
          if (_this.forward) {
            _this.currentEntryOffset=_this.nextEntryOffset;
            _this.nextEntryOffset+=LogManager.HEADER_BYTES + _this.currentEntrySize;
          }
          this.hook474();
          if (isTargetEntry) {
            if (_this.processEntry(dataBuffer)) {
              foundEntry=true;
              _this.nRead++;
            }
          }
 else           if (collectData) {
            _this.threadSafeBufferPosition(dataBuffer,_this.threadSafeBufferPosition(dataBuffer) + _this.currentEntrySize);
          }
        }
      }
 catch (      EOFException e) {
        _this.eof=true;
      }
catch (      DatabaseException e) {
        this.hook468();
        throw e;
      }
      return foundEntry;
    }
    protected FileReader _this;
    protected boolean foundEntry;
    protected ByteBuffer dataBuffer;
    protected boolean isTargetEntry;
    protected boolean doValidate;
    protected boolean collectData;
    protected LogEntryType problemType;
    protected void hook468() throws DatabaseException, IOException {
    }
    protected void hook474() throws DatabaseException, IOException, EOFException {
    }
    protected void hook475() throws DatabaseException, IOException, EOFException {
    }
    protected void hook476() throws DatabaseException, IOException, EOFException {
    }
  }
  protected void hook469(  FileHandle fileHandle) throws IOException, DatabaseException, EOFException {
    readBuffer.clear();
    fileManager.readFromFile(fileHandle.getFile(),readBuffer,readBufferFileStart);
    assert EnvironmentImpl.maybeForceYield();
  }
  protected void hook470(  FileHandle fileHandle) throws DatabaseException, EOFException, IOException {
  }
  protected void hook471(  FileHandle fileHandle) throws DatabaseException, EOFException {
  }
  protected void hook472() throws IOException, DatabaseException {
  }
  protected void hook473(  EnvironmentImpl env) throws IOException, DatabaseException {
  }
}
\00base/com/sleepycat/je/log/SyncedLogManager.java:package com.sleepycat.je.log;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.List;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.cleaner.TrackedFileSummary;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.dbi.EnvironmentImpl;
import de.ovgu.cide.jakutil.*;
/** 
 * The SyncedLogManager uses the synchronized keyword to implement protected
 * regions.
 */
public class SyncedLogManager extends LogManager {
  /** 
 * There is a single log manager per database environment.
 */
  public SyncedLogManager(  EnvironmentImpl envImpl,  boolean readOnly) throws DatabaseException {
    super(envImpl,readOnly);
  }
  protected LogResult logItem(  LoggableObject item,  boolean isProvisional,  boolean flushRequired,  boolean forceNewLogFile,  long oldNodeLsn,  boolean marshallOutsideLatch,  ByteBuffer marshalledBuffer,  UtilizationTracker tracker) throws IOException, DatabaseException {
    try {
      this.hook511(item,isProvisional,flushRequired,forceNewLogFile,oldNodeLsn,marshallOutsideLatch,marshalledBuffer,tracker);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (LogResult)r.value;
    }
  }
  protected void flushInternal() throws LogException, DatabaseException {
    try {
      this.hook512();
    }
 catch (    IOException e) {
      throw new LogException(e.getMessage());
    }
  }
  /** 
 * @see LogManager#getUnflushableTrackedSummary
 */
  public TrackedFileSummary getUnflushableTrackedSummary(  long file) throws DatabaseException {
    try {
      this.hook513(file);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (TrackedFileSummary)r.value;
    }
  }
  /** 
 * @see LogManager#countObsoleteLNs
 */
  public void countObsoleteNode(  long lsn,  LogEntryType type) throws DatabaseException {
    UtilizationTracker tracker=envImpl.getUtilizationTracker();
    this.hook514(lsn,type,tracker);
  }
  /** 
 * @see LogManager#countObsoleteNodes
 */
  public void countObsoleteNodes(  TrackedFileSummary[] summaries) throws DatabaseException {
    UtilizationTracker tracker=envImpl.getUtilizationTracker();
    this.hook515(summaries,tracker);
  }
  /** 
 * @see LogManager#countObsoleteINs
 */
  public void countObsoleteINs(  List lsnList) throws DatabaseException {
    countObsoleteINsInternal(lsnList);
  }
  protected void hook511(  LoggableObject item,  boolean isProvisional,  boolean flushRequired,  boolean forceNewLogFile,  long oldNodeLsn,  boolean marshallOutsideLatch,  ByteBuffer marshalledBuffer,  UtilizationTracker tracker) throws IOException, DatabaseException {
    throw new ReturnObject(logInternal(item,isProvisional,flushRequired,forceNewLogFile,oldNodeLsn,marshallOutsideLatch,marshalledBuffer,tracker));
  }
  protected void hook512() throws LogException, DatabaseException, IOException {
    logBufferPool.writeBufferToFile(0);
  }
  protected void hook513(  long file) throws DatabaseException {
    throw new ReturnObject(getUnflushableTrackedSummaryInternal(file));
  }
  protected void hook514(  long lsn,  LogEntryType type,  UtilizationTracker tracker) throws DatabaseException {
    countObsoleteNodeInternal(tracker,lsn,type);
  }
  protected void hook515(  TrackedFileSummary[] summaries,  UtilizationTracker tracker) throws DatabaseException {
    countObsoleteNodesInternal(tracker,summaries);
  }
}
\00base/com/sleepycat/je/log/FileManager.java:package com.sleepycat.je.log;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.ClosedChannelException;
import java.nio.channels.FileChannel;
import java.nio.channels.FileLock;
import java.nio.channels.OverlappingFileLockException;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Hashtable;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Map;
import java.util.Random;
import java.util.Set;
import java.util.zip.Checksum;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.log.entry.LogEntry;
import com.sleepycat.je.utilint.Adler32;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.HexFormatter;
import de.ovgu.cide.jakutil.*;
/** 
 * The FileManager presents the abstraction of one contiguous file. It doles out
 * LSNs.
 */
public class FileManager {
public static class FileMode {
    public static final FileMode READ_MODE=new FileMode("r");
    public static final FileMode READWRITE_MODE=new FileMode("rw");
    private String fileModeValue;
    private FileMode(    String fileModeValue){
      this.fileModeValue=fileModeValue;
    }
    public String getModeValue(){
      return fileModeValue;
    }
  }
  static boolean IO_EXCEPTION_TESTING=false;
  private static final String DEBUG_NAME=FileManager.class.getName();
  private static long writeCount=0;
  private static long stopOnWriteCount=Long.MAX_VALUE;
  public static final String JE_SUFFIX=".jdb";
  public static final String CIF_SUFFIX=".cif";
  public static final String DEL_SUFFIX=".del";
  public static final String BAD_SUFFIX=".bad";
  public static final String LOCK_SUFFIX=".lck";
  static final String[] DEL_SUFFIXES={DEL_SUFFIX};
  static final String[] JE_SUFFIXES={JE_SUFFIX};
  private static final String[] JE_AND_DEL_SUFFIXES={JE_SUFFIX,DEL_SUFFIX};
  private boolean syncAtFileEnd=true;
  private EnvironmentImpl envImpl;
  private long maxFileSize;
  private File dbEnvHome;
  private boolean includeDeletedFiles=false;
  private boolean readOnly;
  private long currentFileNum;
  private long nextAvailableLsn;
  private long lastUsedLsn;
  private long prevOffset;
  private boolean forceNewFile;
  private long savedCurrentFileNum;
  private long savedNextAvailableLsn;
  private long savedLastUsedLsn;
  private long savedPrevOffset;
  private boolean savedForceNewFile;
  private LogEndFileDescriptor endOfLog;
  private Map perFileLastUsedLsn;
  /** 
 * Set up the file cache and initialize the file manager to point to the
 * beginning of the log.
 * @param configManager
 * @param dbEnvHomeenvironment home directory
 */
  public FileManager(  EnvironmentImpl envImpl,  File dbEnvHome,  boolean readOnly) throws DatabaseException {
    this.envImpl=envImpl;
    this.dbEnvHome=dbEnvHome;
    this.readOnly=readOnly;
    DbConfigManager configManager=envImpl.getConfigManager();
    maxFileSize=configManager.getLong(EnvironmentParams.LOG_FILE_MAX);
    this.hook456(configManager);
    this.hook467(readOnly);
    this.hook457(configManager);
    this.hook449(envImpl);
    if (!dbEnvHome.exists()) {
      throw new LogException("Environment home " + dbEnvHome + " doesn't exist");
    }
    currentFileNum=0L;
    nextAvailableLsn=DbLsn.makeLsn(currentFileNum,firstLogEntryOffset());
    lastUsedLsn=DbLsn.NULL_LSN;
    perFileLastUsedLsn=new HashMap();
    prevOffset=0L;
    endOfLog=new LogEndFileDescriptor();
    forceNewFile=false;
    saveLastPosition();
    String stopOnWriteProp=System.getProperty("je.debug.stopOnWrite");
    if (stopOnWriteProp != null) {
      stopOnWriteCount=Long.parseLong(stopOnWriteProp);
    }
    this.hook452(envImpl);
  }
  /** 
 * Set the file manager's "end of log".
 * @param nextAvailableLsnLSN to be used for the next log entry
 * @param lastUsedLsnlast LSN to have a valid entry, may be null
 * @param prevOffsetvalue to use for the prevOffset of the next entry. If the
 * beginning of the file, this is 0.
 */
  public void setLastPosition(  long nextAvailableLsn,  long lastUsedLsn,  long prevOffset){
    this.lastUsedLsn=lastUsedLsn;
    perFileLastUsedLsn.put(new Long(DbLsn.getFileNumber(lastUsedLsn)),new Long(lastUsedLsn));
    this.nextAvailableLsn=nextAvailableLsn;
    currentFileNum=DbLsn.getFileNumber(this.nextAvailableLsn);
    this.prevOffset=prevOffset;
    saveLastPosition();
  }
  void saveLastPosition(){
    savedNextAvailableLsn=nextAvailableLsn;
    savedLastUsedLsn=lastUsedLsn;
    savedPrevOffset=prevOffset;
    savedForceNewFile=forceNewFile;
    savedCurrentFileNum=currentFileNum;
  }
  void restoreLastPosition(){
    nextAvailableLsn=savedNextAvailableLsn;
    lastUsedLsn=savedLastUsedLsn;
    prevOffset=savedPrevOffset;
    forceNewFile=savedForceNewFile;
    currentFileNum=savedCurrentFileNum;
  }
  /** 
 * May be used to disable sync at file end to speed unit tests. Must only be
 * used for unit testing, since log corruption may result.
 */
  public void setSyncAtFileEnd(  boolean sync){
    syncAtFileEnd=sync;
  }
  /** 
 * public for cleaner.
 * @return the number of the first file in this environment.
 */
  public Long getFirstFileNum(){
    return getFileNum(true);
  }
  public boolean getReadOnly(){
    return readOnly;
  }
  /** 
 * @return the number of the last file in this environment.
 */
  public Long getLastFileNum(){
    return getFileNum(false);
  }
  public long getCurrentFileNum(){
    return currentFileNum;
  }
  public void setIncludeDeletedFiles(  boolean includeDeletedFiles){
    this.includeDeletedFiles=includeDeletedFiles;
  }
  /** 
 * Get all JE file numbers.
 * @return an array of all JE file numbers.
 */
  public Long[] getAllFileNumbers(){
    String[] names=listFiles(JE_SUFFIXES);
    Long[] nums=new Long[names.length];
    for (int i=0; i < nums.length; i+=1) {
      nums[i]=getNumFromName(names[i]);
    }
    return nums;
  }
  /** 
 * Get the next file number before/after currentFileNum.
 * @param currentFileNumthe file we're at right now. Note that it may not exist, if
 * it's been cleaned and renamed.
 * @param forwardif true, we want the next larger file, if false we want the
 * previous file
 * @return null if there is no following file, or if filenum doesn't exist
 */
  public Long getFollowingFileNum(  long currentFileNum,  boolean forward){
    String[] names=listFiles(JE_SUFFIXES);
    String searchName=getFileName(currentFileNum,JE_SUFFIX);
    int foundIdx=Arrays.binarySearch(names,searchName);
    boolean foundTarget=false;
    if (foundIdx >= 0) {
      if (forward) {
        foundIdx++;
      }
 else {
        foundIdx--;
      }
    }
 else {
      foundIdx=Math.abs(foundIdx + 1);
      if (!forward) {
        foundIdx--;
      }
    }
    if (forward && (foundIdx < names.length)) {
      foundTarget=true;
    }
 else     if (!forward && (foundIdx > -1)) {
      foundTarget=true;
    }
    if (foundTarget) {
      return getNumFromName(names[foundIdx]);
    }
 else {
      return null;
    }
  }
  /** 
 * @return true if there are any files at all.
 */
  public boolean filesExist(){
    String[] names=listFiles(JE_SUFFIXES);
    return (names.length != 0);
  }
  /** 
 * Get the first or last file number in the set of je files.
 * @param firstif true, get the first file, else get the last file
 * @return the file number or null if no files exist
 */
  private Long getFileNum(  boolean first){
    String[] names=listFiles(JE_SUFFIXES);
    if (names.length == 0) {
      return null;
    }
 else {
      int index=0;
      if (!first) {
        index=names.length - 1;
      }
      return getNumFromName(names[index]);
    }
  }
  /** 
 * Get the file number from a file name.
 * @param thefile name
 * @return the file number
 */
  private Long getNumFromName(  String fileName){
    String fileNumber=fileName.substring(0,fileName.indexOf("."));
    return new Long(Long.parseLong(fileNumber,16));
  }
  /** 
 * Find je files. Return names sorted in ascending fashion.
 * @param suffixwhich type of file we're looking for
 * @return array of file names
 */
  String[] listFiles(  String[] suffixes){
    String[] fileNames=dbEnvHome.list(new JEFileFilter(suffixes));
    Arrays.sort(fileNames);
    return fileNames;
  }
  /** 
 * Find je files, flavor for unit test support.
 * @param suffixwhich type of file we're looking for
 * @return array of file names
 */
  public static String[] listFiles(  File envDirFile,  String[] suffixes){
    String[] fileNames=envDirFile.list(new JEFileFilter(suffixes));
    Arrays.sort(fileNames);
    return fileNames;
  }
  /** 
 * @return the full file name and path for the nth je file.
 */
  String[] getFullFileNames(  long fileNum){
    if (includeDeletedFiles) {
      int nSuffixes=JE_AND_DEL_SUFFIXES.length;
      String[] ret=new String[nSuffixes];
      for (int i=0; i < nSuffixes; i++) {
        ret[i]=getFullFileName(getFileName(fileNum,JE_AND_DEL_SUFFIXES[i]));
      }
      return ret;
    }
 else {
      return new String[]{getFullFileName(getFileName(fileNum,JE_SUFFIX))};
    }
  }
  /** 
 * @return the full file name and path for the given file number and suffix.
 */
  public String getFullFileName(  long fileNum,  String suffix){
    return getFullFileName(getFileName(fileNum,suffix));
  }
  /** 
 * @return the full file name and path for this file name.
 */
  private String getFullFileName(  String fileName){
    return dbEnvHome + File.separator + fileName;
  }
  /** 
 * @return the file name for the nth file.
 */
  public static String getFileName(  long fileNum,  String suffix){
    return (HexFormatter.formatLong(fileNum).substring(10) + suffix);
  }
  /** 
 * Rename this file to NNNNNNNN.suffix. If that file already exists, try
 * NNNNNNNN.suffix.1, etc. Used for deleting files or moving corrupt files
 * aside.
 * @param fileNumthe file we want to move
 * @param newSuffixthe new file suffix
 */
  public void renameFile(  long fileNum,  String newSuffix) throws DatabaseException, IOException {
    int repeatNum=0;
    boolean renamed=false;
    while (!renamed) {
      String generation="";
      if (repeatNum > 0) {
        generation="." + repeatNum;
      }
      String newName=getFullFileName(getFileName(fileNum,newSuffix) + generation);
      File targetFile=new File(newName);
      if (targetFile.exists()) {
        repeatNum++;
      }
 else {
        String oldFileName=getFullFileNames(fileNum)[0];
        this.hook458(fileNum);
        File oldFile=new File(oldFileName);
        if (oldFile.renameTo(targetFile)) {
          renamed=true;
        }
 else {
          throw new LogException("Couldn't rename " + oldFileName + " to "+ newName);
        }
      }
    }
  }
  /** 
 * Delete log file NNNNNNNN.
 * @param fileNumthe file we want to move
 */
  public void deleteFile(  long fileNum) throws DatabaseException, IOException {
    String fileName=getFullFileNames(fileNum)[0];
    this.hook459(fileNum);
    File file=new File(fileName);
    boolean done=file.delete();
    if (!done) {
      throw new LogException("Couldn't delete " + file);
    }
  }
  /** 
 * Return a read only file handle that corresponds the this file number.
 * Retrieve it from the cache or open it anew and validate the file header.
 * This method takes a latch on this file, so that the file descriptor will
 * be held in the cache as long as it's in use. When the user is done with
 * the file, the latch must be released.
 * @param fileNumwhich file
 * @return the file handle for the existing or newly created file
 */
  FileHandle getFileHandle(  long fileNum) throws LogException, DatabaseException {
    try {
      Long fileId=new Long(fileNum);
      FileHandle fileHandle=null;
      this.hook460(fileNum,fileId,fileHandle);
      throw ReturnHack.returnObject;
    }
 catch (    ReturnObject r) {
      return (FileHandle)r.value;
    }
  }
  private FileHandle makeFileHandle(  long fileNum,  FileMode mode) throws DatabaseException {
    String[] fileNames=getFullFileNames(fileNum);
    RandomAccessFile newFile=null;
    String fileName=null;
    try {
      FileNotFoundException FNFE=null;
      for (int i=0; i < fileNames.length; i++) {
        fileName=fileNames[i];
        try {
          newFile=new RandomAccessFile(fileName,mode.getModeValue());
          break;
        }
 catch (        FileNotFoundException e) {
          if (FNFE == null) {
            FNFE=e;
          }
        }
      }
      if (newFile == null) {
        throw FNFE;
      }
      boolean oldHeaderVersion=false;
      if (newFile.length() == 0) {
        if (mode == FileMode.READWRITE_MODE) {
          long lastLsn=DbLsn.longToLsn((Long)perFileLastUsedLsn.remove(new Long(fileNum - 1)));
          long headerPrevOffset=0;
          if (lastLsn != DbLsn.NULL_LSN) {
            headerPrevOffset=DbLsn.getFileOffset(lastLsn);
          }
          FileHeader fileHeader=new FileHeader(fileNum,headerPrevOffset);
          writeFileHeader(newFile,fileName,fileHeader);
        }
      }
 else {
        oldHeaderVersion=readAndValidateFileHeader(newFile,fileName,fileNum);
      }
      return new FileHandle(newFile,fileName,envImpl,oldHeaderVersion);
    }
 catch (    FileNotFoundException e) {
      throw new LogFileNotFoundException("Couldn't open file " + fileName + ": "+ e.getMessage());
    }
catch (    DbChecksumException e) {
      closeFileInErrorCase(newFile);
      throw new DbChecksumException(envImpl,"Couldn't open file " + fileName,e);
    }
catch (    Throwable t) {
      closeFileInErrorCase(newFile);
      throw new DatabaseException("Couldn't open file " + fileName + ": "+ t,t);
    }
  }
  /** 
 * Close this file and eat any exceptions. Used in catch clauses.
 */
  private void closeFileInErrorCase(  RandomAccessFile file){
    try {
      if (file != null) {
        file.close();
      }
    }
 catch (    IOException e) {
    }
  }
  /** 
 * Read the given je log file and validate the header.
 * @throws DatabaseExceptionif the file header isn't valid
 * @return whether the file header has an old version number.
 */
  private boolean readAndValidateFileHeader(  RandomAccessFile file,  String fileName,  long fileNum) throws DatabaseException, IOException {
    LogManager logManager=envImpl.getLogManager();
    LogEntry headerEntry=logManager.getLogEntry(DbLsn.makeLsn(fileNum,0),file);
    FileHeader header=(FileHeader)headerEntry.getMainItem();
    return header.validate(fileName,fileNum);
  }
  /** 
 * Write a proper file header to the given file.
 */
  private void writeFileHeader(  RandomAccessFile file,  String fileName,  FileHeader header) throws DatabaseException, IOException {
    envImpl.checkIfInvalid();
    if (envImpl.mayNotWrite()) {
      return;
    }
    int headerSize=header.getLogSize();
    int entrySize=headerSize + LogManager.HEADER_BYTES;
    ByteBuffer headerBuf=envImpl.getLogManager().putIntoBuffer(header,headerSize,0,false,entrySize);
    if (++writeCount >= stopOnWriteCount) {
      Runtime.getRuntime().halt(0xff);
    }
    int bytesWritten;
    try {
      if (RUNRECOVERY_EXCEPTION_TESTING) {
        generateRunRecoveryException(file,headerBuf,0);
      }
      bytesWritten=writeToFile(file,headerBuf,0);
    }
 catch (    ClosedChannelException e) {
      throw new RunRecoveryException(envImpl,"Channel closed, may be due to thread interrupt",e);
    }
catch (    IOException e) {
      throw new RunRecoveryException(envImpl,"IOException caught: " + e);
    }
    if (bytesWritten != entrySize) {
      throw new LogException("File " + fileName + " was created with an incomplete header. Only "+ bytesWritten+ " bytes were written.");
    }
  }
  /** 
 * @return the prevOffset field stored in the file header.
 */
  long getFileHeaderPrevOffset(  long fileNum) throws IOException, DatabaseException {
    LogEntry headerEntry=envImpl.getLogManager().getLogEntry(DbLsn.makeLsn(fileNum,0));
    FileHeader header=(FileHeader)headerEntry.getMainItem();
    return header.getLastEntryInPrevFileOffset();
  }
  /** 
 * @return the file offset of the last LSN that was used. For constructing
 * the headers of log entries. If the last LSN that was used was in
 * a previous file, or this is the very first LSN of the whole
 * system, return 0.
 */
  long getPrevEntryOffset(){
    return prevOffset;
  }
  /** 
 * Increase the current log position by "size" bytes. Move the prevOffset
 * pointer along.
 * @param sizeis an unsigned int
 * @return true if we flipped to the next log file.
 */
  boolean bumpLsn(  long size){
    saveLastPosition();
    boolean flippedFiles=false;
    if (forceNewFile || (DbLsn.getFileOffset(nextAvailableLsn) + size) > maxFileSize) {
      forceNewFile=false;
      currentFileNum++;
      if (lastUsedLsn != DbLsn.NULL_LSN) {
        perFileLastUsedLsn.put(new Long(DbLsn.getFileNumber(lastUsedLsn)),new Long(lastUsedLsn));
      }
      prevOffset=0;
      lastUsedLsn=DbLsn.makeLsn(currentFileNum,firstLogEntryOffset());
      flippedFiles=true;
    }
 else {
      if (lastUsedLsn == DbLsn.NULL_LSN) {
        prevOffset=0;
      }
 else {
        prevOffset=DbLsn.getFileOffset(lastUsedLsn);
      }
      lastUsedLsn=nextAvailableLsn;
    }
    nextAvailableLsn=DbLsn.makeLsn(DbLsn.getFileNumber(lastUsedLsn),(DbLsn.getFileOffset(lastUsedLsn) + size));
    return flippedFiles;
  }
  /** 
 * Write out a log buffer to the file.
 * @param fullBufferbuffer to write
 */
  void writeLogBuffer(  LogBuffer fullBuffer) throws DatabaseException {
    envImpl.checkIfInvalid();
    if (envImpl.mayNotWrite()) {
      return;
    }
    long firstLsn=fullBuffer.getFirstLsn();
    if (firstLsn != DbLsn.NULL_LSN) {
      RandomAccessFile file=endOfLog.getWritableFile(DbLsn.getFileNumber(firstLsn));
      ByteBuffer data=fullBuffer.getDataBuffer();
      if (++writeCount >= stopOnWriteCount) {
        Runtime.getRuntime().halt(0xff);
      }
      try {
        this.hook465(fullBuffer,firstLsn,file);
        if (IO_EXCEPTION_TESTING) {
          throw new IOException("generated for testing");
        }
        if (RUNRECOVERY_EXCEPTION_TESTING) {
          generateRunRecoveryException(file,data,DbLsn.getFileOffset(firstLsn));
        }
        writeToFile(file,data,DbLsn.getFileOffset(firstLsn));
      }
 catch (      ClosedChannelException e) {
        throw new RunRecoveryException(envImpl,"File closed, may be due to thread interrupt",e);
      }
catch (      IOException IOE) {
        abortCommittedTxns(data);
        this.hook466(fullBuffer,firstLsn,file,data,IOE);
      }
      assert EnvironmentImpl.maybeForceYield();
    }
  }
  /** 
 * Write a buffer to a file at a given offset, using NIO if so configured.
 */
  private int writeToFile(  RandomAccessFile file,  ByteBuffer data,  long destOffset) throws IOException, DatabaseException {
    return new FileManager_writeToFile(this,file,data,destOffset).execute();
  }
  /** 
 * Read a buffer from a file at a given offset, using NIO if so configured.
 */
  void readFromFile(  RandomAccessFile file,  ByteBuffer readBuffer,  long offset) throws IOException {
    new FileManager_readFromFile(this,file,readBuffer,offset).execute();
  }
  private void abortCommittedTxns(  ByteBuffer data){
    final byte commitType=LogEntryType.LOG_TXN_COMMIT.getTypeNum();
    final byte abortType=LogEntryType.LOG_TXN_ABORT.getTypeNum();
    this.hook461(data);
    while (data.remaining() > 0) {
      int recStartPos=data.position();
      data.position(recStartPos + LogManager.HEADER_ENTRY_TYPE_OFFSET);
      int typePos=data.position();
      byte entryType=data.get();
      boolean recomputeChecksum=false;
      if (entryType == commitType) {
        data.position(typePos);
        data.put(abortType);
        recomputeChecksum=true;
      }
      byte version=data.get();
      data.position(data.position() + LogManager.PREV_BYTES);
      int itemSize=LogUtils.readInt(data);
      int itemDataStartPos=data.position();
      if (recomputeChecksum) {
        Checksum checksum=Adler32.makeChecksum();
        data.position(recStartPos);
        int nChecksumBytes=itemSize + (LogManager.HEADER_BYTES - LogManager.CHECKSUM_BYTES);
        byte[] checksumBytes=new byte[nChecksumBytes];
        System.arraycopy(data.array(),recStartPos + LogManager.CHECKSUM_BYTES,checksumBytes,0,nChecksumBytes);
        checksum.update(checksumBytes,0,nChecksumBytes);
        LogUtils.writeUnsignedInt(data,checksum.getValue());
      }
      data.position(itemDataStartPos + itemSize);
    }
    data.position(0);
  }
  /** 
 * FSync the end of the log.
 */
  void syncLogEnd() throws DatabaseException {
    try {
      endOfLog.force();
    }
 catch (    IOException e) {
      throw new DatabaseException(e);
    }
  }
  /** 
 * Sync the end of the log, close off this log file. Should only be called
 * under the log write latch.
 */
  void syncLogEndAndFinishFile() throws DatabaseException, IOException {
    if (syncAtFileEnd) {
      syncLogEnd();
    }
    endOfLog.close();
  }
  /** 
 * Close all file handles and empty the cache.
 */
  public void clear() throws IOException, DatabaseException {
    endOfLog.close();
  }
  /** 
 * Clear the file lock.
 */
  public void close() throws IOException, DatabaseException {
  }
  /** 
 * Ensure that if the environment home dir is on readonly media or in a
 * readonly directory that the environment has been opened for readonly
 * access.
 * @return true if the environment home dir is readonly.
 */
  private boolean checkEnvHomePermissions(  boolean readOnly) throws DatabaseException {
    boolean envDirIsReadOnly=!dbEnvHome.canWrite();
    if (envDirIsReadOnly && !readOnly) {
      throw new DatabaseException("The Environment directory " + dbEnvHome + " is not writable, but the "+ "Environment was opened for read-write access.");
    }
    return envDirIsReadOnly;
  }
  /** 
 * Truncate a log at this position. Used by recovery to a timestamp
 * utilities and by recovery to set the end-of-log position.
 * <p>
 * This method forces a new log file to be written next, if the last file
 * (the file truncated to) has an old version in its header. This ensures
 * that when the log is opened by an old version of JE, a version
 * incompatibility will be detected. [#11243]
 * </p>
 */
  public void truncateLog(  long fileNum,  long offset) throws IOException, DatabaseException {
    FileHandle handle=makeFileHandle(fileNum,FileMode.READWRITE_MODE);
    RandomAccessFile file=handle.getFile();
    try {
      file.getChannel().truncate(offset);
    }
  finally {
      file.close();
    }
    if (handle.isOldHeaderVersion()) {
      forceNewFile=true;
    }
  }
  /** 
 * Set the flag that causes a new file to be written before the next write.
 */
  void forceNewLogFile(){
    forceNewFile=true;
  }
  /** 
 * @return the size in bytes of the file header log entry.
 */
  public static int firstLogEntryOffset(){
    return FileHeader.entrySize() + LogManager.HEADER_BYTES;
  }
  /** 
 * Return the next available LSN in the log. Note that this is
 * unsynchronized, so is only valid as an approximation of log size.
 */
  public long getNextLsn(){
    return nextAvailableLsn;
  }
  /** 
 * Return the last allocated LSN in the log. Note that this is
 * unsynchronized, so if it is called outside the log write latch it is only
 * valid as an approximation of log size.
 */
  public long getLastUsedLsn(){
    return lastUsedLsn;
  }
  /** 
 * The LogEndFileDescriptor is used to write and fsync the end of the log.
 * Because the JE log is append only, there is only one logical R/W file
 * descriptor for the whole environment. This class actually implements two
 * RandomAccessFile instances, one for writing and one for fsyncing, so the
 * two types of operations don't block each other.
 * The write file descriptor is considered the master. Manipulation of this
 * class is done under the log write latch. Here's an explanation of why the
 * log write latch is sufficient to safeguard all operations.
 * There are two types of callers who may use this file descriptor: the
 * thread that is currently writing to the end of the log and any threads
 * that are fsyncing on behalf of the FSyncManager.
 * The writing thread appends data to the file and fsyncs the file when we
 * flip over to a new log file. The file is only instantiated at the point
 * that it must do so -- which is either when the first fsync is required by
 * JE or when the log file is full and we flip files. Therefore, the writing
 * thread has two actions that change this descriptor -- we initialize the
 * file descriptor for the given log file at the first write to the file,
 * and we close the file descriptor when the log file is full. Therefore is
 * a period when there is no log descriptor -- when we have not yet written
 * a log buffer into a given log file.
 * The fsyncing threads ask for the log end file descriptor asynchronously,
 * but will never modify it. These threads may arrive at the point when the
 * file descriptor is null, and therefore skip their fysnc, but that is fine
 * because it means a writing thread already flipped that target file and
 * has moved on to the next file.
 * Time Activity 10 thread 1 writes log entry A into file 0x0, issues fsync
 * outside of log write latch, yields the processor 20 thread 2 writes log
 * entry B, piggybacks off thread 1 30 thread 3 writes log entry C, but no
 * room left in that file, so it flips the log, and fsyncs file 0x0, all
 * under the log write latch. It nulls out endOfLogRWFile, moves onto file
 * 0x1, but doesn't create the file yet. 40 thread 1 finally comes along,
 * but endOfLogRWFile is null-- no need to fsync in that case, 0x0 got
 * fsynced.
 */
class LogEndFileDescriptor {
    private RandomAccessFile endOfLogRWFile=null;
    private RandomAccessFile endOfLogSyncFile=null;
    /** 
 * getWritableFile must be called under the log write latch.
 */
    RandomAccessFile getWritableFile(    long fileNumber) throws RunRecoveryException {
      try {
        if (endOfLogRWFile == null) {
          endOfLogRWFile=makeFileHandle(fileNumber,FileMode.READWRITE_MODE).getFile();
          endOfLogSyncFile=makeFileHandle(fileNumber,FileMode.READWRITE_MODE).getFile();
        }
        return endOfLogRWFile;
      }
 catch (      Exception e) {
        throw new RunRecoveryException(envImpl,e);
      }
    }
    /** 
 * FSync the log file that makes up the end of the log.
 */
    void force() throws DatabaseException, IOException {
      RandomAccessFile file=endOfLogSyncFile;
      if (file != null) {
        FileChannel channel=file.getChannel();
        try {
          channel.force(false);
        }
 catch (        ClosedChannelException e) {
          throw new RunRecoveryException(envImpl,"Channel closed, may be due to thread interrupt",e);
        }
        assert EnvironmentImpl.maybeForceYield();
      }
    }
    /** 
 * Close the end of the log file descriptor. Use atomic assignment to
 * ensure that we won't force and close on the same descriptor.
 */
    void close() throws IOException {
      IOException firstException=null;
      if (endOfLogRWFile != null) {
        RandomAccessFile file=endOfLogRWFile;
        endOfLogRWFile=null;
        try {
          file.close();
        }
 catch (        IOException e) {
          firstException=e;
        }
      }
      if (endOfLogSyncFile != null) {
        RandomAccessFile file=endOfLogSyncFile;
        endOfLogSyncFile=null;
        file.close();
      }
      if (firstException != null) {
        throw firstException;
      }
    }
  }
  static boolean RUNRECOVERY_EXCEPTION_TESTING=false;
  private static final int RUNRECOVERY_EXCEPTION_MAX=100;
  private int runRecoveryExceptionCounter=0;
  private boolean runRecoveryExceptionThrown=false;
  private Random runRecoveryExceptionRandom=null;
  private void generateRunRecoveryException(  RandomAccessFile file,  ByteBuffer data,  long destOffset) throws DatabaseException, IOException {
    if (runRecoveryExceptionThrown) {
      try {
        throw new Exception("Write after RunRecoveryException");
      }
 catch (      Exception e) {
        e.printStackTrace();
      }
    }
    runRecoveryExceptionCounter+=1;
    if (runRecoveryExceptionCounter >= RUNRECOVERY_EXCEPTION_MAX) {
      runRecoveryExceptionCounter=0;
    }
    if (runRecoveryExceptionRandom == null) {
      runRecoveryExceptionRandom=new Random(System.currentTimeMillis());
    }
    if (runRecoveryExceptionCounter == runRecoveryExceptionRandom.nextInt(RUNRECOVERY_EXCEPTION_MAX)) {
      int len=runRecoveryExceptionRandom.nextInt(data.remaining());
      if (len > 0) {
        byte[] a=new byte[len];
        data.get(a,0,len);
        ByteBuffer buf=ByteBuffer.wrap(a);
        writeToFile(file,buf,destOffset);
      }
      runRecoveryExceptionThrown=true;
      throw new RunRecoveryException(envImpl,"Randomly generated for testing");
    }
  }
@MethodObject static class FileManager_writeToFile {
    FileManager_writeToFile(    FileManager _this,    RandomAccessFile file,    ByteBuffer data,    long destOffset){
      this._this=_this;
      this.file=file;
      this.data=data;
      this.destOffset=destOffset;
    }
    int execute() throws IOException, DatabaseException {
      totalBytesWritten=0;
      this.hook455();
      this.hook445();
      return totalBytesWritten;
    }
    protected FileManager _this;
    protected RandomAccessFile file;
    protected ByteBuffer data;
    protected long destOffset;
    protected int totalBytesWritten;
    protected FileChannel channel;
    protected ByteBuffer useData;
    protected int origLimit;
    protected int bytesWritten;
    protected int pos;
    protected int size;
    protected void hook445() throws IOException, DatabaseException {
    }
    protected void hook455() throws IOException, DatabaseException {
    }
  }
@MethodObject static class FileManager_readFromFile {
    FileManager_readFromFile(    FileManager _this,    RandomAccessFile file,    ByteBuffer readBuffer,    long offset){
      this._this=_this;
      this.file=file;
      this.readBuffer=readBuffer;
      this.offset=offset;
    }
    void execute() throws IOException {
      this.hook446();
    }
    protected FileManager _this;
    protected RandomAccessFile file;
    protected ByteBuffer readBuffer;
    protected long offset;
    protected FileChannel channel;
    protected int readLength;
    protected long currentPosition;
    protected int bytesRead1;
    protected int pos;
    protected int size;
    protected int bytesRead2;
    protected void hook446() throws IOException {
    }
  }
  protected void hook449(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected FileHandle hook450(  long fileNum,  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    fileHandle=this.hook462(fileNum,fileId,fileHandle);
    return fileHandle;
  }
  protected void hook452(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected void hook453(  FileHandle fileHandle) throws LogException, DatabaseException {
  }
  protected void hook454(  FileHandle fileHandle) throws LogException, DatabaseException {
  }
  protected void hook456(  DbConfigManager configManager) throws DatabaseException {
  }
  protected void hook457(  DbConfigManager configManager) throws DatabaseException {
  }
  protected void hook458(  long fileNum) throws DatabaseException, IOException {
  }
  protected void hook459(  long fileNum) throws DatabaseException, IOException {
  }
  protected void hook460(  long fileNum,  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    fileHandle=this.hook463(fileNum,fileId,fileHandle);
    this.hook453(fileHandle);
    if (fileHandle.getFile() == null) {
      this.hook454(fileHandle);
    }
 else {
      throw new ReturnObject(fileHandle);
    }
  }
  protected void hook461(  ByteBuffer data){
  }
  protected FileHandle hook462(  long fileNum,  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    fileHandle=makeFileHandle(fileNum,FileMode.READ_MODE);
    this.hook464(fileId,fileHandle);
    return fileHandle;
  }
  protected FileHandle hook463(  long fileNum,  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
    fileHandle=this.hook450(fileNum,fileId,fileHandle);
    return fileHandle;
  }
  protected void hook464(  Long fileId,  FileHandle fileHandle) throws LogException, DatabaseException {
  }
  protected void hook465(  LogBuffer fullBuffer,  long firstLsn,  RandomAccessFile file) throws DatabaseException, ClosedChannelException, IOException {
  }
  protected void hook466(  LogBuffer fullBuffer,  long firstLsn,  RandomAccessFile file,  ByteBuffer data,  IOException IOE) throws DatabaseException {
    throw new DatabaseException(IOE);
  }
  protected void hook467(  boolean readOnly) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/log/LogManager.java:package com.sleepycat.je.log;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.BufferOverflowException;
import java.nio.ByteBuffer;
import java.nio.channels.ClosedChannelException;
import java.util.List;
import java.util.zip.Checksum;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.RunRecoveryException;
import com.sleepycat.je.cleaner.TrackedFileSummary;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.log.entry.LogEntry;
import com.sleepycat.je.utilint.Adler32;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.TestHook;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * The LogManager supports reading and writing to the JE log.
 */
abstract public class LogManager {
  private static final String DEBUG_NAME=LogManager.class.getName();
  static final int HEADER_BYTES=14;
  static final int CHECKSUM_BYTES=4;
  static final int PREV_BYTES=4;
  static final int HEADER_ENTRY_TYPE_OFFSET=4;
  static final int HEADER_VERSION_OFFSET=5;
  static final int HEADER_PREV_OFFSET=6;
  static final int HEADER_SIZE_OFFSET=6 + 4;
  static int HEADER_CONTENT_BYTES(){
    int r=HEADER_BYTES;
    r=hook504(r);
    return r;
  }
  protected LogBufferPool logBufferPool;
  private FileManager fileManager;
  protected EnvironmentImpl envImpl;
  private boolean readOnly;
  private int readBufferSize;
  private long lastLsnAtRecovery=DbLsn.NULL_LSN;
  private TestHook readHook;
  /** 
 * There is a single log manager per database environment.
 */
  public LogManager(  EnvironmentImpl envImpl,  boolean readOnly) throws DatabaseException {
    this.envImpl=envImpl;
    this.fileManager=envImpl.getFileManager();
    DbConfigManager configManager=envImpl.getConfigManager();
    this.readOnly=readOnly;
    logBufferPool=new LogBufferPool(fileManager,envImpl);
    this.hook505(configManager);
    this.hook502(envImpl);
    readBufferSize=configManager.getInt(EnvironmentParams.LOG_FAULT_READ_SIZE);
    this.hook498(envImpl);
  }
  public long getLastLsnAtRecovery(){
    return lastLsnAtRecovery;
  }
  public void setLastLsnAtRecovery(  long lastLsnAtRecovery){
    this.lastLsnAtRecovery=lastLsnAtRecovery;
  }
  /** 
 * Reset the pool when the cache is resized. This method is called after the
 * memory budget has been calculated.
 */
  public void resetPool(  DbConfigManager configManager) throws DatabaseException {
    logBufferPool.reset(configManager);
  }
  /** 
 * Log this single object and force a write of the log files.
 * @param itemobject to be logged
 * @param fsyncRequiredif true, log files should also be fsynced.
 * @return LSN of the new log entry
 */
  public long logForceFlush(  LoggableObject item,  boolean fsyncRequired) throws DatabaseException {
    return log(item,false,true,fsyncRequired,false,DbLsn.NULL_LSN);
  }
  /** 
 * Log this single object and force a flip of the log files.
 * @param itemobject to be logged
 * @param fsyncRequiredif true, log files should also be fsynced.
 * @return LSN of the new log entry
 */
  public long logForceFlip(  LoggableObject item) throws DatabaseException {
    return log(item,false,true,false,true,DbLsn.NULL_LSN);
  }
  /** 
 * Write a log entry.
 * @return LSN of the new log entry
 */
  public long log(  LoggableObject item) throws DatabaseException {
    return log(item,false,false,false,false,DbLsn.NULL_LSN);
  }
  /** 
 * Write a log entry.
 * @return LSN of the new log entry
 */
  public long log(  LoggableObject item,  boolean isProvisional,  long oldNodeLsn) throws DatabaseException {
    return log(item,isProvisional,false,false,false,oldNodeLsn);
  }
  /** 
 * Write a log entry.
 * @param itemis the item to be logged.
 * @param isProvisionaltrue if this entry should not be read during recovery.
 * @param flushRequiredif true, write the log to the file after adding the item. i.e.
 * call java.nio.channel.FileChannel.write().
 * @param fsyncRequiredif true, fsync the last file after adding the item.
 * @param oldNodeLsnis the previous version of the node to be counted as obsolete,
 * or null if the item is not a node or has no old LSN.
 * @return LSN of the new log entry
 */
  private long log(  LoggableObject item,  boolean isProvisional,  boolean flushRequired,  boolean fsyncRequired,  boolean forceNewLogFile,  long oldNodeLsn) throws DatabaseException {
    if (readOnly) {
      return DbLsn.NULL_LSN;
    }
    boolean marshallOutsideLatch=item.marshallOutsideWriteLatch();
    ByteBuffer marshalledBuffer=null;
    UtilizationTracker tracker=envImpl.getUtilizationTracker();
    LogResult logResult=null;
    try {
      if (marshallOutsideLatch) {
        int itemSize=item.getLogSize();
        int entrySize=itemSize + HEADER_BYTES;
        marshalledBuffer=marshallIntoBuffer(item,itemSize,isProvisional,entrySize);
      }
      logResult=logItem(item,isProvisional,flushRequired,forceNewLogFile,oldNodeLsn,marshallOutsideLatch,marshalledBuffer,tracker);
    }
 catch (    BufferOverflowException e) {
      throw new RunRecoveryException(envImpl,e);
    }
catch (    IOException e) {
      throw new DatabaseException(Tracer.getStackTrace(e),e);
    }
    this.hook501(fsyncRequired);
    this.hook499(logResult);
    if (logResult.wakeupCleaner) {
      tracker.activateCleaner();
    }
    return logResult.currentLsn;
  }
  abstract protected LogResult logItem(  LoggableObject item,  boolean isProvisional,  boolean flushRequired,  boolean forceNewLogFile,  long oldNodeLsn,  boolean marshallOutsideLatch,  ByteBuffer marshalledBuffer,  UtilizationTracker tracker) throws IOException, DatabaseException ;
  /** 
 * Called within the log write critical section.
 */
  protected LogResult logInternal(  LoggableObject item,  boolean isProvisional,  boolean flushRequired,  boolean forceNewLogFile,  long oldNodeLsn,  boolean marshallOutsideLatch,  ByteBuffer marshalledBuffer,  UtilizationTracker tracker) throws IOException, DatabaseException {
    LogEntryType entryType=item.getLogType();
    if (oldNodeLsn != DbLsn.NULL_LSN) {
      tracker.countObsoleteNode(oldNodeLsn,entryType);
    }
    int entrySize;
    if (marshallOutsideLatch) {
      entrySize=marshalledBuffer.limit();
    }
 else {
      entrySize=item.getLogSize() + HEADER_BYTES;
    }
    if (forceNewLogFile) {
      fileManager.forceNewLogFile();
    }
    boolean flippedFile=fileManager.bumpLsn(entrySize);
    long currentLsn=DbLsn.NULL_LSN;
    boolean wakeupCleaner=false;
    boolean usedTemporaryBuffer=false;
    try {
      currentLsn=fileManager.getLastUsedLsn();
      wakeupCleaner=tracker.countNewLogEntry(currentLsn,entryType,entrySize);
      if (item.countAsObsoleteWhenLogged()) {
        tracker.countObsoleteNodeInexact(currentLsn,entryType);
      }
      if (!marshallOutsideLatch) {
        marshalledBuffer=marshallIntoBuffer(item,entrySize - HEADER_BYTES,isProvisional,entrySize);
      }
      if (entrySize != marshalledBuffer.limit()) {
        throw new DatabaseException("Logged item entrySize= " + entrySize + " but marshalledSize="+ marshalledBuffer.limit()+ " type="+ entryType+ " currentLsn="+ DbLsn.getNoFormatString(currentLsn));
      }
      LogBuffer useLogBuffer=logBufferPool.getWriteBuffer(entrySize,flippedFile);
      marshalledBuffer=addPrevOffsetAndChecksum(marshalledBuffer,fileManager.getPrevEntryOffset(),entrySize);
      usedTemporaryBuffer=this.hook503(marshalledBuffer,entrySize,currentLsn,usedTemporaryBuffer,useLogBuffer);
    }
 catch (    Exception e) {
      fileManager.restoreLastPosition();
      if (e instanceof DatabaseException) {
        throw (DatabaseException)e;
      }
 else       if (e instanceof IOException) {
        throw (IOException)e;
      }
 else {
        throw new DatabaseException(e);
      }
    }
    if (!usedTemporaryBuffer) {
      logBufferPool.writeCompleted(currentLsn,flushRequired);
    }
    item.postLogWork(currentLsn);
    boolean wakeupCheckpointer=false;
    wakeupCheckpointer=this.hook500(item,entrySize,wakeupCheckpointer);
    return new LogResult(currentLsn,wakeupCheckpointer,wakeupCleaner);
  }
  /** 
 * Serialize a loggable object into this buffer.
 */
  private ByteBuffer marshallIntoBuffer(  LoggableObject item,  int itemSize,  boolean isProvisional,  int entrySize) throws DatabaseException {
    ByteBuffer destBuffer=ByteBuffer.allocate(entrySize);
    destBuffer.position(CHECKSUM_BYTES);
    writeHeader(destBuffer,item.getLogType(),itemSize,isProvisional);
    item.writeToLog(destBuffer);
    destBuffer.flip();
    return destBuffer;
  }
  private ByteBuffer addPrevOffsetAndChecksum(  ByteBuffer destBuffer,  long lastOffset,  int entrySize){
    Checksum checksum=Adler32.makeChecksum();
    destBuffer.position(HEADER_PREV_OFFSET);
    LogUtils.writeUnsignedInt(destBuffer,lastOffset);
    checksum.update(destBuffer.array(),CHECKSUM_BYTES,(entrySize - CHECKSUM_BYTES));
    destBuffer.position(0);
    LogUtils.writeUnsignedInt(destBuffer,checksum.getValue());
    destBuffer.position(0);
    return destBuffer;
  }
  /** 
 * Serialize a loggable object into this buffer. Return it ready for a copy.
 */
  ByteBuffer putIntoBuffer(  LoggableObject item,  int itemSize,  long prevLogEntryOffset,  boolean isProvisional,  int entrySize) throws DatabaseException {
    ByteBuffer destBuffer=marshallIntoBuffer(item,itemSize,isProvisional,entrySize);
    return addPrevOffsetAndChecksum(destBuffer,0,entrySize);
  }
  /** 
 * Helper to write the common entry header.
 * @param destBufferdestination
 * @param itemobject being logged
 * @param itemSizeWe could ask the item for this, but are passing it as a
 * parameter for efficiency, because it's already available
 */
  private void writeHeader(  ByteBuffer destBuffer,  LogEntryType itemType,  int itemSize,  boolean isProvisional){
    byte typeNum=itemType.getTypeNum();
    destBuffer.put(typeNum);
    byte version=itemType.getVersion();
    if (isProvisional)     version=LogEntryType.setProvisional(version);
    destBuffer.put(version);
    destBuffer.position(HEADER_SIZE_OFFSET);
    LogUtils.writeInt(destBuffer,itemSize);
  }
  /** 
 * Instantiate all the objects in the log entry at this LSN.
 * @param lsnlocation of entry in log.
 * @return log entry that embodies all the objects in the log entry.
 */
  public LogEntry getLogEntry(  long lsn) throws DatabaseException {
    envImpl.checkIfInvalid();
    LogSource logSource=getLogSource(lsn);
    return getLogEntryFromLogSource(lsn,logSource);
  }
  LogEntry getLogEntry(  long lsn,  RandomAccessFile file) throws DatabaseException {
    return getLogEntryFromLogSource(lsn,new FileSource(file,readBufferSize,fileManager));
  }
  /** 
 * Instantiate all the objects in the log entry at this LSN. This will
 * release the log source at the first opportunity.
 * @param lsnlocation of entry in log
 * @return log entry that embodies all the objects in the log entry
 */
  private LogEntry getLogEntryFromLogSource(  long lsn,  LogSource logSource) throws DatabaseException {
    return new LogManager_getLogEntryFromLogSource(this,lsn,logSource).execute();
  }
  /** 
 * Fault in the first object in the log entry log entry at this LSN.
 * @param lsnlocation of object in log
 * @return the object in the log
 */
  public Object get(  long lsn) throws DatabaseException {
    LogEntry entry=getLogEntry(lsn);
    return entry.getMainItem();
  }
  /** 
 * Find the LSN, whether in a file or still in the log buffers.
 */
  private LogSource getLogSource(  long lsn) throws DatabaseException {
    LogBuffer logBuffer=logBufferPool.getReadBuffer(lsn);
    if (logBuffer == null) {
      try {
        return new FileHandleSource(fileManager.getFileHandle(DbLsn.getFileNumber(lsn)),readBufferSize,fileManager);
      }
 catch (      LogFileNotFoundException e) {
        throw new LogFileNotFoundException(DbLsn.getNoFormatString(lsn) + ' ' + e.getMessage());
      }
    }
 else {
      return logBuffer;
    }
  }
  /** 
 * Flush all log entries, fsync the log file.
 */
  public void flush() throws DatabaseException {
    if (readOnly) {
      return;
    }
    flushInternal();
    fileManager.syncLogEnd();
  }
  abstract protected void flushInternal() throws LogException, DatabaseException ;
  /** 
 * Returns a tracked summary for the given file which will not be flushed.
 * Used for watching changes that occur while a file is being cleaned.
 */
  abstract public TrackedFileSummary getUnflushableTrackedSummary(  long file) throws DatabaseException ;
  protected TrackedFileSummary getUnflushableTrackedSummaryInternal(  long file) throws DatabaseException {
    return envImpl.getUtilizationTracker().getUnflushableTrackedSummary(file);
  }
  /** 
 * Count node as obsolete under the log write latch. This is done here
 * because the log write latch is managed here, and all utilization counting
 * must be performed under the log write latch.
 */
  abstract public void countObsoleteNode(  long lsn,  LogEntryType type) throws DatabaseException ;
  protected void countObsoleteNodeInternal(  UtilizationTracker tracker,  long lsn,  LogEntryType type) throws DatabaseException {
    tracker.countObsoleteNode(lsn,type);
  }
  /** 
 * Counts file summary info under the log write latch.
 */
  abstract public void countObsoleteNodes(  TrackedFileSummary[] summaries) throws DatabaseException ;
  protected void countObsoleteNodesInternal(  UtilizationTracker tracker,  TrackedFileSummary[] summaries) throws DatabaseException {
    for (int i=0; i < summaries.length; i+=1) {
      TrackedFileSummary summary=summaries[i];
      tracker.addSummary(summary.getFileNumber(),summary);
    }
  }
  /** 
 * Counts the given obsolete IN LSNs under the log write latch.
 */
  abstract public void countObsoleteINs(  List lsnList) throws DatabaseException ;
  protected void countObsoleteINsInternal(  List lsnList) throws DatabaseException {
    UtilizationTracker tracker=envImpl.getUtilizationTracker();
    for (int i=0; i < lsnList.size(); i+=1) {
      Long offset=(Long)lsnList.get(i);
      tracker.countObsoleteNode(offset.longValue(),LogEntryType.LOG_IN);
    }
  }
  public void setReadHook(  TestHook hook){
    readHook=hook;
  }
@MethodObject static class LogManager_getLogEntryFromLogSource {
    LogManager_getLogEntryFromLogSource(    LogManager _this,    long lsn,    LogSource logSource){
      this._this=_this;
      this.lsn=lsn;
      this.logSource=logSource;
    }
    LogEntry execute() throws DatabaseException {
      try {
        fileOffset=DbLsn.getFileOffset(lsn);
        entryBuffer=logSource.getBytes(fileOffset);
        this.hook507();
        loggableType=entryBuffer.get();
        version=entryBuffer.get();
        entryBuffer.position(entryBuffer.position() + _this.PREV_BYTES);
        itemSize=LogUtils.readInt(entryBuffer);
        if (entryBuffer.remaining() < itemSize) {
          entryBuffer=logSource.getBytes(fileOffset + _this.HEADER_BYTES,itemSize);
          this.hook508();
        }
        this.hook506();
        assert LogEntryType.isValidType(loggableType) : "Read non-valid log entry type: " + loggableType;
        logEntry=LogEntryType.findType(loggableType,version).getNewLogEntry();
        logEntry.readEntry(entryBuffer,itemSize,version,true);
        if (_this.readHook != null) {
          _this.readHook.doIOHook();
        }
        return logEntry;
      }
 catch (      DatabaseException e) {
        throw e;
      }
catch (      ClosedChannelException e) {
        throw new RunRecoveryException(_this.envImpl,"Channel closed, may be " + "due to thread interrupt",e);
      }
catch (      Exception e) {
        throw new DatabaseException(e);
      }
 finally {
        if (logSource != null) {
          logSource.release();
        }
      }
    }
    protected LogManager _this;
    protected long lsn;
    protected LogSource logSource;
    protected long fileOffset;
    protected ByteBuffer entryBuffer;
    protected long storedChecksum;
    protected byte loggableType;
    protected byte version;
    protected int itemSize;
    protected LogEntry logEntry;
    protected void hook506() throws DatabaseException, ClosedChannelException, Exception {
    }
    protected void hook507() throws DatabaseException, ClosedChannelException, Exception {
    }
    protected void hook508() throws DatabaseException, ClosedChannelException, Exception {
    }
  }
  protected void hook498(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected void hook499(  LogResult logResult) throws DatabaseException {
  }
  protected boolean hook500(  LoggableObject item,  int entrySize,  boolean wakeupCheckpointer) throws IOException, DatabaseException {
    return wakeupCheckpointer;
  }
  protected void hook501(  boolean fsyncRequired) throws DatabaseException {
  }
  protected void hook502(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected boolean hook503(  ByteBuffer marshalledBuffer,  int entrySize,  long currentLsn,  boolean usedTemporaryBuffer,  LogBuffer useLogBuffer) throws IOException, DatabaseException, Exception {
    ByteBuffer useBuffer=useLogBuffer.getDataBuffer();
    if (useBuffer.capacity() - useBuffer.position() < entrySize) {
      fileManager.writeLogBuffer(new LogBuffer(marshalledBuffer,currentLsn));
      usedTemporaryBuffer=true;
      assert useBuffer.position() == 0;
      this.hook509();
    }
 else {
      useBuffer.put(marshalledBuffer);
    }
    return usedTemporaryBuffer;
  }
  protected static int hook504(  int r){
    return r;
  }
  protected void hook505(  DbConfigManager configManager) throws DatabaseException {
  }
  protected void hook509() throws IOException, DatabaseException, Exception {
  }
}
\00base/com/sleepycat/je/log/LogBuffer.java:package com.sleepycat.je.log;
import java.nio.ByteBuffer;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.utilint.DbLsn;
import de.ovgu.cide.jakutil.*;
/** 
 * DbLogBuffers hold outgoing, newly written log entries.
 */
class LogBuffer implements LogSource {
  private static final String DEBUG_NAME=LogBuffer.class.getName();
  private ByteBuffer buffer;
  private long firstLsn;
  private long lastLsn;
  LogBuffer(  int capacity,  EnvironmentImpl env) throws DatabaseException {
    this.hook481(capacity);
    this.hook482(capacity);
    this.hook479(env);
    reinit();
  }
  LogBuffer(  ByteBuffer buffer,  long firstLsn) throws DatabaseException {
    this.buffer=buffer;
    this.firstLsn=firstLsn;
    this.lastLsn=firstLsn;
  }
  void reinit() throws DatabaseException {
    buffer.clear();
    firstLsn=DbLsn.NULL_LSN;
    lastLsn=DbLsn.NULL_LSN;
  }
  /** 
 * Return first LSN held in this buffer. Assumes the log write latch is
 * held.
 */
  long getFirstLsn(){
    return firstLsn;
  }
  /** 
 * This LSN has been written to the log.
 */
  void registerLsn(  long lsn) throws DatabaseException {
    if (lastLsn != DbLsn.NULL_LSN) {
      assert (DbLsn.compareTo(lsn,lastLsn) > 0);
    }
    lastLsn=lsn;
    if (firstLsn == DbLsn.NULL_LSN) {
      firstLsn=lsn;
    }
  }
  /** 
 * Check capacity of buffer. Assumes that the log write latch is held.
 * @return true if this buffer can hold this many more bytes.
 */
  boolean hasRoom(  int numBytes){
    return (numBytes <= (buffer.capacity() - buffer.position()));
  }
  /** 
 * @return the actual data buffer.
 */
  ByteBuffer getDataBuffer(){
    return buffer;
  }
  /** 
 * @return capacity in bytes
 */
  int getCapacity(){
    return buffer.capacity();
  }
  /** 
 * Support for reading a log entry out of a still-in-memory log
 * @return true if this buffer holds the entry at this LSN. The buffer will
 * be latched for read. Returns false if LSN is not here, and
 * releases the read latch.
 */
  boolean containsLsn(  long lsn) throws DatabaseException {
    boolean found=false;
    if ((firstLsn != DbLsn.NULL_LSN) && ((DbLsn.compareTo(firstLsn,lsn) <= 0) && (DbLsn.compareTo(lastLsn,lsn) >= 0))) {
      found=true;
    }
    if (found) {
      return true;
    }
 else {
      this.hook480();
      return false;
    }
  }
  /** 
 * @see LogSource#getBytes
 */
  public ByteBuffer getBytes(  long fileOffset){
    ByteBuffer copy=null;
    while (true) {
      try {
        copy=buffer.duplicate();
        copy.position((int)(fileOffset - DbLsn.getFileOffset(firstLsn)));
        break;
      }
 catch (      IllegalArgumentException IAE) {
        continue;
      }
    }
    return copy;
  }
  /** 
 * @see LogSource#getBytes
 */
  public ByteBuffer getBytes(  long fileOffset,  int numBytes){
    ByteBuffer copy=getBytes(fileOffset);
    assert (copy.remaining() >= numBytes) : "copy.remaining=" + copy.remaining() + " numBytes="+ numBytes;
    return copy;
  }
  protected void hook479(  EnvironmentImpl env) throws DatabaseException {
  }
  protected void hook480() throws DatabaseException {
  }
  protected void hook481(  int capacity) throws DatabaseException {
  }
  protected void hook482(  int capacity) throws DatabaseException {
  }
  
  public void release() throws DatabaseException {
	  
  }
}
\00base/com/sleepycat/je/recovery/Checkpointer.java:package com.sleepycat.je.recovery;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import java.util.SortedMap;
import java.util.TreeMap;
import java.util.logging.Level;
import com.sleepycat.je.CheckpointConfig;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.cleaner.Cleaner;
import com.sleepycat.je.cleaner.TrackedFileSummary;
import com.sleepycat.je.cleaner.UtilizationProfile;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.INList;
import com.sleepycat.je.dbi.MemoryBudget;
import com.sleepycat.je.log.LogManager;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.tree.SearchResult;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.utilint.DaemonThread;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.PropUtil;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
/** 
 * The Checkpointer looks through the tree for internal nodes that must be
 * flushed to the log. Checkpoint flushes must be done in ascending order from
 * the bottom of the tree up.
 */
public class Checkpointer {
  private EnvironmentImpl envImpl;
  private LogManager logManager;
  private long checkpointId;
  private long logFileMax;
  private long lastCheckpointMillis;
  private long lastFirstActiveLsn;
  private long lastCheckpointEnd;
  private volatile int highestFlushLevel;
  public Checkpointer(  EnvironmentImpl envImpl,  long waitTime,  String name) throws DatabaseException {
    this.hook538(envImpl,waitTime,name);
    this.envImpl=envImpl;
    this.hook539(envImpl);
    logFileMax=envImpl.getConfigManager().getLong(EnvironmentParams.LOG_FILE_MAX);
    this.hook531();
    this.hook545(waitTime);
    lastCheckpointMillis=0;
    highestFlushLevel=IN.MIN_LEVEL;
    logManager=envImpl.getLogManager();
  }
  public int getHighestFlushLevel(){
    return highestFlushLevel;
  }
  /** 
 * Figure out the wakeup period. Supplied through this static method because
 * we need to pass wakeup period to the superclass and need to do the
 * calcuation outside this constructor.
 */
  public static long getWakeupPeriod(  DbConfigManager configManager) throws IllegalArgumentException, DatabaseException {
    return new Checkpointer_getWakeupPeriod(configManager).execute();
  }
  /** 
 * Set checkpoint id -- can only be done after recovery.
 */
  synchronized public void setCheckpointId(  long lastCheckpointId){
    checkpointId=lastCheckpointId;
  }
  /** 
 * @return the first active LSN point of the last completed checkpoint. If
 * no checkpoint has run, return null.
 */
  public long getFirstActiveLsn(){
    return lastFirstActiveLsn;
  }
  /** 
 * Initialize the FirstActiveLsn during recovery. The cleaner needs this.
 */
  public void setFirstActiveLsn(  long lastFirstActiveLsn){
    this.lastFirstActiveLsn=lastFirstActiveLsn;
  }
  /** 
 * Determine whether a checkpoint should be run.
 * 1. If the force parameter is specified, always checkpoint.
 * 2. If the config object specifies time or log size, use that.
 * 3. If the environment is configured to use log size based checkpointing,
 * check the log.
 * 4. Lastly, use time based checking.
 */
  private boolean isRunnable(  CheckpointConfig config) throws DatabaseException {
    return new Checkpointer_isRunnable(this,config).execute();
  }
  /** 
 * The real work to do a checkpoint. This may be called by the checkpoint
 * thread when waking up, or it may be invoked programatically through the
 * api.
 * @param allowDeltasif true, this checkpoint may opt to log BIN deltas instead of
 * the full node.
 * @param flushAllif true, this checkpoint must flush all the way to the top of
 * the dbtree, instead of stopping at the highest level last
 * modified.
 * @param invokingSourcea debug aid, to indicate who invoked this checkpoint. (i.e.
 * recovery, the checkpointer daemon, the cleaner,
 * programatically)
 */
  public synchronized void doCheckpoint(  CheckpointConfig config,  boolean flushAll,  String invokingSource) throws DatabaseException {
    new Checkpointer_doCheckpoint(this,config,flushAll,invokingSource).execute();
  }
  /** 
 * Flush a FileSummaryLN node for each TrackedFileSummary that is currently
 * active. Tell the UtilizationProfile about the updated file summary.
 */
  private void flushUtilizationInfo() throws DatabaseException {
    if (!DbInternal.getCheckpointUP(envImpl.getConfigManager().getEnvironmentConfig())) {
      return;
    }
    UtilizationProfile profile=envImpl.getUtilizationProfile();
    TrackedFileSummary[] activeFiles=envImpl.getUtilizationTracker().getTrackedFiles();
    for (int i=0; i < activeFiles.length; i+=1) {
      profile.flushFileSummary(activeFiles[i]);
    }
  }
  /** 
 * Flush the nodes in order, from the lowest level to highest level. As a
 * flush dirties its parent, add it to the dirty map, thereby cascading the
 * writes up the tree. If flushAll wasn't specified, we need only cascade up
 * to the highest level set at the start of checkpointing.
 * Note that all but the top level INs and the BINDeltas are logged
 * provisionally. That's because we don't need to process lower INs because
 * the higher INs will end up pointing at them.
 */
  private void flushDirtyNodes(  SortedMap dirtyMap,  boolean flushAll,  boolean allowDeltas,  boolean flushExtraLevel,  long checkpointStart) throws DatabaseException {
    while (dirtyMap.size() > 0) {
      Integer currentLevel=(Integer)dirtyMap.firstKey();
      boolean logProvisionally=(currentLevel.intValue() != highestFlushLevel);
      Set nodeSet=(Set)dirtyMap.get(currentLevel);
      Iterator iter=nodeSet.iterator();
      while (iter.hasNext()) {
        CheckpointReference targetRef=(CheckpointReference)iter.next();
        this.hook520();
        this.hook546(dirtyMap,allowDeltas,checkpointStart,currentLevel,logProvisionally,targetRef);
        iter.remove();
      }
      dirtyMap.remove(currentLevel);
      if (currentLevel.intValue() == highestFlushLevel) {
        break;
      }
    }
  }
  /** 
 * Scan the INList for all dirty INs. Arrange them in level sorted map for
 * level ordered flushing.
 */
  private SortedMap selectDirtyINs(  boolean flushAll,  boolean flushExtraLevel) throws DatabaseException {
    return new Checkpointer_selectDirtyINs(this,flushAll,flushExtraLevel).execute();
  }
  /** 
 * Flush the target IN.
 */
  private void flushIN(  CheckpointReference targetRef,  Map dirtyMap,  int currentLevel,  boolean logProvisionally,  boolean allowDeltas,  long checkpointStart) throws DatabaseException {
    Tree tree=targetRef.db.getTree();
    boolean targetWasRoot=false;
    if (targetRef.isDbRoot) {
      RootFlusher flusher=new RootFlusher(targetRef.db,logManager,targetRef.nodeId);
      tree.withRootLatchedExclusive(flusher);
      boolean flushed=flusher.getFlushed();
      targetWasRoot=flusher.stillRoot();
      if (flushed) {
        DbTree dbTree=targetRef.db.getDbEnvironment().getDbMapTree();
        dbTree.modifyDbRoot(targetRef.db);
        this.hook532();
      }
    }
    if (!targetWasRoot) {
      SearchResult result=tree.getParentINForChildIN(targetRef.nodeId,targetRef.containsDuplicates,false,targetRef.mainTreeKey,targetRef.dupTreeKey,false,false,-1,null,false);
      if (result.parent != null) {
        boolean mustLogParent=false;
        this.hook526(targetRef,dirtyMap,currentLevel,logProvisionally,allowDeltas,checkpointStart,tree,result,mustLogParent);
      }
    }
  }
  /** 
 * @return true if this parent is appropriately 1 level above the child.
 */
  private boolean checkParentChildRelationship(  SearchResult result,  int childLevel){
    if (result.childNotResident && !result.exactParentFound) {
      return true;
    }
    int parentLevel=result.parent.getLevel();
    boolean isMapTree=(childLevel & IN.DBMAP_LEVEL) != 0;
    boolean isMainTree=(childLevel & IN.MAIN_LEVEL) != 0;
    boolean checkOk=false;
    if (isMapTree || isMainTree) {
      if (parentLevel == (childLevel + 1)) {
        checkOk=true;
      }
    }
 else {
      if (childLevel == 1) {
        if (parentLevel == 2) {
          checkOk=true;
        }
      }
 else {
        if ((parentLevel == IN.BIN_LEVEL) || (parentLevel == childLevel + 1)) {
          checkOk=true;
        }
      }
    }
    return checkOk;
  }
  private String dumpParentChildInfo(  SearchResult result,  IN parent,  long childNodeId,  int currentLevel,  Tree tree) throws DatabaseException {
    StringBuffer sb=new StringBuffer();
    sb.append("ckptId=").append(checkpointId);
    sb.append(" result=").append(result);
    sb.append(" parent node=").append(parent.getNodeId());
    sb.append(" level=").append(parent.getLevel());
    sb.append(" child node=").append(childNodeId);
    sb.append(" level=").append(currentLevel);
    return sb.toString();
  }
  private boolean logTargetAndUpdateParent(  IN target,  IN parent,  int index,  boolean allowDeltas,  long checkpointStart,  boolean logProvisionally) throws DatabaseException {
    target.latch(false);
    long newLsn=DbLsn.NULL_LSN;
    boolean mustLogParent=true;
    this.hook527(target,parent,allowDeltas,checkpointStart,logProvisionally,newLsn,mustLogParent);
    if (newLsn != DbLsn.NULL_LSN) {
      this.hook533(target);
      parent.updateEntry(index,newLsn);
    }
    return mustLogParent;
  }
  /** 
 * Add a node to the dirty map. The dirty map is keyed by level (Integers)
 * and holds sets of IN references.
 */
  private void addToDirtyMap(  Map dirtyMap,  IN in){
    Integer inLevel=new Integer(in.getLevel());
    Set inSet=(Set)dirtyMap.get(inLevel);
    if (inSet == null) {
      inSet=new HashSet();
      dirtyMap.put(inLevel,inSet);
    }
    inSet.add(new CheckpointReference(in.getDatabase(),in.getNodeId(),in.containsDuplicates(),in.isDbRoot(),in.getMainTreeKey(),in.getDupTreeKey()));
  }
public static class CheckpointReference {
    DatabaseImpl db;
    long nodeId;
    boolean containsDuplicates;
    boolean isDbRoot;
    byte[] mainTreeKey;
    byte[] dupTreeKey;
    public CheckpointReference(    DatabaseImpl db,    long nodeId,    boolean containsDuplicates,    boolean isDbRoot,    byte[] mainTreeKey,    byte[] dupTreeKey){
      this.db=db;
      this.nodeId=nodeId;
      this.containsDuplicates=containsDuplicates;
      this.isDbRoot=isDbRoot;
      this.mainTreeKey=mainTreeKey;
      this.dupTreeKey=dupTreeKey;
    }
    public boolean equals(    Object o){
      if (!(o instanceof CheckpointReference)) {
        return false;
      }
      CheckpointReference other=(CheckpointReference)o;
      return nodeId == other.nodeId;
    }
    public int hashCode(){
      return (int)nodeId;
    }
  }
@MethodObject static class Checkpointer_getWakeupPeriod {
    Checkpointer_getWakeupPeriod(    DbConfigManager configManager){
      this.configManager=configManager;
    }
    long execute() throws IllegalArgumentException, DatabaseException {
      this.hook541();
      this.hook519();
      result=0;
      this.hook540();
      return result;
    }
    protected DbConfigManager configManager;
    protected long wakeupPeriod;
    protected long bytePeriod;
    protected int result;
    protected void hook519() throws IllegalArgumentException, DatabaseException {
    }
    protected void hook540() throws IllegalArgumentException, DatabaseException {
    }
    protected void hook541() throws IllegalArgumentException, DatabaseException {
    }
  }
@MethodObject static class Checkpointer_isRunnable {
    Checkpointer_isRunnable(    Checkpointer _this,    CheckpointConfig config){
      this._this=_this;
      this.config=config;
    }
    boolean execute() throws DatabaseException {
      try {
        useBytesInterval=0;
        useTimeInterval=0;
        nextLsn=DbLsn.NULL_LSN;
        try {
          if (config.getForce()) {
            return true;
          }
 else {
            this.hook543();
          }
          this.hook542();
        }
  finally {
          this.hook521();
        }
        throw ReturnHack.returnBoolean;
      }
 catch (      ReturnBoolean r) {
        return r.value;
      }
    }
    protected Checkpointer _this;
    protected CheckpointConfig config;
    protected long useBytesInterval;
    protected long useTimeInterval;
    protected long nextLsn;
    protected long lastUsedLsn;
    protected StringBuffer sb;
    protected void hook521() throws DatabaseException {
    }
    protected void hook542() throws DatabaseException {
      throw new ReturnBoolean(false);
    }
    protected void hook543() throws DatabaseException {
      this.hook544();
    }
    protected void hook544() throws DatabaseException {
    }
  }
@MethodObject static class Checkpointer_doCheckpoint {
    Checkpointer_doCheckpoint(    Checkpointer _this,    CheckpointConfig config,    boolean flushAll,    String invokingSource){
      this._this=_this;
      this.config=config;
      this.flushAll=flushAll;
      this.invokingSource=invokingSource;
    }
    void execute() throws DatabaseException {
      if (_this.envImpl.isReadOnly()) {
        return;
      }
      if (!_this.isRunnable(config)) {
        return;
      }
      flushExtraLevel=false;
      cleaner=_this.envImpl.getCleaner();
      cleanerFiles=cleaner.getFilesAtCheckpointStart();
      if (cleanerFiles != null) {
        flushExtraLevel=true;
      }
      _this.lastCheckpointMillis=System.currentTimeMillis();
      this.hook535();
      _this.checkpointId++;
      this.hook534();
      success=false;
      this.hook522();
      this.hook548();
      mb=_this.envImpl.getMemoryBudget();
      try {
        this.hook525();
      }
  finally {
        this.hook549();
        this.hook524();
      }
    }
    protected Checkpointer _this;
    protected CheckpointConfig config;
    protected boolean flushAll;
    protected String invokingSource;
    protected boolean flushExtraLevel;
    protected Cleaner cleaner;
    protected Set[] cleanerFiles;
    protected boolean success;
    protected boolean traced;
    protected int dirtyMapMemSize;
    protected MemoryBudget mb;
    protected long checkpointStart;
    protected long firstActiveLsn;
    protected SortedMap dirtyMap;
    protected CheckpointStart startEntry;
    protected int totalSize;
    protected Set nodeSet;
    protected int size;
    protected boolean allowDeltas;
    protected CheckpointEnd endEntry;
    protected void hook522() throws DatabaseException {
    }
    protected void hook523() throws DatabaseException {
    }
    protected void hook524() throws DatabaseException {
    }
    protected void hook525() throws DatabaseException {
      checkpointStart=DbLsn.NULL_LSN;
      firstActiveLsn=DbLsn.NULL_LSN;
      dirtyMap=null;
      this.hook547();
      this.hook551();
      for (Iterator i=dirtyMap.values().iterator(); i.hasNext(); ) {
        nodeSet=(Set)i.next();
        this.hook552();
      }
      this.hook550();
      allowDeltas=!config.getMinimizeRecoveryTime();
      _this.flushDirtyNodes(dirtyMap,flushAll,allowDeltas,flushExtraLevel,checkpointStart);
      _this.flushUtilizationInfo();
      endEntry=new CheckpointEnd(invokingSource,checkpointStart,_this.envImpl.getRootLsn(),firstActiveLsn,Node.getLastId(),_this.envImpl.getDbMapTree().getLastDbId(),_this.envImpl.getTxnManager().getLastTxnId(),_this.checkpointId);
      this.hook523();
      _this.lastCheckpointEnd=_this.logManager.logForceFlush(endEntry,true);
      _this.lastFirstActiveLsn=firstActiveLsn;
      this.hook536();
      _this.highestFlushLevel=IN.MIN_LEVEL;
      success=true;
      if (cleanerFiles != null) {
        cleaner.updateFilesAtCheckpointEnd(cleanerFiles);
      }
    }
    protected void hook534() throws DatabaseException {
    }
    protected void hook535() throws DatabaseException {
    }
    protected void hook536() throws DatabaseException {
    }
    protected void hook547() throws DatabaseException {
      startEntry=new CheckpointStart(_this.checkpointId,invokingSource);
      checkpointStart=_this.logManager.log(startEntry);
      firstActiveLsn=_this.envImpl.getTxnManager().getFirstActiveLsn();
      if (firstActiveLsn == DbLsn.NULL_LSN) {
        firstActiveLsn=checkpointStart;
      }
 else {
        if (DbLsn.compareTo(checkpointStart,firstActiveLsn) < 0) {
          firstActiveLsn=checkpointStart;
        }
      }
      dirtyMap=_this.selectDirtyINs(flushAll,flushExtraLevel);
    }
    protected void hook548() throws DatabaseException {
    }
    protected void hook549() throws DatabaseException {
    }
    protected void hook550() throws DatabaseException {
    }
    protected void hook551() throws DatabaseException {
    }
    protected void hook552() throws DatabaseException {
    }
  }
@MethodObject static class Checkpointer_selectDirtyINs {
    Checkpointer_selectDirtyINs(    Checkpointer _this,    boolean flushAll,    boolean flushExtraLevel){
      this._this=_this;
      this.flushAll=flushAll;
      this.flushExtraLevel=flushExtraLevel;
    }
    SortedMap execute() throws DatabaseException {
      newDirtyMap=new TreeMap();
      inMemINs=_this.envImpl.getInMemoryINs();
      this.hook529();
      this.hook553();
      this.hook528();
      return newDirtyMap;
    }
    protected Checkpointer _this;
    protected boolean flushAll;
    protected boolean flushExtraLevel;
    protected SortedMap newDirtyMap;
    protected INList inMemINs;
    protected long totalSize;
    protected MemoryBudget mb;
    protected Iterator iter;
    protected IN in;
    protected Integer level;
    protected Set dirtySet;
    protected void hook528() throws DatabaseException {
      iter=inMemINs.iterator();
      while (iter.hasNext()) {
        in=(IN)iter.next();
        in.latch(false);
        this.hook530();
      }
      this.hook554();
      if (newDirtyMap.size() > 0) {
        if (flushAll) {
          _this.highestFlushLevel=_this.envImpl.getDbMapTree().getHighestLevel();
        }
 else {
          _this.highestFlushLevel=((Integer)newDirtyMap.lastKey()).intValue();
          if (flushExtraLevel) {
            _this.highestFlushLevel+=1;
          }
        }
      }
 else {
        _this.highestFlushLevel=IN.MAX_LEVEL;
      }
    }
    protected void hook529() throws DatabaseException {
    }
    protected void hook530() throws DatabaseException {
      if (in.getDirty()) {
        level=new Integer(in.getLevel());

        if (newDirtyMap.containsKey(level)) {
          dirtySet=(Set)newDirtyMap.get(level);
        }
 else {
          dirtySet=new HashSet();
          newDirtyMap.put(level,dirtySet);
        }
        dirtySet.add(new CheckpointReference(in.getDatabase(),in.getNodeId(),in.containsDuplicates(),in.isDbRoot(),in.getMainTreeKey(),in.getDupTreeKey()));
      }
    }
    protected void hook553() throws DatabaseException {
    }
    protected void hook554() throws DatabaseException {
    }
  }
  protected void hook520() throws DatabaseException {
  }
  protected void hook526(  CheckpointReference targetRef,  Map dirtyMap,  int currentLevel,  boolean logProvisionally,  boolean allowDeltas,  long checkpointStart,  Tree tree,  SearchResult result,  boolean mustLogParent) throws DatabaseException {
    if (result.exactParentFound) {
      IN renewedTarget=(IN)result.parent.getTarget(result.index);
      if (renewedTarget == null) {
        mustLogParent=true;
      }
 else {
        mustLogParent=logTargetAndUpdateParent(renewedTarget,result.parent,result.index,allowDeltas,checkpointStart,logProvisionally);
      }
    }
 else {
      if (result.childNotResident) {
        if (result.parent.getLevel() > currentLevel) {
          mustLogParent=true;
        }
      }
    }
    if (mustLogParent) {
      assert checkParentChildRelationship(result,currentLevel) : dumpParentChildInfo(result,result.parent,targetRef.nodeId,currentLevel,tree);
      addToDirtyMap(dirtyMap,result.parent);
    }
  }
  protected void hook527(  IN target,  IN parent,  boolean allowDeltas,  long checkpointStart,  boolean logProvisionally,  long newLsn,  boolean mustLogParent) throws DatabaseException {
    if (target.getDirty()) {
      newLsn=target.log(logManager,allowDeltas,logProvisionally,true,parent);
      if (allowDeltas && newLsn == DbLsn.NULL_LSN) {
        this.hook537();
        long lastFullLsn=target.getLastFullVersion();
        if (DbLsn.compareTo(lastFullLsn,checkpointStart) < 0) {
          mustLogParent=false;
        }
      }
    }
  }
  protected void hook531() throws DatabaseException {
  }
  protected void hook532() throws DatabaseException {
  }
  protected void hook533(  IN target) throws DatabaseException {
  }
  protected void hook537() throws DatabaseException {
  }
  protected void hook538(  EnvironmentImpl envImpl,  long waitTime,  String name) throws DatabaseException {
  }
  protected void hook539(  EnvironmentImpl envImpl) throws DatabaseException {
  }
  protected void hook545(  long waitTime) throws DatabaseException {
  }
  protected void hook546(  SortedMap dirtyMap,  boolean allowDeltas,  long checkpointStart,  Integer currentLevel,  boolean logProvisionally,  CheckpointReference targetRef) throws DatabaseException {
  }
}
\00base/com/sleepycat/je/recovery/RecoveryManager.java:package com.sleepycat.je.recovery;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.CheckpointConfig;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.DbInternal;
import com.sleepycat.je.TransactionConfig;
import com.sleepycat.je.cleaner.UtilizationTracker;
import com.sleepycat.je.config.EnvironmentParams;
import com.sleepycat.je.dbi.DatabaseId;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.DbConfigManager;
import com.sleepycat.je.dbi.DbTree;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.log.CheckpointFileReader;
import com.sleepycat.je.log.FileManager;
import com.sleepycat.je.log.INFileReader;
import com.sleepycat.je.log.LNFileReader;
import com.sleepycat.je.log.LastFileReader;
import com.sleepycat.je.log.LogEntryType;
import com.sleepycat.je.tree.BIN;
import com.sleepycat.je.tree.ChildReference;
import com.sleepycat.je.tree.DIN;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Key;
import com.sleepycat.je.tree.LN;
import com.sleepycat.je.tree.Node;
import com.sleepycat.je.tree.SearchResult;
import com.sleepycat.je.tree.TrackingInfo;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.TreeLocation;
import com.sleepycat.je.tree.WithRootLatched;
import com.sleepycat.je.txn.LockType;
import com.sleepycat.je.txn.Txn;
import com.sleepycat.je.utilint.DbLsn;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
public class RecoveryManager {
  private static final String TRACE_DUP_ROOT_REPLACE="DupRootRecover:";
  private static final String TRACE_LN_REDO="LNRedo:";
  private static final String TRACE_LN_UNDO="LNUndo";
  private static final String TRACE_IN_REPLACE="INRecover:";
  private static final String TRACE_ROOT_REPLACE="RootRecover:";
  private static final String TRACE_IN_DEL_REPLAY="INDelReplay:";
  private static final String TRACE_IN_DUPDEL_REPLAY="INDupDelReplay:";
  private static final String TRACE_ROOT_DELETE="RootDelete:";
  private static final int CLEAR_INCREMENT=50;
  private EnvironmentImpl env;
  private int readBufferSize;
  private RecoveryInfo info;
  private Set committedTxnIds;
  private Set abortedTxnIds;
  private Map preparedTxns;
  private Set inListRebuildDbIds;
  private Level detailedTraceLevel;
  private Map fileSummaryLsns;
  private int inListClearCounter;
  /** 
 * Make a recovery manager
 */
  public RecoveryManager(  EnvironmentImpl env) throws DatabaseException {
    this.env=env;
    DbConfigManager cm=env.getConfigManager();
    readBufferSize=cm.getInt(EnvironmentParams.LOG_ITERATOR_READ_SIZE);
    committedTxnIds=new HashSet();
    abortedTxnIds=new HashSet();
    preparedTxns=new HashMap();
    inListRebuildDbIds=new HashSet();
    fileSummaryLsns=new HashMap();
    this.hook578(env);
  }
  /** 
 * Look for an existing log and use it to create an in memory structure for
 * accessing existing databases. The file manager and logging system are
 * only available after recovery.
 * @return RecoveryInfo statistics about the recovery process.
 */
  public RecoveryInfo recover(  boolean readOnly) throws DatabaseException {
    info=new RecoveryInfo();
    try {
      FileManager fileManager=env.getFileManager();
      DbConfigManager configManager=env.getConfigManager();
      boolean forceCheckpoint=configManager.getBoolean(EnvironmentParams.ENV_RECOVERY_FORCE_CHECKPOINT);
      if (fileManager.filesExist()) {
        findEndOfLog(readOnly);
        this.hook559();
        findLastCheckpoint();
        env.getLogManager().setLastLsnAtRecovery(fileManager.getLastUsedLsn());
        this.hook558();
        env.readMapTreeFromLog(info.useRootLsn);
        buildTree();
      }
 else {
        this.hook556();
        this.hook560();
        env.logMapTreeRoot();
        forceCheckpoint=true;
      }
      if (preparedTxns.size() > 0) {
        this.hook573();
        preparedTxns=null;
      }
      if (DbInternal.getCreateUP(env.getConfigManager().getEnvironmentConfig())) {
        env.getUtilizationProfile().populateCache();
      }
      if (!readOnly && (env.getLogManager().getLastLsnAtRecovery() != info.checkpointEndLsn || forceCheckpoint)) {
        CheckpointConfig config=new CheckpointConfig();
        config.setForce(true);
        config.setMinimizeRecoveryTime(true);
        env.invokeCheckpoint(config,false,"recovery");
      }
    }
 catch (    IOException e) {
      this.hook575(e);
      throw new RecoveryException(env,"Couldn't recover: " + e.getMessage(),e);
    }
 finally {
      Tracer.trace(Level.CONFIG,env,"Recovery finished: " + info);
    }
    return info;
  }
  /** 
 * Find the end of the log, initialize the FileManager. While we're perusing
 * the log, return the last checkpoint LSN if we happen to see it.
 */
  private void findEndOfLog(  boolean readOnly) throws IOException, DatabaseException {
    LastFileReader reader=new LastFileReader(env,readBufferSize);
    while (reader.readNextEntry()) {
      LogEntryType type=reader.getEntryType();
      if (LogEntryType.LOG_CKPT_END.equals(type)) {
        info.checkpointEndLsn=reader.getLastLsn();
        info.partialCheckpointStartLsn=DbLsn.NULL_LSN;
      }
 else       if (LogEntryType.LOG_CKPT_START.equals(type)) {
        if (info.partialCheckpointStartLsn == DbLsn.NULL_LSN) {
          info.partialCheckpointStartLsn=reader.getLastLsn();
        }
      }
    }
    assert (reader.getLastValidLsn() != reader.getEndOfLog()) : "lastUsed=" + DbLsn.getNoFormatString(reader.getLastValidLsn()) + " end="+ DbLsn.getNoFormatString(reader.getEndOfLog());
    if (!readOnly) {
      reader.setEndOfFile();
    }
    info.lastUsedLsn=reader.getLastValidLsn();
    info.nextAvailableLsn=reader.getEndOfLog();
    info.nRepeatIteratorReads+=reader.getNRepeatIteratorReads();
    env.getFileManager().setLastPosition(info.nextAvailableLsn,info.lastUsedLsn,reader.getPrevOffset());
  }
  /** 
 * Find the last checkpoint and establish the firstActiveLsn point,
 * checkpoint start, and checkpoint end.
 */
  private void findLastCheckpoint() throws IOException, DatabaseException {
    if (info.checkpointEndLsn == DbLsn.NULL_LSN) {
      CheckpointFileReader searcher=new CheckpointFileReader(env,readBufferSize,false,info.lastUsedLsn,DbLsn.NULL_LSN,info.nextAvailableLsn);
      while (searcher.readNextEntry()) {
        if (searcher.isCheckpointEnd()) {
          info.checkpointEndLsn=searcher.getLastLsn();
          break;
        }
 else         if (searcher.isCheckpointStart()) {
          info.partialCheckpointStartLsn=searcher.getLastLsn();
        }
 else         if (searcher.isRoot()) {
          if (info.useRootLsn == DbLsn.NULL_LSN) {
            info.useRootLsn=searcher.getLastLsn();
          }
        }
      }
      info.nRepeatIteratorReads+=searcher.getNRepeatIteratorReads();
    }
    if (info.checkpointEndLsn == DbLsn.NULL_LSN) {
      info.checkpointStartLsn=DbLsn.NULL_LSN;
      info.firstActiveLsn=DbLsn.NULL_LSN;
    }
 else {
      CheckpointEnd checkpointEnd=(CheckpointEnd)(env.getLogManager().get(info.checkpointEndLsn));
      info.checkpointEnd=checkpointEnd;
      info.checkpointStartLsn=checkpointEnd.getCheckpointStartLsn();
      info.firstActiveLsn=checkpointEnd.getFirstActiveLsn();
      if (checkpointEnd.getRootLsn() != DbLsn.NULL_LSN) {
        info.useRootLsn=checkpointEnd.getRootLsn();
      }
      env.getCheckpointer().setCheckpointId(checkpointEnd.getId());
      env.getCheckpointer().setFirstActiveLsn(checkpointEnd.getFirstActiveLsn());
    }
    if (info.useRootLsn == DbLsn.NULL_LSN) {
      throw new RecoveryException(env,"This environment's log file has no root. Since the root " + "is the first entry written into a log at environment " + "creation, this should only happen if the initial creation "+ "of the environment was never checkpointed or synced. "+ "Please move aside the existing log files to allow the "+ "creation of a new environment");
    }
  }
  /** 
 * Use the log to recreate an in memory tree.
 */
  private void buildTree() throws IOException, DatabaseException {
    inListClearCounter=0;
    this.hook572();
    long start=System.currentTimeMillis();
    readINsAndTrackIds(info.checkpointStartLsn);
    long end=System.currentTimeMillis();
    this.hook571(start,end);
    start=System.currentTimeMillis();
    info.numOtherINs+=readINs(info.checkpointStartLsn,true,LogEntryType.LOG_BIN_DELTA,null,null,true);
    end=System.currentTimeMillis();
    this.hook570(start,end);
    start=System.currentTimeMillis();
    Set mapLNSet=new HashSet();
    mapLNSet.add(LogEntryType.LOG_MAPLN_TRANSACTIONAL);
    mapLNSet.add(LogEntryType.LOG_TXN_COMMIT);
    mapLNSet.add(LogEntryType.LOG_TXN_ABORT);
    mapLNSet.add(LogEntryType.LOG_TXN_PREPARE);
    undoLNs(info,mapLNSet);
    end=System.currentTimeMillis();
    this.hook569(start,end);
    start=System.currentTimeMillis();
    mapLNSet.add(LogEntryType.LOG_MAPLN);
    redoLNs(info,mapLNSet);
    end=System.currentTimeMillis();
    this.hook568(start,end);
    start=System.currentTimeMillis();
    info.numOtherINs+=readINs(info.checkpointStartLsn,false,LogEntryType.LOG_IN,LogEntryType.LOG_BIN,LogEntryType.LOG_IN_DELETE_INFO,false);
    end=System.currentTimeMillis();
    this.hook567(start,end);
    start=System.currentTimeMillis();
    info.numBinDeltas=readINs(info.checkpointStartLsn,false,LogEntryType.LOG_BIN_DELTA,null,null,true);
    end=System.currentTimeMillis();
    this.hook566(start,end);
    start=System.currentTimeMillis();
    info.numDuplicateINs+=readINs(info.checkpointStartLsn,false,LogEntryType.LOG_DIN,LogEntryType.LOG_DBIN,LogEntryType.LOG_IN_DUPDELETE_INFO,true);
    end=System.currentTimeMillis();
    this.hook565(start,end);
    start=System.currentTimeMillis();
    info.numBinDeltas+=readINs(info.checkpointStartLsn,false,LogEntryType.LOG_DUP_BIN_DELTA,null,null,true);
    end=System.currentTimeMillis();
    this.hook564(start,end);
    rebuildINList();
    this.hook596();
    this.hook563();
    start=System.currentTimeMillis();
    Set lnSet=new HashSet();
    lnSet.add(LogEntryType.LOG_LN_TRANSACTIONAL);
    lnSet.add(LogEntryType.LOG_NAMELN_TRANSACTIONAL);
    lnSet.add(LogEntryType.LOG_DEL_DUPLN_TRANSACTIONAL);
    lnSet.add(LogEntryType.LOG_DUPCOUNTLN_TRANSACTIONAL);
    undoLNs(info,lnSet);
    end=System.currentTimeMillis();
    this.hook562(start,end);
    start=System.currentTimeMillis();
    lnSet.add(LogEntryType.LOG_LN);
    lnSet.add(LogEntryType.LOG_NAMELN);
    lnSet.add(LogEntryType.LOG_DEL_DUPLN);
    lnSet.add(LogEntryType.LOG_DUPCOUNTLN);
    lnSet.add(LogEntryType.LOG_FILESUMMARYLN);
    redoLNs(info,lnSet);
    end=System.currentTimeMillis();
    this.hook561(start,end);
  }
  private void readINsAndTrackIds(  long rollForwardLsn) throws IOException, DatabaseException {
    INFileReader reader=new INFileReader(env,readBufferSize,rollForwardLsn,info.nextAvailableLsn,true,false,info.partialCheckpointStartLsn,fileSummaryLsns);
    reader.addTargetType(LogEntryType.LOG_IN);
    reader.addTargetType(LogEntryType.LOG_BIN);
    reader.addTargetType(LogEntryType.LOG_IN_DELETE_INFO);
    this.hook593(reader);
    try {
      info.numMapINs=0;
      DbTree dbMapTree=env.getDbMapTree();
      while (reader.readNextEntry()) {
        DatabaseId dbId=reader.getDatabaseId();
        if (dbId.equals(DbTree.ID_DB_ID)) {
          DatabaseImpl db=dbMapTree.getDb(dbId);
          replayOneIN(reader,db,false);
          info.numMapINs++;
        }
      }
      info.useMaxNodeId=reader.getMaxNodeId();
      info.useMaxDbId=reader.getMaxDbId();
      info.useMaxTxnId=reader.getMaxTxnId();
      if (info.checkpointEnd != null) {
        if (info.useMaxNodeId < info.checkpointEnd.getLastNodeId()) {
          info.useMaxNodeId=info.checkpointEnd.getLastNodeId();
        }
        if (info.useMaxDbId < info.checkpointEnd.getLastDbId()) {
          info.useMaxDbId=info.checkpointEnd.getLastDbId();
        }
        if (info.useMaxTxnId < info.checkpointEnd.getLastTxnId()) {
          info.useMaxTxnId=info.checkpointEnd.getLastTxnId();
        }
      }
      Node.setLastNodeId(info.useMaxNodeId);
      env.getDbMapTree().setLastDbId(info.useMaxDbId);
      env.getTxnManager().setLastTxnId(info.useMaxTxnId);
      info.nRepeatIteratorReads+=reader.getNRepeatIteratorReads();
    }
 catch (    Exception e) {
      traceAndThrowException(reader.getLastLsn(),"readMapIns",e);
    }
  }
  /** 
 * Read INs and process.
 */
  private int readINs(  long rollForwardLsn,  boolean mapDbOnly,  LogEntryType inType1,  LogEntryType inType2,  LogEntryType inType3,  boolean requireExactMatch) throws IOException, DatabaseException {
    INFileReader reader=new INFileReader(env,readBufferSize,rollForwardLsn,info.nextAvailableLsn,false,mapDbOnly,info.partialCheckpointStartLsn,fileSummaryLsns);
    if (inType1 != null) {
      reader.addTargetType(inType1);
    }
    if (inType2 != null) {
      reader.addTargetType(inType2);
    }
    if (inType3 != null) {
      reader.addTargetType(inType3);
    }
    int numINsSeen=0;
    try {
      DbTree dbMapTree=env.getDbMapTree();
      while (reader.readNextEntry()) {
        DatabaseId dbId=reader.getDatabaseId();
        boolean isMapDb=dbId.equals(DbTree.ID_DB_ID);
        boolean isTarget=false;
        if (mapDbOnly && isMapDb) {
          isTarget=true;
        }
 else         if (!mapDbOnly && !isMapDb) {
          isTarget=true;
        }
        if (isTarget) {
          DatabaseImpl db=dbMapTree.getDb(dbId);
          if (db == null) {
          }
 else {
            replayOneIN(reader,db,requireExactMatch);
            numINsSeen++;
            inListRebuildDbIds.add(dbId);
          }
        }
      }
      info.nRepeatIteratorReads+=reader.getNRepeatIteratorReads();
      return numINsSeen;
    }
 catch (    Exception e) {
      traceAndThrowException(reader.getLastLsn(),"readNonMapIns",e);
      return 0;
    }
  }
  /** 
 * Get an IN from the reader, set its database, and fit into tree.
 */
  private void replayOneIN(  INFileReader reader,  DatabaseImpl db,  boolean requireExactMatch) throws DatabaseException {
    if (reader.isDeleteInfo()) {
      replayINDelete(db,reader.getDeletedNodeId(),false,reader.getDeletedIdKey(),null,reader.getLastLsn());
    }
 else     if (reader.isDupDeleteInfo()) {
      replayINDelete(db,reader.getDupDeletedNodeId(),true,reader.getDupDeletedMainKey(),reader.getDupDeletedDupKey(),reader.getLastLsn());
    }
 else {
      IN in=reader.getIN();
      long inLsn=reader.getLsnOfIN();
      in.postRecoveryInit(db,inLsn);
      this.hook585(in);
      replaceOrInsert(db,in,reader.getLastLsn(),inLsn,requireExactMatch);
    }
    if ((++inListClearCounter % CLEAR_INCREMENT) == 0) {
      env.getInMemoryINs().clear();
    }
  }
  /** 
 * Undo all aborted LNs. To do so, walk the log backwards, keeping a
 * collection of committed txns. If we see a log entry that doesn't have a
 * committed txn, undo it.
 */
  private void undoLNs(  RecoveryInfo info,  Set lnTypes) throws IOException, DatabaseException {
    long firstActiveLsn=info.firstActiveLsn;
    long lastUsedLsn=info.lastUsedLsn;
    long endOfFileLsn=info.nextAvailableLsn;
    LNFileReader reader=new LNFileReader(env,readBufferSize,lastUsedLsn,false,endOfFileLsn,firstActiveLsn,null);
    Iterator iter=lnTypes.iterator();
    while (iter.hasNext()) {
      LogEntryType lnType=(LogEntryType)iter.next();
      reader.addTargetType(lnType);
    }
    Map countedFileSummaries=new HashMap();
    Set countedAbortLsnNodes=new HashSet();
    DbTree dbMapTree=env.getDbMapTree();
    TreeLocation location=new TreeLocation();
    try {
      while (reader.readNextEntry()) {
        if (reader.isLN()) {
          Long txnId=reader.getTxnId();
          if (txnId != null && !committedTxnIds.contains(txnId)) {
            this.hook597();
            LN ln=reader.getLN();
            long logLsn=reader.getLastLsn();
            long abortLsn=reader.getAbortLsn();
            boolean abortKnownDeleted=reader.getAbortKnownDeleted();
            DatabaseId dbId=reader.getDatabaseId();
            DatabaseImpl db=dbMapTree.getDb(dbId);
            if (db != null) {
              ln.postFetchInit(db,logLsn);
              this.hook586(info,reader,location,ln,logLsn,abortLsn,abortKnownDeleted,db);
              TxnNodeId txnNodeId=new TxnNodeId(reader.getNodeId(),txnId.longValue());
              undoUtilizationInfo(ln,logLsn,abortLsn,abortKnownDeleted,txnNodeId,countedFileSummaries,countedAbortLsnNodes);
              inListRebuildDbIds.add(dbId);
            }
          }
        }
 else         if (reader.isPrepare()) {
          long prepareId=reader.getTxnPrepareId();
          Long prepareIdL=new Long(prepareId);
          if (!committedTxnIds.contains(prepareIdL) && !abortedTxnIds.contains(prepareIdL)) {
            TransactionConfig txnConf=new TransactionConfig();
            Txn preparedTxn=new Txn(env,txnConf,prepareId);
            preparedTxn.setLockTimeout(0);
            preparedTxns.put(prepareIdL,preparedTxn);
            env.getTxnManager().registerXATxn(reader.getTxnPrepareXid(),preparedTxn,true);
            this.hook574(reader);
          }
        }
 else         if (reader.isAbort()) {
          abortedTxnIds.add(new Long(reader.getTxnAbortId()));
        }
 else {
          committedTxnIds.add(new Long(reader.getTxnCommitId()));
        }
      }
      info.nRepeatIteratorReads+=reader.getNRepeatIteratorReads();
    }
 catch (    Exception e) {
      traceAndThrowException(reader.getLastLsn(),"undoLNs",e);
    }
  }
  /** 
 * Apply all committed LNs.
 * @param rollForwardLsnstart redoing from this point
 * @param lnType1targetted LN
 * @param lnType2targetted LN
 */
  private void redoLNs(  RecoveryInfo info,  Set lnTypes) throws IOException, DatabaseException {
    long endOfFileLsn=info.nextAvailableLsn;
    long rollForwardLsn=info.checkpointStartLsn;
    LNFileReader reader=new LNFileReader(env,readBufferSize,rollForwardLsn,true,DbLsn.NULL_LSN,endOfFileLsn,null);
    Iterator iter=lnTypes.iterator();
    while (iter.hasNext()) {
      LogEntryType lnType=(LogEntryType)iter.next();
      reader.addTargetType(lnType);
    }
    Set countedAbortLsnNodes=new HashSet();
    DbTree dbMapTree=env.getDbMapTree();
    TreeLocation location=new TreeLocation();
    try {
      while (reader.readNextEntry()) {
        if (reader.isLN()) {
          Long txnId=reader.getTxnId();
          boolean processThisLN=false;
          boolean lnIsCommitted=false;
          boolean lnIsPrepared=false;
          Txn preparedTxn=null;
          if (txnId == null) {
            processThisLN=true;
          }
 else {
            lnIsCommitted=committedTxnIds.contains(txnId);
            if (!lnIsCommitted) {
              preparedTxn=(Txn)preparedTxns.get(txnId);
              lnIsPrepared=preparedTxn != null;
            }
            if (lnIsCommitted || lnIsPrepared) {
              processThisLN=true;
            }
          }
          if (processThisLN) {
            this.hook598();
            LN ln=reader.getLN();
            DatabaseId dbId=reader.getDatabaseId();
            DatabaseImpl db=dbMapTree.getDb(dbId);
            long logLsn=reader.getLastLsn();
            long treeLsn=DbLsn.NULL_LSN;
            if (db != null) {
              ln.postFetchInit(db,logLsn);
              if (preparedTxn != null) {
                preparedTxn.addLogInfo(logLsn);
                preparedTxn.lock(ln.getNodeId(),LockType.WRITE,false,db);
                preparedTxn.setPrepared(true);
              }
              treeLsn=redo(db,location,ln,reader.getKey(),reader.getDupTreeKey(),logLsn,info);
              inListRebuildDbIds.add(dbId);
            }
            TxnNodeId txnNodeId=null;
            if (txnId != null) {
              txnNodeId=new TxnNodeId(reader.getNodeId(),txnId.longValue());
            }
            redoUtilizationInfo(logLsn,treeLsn,reader.getAbortLsn(),reader.getAbortKnownDeleted(),ln,txnNodeId,countedAbortLsnNodes);
          }
        }
      }
      info.nRepeatIteratorReads+=reader.getNRepeatIteratorReads();
    }
 catch (    Exception e) {
      traceAndThrowException(reader.getLastLsn(),"redoLns",e);
    }
  }
  /** 
 * Rebuild the in memory inList with INs that have been made resident by the
 * recovery process.
 */
  private void rebuildINList() throws DatabaseException {
    env.getInMemoryINs().clear();
    env.getDbMapTree().rebuildINListMapDb();
    Iterator iter=inListRebuildDbIds.iterator();
    while (iter.hasNext()) {
      DatabaseId dbId=(DatabaseId)iter.next();
      if (!dbId.equals(DbTree.ID_DB_ID)) {
        DatabaseImpl db=env.getDbMapTree().getDb(dbId);
        if (db != null) {
          db.getTree().rebuildINList();
        }
      }
    }
  }
private static class TxnNodeId {
    long nodeId;
    long txnId;
    TxnNodeId(    long nodeId,    long txnId){
      this.nodeId=nodeId;
      this.txnId=txnId;
    }
    /** 
 * Compare two TxnNodeId objects
 */
    public boolean equals(    Object obj){
      if (this == obj) {
        return true;
      }
      if (!(obj instanceof TxnNodeId)) {
        return false;
      }
      return ((((TxnNodeId)obj).txnId == txnId) && (((TxnNodeId)obj).nodeId == nodeId));
    }
    public int hashCode(){
      return (int)(txnId + nodeId);
    }
    public String toString(){
      return "txnId=" + txnId + "/nodeId="+ nodeId;
    }
  }
  /** 
 * Recover an internal node. If inFromLog is: - not found, insert it in the
 * appropriate location. - if found and there is a physical match (LSNs are
 * the same) do nothing. - if found and there is a logical match (LSNs are
 * different, another version of this IN is in place, replace the found node
 * with the node read from the log only if the log version's LSN is greater.
 * InFromLog should be latched upon entering this method and it will not be
 * latched upon exiting.
 * @param inFromLog -
 * the new node to put in the tree. The identifier key and node
 * id are used to find the existing version of the node.
 * @param logLsn -
 * the location of log entry in in the log.
 * @param inLsnLSN of this in -- may not be the same as the log LSN if the
 * current entry is a BINDelta
 * @param requireExactMatch -
 * true if we won't place this node in the tree unless we find
 * exactly that parent. Used for BINDeltas, where we want to only
 * apply the BINDelta to that exact node.
 */
  private void replaceOrInsert(  DatabaseImpl db,  IN inFromLog,  long logLsn,  long inLsn,  boolean requireExactMatch) throws DatabaseException {
    List trackingList=null;
    try {
      if (inFromLog.isRoot()) {
        if (inFromLog.containsDuplicates()) {
          replaceOrInsertDuplicateRoot(db,(DIN)inFromLog,logLsn);
        }
 else {
          replaceOrInsertRoot(db,inFromLog,logLsn);
        }
      }
 else {
        trackingList=new ArrayList();
        replaceOrInsertChild(db,inFromLog,logLsn,inLsn,trackingList,requireExactMatch);
      }
    }
 catch (    Exception e) {
      String trace=printTrackList(trackingList);
      this.hook576(db,logLsn,e,trace);
      throw new DatabaseException("lsnFromLog=" + DbLsn.getNoFormatString(logLsn),e);
    }
 finally {
      this.hook587(inFromLog,logLsn);
    }
  }
  /** 
 * Dump a tracking list into a string.
 */
  private String printTrackList(  List trackingList){
    if (trackingList != null) {
      StringBuffer sb=new StringBuffer();
      Iterator iter=trackingList.iterator();
      sb.append("Trace list:");
      sb.append('\n');
      while (iter.hasNext()) {
        sb.append((TrackingInfo)iter.next());
        sb.append('\n');
      }
      return sb.toString();
    }
 else {
      return null;
    }
  }
  /** 
 * Replay an IN delete. Remove an entry from an IN to reflect a reverse
 * split.
 */
  private void replayINDelete(  DatabaseImpl db,  long nodeId,  boolean containsDuplicates,  byte[] mainKey,  byte[] dupKey,  long logLsn) throws DatabaseException {
    boolean found=false;
    boolean deleted=false;
    Tree tree=db.getTree();
    SearchResult result=new SearchResult();
    try {
      result=db.getTree().getParentINForChildIN(nodeId,containsDuplicates,false,mainKey,dupKey,false,false,-1,null,true);
      if (result.parent == null) {
        tree.withRootLatchedExclusive(new RootDeleter(tree));
        DbTree dbTree=db.getDbEnvironment().getDbMapTree();
        dbTree.modifyDbRoot(db);
        this.hook557(db);
        deleted=true;
      }
 else       if (result.exactParentFound) {
        found=true;
        deleted=result.parent.deleteEntry(result.index,false);
      }
    }
  finally {
      this.hook588(result);
      this.hook579(nodeId,containsDuplicates,logLsn,found,deleted,result);
    }
  }
private static class RootDeleter implements WithRootLatched {
    Tree tree;
    RootDeleter(    Tree tree){
      this.tree=tree;
    }
    /** 
 * @return true if the in-memory root was replaced.
 */
    public IN doWork(    ChildReference root) throws DatabaseException {
      tree.setRoot(null,false);
      return null;
    }
  }
  /** 
 * If the root of this tree is null, use this IN from the log as a root.
 * Note that we should really also check the LSN of the mapLN, because
 * perhaps the root is null because it's been deleted. However, the replay
 * of all the LNs will end up adjusting the tree correctly.
 * If there is a root, check if this IN is a different LSN and if so,
 * replace it.
 */
  private void replaceOrInsertRoot(  DatabaseImpl db,  IN inFromLog,  long lsn) throws DatabaseException {
    boolean success=true;
    Tree tree=db.getTree();
    RootUpdater rootUpdater=new RootUpdater(tree,inFromLog,lsn);
    try {
      tree.withRootLatchedExclusive(rootUpdater);
      if (rootUpdater.updateDone()) {
        EnvironmentImpl env=db.getDbEnvironment();
        env.getDbMapTree().modifyDbRoot(db);
      }
    }
 catch (    Exception e) {
      success=false;
      throw new DatabaseException("lsnFromLog=" + DbLsn.getNoFormatString(lsn),e);
    }
 finally {
      this.hook580(db,inFromLog,lsn,success,rootUpdater);
    }
  }
  /** 
 * Recover this root of a duplicate tree.
 */
  private void replaceOrInsertDuplicateRoot(  DatabaseImpl db,  DIN inFromLog,  long lsn) throws DatabaseException {
    boolean found=true;
    boolean inserted=false;
    boolean replaced=false;
    long origLsn=DbLsn.NULL_LSN;
    byte[] mainTreeKey=inFromLog.getMainTreeKey();
    IN parent=null;
    int index=-1;
    boolean success=false;
    try {
      parent=db.getTree().searchSplitsAllowed(mainTreeKey,-1,true);
      assert parent instanceof BIN;
      ChildReference newRef=new ChildReference(inFromLog,mainTreeKey,lsn);
      index=parent.insertEntry1(newRef);
      if ((index >= 0 && (index & IN.EXACT_MATCH) != 0)) {
        index&=~IN.EXACT_MATCH;
        if (parent.isEntryKnownDeleted(index)) {
          parent.setEntry(index,inFromLog,mainTreeKey,lsn,(byte)0);
          replaced=true;
        }
 else {
          origLsn=parent.getLsn(index);
          if (DbLsn.compareTo(origLsn,lsn) < 0) {
            parent.setEntry(index,inFromLog,mainTreeKey,lsn,parent.getState(index));
            replaced=true;
          }
        }
      }
 else {
        found=false;
      }
      success=true;
    }
  finally {
      this.hook589(parent);
      this.hook581(db,inFromLog,lsn,found,inserted,replaced,origLsn,parent,index,success);
    }
  }
  private void replaceOrInsertChild(  DatabaseImpl db,  IN inFromLog,  long logLsn,  long inLsn,  List trackingList,  boolean requireExactMatch) throws DatabaseException {
    boolean inserted=false;
    boolean replaced=false;
    long origLsn=DbLsn.NULL_LSN;
    boolean success=false;
    SearchResult result=new SearchResult();
    try {
      result=db.getTree().getParentINForChildIN(inFromLog,requireExactMatch,false,-1,trackingList);
      if (result.parent == null) {
        return;
      }
      if (result.index >= 0) {
        if (result.parent.getLsn(result.index) == logLsn) {
        }
 else {
          if (result.exactParentFound) {
            origLsn=result.parent.getLsn(result.index);
            if (DbLsn.compareTo(origLsn,logLsn) < 0) {
              result.parent.updateEntry(result.index,inFromLog,inLsn);
              replaced=true;
            }
          }
        }
      }
      success=true;
    }
  finally {
      this.hook590(result);
      this.hook582(db,inFromLog,logLsn,inserted,replaced,origLsn,success,result);
    }
  }
  /** 
 * Redo a committed LN for recovery.
 * <pre>
 * log LN found  | logLSN &gt; LSN | LN is deleted | action
 * in tree     | in tree      |               |
 * --------------+--------------+---------------+------------------------
 * Y         |    N         |    n/a        | no action
 * --------------+--------------+---------------+------------------------
 * Y         |    Y         |     N         | replace w/log LSN
 * --------------+--------------+---------------+------------------------
 * Y         |    Y         |     Y         | replace w/log LSN, put
 * |              |               | on compressor queue
 * --------------+--------------+---------------+------------------------
 * N         |    n/a       |     N         | insert into tree
 * --------------+--------------+---------------+------------------------
 * N         |    n/a       |     Y         | no action
 * --------------+--------------+---------------+------------------------
 * </pre>
 * @param locationholds state about the search in the tree. Passed in from the
 * recovery manager to reduce objection creation overhead.
 * @param lnFromLog -
 * the new node to put in the tree.
 * @param mainKeyis the key that navigates us through the main tree
 * @param dupTreeKeyis the key that navigates us through the duplicate tree
 * @param logLsnis the LSN from the just-read log entry
 * @param infois a recovery stats object.
 * @return the LSN found in the tree, or null if not found.
 */
  private long redo(  DatabaseImpl db,  TreeLocation location,  LN lnFromLog,  byte[] mainKey,  byte[] dupKey,  long logLsn,  RecoveryInfo info) throws DatabaseException {
    boolean found=false;
    boolean replaced=false;
    boolean inserted=false;
    boolean success=false;
    try {
      location.reset();
      found=db.getTree().getParentBINForChildLN(location,mainKey,dupKey,lnFromLog,true,false,true,true);
      if (!found && (location.bin == null)) {
        success=true;
        return DbLsn.NULL_LSN;
      }
      if (lnFromLog.containsDuplicates()) {
        if (found) {
          DIN duplicateRoot=(DIN)location.bin.fetchTarget(location.index);
          if (DbLsn.compareTo(logLsn,location.childLsn) >= 0) {
            duplicateRoot.updateDupCountLNRefAndNullTarget(logLsn);
          }
        }
      }
 else {
        if (found) {
          info.lnFound++;
          if (DbLsn.compareTo(logLsn,location.childLsn) > 0) {
            info.lnReplaced++;
            replaced=true;
            location.bin.updateEntry(location.index,null,logLsn);
          }
          if (DbLsn.compareTo(logLsn,location.childLsn) >= 0 && lnFromLog.isDeleted()) {
            location.bin.setKnownDeletedLeaveTarget(location.index);
            byte[] deletedKey=location.bin.containsDuplicates() ? dupKey : mainKey;
            this.hook594(db,location,deletedKey);
          }
        }
 else {
          info.lnNotFound++;
          if (!lnFromLog.isDeleted()) {
            info.lnInserted++;
            inserted=true;
            boolean insertOk=insertRecovery(db,location,logLsn);
            assert insertOk;
          }
        }
      }
      success=true;
      return found ? location.childLsn : DbLsn.NULL_LSN;
    }
  finally {
      this.hook591(location);
      this.hook583(db,location,lnFromLog,logLsn,found,replaced,inserted,success);
    }
  }
  /** 
 * Undo the changes to this node. Here are the rules that govern the action
 * taken.
 * <pre>
 * found LN in  | abortLsn is | logLsn ==       | action taken
 * tree      | null        | LSN in tree     | by undo
 * -------------+-------------+----------------------------------------
 * Y       |     N       |      Y          | replace w/abort LSN
 * ------------ +-------------+-----------------+-----------------------
 * Y       |     Y       |      Y          | remove from tree
 * ------------ +-------------+-----------------+-----------------------
 * Y       |     N/A     |      N          | no action
 * ------------ +-------------+-----------------+-----------------------
 * N       |     N/A     |    N/A          | no action (*)
 * (*) If this key is not present in the tree, this record doesn't
 * reflect the IN state of the tree and this log entry is not applicable.
 * </pre>
 * @param locationholds state about the search in the tree. Passed in from the
 * recovery manager to reduce objection creation overhead.
 * @param lnFromLog -
 * the new node to put in the tree.
 * @param mainKeyis the key that navigates us through the main tree
 * @param dupTreeKeyis the key that navigates us through the duplicate tree
 * @param logLsnis the LSN from the just-read log entry
 * @param abortLsngives us the location of the original version of the node
 * @param infois a recovery stats object.
 */
  public static void undo(  Level traceLevel,  DatabaseImpl db,  TreeLocation location,  LN lnFromLog,  byte[] mainKey,  byte[] dupKey,  long logLsn,  long abortLsn,  boolean abortKnownDeleted,  RecoveryInfo info,  boolean splitsAllowed) throws DatabaseException {
    boolean found=false;
    boolean replaced=false;
    boolean success=false;
    hook584(traceLevel,db,location,lnFromLog,mainKey,dupKey,logLsn,abortLsn,abortKnownDeleted,info,splitsAllowed,found,replaced,success);
  }
  /** 
 * Inserts a LN into the tree for recovery redo processing. In this case, we
 * know we don't have to lock when checking child LNs for deleted status
 * (there can be no other thread running on this tree) and we don't have to
 * log the new entry. (it's in the log already)
 * @param db
 * @param locationthis embodies the parent bin, the index, the key that
 * represents this entry in the bin.
 * @param logLsnLSN of this current ln
 * @param keyto use when creating a new ChildReference object.
 * @return true if LN was inserted, false if it was a duplicate duplicate or
 * if an attempt was made to insert a duplicate when allowDuplicates
 * was false.
 */
  private static boolean insertRecovery(  DatabaseImpl db,  TreeLocation location,  long logLsn) throws DatabaseException {
    ChildReference newLNRef=new ChildReference(null,location.lnKey,logLsn);
    BIN parentBIN=location.bin;
    int entryIndex=parentBIN.insertEntry1(newLNRef);
    if ((entryIndex & IN.INSERT_SUCCESS) == 0) {
      entryIndex&=~IN.EXACT_MATCH;
      boolean canOverwrite=false;
      if (parentBIN.isEntryKnownDeleted(entryIndex)) {
        canOverwrite=true;
      }
 else {
        LN currentLN=(LN)parentBIN.fetchTarget(entryIndex);
        if (currentLN == null || currentLN.isDeleted()) {
          canOverwrite=true;
        }
        parentBIN.updateEntry(entryIndex,null);
      }
      if (canOverwrite) {
        parentBIN.updateEntry(entryIndex,null,logLsn,location.lnKey);
        parentBIN.clearKnownDeleted(entryIndex);
        location.index=entryIndex;
        return true;
      }
 else {
        return false;
      }
    }
    location.index=entryIndex & ~IN.INSERT_SUCCESS;
    return true;
  }
  /** 
 * Update file utilization info during redo.
 */
  private void redoUtilizationInfo(  long logLsn,  long treeLsn,  long abortLsn,  boolean abortKnownDeleted,  LN ln,  TxnNodeId txnNodeId,  Set countedAbortLsnNodes){
    UtilizationTracker tracker=env.getUtilizationTracker();
    if (ln.isDeleted()) {
      Long logFileNum=new Long(DbLsn.getFileNumber(logLsn));
      long fileSummaryLsn=DbLsn.longToLsn((Long)fileSummaryLsns.get(logFileNum));
      int cmpFsLsnToLogLsn=(fileSummaryLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(fileSummaryLsn,logLsn) : -1;
      if (cmpFsLsnToLogLsn < 0) {
        tracker.countObsoleteNode(logLsn,null);
      }
    }
    if (treeLsn != DbLsn.NULL_LSN) {
      int cmpLogLsnToTreeLsn=DbLsn.compareTo(logLsn,treeLsn);
      if (cmpLogLsnToTreeLsn != 0) {
        long newLsn=(cmpLogLsnToTreeLsn < 0) ? treeLsn : logLsn;
        long oldLsn=(cmpLogLsnToTreeLsn > 0) ? treeLsn : logLsn;
        Long oldLsnFile=new Long(DbLsn.getFileNumber(oldLsn));
        long oldFsLsn=DbLsn.longToLsn((Long)fileSummaryLsns.get(oldLsnFile));
        int cmpOldFsLsnToNewLsn=(oldFsLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(oldFsLsn,newLsn) : -1;
        if (cmpOldFsLsnToNewLsn < 0) {
          tracker.countObsoleteNode(oldLsn,null);
        }
      }
      if (cmpLogLsnToTreeLsn <= 0 && abortLsn != DbLsn.NULL_LSN && !abortKnownDeleted && !countedAbortLsnNodes.contains(txnNodeId)) {
        Long abortFileNum=new Long(DbLsn.getFileNumber(abortLsn));
        long abortFsLsn=DbLsn.longToLsn((Long)fileSummaryLsns.get(abortFileNum));
        int cmpAbortFsLsnToLogLsn=(abortFsLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(abortFsLsn,logLsn) : -1;
        if (cmpAbortFsLsnToLogLsn < 0) {
          tracker.countObsoleteNodeInexact(abortLsn,null);
          countedAbortLsnNodes.add(txnNodeId);
        }
      }
    }
  }
  /** 
 * Update file utilization info during recovery undo (not abort undo).
 */
  private void undoUtilizationInfo(  LN ln,  long logLsn,  long abortLsn,  boolean abortKnownDeleted,  TxnNodeId txnNodeId,  Map countedFileSummaries,  Set countedAbortLsnNodes){
    UtilizationTracker tracker=env.getUtilizationTracker();
    Long logFileNum=new Long(DbLsn.getFileNumber(logLsn));
    long fileSummaryLsn=DbLsn.longToLsn((Long)fileSummaryLsns.get(logFileNum));
    int cmpFsLsnToLogLsn=(fileSummaryLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(fileSummaryLsn,logLsn) : -1;
    if (cmpFsLsnToLogLsn < 0) {
      tracker.countObsoleteNode(logLsn,null);
    }
    if (cmpFsLsnToLogLsn > 0) {
      Long countedFile=(Long)countedFileSummaries.get(txnNodeId);
      if (countedFile == null || countedFile.longValue() > logFileNum.longValue()) {
        if (!ln.isDeleted()) {
          tracker.countObsoleteNode(logLsn,null);
        }
        countedFileSummaries.put(txnNodeId,logFileNum);
      }
    }
  }
  /** 
 * Concoct a header for the recovery pass trace info.
 */
  private String passStartHeader(  int passNum){
    return "Recovery Pass " + passNum + " start: ";
  }
  /** 
 * Concoct a header for the recovery pass trace info.
 */
  private String passEndHeader(  int passNum,  long start,  long end){
    return "Recovery Pass " + passNum + " end ("+ (end - start)+ "): ";
  }
  /** 
 * Send trace messages to the java.util.logger. Don't rely on the logger
 * alone to conditionalize whether we send this message, we don't even want
 * to construct the message if the level is not enabled. This is used to
 * construct verbose trace messages for individual log entry processing.
 */
  private static void trace(  Level level,  DatabaseImpl database,  String debugType,  boolean success,  Node node,  long logLsn,  IN parent,  boolean found,  boolean replaced,  boolean inserted,  long replacedLsn,  long abortLsn,  int index){
    new RecoveryManager_trace(level,database,debugType,success,node,logLsn,parent,found,replaced,inserted,replacedLsn,abortLsn,index).execute();
  }
  private void traceAndThrowException(  long badLsn,  String method,  Exception origException) throws DatabaseException {
    String badLsnString=DbLsn.getNoFormatString(badLsn);
    this.hook577(method,origException,badLsnString);
    throw new DatabaseException("last LSN=" + badLsnString,origException);
  }
@MethodObject static class RecoveryManager_trace {
    RecoveryManager_trace(    Level level,    DatabaseImpl database,    String debugType,    boolean success,    Node node,    long logLsn,    IN parent,    boolean found,    boolean replaced,    boolean inserted,    long replacedLsn,    long abortLsn,    int index){
      this.level=level;
      this.database=database;
      this.debugType=debugType;
      this.success=success;
      this.node=node;
      this.logLsn=logLsn;
      this.parent=parent;
      this.found=found;
      this.replaced=replaced;
      this.inserted=inserted;
      this.replacedLsn=replacedLsn;
      this.abortLsn=abortLsn;
      this.index=index;
    }
    void execute(){
    }
    protected Level level;
    protected DatabaseImpl database;
    protected String debugType;
    protected boolean success;
    protected Node node;
    protected long logLsn;
    protected IN parent;
    protected boolean found;
    protected boolean replaced;
    protected boolean inserted;
    protected long replacedLsn;
    protected long abortLsn;
    protected int index;
    protected Logger logger;
    protected Level useLevel;
    protected StringBuffer sb;
  }
  protected void hook556() throws DatabaseException, IOException {
  }
  protected void hook557(  DatabaseImpl db) throws DatabaseException {
  }
  protected void hook558() throws DatabaseException, IOException {
  }
  protected void hook559() throws DatabaseException, IOException {
  }
  protected void hook560() throws DatabaseException, IOException {
  }
  protected void hook561(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook562(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook563() throws IOException, DatabaseException {
  }
  protected void hook564(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook565(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook566(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook567(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook568(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook569(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook570(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook571(  long start,  long end) throws IOException, DatabaseException {
  }
  protected void hook572() throws IOException, DatabaseException {
  }
  protected void hook573() throws DatabaseException, IOException {
  }
  protected void hook574(  LNFileReader reader) throws IOException, DatabaseException, Exception {
  }
  protected void hook575(  IOException e) throws DatabaseException {
  }
  protected void hook576(  DatabaseImpl db,  long logLsn,  Exception e,  String trace) throws DatabaseException {
  }
  protected void hook577(  String method,  Exception origException,  String badLsnString) throws DatabaseException {
  }
  protected void hook578(  EnvironmentImpl env) throws DatabaseException {
  }
  protected void hook579(  long nodeId,  boolean containsDuplicates,  long logLsn,  boolean found,  boolean deleted,  SearchResult result) throws DatabaseException {
  }
  protected void hook580(  DatabaseImpl db,  IN inFromLog,  long lsn,  boolean success,  RootUpdater rootUpdater) throws DatabaseException {
  }
  protected void hook581(  DatabaseImpl db,  DIN inFromLog,  long lsn,  boolean found,  boolean inserted,  boolean replaced,  long origLsn,  IN parent,  int index,  boolean success) throws DatabaseException {
  }
  protected void hook582(  DatabaseImpl db,  IN inFromLog,  long logLsn,  boolean inserted,  boolean replaced,  long origLsn,  boolean success,  SearchResult result) throws DatabaseException {
  }
  protected void hook583(  DatabaseImpl db,  TreeLocation location,  LN lnFromLog,  long logLsn,  boolean found,  boolean replaced,  boolean inserted,  boolean success) throws DatabaseException {
  }
  protected static void hook584(  Level traceLevel,  DatabaseImpl db,  TreeLocation location,  LN lnFromLog,  byte[] mainKey,  byte[] dupKey,  long logLsn,  long abortLsn,  boolean abortKnownDeleted,  RecoveryInfo info,  boolean splitsAllowed,  boolean found,  boolean replaced,  boolean success) throws DatabaseException {
    location.reset();
    found=db.getTree().getParentBINForChildLN(location,mainKey,dupKey,lnFromLog,splitsAllowed,true,false,true);
    if (lnFromLog.containsDuplicates()) {
      if (found) {
        DIN duplicateRoot=(DIN)location.bin.fetchTarget(location.index);
        replaced=hook592(location,logLsn,abortLsn,replaced,duplicateRoot);
      }
    }
 else {
      if (found) {
        if (info != null) {
          info.lnFound++;
        }
        boolean updateEntry=DbLsn.compareTo(logLsn,location.childLsn) == 0;
        if (updateEntry) {
          if (abortLsn == DbLsn.NULL_LSN) {
            location.bin.setKnownDeletedLeaveTarget(location.index);
            byte[] deletedKey=location.bin.containsDuplicates() ? dupKey : mainKey;
            hook595(db,location,deletedKey);
          }
 else {
            if (info != null) {
              info.lnReplaced++;
            }
            replaced=true;
            location.bin.updateEntry(location.index,null,abortLsn);
            if (abortKnownDeleted) {
              location.bin.setKnownDeleted(location.index);
            }
 else {
              location.bin.clearKnownDeleted(location.index);
            }
          }
          location.bin.clearPendingDeleted(location.index);
        }
      }
 else {
        if (info != null) {
          info.lnNotFound++;
        }
      }
    }
    success=true;
  }
  protected void hook585(  IN in) throws DatabaseException {
  }
  protected void hook586(  RecoveryInfo info,  LNFileReader reader,  TreeLocation location,  LN ln,  long logLsn,  long abortLsn,  boolean abortKnownDeleted,  DatabaseImpl db) throws IOException, DatabaseException, Exception {
    undo(detailedTraceLevel,db,location,ln,reader.getKey(),reader.getDupTreeKey(),logLsn,abortLsn,abortKnownDeleted,info,true);
  }
  protected void hook587(  IN inFromLog,  long logLsn) throws DatabaseException {
  }
  protected void hook588(  SearchResult result) throws DatabaseException {
  }
  protected void hook589(  IN parent) throws DatabaseException {
  }
  protected void hook590(  SearchResult result) throws DatabaseException {
  }
  protected void hook591(  TreeLocation location) throws DatabaseException {
  }
  protected static boolean hook592(  TreeLocation location,  long logLsn,  long abortLsn,  boolean replaced,  DIN duplicateRoot) throws DatabaseException {
    if (DbLsn.compareTo(logLsn,location.childLsn) == 0) {
      duplicateRoot.updateDupCountLNRefAndNullTarget(abortLsn);
      replaced=true;
    }
    return replaced;
  }
  protected void hook593(  INFileReader reader) throws IOException, DatabaseException {
  }
  protected void hook594(  DatabaseImpl db,  TreeLocation location,  byte[] deletedKey) throws DatabaseException {
  }
  protected static void hook595(  DatabaseImpl db,  TreeLocation location,  byte[] deletedKey) throws DatabaseException {
  }
  protected void hook596() throws IOException, DatabaseException {
  }
  protected void hook597() throws IOException, DatabaseException, Exception {
  }
  protected void hook598() throws IOException, DatabaseException, Exception {
  }
}
\00base/com/sleepycat/je/recovery/RootUpdater.java:/** 
 */
package com.sleepycat.je.recovery;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.tree.ChildReference;
import com.sleepycat.je.tree.IN;
import com.sleepycat.je.tree.Tree;
import com.sleepycat.je.tree.WithRootLatched;
import com.sleepycat.je.utilint.DbLsn;
import de.ovgu.cide.jakutil.*;
class RootUpdater implements WithRootLatched {
  private Tree tree;
  private IN inFromLog;
  private long lsn=DbLsn.NULL_LSN;
  private boolean inserted=false;
  private boolean replaced=false;
  private long origLsn=DbLsn.NULL_LSN;
  RootUpdater(  Tree tree,  IN inFromLog,  long lsn){
    this.tree=tree;
    this.inFromLog=inFromLog;
    this.lsn=lsn;
  }
  public IN doWork(  ChildReference root) throws DatabaseException {
    ChildReference newRoot=tree.makeRootChildReference(inFromLog,new byte[0],lsn);
    this.hook600();
    if (root == null) {
      tree.setRoot(newRoot,false);
      inserted=true;
    }
 else {
      origLsn=root.getLsn();
      if (DbLsn.compareTo(origLsn,lsn) < 0) {
        tree.setRoot(newRoot,false);
        replaced=true;
      }
    }
    return null;
  }
  boolean updateDone(){
    return inserted || replaced;
  }
  boolean getInserted(){
    return inserted;
  }
  boolean getReplaced(){
    return replaced;
  }
  long getOriginalLsn(){
    return origLsn;
  }
  protected void hook600() throws DatabaseException {
  }
}
\00base/com/sleepycat/je/Database.java:package com.sleepycat.je;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.Iterator;
import java.util.List;
import java.util.logging.Level;
import java.util.logging.Logger;
import com.sleepycat.je.dbi.DatabaseImpl;
import com.sleepycat.je.dbi.EnvironmentImpl;
import com.sleepycat.je.dbi.GetMode;
import com.sleepycat.je.dbi.PutMode;
import com.sleepycat.je.dbi.CursorImpl.SearchMode;
import com.sleepycat.je.txn.Locker;
import com.sleepycat.je.txn.LockerFactory;
import com.sleepycat.je.utilint.TinyHashSet;
import com.sleepycat.je.utilint.Tracer;
import de.ovgu.cide.jakutil.*;
public class Database {
static class DbState {
    private String stateName;
    DbState(    String stateName){
      this.stateName=stateName;
    }
    public String toString(){
      return "DbState." + stateName;
    }
  }
  static DbState OPEN=new DbState("OPEN");
  static DbState CLOSED=new DbState("CLOSED");
  static DbState INVALID=new DbState("INVALID");
  private DbState state;
  Environment envHandle;
  private DatabaseImpl databaseImpl;
  DatabaseConfig configuration;
  private boolean isWritable;
  Locker handleLocker;
  private TinyHashSet cursors=new TinyHashSet();
  private List triggerList;
  /** 
 * Creates a database but does not open or fully initialize it.
 * Is protected for use in compat package.
 */
  protected Database(  Environment env){
    this.envHandle=env;
    handleLocker=null;
  }
  /** 
 * Create a database, called by Environment.
 */
  void initNew(  Environment env,  Locker locker,  String databaseName,  DatabaseConfig dbConfig) throws DatabaseException {
    if (dbConfig.getReadOnly()) {
      throw new DatabaseException("DatabaseConfig.setReadOnly() must be set to false " + "when creating a Database");
    }
    init(env,dbConfig);
    EnvironmentImpl environmentImpl=DbInternal.envGetEnvironmentImpl(envHandle);
    databaseImpl=environmentImpl.createDb(locker,databaseName,dbConfig,this);
    databaseImpl.addReferringHandle(this);
  }
  /** 
 * Open a database, called by Environment.
 */
  void initExisting(  Environment env,  Locker locker,  DatabaseImpl databaseImpl,  DatabaseConfig dbConfig) throws DatabaseException {
    validateConfigAgainstExistingDb(dbConfig,databaseImpl);
    init(env,dbConfig);
    this.databaseImpl=databaseImpl;
    databaseImpl.addReferringHandle(this);
    configuration.setSortedDuplicates(databaseImpl.getSortedDuplicates());
    configuration.setTransactional(databaseImpl.isTransactional());
  }
  private void init(  Environment env,  DatabaseConfig config) throws DatabaseException {
    handleLocker=null;
    envHandle=env;
    configuration=config.cloneConfig();
    isWritable=!configuration.getReadOnly();
    state=OPEN;
  }
  /** 
 * See if this new handle's configuration is compatible with the
 * pre-existing database.
 */
  private void validateConfigAgainstExistingDb(  DatabaseConfig config,  DatabaseImpl databaseImpl) throws DatabaseException {
    if (!config.getUseExistingConfig()) {
      if (databaseImpl.getSortedDuplicates() != config.getSortedDuplicates()) {
        throw new DatabaseException("You can't open a Database with a duplicatesAllowed " + "configuration of " + config.getSortedDuplicates() + " if the underlying database was created with a "+ "duplicatesAllowedSetting of "+ databaseImpl.getSortedDuplicates()+ ".");
      }
    }
    if (databaseImpl.hasOpenHandles()) {
      if (!config.getUseExistingConfig()) {
        if (config.getTransactional() != databaseImpl.isTransactional()) {
          throw new DatabaseException("You can't open a Database with a transactional " + "configuration of " + config.getTransactional() + " if the underlying database was created with a "+ "transactional configuration of "+ databaseImpl.isTransactional()+ ".");
        }
      }
    }
 else {
      databaseImpl.setTransactional(config.getTransactional());
    }
    if (config.getOverrideBtreeComparator()) {
      databaseImpl.setBtreeComparator(config.getBtreeComparator());
    }
    if (config.getOverrideDuplicateComparator()) {
      databaseImpl.setDuplicateComparator(config.getDuplicateComparator());
    }
  }
  public synchronized void close() throws DatabaseException {
    StringBuffer errors=null;
    checkEnv();
    checkProhibitedDbState(CLOSED,"Can't close Database:");
    this.hook44();
    removeAllTriggers();
    envHandle.removeReferringHandle(this);
    if (cursors.size() > 0) {
      errors=new StringBuffer("There are open cursors against the database.\n");
      errors.append("They will be closed.\n");
      Iterator iter=cursors.copy().iterator();
      while (iter.hasNext()) {
        Cursor dbc=(Cursor)iter.next();
        try {
          dbc.close();
        }
 catch (        DatabaseException DBE) {
          errors.append("Exception while closing cursors:\n");
          errors.append(DBE.toString());
        }
      }
    }
    if (databaseImpl != null) {
      databaseImpl.removeReferringHandle(this);
      databaseImpl=null;
      handleLocker.setHandleLockOwner(true,this,true);
      handleLocker.operationEnd(true);
      state=CLOSED;
    }
    if (errors != null) {
      throw new DatabaseException(errors.toString());
    }
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public Sequence openSequence(  Transaction txn,  DatabaseEntry key,  SequenceConfig config) throws DatabaseException {
    checkEnv();
    DatabaseUtil.checkForNullDbt(key,"key",true);
    checkRequiredDbState(OPEN,"Can't call Database.openSequence:");
    checkWritable("openSequence");
    this.hook45(txn,key);
    return new Sequence(this,txn,key,config);
  }
  /** 
 * Javadoc for this public method is generated via
 * the doc templates in the doc_src directory.
 */
  public void removeSequence(  Transaction txn,  DatabaseEntry key) throws DatabaseException {
    delete(txn,key);
  }
  public synchronized Cursor openCursor(  Transaction txn,  CursorConfig cursorConfig) throws DatabaseException {
    checkEnv();
    checkRequiredDbState(OPEN,"Can't open a cursor");
    CursorConfig useConfig=(cursorConfig == null) ? CursorConfig.DEFAULT : cursorConfig;
    if (useConfig.getReadUncommitted() && useConfig.getReadCommitted()) {
      throw new IllegalArgumentException("Only one may be specified: ReadCommitted or ReadUncommitted");
    }
    this.hook46(txn,cursorConfig);
    Cursor ret=newDbcInstance(txn,useConfig);
    return ret;
  }
  /** 
 * Is overridden by SecondaryDatabase.
 */
  Cursor newDbcInstance(  Transaction txn,  CursorConfig cursorConfig) throws DatabaseException {
    return new Cursor(this,txn,cursorConfig);
  }
  public OperationStatus delete(  Transaction txn,  DatabaseEntry key) throws DatabaseException {
    checkEnv();
    DatabaseUtil.checkForNullDbt(key,"key",true);
    checkRequiredDbState(OPEN,"Can't call Database.delete:");
    checkWritable("delete");
    this.hook47(txn,key);
    OperationStatus commitStatus=OperationStatus.NOTFOUND;
    Locker locker=null;
    try {
      locker=LockerFactory.getWritableLocker(envHandle,txn,isTransactional());
      commitStatus=deleteInternal(locker,key);
      return commitStatus;
    }
  finally {
      if (locker != null) {
        locker.operationEnd(commitStatus);
      }
    }
  }
  /** 
 * Internal version of delete() that does no parameter checking.  Notify
 * triggers.  Deletes all duplicates.
 */
  OperationStatus deleteInternal(  Locker locker,  DatabaseEntry key) throws DatabaseException {
    Cursor cursor=null;
    try {
      cursor=new Cursor(this,locker,null);
      cursor.setNonCloning(true);
      OperationStatus commitStatus=OperationStatus.NOTFOUND;
      DatabaseEntry oldData=new DatabaseEntry();
      OperationStatus searchStatus=cursor.search(key,oldData,LockMode.RMW,SearchMode.SET);
      if (searchStatus == OperationStatus.SUCCESS) {
        do {
          if (hasTriggers()) {
            notifyTriggers(locker,key,oldData,null);
          }
          commitStatus=cursor.deleteNoNotify();
          if (commitStatus != OperationStatus.SUCCESS) {
            return commitStatus;
          }
          if (databaseImpl.getSortedDuplicates()) {
            searchStatus=cursor.retrieveNext(key,oldData,LockMode.RMW,GetMode.NEXT_DUP);
          }
 else {
            searchStatus=OperationStatus.NOTFOUND;
          }
        }
 while (searchStatus == OperationStatus.SUCCESS);
        commitStatus=OperationStatus.SUCCESS;
      }
      return commitStatus;
    }
  finally {
      if (cursor != null) {
        cursor.close();
      }
    }
  }
  public OperationStatus get(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkEnv();
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",false);
    checkRequiredDbState(OPEN,"Can't call Database.get:");
    this.hook48(txn,key,lockMode);
    CursorConfig cursorConfig=CursorConfig.DEFAULT;
    if (lockMode == LockMode.READ_COMMITTED) {
      cursorConfig=CursorConfig.READ_COMMITTED;
      lockMode=null;
    }
    Cursor cursor=null;
    try {
      cursor=new Cursor(this,txn,cursorConfig);
      cursor.setNonCloning(true);
      return cursor.search(key,data,lockMode,SearchMode.SET);
    }
  finally {
      if (cursor != null) {
        cursor.close();
      }
    }
  }
  public OperationStatus getSearchBoth(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
    checkEnv();
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    checkRequiredDbState(OPEN,"Can't call Database.getSearchBoth:");
    this.hook49(txn,key,data,lockMode);
    CursorConfig cursorConfig=CursorConfig.DEFAULT;
    if (lockMode == LockMode.READ_COMMITTED) {
      cursorConfig=CursorConfig.READ_COMMITTED;
      lockMode=null;
    }
    Cursor cursor=null;
    try {
      cursor=new Cursor(this,txn,cursorConfig);
      cursor.setNonCloning(true);
      return cursor.search(key,data,lockMode,SearchMode.BOTH);
    }
  finally {
      if (cursor != null) {
        cursor.close();
      }
    }
  }
  public OperationStatus put(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    checkEnv();
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    DatabaseUtil.checkForPartialKey(key);
    checkRequiredDbState(OPEN,"Can't call Database.put");
    checkWritable("put");
    this.hook50(txn,key,data);
    return putInternal(txn,key,data,PutMode.OVERWRITE);
  }
  public OperationStatus putNoOverwrite(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    checkEnv();
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    DatabaseUtil.checkForPartialKey(key);
    checkRequiredDbState(OPEN,"Can't call Database.putNoOverWrite");
    checkWritable("putNoOverwrite");
    this.hook51(txn,key,data);
    return putInternal(txn,key,data,PutMode.NOOVERWRITE);
  }
  public OperationStatus putNoDupData(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
    checkEnv();
    DatabaseUtil.checkForNullDbt(key,"key",true);
    DatabaseUtil.checkForNullDbt(data,"data",true);
    DatabaseUtil.checkForPartialKey(key);
    checkRequiredDbState(OPEN,"Can't call Database.putNoDupData");
    checkWritable("putNoDupData");
    this.hook52(txn,key,data);
    return putInternal(txn,key,data,PutMode.NODUP);
  }
  /** 
 * Internal version of put() that does no parameter checking.
 */
  OperationStatus putInternal(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data,  PutMode putMode) throws DatabaseException {
    Locker locker=null;
    Cursor cursor=null;
    OperationStatus commitStatus=OperationStatus.KEYEXIST;
    try {
      locker=LockerFactory.getWritableLocker(envHandle,txn,isTransactional());
      cursor=new Cursor(this,locker,null);
      cursor.setNonCloning(true);
      commitStatus=cursor.putInternal(key,data,putMode);
      return commitStatus;
    }
  finally {
      if (cursor != null) {
        cursor.close();
      }
      if (locker != null) {
        locker.operationEnd(commitStatus);
      }
    }
  }
  /** 
 */
  public JoinCursor join(  Cursor[] cursors,  JoinConfig config) throws DatabaseException {
    checkEnv();
    checkRequiredDbState(OPEN,"Can't call Database.join");
    DatabaseUtil.checkForNullParam(cursors,"cursors");
    if (cursors.length == 0) {
      throw new IllegalArgumentException("At least one cursor is required.");
    }
    Locker locker=cursors[0].getCursorImpl().getLocker();
    if (!locker.isTransactional()) {
      EnvironmentImpl env=envHandle.getEnvironmentImpl();
      for (int i=1; i < cursors.length; i+=1) {
        Locker locker2=cursors[i].getCursorImpl().getLocker();
        if (locker2.isTransactional()) {
          throw new IllegalArgumentException("All cursors must use the same transaction.");
        }
        EnvironmentImpl env2=cursors[i].getDatabaseImpl().getDbEnvironment();
        if (env != env2) {
          throw new IllegalArgumentException("All cursors must use the same environment.");
        }
      }
      locker=null;
    }
 else {
      for (int i=1; i < cursors.length; i+=1) {
        Locker locker2=cursors[i].getCursorImpl().getLocker();
        if (locker.getTxnLocker() != locker2.getTxnLocker()) {
          throw new IllegalArgumentException("All cursors must use the same transaction.");
        }
      }
    }
    return new JoinCursor(locker,this,cursors,config);
  }
  public void preload(  long maxBytes) throws DatabaseException {
    checkEnv();
    checkRequiredDbState(OPEN,"Can't call Database.preload");
    this.hook55();
    PreloadConfig config=new PreloadConfig();
    config.setMaxBytes(maxBytes);
    databaseImpl.preload(config);
  }
  public void preload(  long maxBytes,  long maxMillisecs) throws DatabaseException {
    checkEnv();
    checkRequiredDbState(OPEN,"Can't call Database.preload");
    this.hook56();
    PreloadConfig config=new PreloadConfig();
    config.setMaxBytes(maxBytes);
    config.setMaxMillisecs(maxMillisecs);
    databaseImpl.preload(config);
  }
  public PreloadStats preload(  PreloadConfig config) throws DatabaseException {
    checkEnv();
    checkRequiredDbState(OPEN,"Can't call Database.preload");
    this.hook57();
    return databaseImpl.preload(config);
  }
  public String getDatabaseName() throws DatabaseException {
    checkEnv();
    if (databaseImpl != null) {
      return databaseImpl.getName();
    }
 else {
      return null;
    }
  }
  String getDebugName(){
    if (databaseImpl != null) {
      return databaseImpl.getDebugName();
    }
 else {
      return null;
    }
  }
  public DatabaseConfig getConfig() throws DatabaseException {
    DatabaseConfig showConfig=configuration.cloneConfig();
    Comparator btComp=(databaseImpl == null ? null : databaseImpl.getBtreeComparator());
    Comparator dupComp=(databaseImpl == null ? null : databaseImpl.getDuplicateComparator());
    showConfig.setBtreeComparator(btComp == null ? null : btComp.getClass());
    showConfig.setDuplicateComparator(dupComp == null ? null : dupComp.getClass());
    return showConfig;
  }
  /** 
 * Equivalent to getConfig().getTransactional() but cheaper.
 */
  boolean isTransactional() throws DatabaseException {
    return databaseImpl.isTransactional();
  }
  public Environment getEnvironment() throws DatabaseException {
    return envHandle;
  }
  public List getSecondaryDatabases() throws DatabaseException {
    List list=new ArrayList();
    if (hasTriggers()) {
      acquireTriggerListReadLock();
      this.hook53(list);
    }
 else {
    }
    return list;
  }
  /** 
 * @return true if the Database was opened read/write.
 */
  boolean isWritable(){
    return isWritable;
  }
  /** 
 * Return the databaseImpl object instance.
 */
  DatabaseImpl getDatabaseImpl(){
    return databaseImpl;
  }
  /** 
 * The handleLocker is the one that holds the db handle lock.
 */
  void setHandleLocker(  Locker locker){
    handleLocker=locker;
  }
  synchronized void removeCursor(  Cursor dbc){
    cursors.remove(dbc);
  }
  synchronized void addCursor(  Cursor dbc){
    cursors.add(dbc);
  }
  /** 
 * @throws DatabaseException if the Database state is not this value.
 */
  void checkRequiredDbState(  DbState required,  String msg) throws DatabaseException {
    if (state != required) {
      throw new DatabaseException(msg + " Database state can't be " + state+ " must be "+ required);
    }
  }
  /** 
 * @throws DatabaseException if the Database state is this value.
 */
  void checkProhibitedDbState(  DbState prohibited,  String msg) throws DatabaseException {
    if (state == prohibited) {
      throw new DatabaseException(msg + " Database state must not be " + prohibited);
    }
  }
  /** 
 * @throws RunRecoveryException if the underlying environment is
 * invalid
 */
  void checkEnv() throws RunRecoveryException {
    EnvironmentImpl env=envHandle.getEnvironmentImpl();
    if (env != null) {
      env.checkIfInvalid();
    }
  }
  /** 
 * Invalidate the handle, called by txn.abort by way of DbInternal.
 */
  synchronized void invalidate(){
    state=INVALID;
    envHandle.removeReferringHandle(this);
    if (databaseImpl != null) {
      databaseImpl.removeReferringHandle(this);
    }
  }
  /** 
 * Check that write operations aren't used on a readonly Database.
 */
  private void checkWritable(  String operation) throws DatabaseException {
    if (!isWritable) {
      throw new DatabaseException("Database is Read Only: " + operation);
    }
  }
  /** 
 * Returns whether any triggers are currently associated with this primary.
 * Note that an update of the trigger list may be in progress and this
 * method does not wait for that update to be completed.
 */
  boolean hasTriggers(){
    return triggerList != null;
  }
  /** 
 * Gets a read-lock on the list of triggers.  releaseTriggerListReadLock()
 * must be called to release the lock.  Called by all primary put and
 * delete operations.
 */
  private void acquireTriggerListReadLock() throws DatabaseException {
    new Database_acquireTriggerListReadLock(this).execute();
  }
  /** 
 * Gets a write lock on the list of triggers.  An empty list is created if
 * necessary, so null is never returned.  releaseTriggerListWriteLock()
 * must always be called to release the lock.
 */
  private void acquireTriggerListWriteLock() throws DatabaseException {
    new Database_acquireTriggerListWriteLock(this).execute();
  }
  /** 
 * Releases a lock acquired by calling acquireTriggerListWriteLock().  If
 * the list is now empty then it is set to null, that is, hasTriggers()
 * will subsequently return false.
 */
  private void releaseTriggerListWriteLock() throws DatabaseException {
    new Database_releaseTriggerListWriteLock(this).execute();
  }
  /** 
 * Adds a given trigger to the list of triggers.  Called while opening
 * a SecondaryDatabase.
 * @param insertAtFront true to insert at the front, or false to append.
 */
  void addTrigger(  DatabaseTrigger trigger,  boolean insertAtFront) throws DatabaseException {
    acquireTriggerListWriteLock();
    try {
      if (insertAtFront) {
        triggerList.add(0,trigger);
      }
 else {
        triggerList.add(trigger);
      }
      trigger.triggerAdded(this);
    }
  finally {
      releaseTriggerListWriteLock();
    }
  }
  /** 
 * Removes a given trigger from the list of triggers.  Called by
 * SecondaryDatabase.close().
 */
  void removeTrigger(  DatabaseTrigger trigger) throws DatabaseException {
    acquireTriggerListWriteLock();
    try {
      triggerList.remove(trigger);
      trigger.triggerRemoved(this);
    }
  finally {
      releaseTriggerListWriteLock();
    }
  }
  /** 
 * Clears the list of triggers.  Called by close(), this allows closing the
 * primary before its secondaries, although we document that secondaries
 * should be closed first.
 */
  private void removeAllTriggers() throws DatabaseException {
    acquireTriggerListWriteLock();
    try {
      for (int i=0; i < triggerList.size(); i+=1) {
        DatabaseTrigger trigger=(DatabaseTrigger)triggerList.get(i);
        trigger.triggerRemoved(this);
      }
      triggerList.clear();
    }
  finally {
      releaseTriggerListWriteLock();
    }
  }
  /** 
 * Notifies associated triggers when a put() or delete() is performed on
 * the primary.  This method is normally called only if hasTriggers() has
 * returned true earlier.  This avoids acquiring a shared latch for
 * primaries with no triggers.  If a trigger is added during the update
 * process, there is no requirement to immediately start updating it.
 * @param locker the internal locker.
 * @param priKey the primary key.
 * @param oldData the primary data before the change, or null if the record
 * did not previously exist.
 * @param newData the primary data after the change, or null if the record
 * has been deleted.
 */
  void notifyTriggers(  Locker locker,  DatabaseEntry priKey,  DatabaseEntry oldData,  DatabaseEntry newData) throws DatabaseException {
    acquireTriggerListReadLock();
    this.hook54(locker,priKey,oldData,newData);
  }
@MethodObject static class Database_acquireTriggerListReadLock {
    Database_acquireTriggerListReadLock(    Database _this){
      this._this=_this;
    }
    void execute() throws DatabaseException {
      if (_this.triggerList == null) {
        _this.triggerList=new ArrayList();
      }
    }
    protected Database _this;
    protected EnvironmentImpl env;
  }
@MethodObject static class Database_acquireTriggerListWriteLock {
    Database_acquireTriggerListWriteLock(    Database _this){
      this._this=_this;
    }
    void execute() throws DatabaseException {
      if (_this.triggerList == null) {
        _this.triggerList=new ArrayList();
      }
    }
    protected Database _this;
    protected EnvironmentImpl env;
  }
@MethodObject static class Database_releaseTriggerListWriteLock {
    Database_releaseTriggerListWriteLock(    Database _this){
      this._this=_this;
    }
    void execute() throws DatabaseException {
      if (_this.triggerList.size() == 0) {
        _this.triggerList=null;
      }
    }
    protected Database _this;
    protected EnvironmentImpl env;
  }
  protected void hook44() throws DatabaseException {
  }
  protected void hook45(  Transaction txn,  DatabaseEntry key) throws DatabaseException {
  }
  protected void hook46(  Transaction txn,  CursorConfig cursorConfig) throws DatabaseException {
  }
  protected void hook47(  Transaction txn,  DatabaseEntry key) throws DatabaseException {
  }
  protected void hook48(  Transaction txn,  DatabaseEntry key,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook49(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data,  LockMode lockMode) throws DatabaseException {
  }
  protected void hook50(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
  }
  protected void hook51(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
  }
  protected void hook52(  Transaction txn,  DatabaseEntry key,  DatabaseEntry data) throws DatabaseException {
  }
  protected void hook53(  List list) throws DatabaseException {
    for (int i=0; i < triggerList.size(); i+=1) {
      Object obj=triggerList.get(i);
      if (obj instanceof SecondaryTrigger) {
        list.add(((SecondaryTrigger)obj).getDb());
      }
    }
  }
  protected void hook54(  Locker locker,  DatabaseEntry priKey,  DatabaseEntry oldData,  DatabaseEntry newData) throws DatabaseException {
    for (int i=0; i < triggerList.size(); i+=1) {
      DatabaseTrigger trigger=(DatabaseTrigger)triggerList.get(i);
      trigger.databaseUpdated(this,locker,priKey,oldData,newData);
    }
  }
  protected void hook55() throws DatabaseException {
  }
  protected void hook56() throws DatabaseException {
  }
  protected void hook57() throws DatabaseException {
  }
}
\00LoggingFinest/com/sleepycat/je/recovery/Checkpointer.java:package com.sleepycat.je.recovery;
public class Checkpointer {
@MethodObject static class Checkpointer_isRunnable {
    protected void hook517() throws DatabaseException {
    }
    protected void hook518() throws DatabaseException {
    }
    protected void hook521() throws DatabaseException {
      sb=new StringBuffer();
{
        this.hook517();
      }
      if (nextLsn != DbLsn.NULL_LSN) {
        sb.append(" " + "nextLsn=").append(DbLsn.getNoFormatString(nextLsn));
      }
      if (_this.lastCheckpointEnd != DbLsn.NULL_LSN) {
        sb.append(" lastCkpt=");
        sb.append(DbLsn.getNoFormatString(_this.lastCheckpointEnd));
      }
{
        this.hook518();
      }
      sb.append(" force=").append(config.getForce());
      Tracer.trace(Level.FINEST,_this.envImpl,sb.toString());
      original();
    }
  }
}
\00TruncateOp/com/sleepycat/je/Database.java:package com.sleepycat.je;
import com.sleepycat.je.dbi.TruncateResult;
public class Database {
  /** 
 * @deprecated It has not been possible to implement this method with
 * correct transactional semantics without incurring a performance penalty
 * on all Database operations. Truncate functionality has been moved to
 * Environment.truncateDatabase(), which requires that all Database handles
 * on the database are closed before the truncate operation can execute.
 */
  public int truncate(  Transaction txn,  boolean countRecords) throws DatabaseException {
    return new Database_truncate(this,txn,countRecords).execute();
  }
  /** 
 * Internal unchecked truncate that optionally counts records.
 * @deprecated
 */
  int truncateInternal(  Locker locker,  boolean countRecords) throws DatabaseException {
    if (databaseImpl == null) {
      throw new DatabaseException("couldn't find database - truncate");
    }
    this.hook43();
    if (handleLocker.isHandleLockTransferrable()) {
      handleLocker.transferHandleLock(this,locker,false);
    }
    boolean operationOk=false;
    try {
      TruncateResult result=envHandle.getEnvironmentImpl().truncate(locker,databaseImpl);
      databaseImpl=result.getDatabase();
      operationOk=true;
      return countRecords ? result.getRecordCount() : -1;
    }
  finally {
      locker.setHandleLockOwner(operationOk,this,false);
    }
  }
  protected void hook43() throws DatabaseException {
  }
@MethodObject static class Database_truncate {
    Database_truncate(    Database _this,    Transaction txn,    boolean countRecords){
      this._this=_this;
      this.txn=txn;
      this.countRecords=countRecords;
    }
    int execute() throws DatabaseException {
      _this.checkEnv();
      _this.checkRequiredDbState(_this.OPEN,"Can't call Database.truncate");
      _this.checkWritable("truncate");
      this.hook39();
      locker=null;
      this.hook40();
      operationOk=false;
      try {
        locker=LockerFactory.getWritableLocker(_this.envHandle,txn,_this.isTransactional(),true,null);
        _this.acquireTriggerListReadLock();
        this.hook41();
        count=_this.truncateInternal(locker,countRecords);
        for (int i=0; i < _this.triggerList.size(); i+=1) {
          obj=_this.triggerList.get(i);
          if (obj instanceof SecondaryTrigger) {
            secDb=((SecondaryTrigger)obj).getDb();
            secDb.truncateInternal(locker,false);
          }
        }
        operationOk=true;
        return count;
      }
  finally {
        if (locker != null) {
          locker.operationEnd(operationOk);
        }
        this.hook42();
      }
    }
    protected Database _this;
    protected Transaction txn;
    protected boolean countRecords;
    protected Locker locker;
    protected boolean triggerLock;
    protected boolean operationOk;
    protected int count;
    protected Object obj;
    protected SecondaryDatabase secDb;
    protected void hook39() throws DatabaseException {
    }
    protected void hook40() throws DatabaseException {
    }
    protected void hook41() throws DatabaseException {
    }
    protected void hook42() throws DatabaseException {
    }
  }
}
\00Derivative_LoggingEvictor_Evictor_LoggingBase/com/sleepycat/je/evictor/Evictor.java:package com.sleepycat.je.evictor;
public class Evictor {
@MethodObject static class Evictor_evictBatch {
    protected void hook368() throws DatabaseException {
    }
    protected void hook369() throws DatabaseException {
    }
    protected void hook371() throws DatabaseException {
      logger=_this.envImpl.getLogger();
      if (logger.isLoggable(_this.detailedTraceLevel)) {
        msg="Evictor: ";
        this.hook369();
        msg+=" finished=" + finished + " source="+ source+ " requiredEvictBytes="+ _this.formatter.format(requiredEvictBytes)+ " evictBytes="+ _this.formatter.format(evictBytes)+ " inListSize="+ inListStartSize+ " nNodesScanned="+ _this.nNodesScannedThisRun;
        this.hook368();
        msg+=" nBatchSets=" + nBatchSets;
        Tracer.trace(_this.detailedTraceLevel,_this.envImpl,msg);
      }
      original();
    }
  }
@MethodObject static class Evictor_isRunnable {
    protected void hook370() throws DatabaseException {
    }
    protected void hook372() throws DatabaseException {
      logger=_this.envImpl.getLogger();
      if (logger.isLoggable(_this.detailedTraceLevel)) {
        r=Runtime.getRuntime();
        totalBytes=r.totalMemory();
        freeBytes=r.freeMemory();
        usedBytes=r.totalMemory() - r.freeMemory();
        sb=new StringBuffer();
        sb.append(" source=").append(source);
        this.hook370();
        sb.append(" requiredEvict=").append(_this.formatter.format(_this.currentRequiredEvictBytes));
        sb.append(" JVMtotalBytes= ").append(_this.formatter.format(totalBytes));
        sb.append(" JVMfreeBytes= ").append(_this.formatter.format(freeBytes));
        sb.append(" JVMusedBytes= ").append(_this.formatter.format(usedBytes));
        logger.log(_this.detailedTraceLevel,sb.toString());
      }
      original();
    }
  }
}
\00
