after LLLLLLL314:  loadStats(StatsConfig ) {
	inCompressor.loadStats(config, stats);
	//original(config, stats);
    }

}
\00after LLLLLLL403:  nDeadlockRetries() {
	wokenUp++;
	//original();
    }

    after LLLLLLL404: doCompress(){

	resetPerRunCounters();
	//original();
    }

    after LLLLLLL405: doCompress() {
// synchronized void doCompress()

	accumulatePerRunCounters();
	//original();
    }

    after LLLLLLL406: pruneBIN(DatabaseImpl , BINReference , byte , boolean ,	    byte , UtilizationTracker ){
	processedBinsThisRun++;
	//original();
    }

    after LLLLLLL407: pruneBIN(DatabaseImpl , BINReference , byte , boolean ,
	    byte , UtilizationTracker ){
	nonEmptyBinsThisRun++;
	//original();
    }

    after LLLLLLL408: pruneBIN(DatabaseImpl , BINReference , byte , boolean ,	    byte , UtilizationTracker ){

	cursorsBinsThisRun++;
	//original();
    }

    after LLLLLLL409:lazyCompress(IN ){
	lazyProcessed++;
	//original();
    }

    after LLLLLLL410:lazyCompress(IN ){
	lazySplit++;
	//original();
    }

    after LLLLLLL411:lazyCompress(IN ){
	lazyEmpty++;
	//original();
    }

    after LLLLLLL412: findDBAndBIN(BINSearch , BINReference , DbTree , Map ){
	dbClosedBinsThisRun++;
	//original();
    }

    after LLLLLLL413: findDBAndBIN(BINSearch , BINReference , DbTree , Map ){
	splitBinsThisRun++;
	//original();
    }

    after LLLLLLL414: compressBin(DatabaseImpl , BIN , BINReference , UtilizationTracker ){
	cursorsBinsThisRun++;
	//original();
    }

}
\00after dump() {
//	original();
	Tracer.trace(Level.INFO, DbInternal.envGetEnvironmentImpl(env), "DbDump.dump of " + dbName + " ending");
    }

  after LLLLLLL834: dump(){
	Tracer.trace(Level.INFO, DbInternal.envGetEnvironmentImpl(env), "DbDump.dump of " + dbName + " starting");
//	original();
    }

}
\00after LLLLLLL573: recover(boolean ) {

	Tracer.trace(Level.INFO, env, "There are " + preparedTxns.size() + " prepared but unfinished txns.");
	//original();
    }

    after LLLLLLL574: undoLNs(RecoveryInfo , Set ){

	Tracer.trace(Level.INFO, env, "Found unfinished prepare record: id: " + reader.getTxnPrepareId() + " Xid: "
		+ reader.getTxnPrepareXid());
	//original(reader);
    }

}
\00after LLLLLLL121: doClean(boolean , boolean , boolean ) {
	Tracer.trace(Level.INFO, env, traceMsg);
//	original(traceMsg);
    }

}
\00after LLLLLLL578: RecoveryManager(EnvironmentImpl ) {
	detailedTraceLevel = Tracer.parseLevel(env, EnvironmentParams.JE_LOGGING_LEVEL_RECOVERY);
	//original(env);
    }

    after LLLLLLL579:  replayINDelete(DatabaseImpl , long , boolean , byte , byte , long ){
	traceINDeleteReplay(nodeId, logLsn, found, deleted, result.index, containsDuplicates);
	//original(nodeId, containsDuplicates, logLsn, found, deleted, result);
    }

    after LLLLLLL580: replaceOrInsertRoot(DatabaseImpl , IN , long ){
	trace(detailedTraceLevel, db, TRACE_ROOT_REPLACE, success, inFromLog, lsn, null, true,
		rootUpdater.getReplaced(), rootUpdater.getInserted(), rootUpdater.getoriginalLsn(), DbLsn.NULL_LSN, -1);
	//original(db, inFromLog, lsn, success, rootUpdater);
    }

    after LLLLLLL581: replaceOrInsertDuplicateRoot(DatabaseImpl , DIN , long ) {

	trace(detailedTraceLevel, db, TRACE_DUP_ROOT_REPLACE, success, inFromLog, lsn, parent, found, replaced,
		inserted, origLsn, DbLsn.NULL_LSN, index);
	//original(db, inFromLog, lsn, found, inserted, replaced, origLsn, parent, index, success);
    }

    after LLLLLLL582: replaceOrInsertChild(DatabaseImpl , IN , long , long , List , boolean ) {
	trace(detailedTraceLevel, db, TRACE_IN_REPLACE, success, inFromLog, logLsn, result.parent,
		result.exactParentFound, replaced, inserted, origLsn, DbLsn.NULL_LSN, result.index);
	//original(db, inFromLog, logLsn, inserted, replaced, origLsn, success, result);
    }

    after LLLLLLL583:  redo(DatabaseImpl , TreeLocation , LN , byte , byte , long ,	  RecoveryInfo ) {
	trace(detailedTraceLevel, db, TRACE_LN_REDO, success, lnFromLog, logLsn, location.bin, found, replaced,
		inserted, location.childLsn, DbLsn.NULL_LSN, location.index);
	//original(db, location, lnFromLog, logLsn, found, replaced, inserted, success);
    }

  after LLLLLLL584: undo(Level , DatabaseImpl , TreeLocation , LN , byte ,
	    byte , long , long , boolean , RecoveryInfo , boolean ){

	try {
	    //original(traceLevel, db, location, lnFromLog, mainKey, dupKey, logLsn, abortLsn, abortKnownDeleted, info, splitsAllowed, found, replaced, success);
	} finally {
	    Label555: YYY //hook555(traceLevel, db, location, lnFromLog, logLsn, abortLsn, found, replaced, success);
	}
    }

}
\00after LLLLLLL373: Evictor(EnvironmentImpl , String ){
	detailedTraceLevel = Tracer.parseLevel(envImpl, EnvironmentParams.JE_LOGGING_LEVEL_EVICTOR);
	//original(envImpl);
    }

}
\00after LLLLLLL391: findDBAndBIN(BINSearch , BINReference , DbTree , Map ){

	env.getEvictor().doCriticalEviction();
	//original();
    }

}
\00after LLLLLLL333:runOrPauseDaemons(DbConfigManager ){
			cleaner.runOrPause(mgr.getBoolean(EnvironmentParams.ENV_RUN_CLEANER)
				&& !mgr.getBoolean(EnvironmentParams.LOG_MEMORY_ONLY));
			//original(mgr);
    }

    after requestShutdownDaemons() {
	//original();
	if (cleaner != null) {
	    cleaner.requestShutdown();
	}
    }

    /** 
     * Ask all daemon threads to shut down.
     */
  before shutdownDaemons()  {
	shutdownCleaner();
//	original();
    }

}
\00after LLLLLLL796: execute(){

        delta+=MemoryBudget.HASHSET_ENTRY_OVERHEAD + MemoryBudget.OBJECT_OVERHEAD;
        _this.updateMemoryUsage(delta);
        //original();
      }
      after LLLLLLL797: execute(){
        delta=0;
        //original();
      }
      after LLLLLLL798:  execute(){ 
        delta+=MemoryBudget.HASHSET_OVERHEAD;
        //original();
      }
    }
}
\00after LLLLLLL849: stats(PrintStream ){
	Tracer.trace(Level.INFO, DbInternal.envGetEnvironmentImpl(env), "DbStat.stats of " + dbName + " ending");
	//original();
    }

    after LLLLLLL850: stats(PrintStream ){
	Tracer.trace(Level.INFO, DbInternal.envGetEnvironmentImpl(env), "DbStat.stats of " + dbName + " starting");
	//original();
    }

}
\00after LLLLLLL298: dbRename(Locker , String , String ){
	nameCursor.releaseBIN();
//	original(nameCursor);
    }

}
\00after LLLLLLL457: FileManager(EnvironmentImpl, File , boolean )
  {
    		fileCache = new FileCache(configManager);
  }
 //   protected void hook457(DbConfigManager configManager) throws DatabaseException {
		//	fileCache = new FileCache(configManager);
//			original(configManager);
    //}

  //  protected void hook458(long fileNum) throws DatabaseException, IOException {
	//		clearFileCache(fileNum);
	//		original(fileNum);
   // }
  after LLLLLLL458:renameFile(long, String )
  {
    clearFileCache(fileNum);
  }

//    protected void hook459(long fileNum) throws DatabaseException, IOException {
//			clearFileCache(fileNum);
//			original(fileNum);
//    }
  after LLLLLLL459: deleteFile(long)
  {
			clearFileCache(fileNum);
  }


  after LLLLLLL461: abortCommittedTxns(ByteBuffer ) {
	data.position(0);

  }
 //   protected void hook461(ByteBuffer data) {
//	data.position(0);
//	original(data);
 //   }

    /** 
     * Close all file handles and empty the cache.
     */
//    public void clear() throws IOException, DatabaseException {
//	{
//	    this.hook451();
//	}
//	original();
 //   }

  before clear()
  {
    fileCache.clear();
  }


  after LLLLLLL462: getFileHandle(long) 
  {
    fileHandle = fileCache.get(fileId);
	  if (fileHandle == null) // continue 
  }
 


  after LLLLLLL464: getFileHandle(long )
	{
	fileCache.add(fileId, fileHandle);
	}    

 //protected void hook464(Long fileId, FileHandle fileHandle) throws LogException, DatabaseException {
	//fileCache.add(fileId, fileHandle);
	//original(fileId, fileHandle);
    //}

	

}
\00after LLLLLLL440: clear() {
	 // this.hook443(fileHandle);
    Label443: YYY
	  fileHandle.close();
	  iter.remove();
  }
 //   protected void hook440(Iterator iter, FileHandle fileHandle) throws IOException, DatabaseException {
//	this.hook443(fileHandle);
//	fileHandle.close();
//	iter.remove();
 //   }

    //protected void hook441(FileHandle evictTarget) throws DatabaseException {
    //}

 //   protected void hook442(FileHandle evictTarget) throws IOException, DatabaseException {
//    }

    //protected void hook443(FileHandle fileHandle) throws IOException, DatabaseException {
   // }

}
\00after LLLLLLL311: doClose(boolean ){ /
	LatchSupport.clearNotes();
	//original();
    }

}
\00after LLLLLLL312: execute(){ // introduced in CheckLeaks_EnvironmentImp_inner.ump
        if (LatchSupport.countLatchesHeld() > 0) {
          clean=false;
          System.out.println("Some latches held at env close.");
          LatchSupport.dumpLatchesHeld();
        }
        //original();
      }
    }
}
\00after LLLLLLL326: runOrPauseDaemons(DbConfigManager ){
			checkpointer.runOrPause(mgr.getBoolean(EnvironmentParams.ENV_RUN_CHECKPOINTER));
	//original(mgr);
    }

    after LLLLLLL327:requestShutdownDaemons() { 
			if (checkpointer != null) {
					checkpointer.requestShutdown();
			}
			//original();
    }

    after LLLLLLL328:shutdownCheckpointer() {

			checkpointer.shutdown();
			checkpointer.clearEnv();
			//original();
    }

}
\00after LLLLLLL538: Checkpointer(EnvironmentImpl , long , String ) {

			super.init(0 + waitTime, name, envImpl);
			//original(envImpl, waitTime, name);
    }

}
\00after LLLLLLL558: recover(boolean ) {
			Tracer.trace(Level.CONFIG, env, "Recovery checkpoint search, " + info);
			//original();
    }

    after LLLLLLL559: recover(boolean ) {
	Tracer.trace(Level.CONFIG, env, "Recovery underway, found end of log");
	//original();
    }

    after LLLLLLL560: recover(boolean ) {
	Tracer.trace(Level.CONFIG, env, "Recovery w/no files.");
	//original();
    }

    after LLLLLLL561: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(10, start, end) + info.toString());
	//original(start, end);
    }

    after LLLLLLL562: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(9, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(10) + "redo LNs");
	//original(start, end);
    }

    after LLLLLLL563: buildTree() {
	Tracer.trace(Level.CONFIG, env, passStartHeader(9) + "undo LNs");
	//original();
    }

    after LLLLLLL564: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(8, start, end) + info.toString());
	//original(start, end);
    }

    after LLLLLLL565: buildTree() {

	Tracer.trace(Level.CONFIG, env, passEndHeader(7, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(8) + "read dup BINDeltas");
	//original(start, end);
    }

    after LLLLLLL566: buildTree() {

	Tracer.trace(Level.CONFIG, env, passEndHeader(6, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(7) + "read dup INs");
	//original(start, end);
    }

    after LLLLLLL567: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(5, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(6) + "read BINDeltas");
	//original(start, end);
    }

    after LLLLLLL568: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(4, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(5) + "read other INs");
	//original(start, end);
    }

    after LLLLLLL569: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(3, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(4) + "redo map LNs");
	//original(start, end);
    }

    after LLLLLLL570: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(2, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(3) + "undo map LNs");
	//original(start, end);
    }

    after LLLLLLL571: buildTree() {
	Tracer.trace(Level.CONFIG, env, passEndHeader(1, start, end) + info.toString());
	Tracer.trace(Level.CONFIG, env, passStartHeader(2) + "read map BINDeltas");
	//original(start, end);
    }

    after LLLLLLL572: buildTree() {
	Tracer.trace(Level.CONFIG, env, passStartHeader(1) + "read map INs");
	//original();
    }

}
\00after LLLLLLL522: execute() {
        traced=false;
        //original();
      }
      after LLLLLLL523: execute() {
        _this.trace(_this.envImpl,invokingSource,true);
        traced=true;
//        original();
      }
      after LLLLLLL524: execute() {

        if (!traced) {
          _this.trace(_this.envImpl,invokingSource,success);
        }
        //original();
      }
    }
}
\00after LLLLLLL38: getStats(StatsConfig ){
	databaseImpl.checkIsDeleted("stat");
	//original();
    }

}
\00after LLLLLLL87: deleteSafeToDeleteFiles(){ //>>  introduced by EnviromentLocking_Cleaner.ump
	Tracer.trace(Level.SEVERE, env,	"Cleaner has " + safeFiles.size() + " files not deleted because of read-only processes.");
	//original(safeFiles);
    }

}
\00after LLLLLLL393: doCompress() {
//synchronized void doCompress()
	assert LatchSupport.countLatchesHeld() == 0;
	//original();
    }

    after LLLLLLL394: doCompress() {
	foundBin.releaseLatch();
	//original(foundBin);
    }

    after LLLLLLL395: doCompress() {
	assert LatchSupport.countLatchesHeld() == 0;
	//original();
    }

    after LLLLLLL396_1: compressBin(DatabaseImpl , BIN , BINReference , UtilizationTracker ){

	    bin.releaseLatch();
	
    }

    after LLLLLLL397_1:searchForBIN(DatabaseImpl , BINReference ) {

//	try {	    //original(mainKey, dupKey, tree, duplicateRoot, duplicateBin, bin);} catch (DatabaseException DBE) {
	
    if (bin != null) {
		bin.releaseLatchIfOwner();
	    }
	    if (duplicateRoot != null) {
		duplicateRoot.releaseLatchIfOwner();
	    }
	    if (duplicateBin != null) {
		duplicateBin.releaseLatchIfOwner();
	    }
//	    throw DBE;	}
    }

    after LLLLLLL398: lazyCompress(IN ){

	assert in.isLatchOwner();
	//original(in);
    }

    after LLLLLLL399: findDBAndBIN(BINSearch , BINReference , DbTree , Map ){
	if (binSearch.bin != null) {
	    binSearch.bin.releaseLatch();
	}
	//original(binSearch);
    }

    after LLLLLLL400:  searchForBIN(DatabaseImpl , byte , byte ){
	bin.releaseLatch();
	//original(bin);
    }

    after LLLLLLL401:  searchForBIN(DatabaseImpl , byte , byte ){
	duplicateRoot.latch();
	bin.releaseLatch();
	//original(duplicateRoot, bin);
    }

    after LLLLLLL402:  searchForBIN(DatabaseImpl , byte , byte ){
	bin.releaseLatch();
	//original(bin);
    }

}
\00after LLLLLLL370: execute(){ //      Lael370 introduced in Evictor_ump >> after LLLLLLL372: execute(){
        sb.append(" doRun=").append(doRun);
        sb.append(" JEusedBytes=").append(_this.formatter.format(currentUsage));
  //      original();
      }
    }
}
\00after LLLLLLL392: doCompress(){

	Tracer.trace(Level.FINE, env, "InCompress.doCompress called, queue size: " + binQueueSize);
	//original(binQueueSize);
    }

}
\00after LLLLLLL85: addPendingDB(DatabaseImpl ) {
	Tracer.trace(detailedTraceLevel, env, "CleanAddPendingDB " + id);
	//original(id);
    }

}
\00after LLLLLLL325: doClose(boolean ){
		try {
					checkLeaks();
					Label311:   YYY //this.hook311();  
			} catch (DatabaseException DBE) {
					errors.append("\nException performing validity checks: ");
					errors.append(DBE.toString()).append("\n");
			}
			//original(errors);
		}

}
\00after LLLLLLL64: toString() {
	sb.append("lockTableLatch:\n").append(lockTableLatchStats);
	//original(sb);
    }

}
\00after LLLLLLL483_1: loadStats(StatsConfig , EnvironmentStats ){

	    bufferPoolLatch.release();
	
    }

    after LLLLLLL484: loadStats(StatsConfig , EnvironmentStats ){
	bufferPoolLatch.acquire();
	//original();
    }

}
\00after LLLLLLL820:
  {

	allTxnLatch.acquire();
  }

 after LLLLLLL820_1: txnStat(StatsConfig ){
	    allTxnLatch.release();
    }

}
\00after LLLLLLL416:acquire() {

	if (lock.isLocked()) {
	    stats.nAcquiresWithContention++;
	} else {
	    stats.nAcquiresNoWaiters++;
	}
	//original();
    }

    after LLLLLLL417:acquire() {
	stats.nAcquiresSelfOwned++;
	//original();
    }

    after LLLLLLL418:acquireNoWait() {
	stats.nAcquiresSelfOwned++;
	//original();
    }

    after LLLLLLL419:acquireNoWait() {
	stats.nAcquireNoWaitSuccessful++;
	//original();
    }

    after LLLLLLL420:acquireNoWait() {
	stats.nAcquireNoWaitUnsuccessful++;
	//original();
    }

    after LLLLLLL421:doRelease(boolean ) {
	stats.nReleases++;
	//original();
    }

}
\00after LLLLLLL422:acquire(){
	stats.nAcquiresSelfOwned++;
	//original();
    }

    after LLLLLLL423:acquire(){
	stats.nAcquiresNoWaiters++;
	//original();
    }

    after LLLLLLL424:acquire(){
	stats.nAcquiresWithContention++;
	//original();
    }

    after LLLLLLL425: acquireNoWait(){ //synchronized boolean acquireNoWait()
	stats.nAcquiresSelfOwned++;
	//original();
    }

    after LLLLLLL426: acquireNoWait(){ //synchronized boolean acquireNoWait()
	stats.nAcquireNoWaitSuccessful++;
	//original();
    }

    after LLLLLLL427: acquireNoWait(){ //synchronized boolean acquireNoWait()
	stats.nAcquireNoWaitUnsuccessful++;
	//original();
    }

    after LLLLLLL428: doRelease(boolean ) {
	stats.nReleases++;
	//original();
    }

}
\00after LLLLLLL429:  acquireExclusive() {  //synchronized void acquireExclusive()

	stats.nAcquiresNoWaiters++;
	//original();
    }

    after LLLLLLL430:  acquireExclusive() {  //synchronized void acquireExclusive()
	stats.nAcquiresWithContention++;
	//original();
    }

    after LLLLLLL431:  acquireExclusiveNoWait() {
	stats.nAcquiresNoWaiters++;
	//original();
    }

    after LLLLLLL432:acquireShared() {

	stats.nAcquireSharedSuccessful++;
	//original();
    }

    after LLLLLLL433: release() { // synchronized void release() 
	stats.nReleases++;
	//original();
    }

}
\00after LLLLLLL200: execute() {
        _this.latchBIN();
        try {
          //original();
        }
    finally {
          _this.releaseBIN();
        }
      }
      after LLLLLLL201: execute() {
        duplicateRoot.latch();
        try {
          //original();
        }
    finally {
          duplicateRoot.releaseLatch();
        }
      }
    }
}
\00after LLLLLLL769: execute(){
        for (int i=0; i < _this.nLockTables; i++) {
          latchStats=(LatchStats)_this.lockTableLatches[i].getLatchStats();
          stats.accumulateLockTableLatchStats(latchStats);
        }
        //original();
      }
    }
}
\00after LLLLLLL482: LogBuffer(int , EnvironmentImpl ){
			buffer = ByteBuffer.allocate(capacity);
			//original(capacity);
    }

}
\00after LLLLLLL445_1: execute() {
//            int result = original(); 
//{
              //  this.hook447();
            //}
            Label447:  YYY
            assert data.hasArray();
            assert data.arrayOffset() == 0;
            pos = data.position();
            size = data.limit() - pos;
            file.seek(destOffset);
            file.write(data.array(), pos, size);
            data.position(pos + size);
            totalBytesWritten = size;
            Label447_1: YYY ;//
            //end 
           // return result;
        }
    }
     static class FileManager_readFromFile {
        //protected void hook448() throws IOException {
        //    assert readBuffer.hasArray();
        //    assert readBuffer.arrayOffset() == 0;
        //    pos = readBuffer.position();
        //    size = readBuffer.limit() - pos;
        //    file.seek(offset);
        //    bytesRead2 = file.read(readBuffer.array(), pos, size);
        //    if (bytesRead2 > 0) {
        //        readBuffer.position(pos + bytesRead2);
        //    }
       // }
        after LLLLLLL446_1: execute() {
            //original(); {
            //    this.hook448();
            //}
           Label448: YYY
           assert readBuffer.hasArray();
           assert readBuffer.arrayOffset() == 0;
           pos = readBuffer.position();
           size = readBuffer.limit() - pos;
           file.seek(offset);
           bytesRead2 = file.read(readBuffer.array(), pos, size);
           if (bytesRead2 > 0) {
                readBuffer.position(pos + bytesRead2);
           }
           Label448_1: YYY   ;//
        }
    }
}
\00after LLLLLLL204: delete(){

	trace(Level.FINER, TRACE_DELETE, targetBin, ln, targetIndex, oldLsn, newLsn);
	//original(ln, oldLsn, newLsn);
    }

    after LLLLLLL205: putCurrent(DatabaseEntry , DatabaseEntry , DatabaseEntry ) {
	trace(Level.FINER, TRACE_MOD, targetBin, ln, targetIndex, oldLsn, newLsn);
//	original(ln, oldLsn, newLsn);
    }

}
\00after LLLLLLL380: selectIN(INList , ScanIterator ){
	if (target != null) {
	    nNodesSelectedThisRun++;
	    nNodesSelected++;
	}
	//original(target);
    }

    after LLLLLLL383:  evict(INList , IN , ScanIterator ){
	if (evictedBytes > 0) {
	    nBINsStrippedThisRun++;
	    nBINsStripped++;
	}
	//original(evictedBytes);
    }

    after LLLLLLL384: evictIN(IN , IN , int , INList , ScanIterator , boolean ){
	nNodesEvictedThisRun++;
	nNodesEvicted++;
	//original();
    }

}
\00after LLLLLLL315: loadStats(StatsConfig ){
	evictor.loadStats(config, stats);
	//original(config, stats);
    }

}
\00after LLLLLLL381: execute() { 

        _this.nBINsStrippedThisRun=0;
        _this.nEvictPasses++;
       // original();
      }
      after LLLLLLL382: execute() { 
        _this.nNodesScanned+=_this.nNodesScannedThisRun;
        //original();
      }
    }
}
\00after LLLLLLL831: measure(PrintStream , File , long , int , int , int ,	    boolean ){
	printStats(out, env, "Stats for internal nodes only (after preload)");
//	original(out, env);
    }

    after LLLLLLL832: measure(PrintStream , File , long , int , int , int ,	    boolean ){
	printStats(out, env, "Stats for internal and leaf nodes (after insert)");
//	original(out, env);/
    }

}
\00after LLLLLLL833:  execute() {
        stats=env.getStats(null);
        if (stats.getNNodesScanned() > 0) {
          out.println("*** Ran out of cache memory at record " + i + " -- try increasing the Java heap size ***");
          throw new ReturnVoid();
        }
       // original();
      }
    }
}
\00after LLLLLLL435: fsync(){
	nFSyncRequests++;
	//original();
    }

    after LLLLLLL436: fsync(){
	synchronized (this) {
	    nTimeouts++;
	}
	//original();
    }

    after LLLLLLL437: fsync(){
	nFSyncs++;
	//original();
    }

}
\00after LLLLLLL60: reset() {
	nFSyncs = 0;
	nFSyncRequests = 0;
	nFSyncTimeouts = 0;
	//original();
    }

    after LLLLLLL61: toString() {
	sb.append("nFSyncs=").append(nFSyncs).append('\n');
	sb.append("nFSyncRequests=").append(nFSyncRequests).append('\n');
	sb.append("nFSyncTimeouts=").append(nFSyncTimeouts).append('\n');
	//original(sb);
    }

}
\00after LLLLLLL497: loadStats(StatsConfig , EnvironmentStats ) {
	fileManager.loadStats(config, stats);
//	original(config, stats);
    }

}
\00after LLLLLLL368: execute(){ //     after LLLLLLL371: execute(){
        msg+=" nNodesSelected=" + _this.nNodesSelectedThisRun + " nEvicted="+ _this.nNodesEvictedThisRun+ " nBINsStripped="+ _this.nBINsStrippedThisRun;
        //original();
      }

      after LLLLLLL369: execute(){ //     after LLLLLLL371: execute(){
        msg+="pass=" + _this.nEvictPasses;
       // original();
      }
    }
}
\00after LLLLLLL363: processIN(IN , Long , int ) {
	verifyNode(node);
	//original(node);
    }

    after LLLLLLL364: processBIN(BIN , Long , int ) {
	verifyNode(node);
	//original(node);
    }

    after LLLLLLL365: processDIN(DIN , Long , int ) {
	verifyNode(node);
	//original(node);
    }

    after LLLLLLL366: processDBIN(DBIN , Long , int ) {
	verifyNode(node);
	//original(node);
    }

    after LLLLLLL367: processDupCountLN(DupCountLN , Long ) {
	verifyNode(node);
	//original(node);
    }

}
\00after initCacheMemoryUsage()  {
	//original();
	assert LatchSupport.countLatchesHeld() == 0;
    }

    after LLLLLLL347: calcTreeCacheUsage(){
	inList.latchMajor();
}

    after LLLLLLL347_1: calcTreeCacheUsage(){
	//try {
	  //  totalSize = original(totalSize, inList);} finally {
	    inList.releaseMajorLatch();
	//}
	//return totalSize;
    }

}
\00after LLLLLLL348:execute(){
        isJVM14=(LatchSupport.getJava5LatchClass() == null);
        //original();
      }
    }
}
\00after LLLLLLL82: get(Transaction , int ) { 
	if (logger.isLoggable(Level.FINEST)) {
	    logger.log(Level.FINEST, "Sequence.get" + " value=" + retVal + " cached=" + cached + " wrapped=" + wrapped);
	}
	//original(cached, wrapped, retVal);
    }

}
\00after LLLLLLL456:FileManager(EnvironmentImpl , File , boolean ) {

			chunkedNIOSize = configManager.getLong(EnvironmentParams.LOG_CHUNKED_NIO);
			//original(configManager);
    }

}
\00after LLLLLLL415:  findDBAndBIN(BINSearch , BINReference , DbTree , Map ){
	close |= binSearch.db.isDeleted();
	//return original(binSearch, close);
    }

}
\00after latch(boolean ) {

	//original(updateGeneration);
	latch.acquire();
    }

    protected void hook619: latchNoWait(boolean ){
	//if (latch.acquireNoWait()) {
	    //original(updateGeneration);
	//} else {
	    if(! latch.acquireNoWait()) 
        throw new ReturnBoolean(false);
	//}
    }

    /** 
     * See if you are the parent of this child. If not, find a child of your's that may be the parent, and return it. If there are no possiblities, return null. Note that the keys of the target are passed in so we don't have to latch the target to look at them. Also, this node is latched upon entry.
     * @param doFetch If true, fetch the child in the pursuit of this search.If false, give up if the child is not resident. In that case, we have a potential ancestor, but are not sure if this is the parent.
     */


    before findParent(Tree.SearchType , long , boolean , boolean , byte, byte, SearchResult ,	boolean , boolean , int , List , boolean ) {
	assert isLatchOwner();
	//original(searchType, targetNodeId, targetContainsDuplicates, targetIsRoot, targetMainTreeKey, targetDupTreeKey,result, requireExactMatch, updateGeneration, targetLevel, trackingList, doFetch);
    }

    protected void hook620: findParent(Tree.SearchType , long , boolean , boolean , byte , byte , SearchResult , boolean , boolean , int , List , boolean ) {
	releaseLatch();
	//original();
    }

    protected void hook621: findParent(Tree.SearchType , long , boolean , boolean , byte , byte , SearchResult , boolean , boolean , int , List , boolean ) {
	releaseLatch();
	//original();
    }

    protected void hook622: findParent(Tree.SearchType , long , boolean , boolean , byte , byte , SearchResult , boolean , boolean , int , List , boolean ) {
	releaseLatch();
	//original();
    }

    protected void hook623: findParent(Tree.SearchType , long , boolean , boolean , byte , byte , SearchResult , boolean , boolean , int , List , boolean ) {
	releaseLatch();
	//original();
    }

    protected void hook624:  descendOnParentSearch(SearchResult , boolean , boolean , long , Node , boolean ) {

	releaseLatch();
	//original();
    }

    protected void hook625:  descendOnParentSearch(SearchResult , boolean , boolean , long , Node , boolean ) {

	((IN) child).releaseLatch();
	//original(child);
    }

    protected void hook626:  descendOnParentSearch(SearchResult , boolean , boolean , long , Node , boolean ) {
	releaseLatch();
	//original();
    }

    protected void hook627: isSoughtNode(long , boolean ){
	releaseLatch();
	//original();
    }

    /** 
     * @see LogReadable#readFromLog
     */
    after readFromLog(ByteBuffer , byte ) {

	//original(itemBuffer, entryTypeVersion);
	latch.setName(shortClassName() + getNodeId());
    }

}
\00after LLLLLLL95: processPendingLN(LN , DatabaseImpl , byte , byte , TreeLocation ){

			if (parentDIN != null) {
					parentDIN.releaseLatchIfOwner();
			}
			if (bin != null) {
					bin.releaseLatchIfOwner();
			}
			//original(bin, parentDIN);
				}

}
\00after LLLLLLL600: doWork(ChildReference ){
	inFromLog.releaseLatch();
	//original();
    }

}
\00after LLLLLLL611: reconstituteBIN(EnvironmentImpl ) {
		fullBIN.releaseLatch();
		//original(fullBIN);
    }

    after LLLLLLL612: reconstituteBIN(EnvironmentImpl ) {
		fullBIN.latch();
		//original(fullBIN);
    }

}
\00after LLLLLLL206:  cloneCursor(boolean , CursorImpl ){

        latchBINs();
        //original();
    }

    after LLLLLLL207:  cloneCursor(boolean , CursorImpl ) {
        releaseBINs();
        //original();
    }

    after LLLLLLL208:  addCursor(BIN ) {

        assert bin.isLatchOwner();
        //original(bin);
    }

    after LLLLLLL209: removeCursorBIN()  {
        abin.releaseLatch();
        //original(abin);
    }

    after LLLLLLL210: removeCursorDBIN() {
        abin.releaseLatch();
        //original(abin);
    }

    after LLLLLLL211: clearDupBIN(boolean ) {
        dupBin.releaseLatch();
        //original();
    }

		before  count(LockType)
		{
				    latchBIN();
		}

    after LLLLLLL212: {
     //   latchBIN();
 //       try {
            //original(lockType);
  //      } finally {
            releaseBIN();
   //     }
    }

    after LLLLLLL213_1: delete() {

        //try {original(isDup, ln, lockResult, dclLockResult, dupRoot);} finally {

            if (dupRoot != null) {
                dupRoot.releaseLatchIfOwner();
            }
      //  }
    }

    after LLLLLLL214: delete() {


        releaseBINs();
        //original();
    }

    after LLLLLLL215: delete() {

        releaseBINs();
        //original();
    }

    after LLLLLLL216:  delete() {
        releaseBINs();
        //original();
    }

    after LLLLLLL217: putLN(byte, LN , boolean ) { 
        assert LatchSupport.countLatchesHeld() == 0;
        //original();
    }

    after LLLLLLL218_1: putCurrent(DatabaseEntry , DatabaseEntry , DatabaseEntry ) {
    //    try {
            //original(data, foundKey, foundData, isDup);
      //  } finally {
            releaseBINs();
        //}
    }

    after LLLLLLL219: putCurrent(DatabaseEntry , DatabaseEntry , DatabaseEntry ) {

        latchBINs();
        //original();
    }

    after LLLLLLL220: getCurrent(DatabaseEntry , DatabaseEntry , LockType ){
        if (dupBin == null) {
            latchBIN();
        } else {
            latchDBIN();
        }
        //original();
    }

    after LLLLLLL221: getCurrentAlreadyLatched(DatabaseEntry , DatabaseEntry , LockType ,        boolean ){
        assert checkAlreadyLatched(true): dumpToString(true);
        try {
                    throw new ReturnObject(fetchCurrent(foundKey, foundData, lockType, first));
										//original(foundKey, foundData, lockType, first);
        } finally {
            releaseBINs();
        }
    }

    after LLLLLLL222:getCurrentLN(LockType ){
        latchBIN();
        //original();
    }

    after LLLLLLL223_1: getCurrentLNAlreadyLatched(LockType ) {

       // try {
            //original(lockType);
        //} finally {
            releaseBINs();
        //}
    }

    after LLLLLLL224: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        assert checkAlreadyLatched(alreadyLatched): dumpToString(true);
        //original(alreadyLatched);
    }

    protected boolean hook225: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        assert checkAlreadyLatched(alreadyLatched): dumpToString(true);
        if (!alreadyLatched) {
            latchBIN();
        } else {
            alreadyLatched = false;
        }
        return //original(alreadyLatched);
    }

    protected boolean hook226:  getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        alreadyLatched = false;
        return //original(alreadyLatched);
    }

    after LLLLLLL227:getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        assert LatchSupport.countLatchesHeld() == 0;
        //original();
    }

    after LLLLLLL228: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        latchBIN();
        //original();
    }

    after LLLLLLL229: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        releaseBIN();
        //original();
    }

    after LLLLLLL230: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        alreadyLatched = true;
        //original(alreadyLatched);
    }

    after LLLLLLL231: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,LockType , boolean, boolean ) {
        assert LatchSupport.countLatchesHeld() == 0: LatchSupport.latchesHeldToString();
        //original();
    }

    before flushBINToBeRemoved() {

        binToBeRemoved.latch();
        //original();
    }

    after LLLLLLL232:flushBINToBeRemoved() {
        binToBeRemoved.releaseLatch();
        //original();
    }

    before flushDBINToBeRemoved() {
        dupBinToBeRemoved.latch();
        //original();
    }

    after LLLLLLL233: flushDBINToBeRemoved(){
        dupBinToBeRemoved.releaseLatch();
        //original();
    }

    after LLLLLLL234_1: positionFirstOrLast(boolean , DIN ) {
        //try {
            //original(first, duplicateRoot, in, found);
       // } 
       catch (DatabaseException e) {
            if ( in != null) 
            { in .releaseLatch();
            }
            throw e;
        }
    }

    after LLLLLLL235_1:searchAndPosition(DatabaseEntry , DatabaseEntry , SearchMode ,        LockType ) {

        //try {
            //original(matchKey, matchData, searchMode, lockType, foundSomething, foundExactKey, foundExactData, foundLast,            exactSearch,            binBoundary);
//    } 
catch (DatabaseException e) {
        releaseBIN();
        throw e;
    }
}

after LLLLLLL236: searchAndPositionBoth(boolean , Node , DatabaseEntry , boolean , LockType , long ){
    duplicateRoot.latch();
    releaseBIN();
    //original(duplicateRoot);
}

after LLLLLLL237: lockLNDeletedAllowed(LN , LockType )  {
    latchBINs();
    //original();
}

after LLLLLLL238: lockLNDeletedAllowed(LN , LockType )  {
    releaseBINs();
    //original();
}

after LLLLLLL239: lockDupCountLN(DIN , LockType )  {
    dupRoot.latch();
    latchDBIN();
    //original(dupRoot);
}

after LLLLLLL240: lockDupCountLN(DIN , LockType )  {

    latchBIN();
    //original();
}

after LLLLLLL241: lockDupCountLN(DIN , LockType )  {
    dupRoot.releaseLatch();
    releaseBINs();
    //original(dupRoot);
}

after LLLLLLL242: getLatchedDupRoot(boolean ){

    if (isDBINLatched) {
        if (!dupRoot.latchNoWait()) {
            releaseDBIN();
            dupRoot.latch();
            latchDBIN();
        }
    } else {
        dupRoot.latch();
    }
    //original(isDBINLatched, dupRoot);
}

after LLLLLLL243: getLatchedDupRoot(boolean ){
    assert bin.isLatchOwner();
    //original();
}

after LLLLLLL264: count(LockType ) {
    dupRoot.releaseLatch();
    //original(dupRoot);
}

after LLLLLLL265: count(LockType ) {

    dupRoot.latch();
    releaseBIN();
    //original(dupRoot);
}

after LLLLLLL266: delete() {
    releaseBINs();
    //original();
}

after LLLLLLL267:: delete() {
    releaseBIN();
    //original();
}

after LLLLLLL268: delete() {
    dupRoot.releaseLatch();
    //original(dupRoot);
}

after LLLLLLL269:putCurrent(DatabaseEntry , DatabaseEntry , DatabaseEntry ) {
    releaseBINs();
    //original();
}

after LLLLLLL270: putCurrent(DatabaseEntry , DatabaseEntry , DatabaseEntry ) {
    releaseBINs();
    //original();
}

after LLLLLLL271: putCurrent(DatabaseEntry , DatabaseEntry , DatabaseEntry ) {
    releaseBINs();
    //original();
}

after LLLLLLL272:  getCurrentLNAlreadyLatched(LockType ) {
    assert checkAlreadyLatched(true): dumpToString(true);
    //original();
}

after LLLLLLL273:  getCurrentLNAlreadyLatched(LockType ) {
    releaseBIN();
    //original();
}

after LLLLLLL274: positionFirstOrLast(boolean , DIN ){
    dupRoot.latch(); in .releaseLatch();
    //original(in, dupRoot);
}

}
\00after LLLLLLL856: init(long , String , EnvironmentImpl ) {
	workQueueLatch = LatchSupport.makeLatch(name + " work queue", env);
	//original(name, env);
    }

    before addToQueue(Object ) {
	workQueueLatch.acquire();
	//original(o);
	} 

   after addToQueue(Object ) {
	workQueueLatch.release();
    }

  before getQueueSize()  {
	workQueueLatch.acquire();
	}

	after getQueueSize() {
	//int result = //original();
		workQueueLatch.release();
		//return result;
		  }

    after LLLLLLL857: run() {
	workQueueLatch.release();
	//original();
    }

    after LLLLLLL858: run() {

	workQueueLatch.acquire();
	//original();
    }

}
\00after LLLLLLL454: getFileHandle(long ){
	fileHandle.release();
//	original(fileHandle);
    }

}
\00after LLLLLLL603: descendOnParentSearch(SearchResult, boolean, boolean, long, Node, boolean) {

        ((IN) child).releaseLatch();
        //original(child);
    }

    after LLLLLLL604: descendOnParentSearch(SearchResult, boolean, boolean, long, Node, boolean) {

        releaseLatch();
        //original();
    }

    after LLLLLLL605: descendOnParentSearch(SearchResult, boolean, boolean, long, Node, boolean) {
        releaseLatch();
        //original();
    }

    after LLLLLLL606: descendOnParentSearch(SearchResult, boolean, boolean, long, Node, boolean) {
        releaseLatch();
        //original();
    }

    /** 
     * Register a cursor with this bin. Caller has this bin already latched.
     * @param cursorCursor to register.
     */
    before addCursor(CursorImpl) {
        assert isLatchOwner();
        //original(cursor);
    }

    /** 
     * Unregister a cursor with this bin. Caller has this bin already latched.
     * @param cursorCursor to unregister.
     */
    before removeCursor(CursorImpl) {
        assert isLatchOwner();
        //original(cursor);
    }

    /** 
     * Adjust any cursors that are referring to this BIN. This method is called during a split operation. "this" is the BIN being split. newSibling is the new BIN into which the entries from "this" between newSiblingLow and newSiblingHigh have been copied.
     * @param newSibling -the newSibling into which "this" has been split.
     * @param newSiblingLow,newSiblingHigh - the low and high entry of "this" that were moved into newSibling.
     */
    before adjustCursors(IN, int, int) {
        assert newSibling.isLatchOwner();
        assert this.isLatchOwner();
        //original(newSibling, newSiblingLow, newSiblingHigh);
    }

    /** 
     * Adjust cursors referring to this BIN following an insert.
     * @param insertIndex -The index of the new entry.
     */
    before adjustCursorsForInsert(int) {
        assert this.isLatchOwner();
        //original(insertIndex);
    }

    /** 
     * Adjust cursors referring to the given binIndex in this BIN following a mutation of the entry from an LN to a DIN. The entry was moved from a BIN to a newly created DBIN so each cursor must be added to the new DBIN.
     * @param binIndex -The index of the DIN (previously LN) entry in the BIN.
     * @param dupBin -The DBIN into which the LN entry was moved.
     * @param dupBinIndex -The index of the moved LN entry in the DBIN.
     * @param excludeCursor -The cursor being used for insertion and that should not be updated.
     */
    before adjustCursorsForMutation(int, DBIN, int, CursorImpl) {
        assert this.isLatchOwner();
        //original(binIndex, dupBin, dupBinIndex, excludeCursor);
    }

    after LLLLLLL607: isValidForDelete() {
        needToLatch = !isLatchOwner();
    }

    after LLLLLLL607_1: isValidForDelete() {
        if (needToLatch && isLatchOwner()) {
            releaseLatch();
        }
    }

    after LLLLLLL608: isValidForDelete() {
        if (needToLatch) {
            latch();
        }
        //original(needToLatch);
    }

}
\00after LLLLLLL469_1: getLogEntryInReadBuffer() {
	    fileHandle.release();
	
    }

    after LLLLLLL470: fillReadBuffer(int ){
	fileHandle.release();
	//original(fileHandle);
    }

    after LLLLLLL471: fillReadBuffer(int ) {
	if (fileHandle != null) {
	    fileHandle.release();
	}
	//original(fileHandle);
    }

}
\00after LLLLLLL477: initStartingPosition(long , Long ){
	fileHandle.release();
	//original(fileHandle);
    }

    after LLLLLLL478:initStartingPosition(long , Long ){
	if (fileHandle != null) {
	    fileHandle.release();
	}
	//original(fileHandle);
    }

}
\00after LLLLLLL338: INList(EnvironmentImpl ) {
	addedINs = new HashSet();
	majorLatch = LatchSupport.makeLatch(DEBUG_NAME + " Major Latch", envImpl);
	minorLatch = LatchSupport.makeLatch(DEBUG_NAME + " Minor Latch", envImpl);
	//original(envImpl);
    }

    after LLLLLLL339: INList(INList , EnvironmentImpl ) {
	majorLatch = LatchSupport.makeLatch(DEBUG_NAME + " Major Latch", envImpl);
	minorLatch = LatchSupport.makeLatch(DEBUG_NAME + " Minor Latch", envImpl);
	//original(envImpl);
    }

    after LLLLLLL340: INList(INList , EnvironmentImpl ) {
	addedINs = new HashSet();
	//original();
    }

    /** 
     * An IN is getting evicted or is displaced by recovery.  Caller is responsible for acquiring the major latch before calling this and releasing it when they're done.
     */
  before removeLatchAlreadyHeld(IN ) {
	assert majorLatch.isOwner();
	//original(in);
    }

    protected boolean hook341: removeLatchAlreadyHeld(IN ){
		if (!removeDone) {
			  minorLatch.acquire();
			  try {
			removeDone = addedINs.remove(in);
			dumpAddedINsIntoMajorSet();
			  } finally {
			minorLatch.release();
			  }
		}
			//	return //original(in, removeDone);
    }

    /** 
     * An IN is getting swept or is displaced by recovery.
     */
  before remove(IN ) {
	assert LatchSupport.countLatchesHeld() == 0;
	majorLatch.acquire();
	}

  after LLLLLLLRemove_1:remove(IN ) {
	   releaseMajorLatch();

    }

  before tailSet(IN ) { 
	assert majorLatch.isOwner();
	//return //original(in);
    }

  before first() {
	assert majorLatch.isOwner();
	//return //original();
    }

    /** 
     * Return an iterator over the main 'ins' set.  Returned iterator will not show the elements in addedINs. The major latch should be held before entering.  The caller is responsible for releasing the major latch when they're finished with the iterator.
     * @return an iterator over the main 'ins' set.
     */
    before iterator() {
	assert majorLatch.isOwner();
	//return //original();
    }

    /** 
     * Clear the entire list during recovery and at shutdown.
     */
  before clear() {
	assert LatchSupport.countLatchesHeld() == 0;
	majorLatch.acquire();
	minorLatch.acquire();
	//original();
    }

    after clear(){
	addedINs.clear();
	minorLatch.release();
	releaseMajorLatch();
	//original();
    }

}
\00after  LaeLLLLLLl53_1:  getSecondaryDatabases() {
	//		try {
					//original(list);
		//	} finally {
					releaseTriggerListReadLock();
			//}
    }

    after LLLLLLL54_1: notifyTriggers(Locker , DatabaseEntry, DatabaseEntry , DatabaseEntry ) {
			//try {
					//original(locker, priKey, oldData, newData);
			//} finally {
					releaseTriggerListReadLock();
			//}
		}

}
\00after LLLLLLL444:  FileHandle(RandomAccessFile , String , EnvironmentImpl , boolean ) {
	fileLatch = LatchSupport.makeLatch(fileName + "_fileHandle", env);
//	original(fileName, env);
    }

}
\00after LLLLLLL526:flushIN(CheckpointReference , Map , int , boolean , boolean , long ) { 
        result.parent.releaseLatch()
    }

    after LLLLLLL527_1:logTargetAndUpdateParent(IN , IN , int , boolean , long , boolean ) {
	    target.releaseLatch(); 
     }


}
\00after LLLLLLL320: EnvironmentImpl(File , EnvironmentConfig ) {
	triggerLatch = LatchSupport.makeSharedLatch("TriggerLatch", this);
	//original();
    }

    after LLLLLLL321: EnvironmentImpl(File , EnvironmentConfig ) { 
			if (fairLatches) {
					logManager = new LatchedLogManager(this, isReadOnly);
			} else 
    }

    after LLLLLLL322: EnvironmentImpl(File , EnvironmentConfig ) { 
	fairLatches = configManager.getBoolean(EnvironmentParams.ENV_FAIR_LATCHES);
	//original();
    }

    after LLLLLLL323: EnvironmentImpl(File , EnvironmentConfig ) { 
	mapTreeRootLatch = LatchSupport.makeLatch("MapTreeRoot", this);
	//original();
    }

    /** 
     * Log the map tree root and save the LSN.
     */

		before logMapTreeRoot() {
			mapTreeRootLatch.acquire();
		}
	
		after logMapTreeRoot_1:logMapTreeRoot() {

			//try {original();} finally {
					mapTreeRootLatch.release();
			//}
    }

    /** 
     * Force a rewrite of the map tree root if required.
     */
  before rewriteMapTreeRoot(long ) {
			mapTreeRootLatch.acquire();
    }

    after LLLLLLL_rewriteMapTreeRoot_1: rewriteMapTreeRoot(long ) {
	    mapTreeRootLatch.release();
	
    }

  after LLLLLLL324: readMapTreeFromLog(long ) {
	mapTreeRootLatch.acquire();
	}

  after LLLLLLL324_1: readMapTreeFromLog(long ) {
	   mapTreeRootLatch.release();
    }

}
\00after LLLLLLL299: modifyDbRoot(DatabaseImpl )  {
	cursor.releaseBINs();
	//original(cursor);
    }

    after LLLLLLL300: lockNameLN(Locker , String , String )  {
	result.nameCursor.releaseBIN();
	//original(result);
    }

    after LLLLLLL301: lockNameLN(Locker , String , String )  {
	result.nameCursor.releaseBIN();
	//original(result);
    }

    after LLLLLLL302: lockNameLN(Locker , String , String )  {
	result.nameCursor.releaseBIN();
	//original(result);
    }

    after LLLLLLL303:  getDb(Locker , String , Database , boolean ){
	nameCursor.releaseBIN();
	//original(nameCursor);
    }

    after LLLLLLL304: getDb(DatabaseId , long , boolean , String ) {
	idCursor.releaseBIN();
	//original(idCursor);
    }

    after LLLLLLL305: getDbName(DatabaseId ) {
	cursor.releaseBINs();
	//original(cursor);
    }

}
\00after LLLLLLL19: deleteNoNotify() {

			dup.latchBINs();
			//original(dup);
    }

    after LLLLLLL20:deleteNoNotify(){

			if (origCursor != null) {
					origCursor.releaseBINs();
			}
			if (dup != null) {
					dup.releaseBINs();
			}
			//original(origCursor, dup);
    }

    after LLLLLLL21: putAllowPhantoms(DatabaseEntry , DatabaseEntry, PutMode ,	DatabaseEntry , CursorImpl ) {

			if (origCursor != null) {
					origCursor.releaseBINs();
			}
			//original(origCursor);
    }

    after LLLLLLL22: positionAllowPhantoms(DatabaseEntry , DatabaseEntry , LockType , boolean ) {
				assert LatchSupport.countLatchesHeld() == 0 : LatchSupport.latchesHeldToString();
      	//original();
    }

    after LLLLLLL23: positionAllowPhantoms(DatabaseEntry , DatabaseEntry , LockType , boolean ) {
				assert LatchSupport.countLatchesHeld() == 1 : LatchSupport.latchesHeldToString();
				//original();
    }

    after LLLLLLL24:  positionAllowPhantoms(DatabaseEntry , DatabaseEntry , LockType , boolean ) {
			
			cursorImpl.releaseBINs();
			//original();
    }

    after LLLLLLL25:searchInternal(CursorImpl , DatabaseEntry , DatabaseEntry , LockType , LockType , SearchMode , boolean ) {
			cursorImpl.releaseBINs();
			if (status != OperationStatus.SUCCESS && dup != cursorImpl) {
				dup.releaseBINs();								
			}
    }

    after LLLLLLL26: getNextDupAndRangeLock(DatabaseEntry , DatabaseEntry , LockMode )  {
			assert LatchSupport.countLatchesHeld() == 0;
		//original();
    }

    after LLLLLLL27:getNextDupAndRangeLock(DatabaseEntry , DatabaseEntry , LockMode )  {
			assert LatchSupport.countLatchesHeld() == 0;
			//original();
    }

    after LLLLLLL28: rangeLockCurrentPosition(GetMode ) { 
			assert LatchSupport.countLatchesHeld() == 0;
		//original();
    }

    after LLLLLLL29: rangeLockCurrentPosition(GetMode ) { 
			assert LatchSupport.countLatchesHeld() == 0;
		//original();
    }

    after LLLLLLL30: retrieveNextAllowPhantoms(DatabaseEntry , DatabaseEntry, LockType , GetMode )  {
			assert LatchSupport.countLatchesHeld() == 0;
	//original();
    }

    after LLLLLLL31: retrieveNextAllowPhantoms(DatabaseEntry , DatabaseEntry, LockType , GetMode )  {
	assert LatchSupport.countLatchesHeld() == 0;
	//original();
    }

    after LLLLLLL32: checkForInsertion(GetMode , CursorImpl , CursorImpl ) {
	origCursor.releaseBINs();
	//original(origCursor);
    }

    after LLLLLLL33: checkForInsertion(GetMode , CursorImpl , CursorImpl )  {
	origCursor.latchBINs();
	//original(origCursor);
    }

    after LLLLLLL34: checkForInsertion(GetMode , CursorImpl , CursorImpl )  {
	origCursor.releaseBINs();
	//original(origCursor);
    }

    after LLLLLLL35: checkForInsertion(GetMode , CursorImpl , CursorImpl )  {
	origCursor.latchBINs();
	//original(origCursor);
    }

}
\00after LLLLLLL599: doWork(ChildReference ) {
			rootIN.latch(false);
    }

    after LLLLLLL599_1: doWork(ChildReference ) {			
//			try {original(root, rootIN);} finally {
					rootIN.releaseLatch();
	//		}
    }

}
\00after LLLLLLL670_1: withRootLatchedExclusive(WithRootLatched ) {
        //	try {	    //original(wrl);} finally {
        rootLatch.release();
        //	}
    }

    after LLLLLLL671_1: withRootLatchedExclusive(WithRootLatched ) {
        //	try {   //original(wrl);	} finally {
        rootLatch.release();
        //}
    }

    after LLLLLLL672: delete(byte , UtilizationTracker ){
        rootLatch.acquireExclusive();
    }


    after LLLLLLL672_1: delete(byte , UtilizationTracker  ){
        //finally {
        releaseNodeLadderLatches(nodeLadder);
        if (rootIN != null) {
            rootIN.releaseLatch();
        }
        rootLatch.release();
    }

    /** 
     * This entire tree is empty, clear the root and log a new MapLN
     * @return the rootIN that has been detached, or null if there hasn't beenany removal.
     */
    before logTreeRemoval(IN, UtilizationTracker) {
        assert rootLatch.isWriteLockedByCurrentThread();
        //	return //original(rootIN, tracker);
    }

    after LLLLLLL673: cascadeUpdates(ArrayList, BIN, int) {
        assert rootLatch.isWriteLockedByCurrentThread();
        //original();
    }

    after LLLLLLL674_1: deleteDupSubtree(byte, BIN, int) {
        //try {// deletedSubtreeRoot = //original(idKey, mainKey, in, deletedSubtreeRoot);} finally {
        in .releaseLatch();
        //}
        //return deletedSubtreeRoot;
    }

    after LLLLLLL675: deleteDupSubtree(byte, BIN, int) {

        if (duplicateRoot != null) {
            duplicateRoot.releaseLatch();
        }
        //original(duplicateRoot);
    }

    after LLLLLLL676: deleteDupSubtree(byte, BIN, int) {
        releaseNodeLadderLatches(nodeLadder);
        //original(nodeLadder);
    }

    after LLLLLLL677: getFirstNode(DIN) {
        assert dupRoot.isLatchOwner();
        //original(dupRoot);
    }

    after LLLLLLL678: getLastNode(DIN) {
        assert dupRoot.isLatchOwner();
        //original(dupRoot);
    }

    after LLLLLLL679: getParentINForChildIN(IN, boolean, boolean, int, List) {
        child.releaseLatch();
        //original(child);
    }

    after LLLLLLL680: getParentINForChildIN(IN, boolean, boolean, int, List) {
        assert child.isLatchOwner();
        //original(child);
    }

    after LLLLLLL681: getParentINForChildIN(long, boolean, boolean, byte, byte, boolean, boolean, int, List, boolean) {

        potentialParent.releaseLatchIfOwner();
        //original(potentialParent);
    }

    after LLLLLLL682: getParentBINForChildLN(TreeLocation, byte, byte, LN, boolean, boolean, boolean, boolean) {
        searchResult.releaseLatchIfOwner();
        //original(searchResult);
    }

    after LLLLLLL683_1: getParentBINForChildLN(TreeLocation, byte, byte, LN, boolean, boolean, boolean, boolean) {
        //	try {	    //original(location, mainKey, dupKey, ln, splitsAllowed, findDeletedEntries, searchDupTree, updateGeneration,		    exactSearch, indicateIfExact, childNode);} 

        catch (DatabaseException e) {
            location.bin.releaseLatchIfOwner();
            throw e;
        }
    }

    after LLLLLLL684: searchDupTreeByNodeId(TreeLocation, Node, LN, boolean, boolean) {
        if (oldBIN != null) {
            oldBIN.releaseLatch();
        }
        //original(oldBIN);
    }

    after LLLLLLL685: searchDupTreeForDBIN(TreeLocation, byte, DIN, LN, boolean, boolean, boolean, boolean, boolean) {
        dupRoot.latch();
    }

    after LLLLLLL685_1: searchDupTreeForDBIN(TreeLocation, byte, DIN, LN, boolean, boolean, boolean, boolean, boolean) {

        //	try {	    //original(location, dupKey, dupRoot, ln, findDeletedEntries, indicateIfExact, exactSearch, splitsAllowed, updateGeneration);} 
        catch (DatabaseException e) {
            dupRoot.releaseLatchIfOwner();
            throw e;
        }
    }

    after LLLLLLL686_1: getNextBinInternal(boolean, BIN, boolean) {
        //					try {							//original(traverseWithinDupTree, forward, idKey, next, parent, nextIN);} 

        catch (DatabaseException e) {
            next.releaseLatchIfOwner();
            if (parent != null) {
                parent.releaseLatchIfOwner();
            }
            if (nextIN != null) {
                nextIN.releaseLatchIfOwner();
            }
            throw e;
        }
    }

    after LLLLLLL687: getNextBinInternal(boolean, BIN, boolean) {
        assert LatchSupport.countLatchesHeld() == 1: LatchSupport.latchesHeldToString();
        //original();
    }

    after LLLLLLL688_1: splitRoot() {

        curRoot.releaseLatch();

    }

    after LLLLLLL689: splitRoot() {

        curRoot.latch();
        //original(curRoot);
    }

    after LLLLLLL690: searchSubTree(IN, byte, SearchType, long, BINBoundary, boolean) {
        assert parent.isLatchOwner();
        //original(parent);
    }

    after LLLLLLL691: searchSubTree(IN, byte, SearchType, long, BINBoundary, boolean) {

        parent.releaseLatch();
        //original(parent);
    }

    after LLLLLLL692: searchSubTree(IN, byte, SearchType, long, BINBoundary, boolean) {
        parent.releaseLatch();
        //original(parent);
    }

    after LLLLLLL693: searchSubTree(IN, byte, SearchType, long, BINBoundary, boolean) {
        child.releaseLatch();
        //original(child);
    }

    after LLLLLLL694: searchSubTree(IN, byte, SearchType, long, BINBoundary, boolean) {
        if (child != null) {
            child.releaseLatchIfOwner();
        }
        parent.releaseLatchIfOwner();
        //original(parent, child);
    }

    after LLLLLLL695: searchDeletableSubTree(IN, byte, ArrayList) {
        assert parent.isLatchOwner();
        //original(parent);
    }

    after LLLLLLL696: searchDeletableSubTree(IN, byte, ArrayList) {
        info5.child.releaseLatch();
        //original(info5);
    }

    after LLLLLLL697: searchDeletableSubTree(IN, byte, ArrayList) {
        releaseNodeLadderLatches(nodeLadder);
        //original(nodeLadder);
    }

    after LLLLLLL698_1: searchSubTreeUntilSplit(IN, byte, long, boolean) {
        catch (DatabaseException e) {
            if (child != null) {
                child.releaseLatchIfOwner();
            }
            parent.releaseLatchIfOwner();
            throw e;
        }
    }

        after LLLLLLL699: searchSubTreeUntilSplit(IN, byte, long, boolean) {
            assert parent.isLatchOwner();
            //original(parent);
        }

        after LLLLLLL700: searchSubTreeUntilSplit(IN, byte, long, boolean) {
            parent.releaseLatch();
            //original(parent);
        }

        after LLLLLLL701_1: getRootIN(boolean) {

            rootLatch.release();

        }

        after LLLLLLL702: getRootIN(boolean) {
            rootLatch.acquireShared();
            //original();
        }

        after LLLLLLL703_1: insert(LN, byte, boolean, CursorImpl, LockResult) {


            cursor.releaseBIN();

        }

        after LLLLLLL704_1: insertDuplicate(byte, BIN, LN, LogManager, INList, CursorImpl, LockResult, boolean) {
            if (dupBin != null) {
                dupBin.releaseLatchIfOwner();
            }
            if (dupRoot != null) {
                dupRoot.releaseLatchIfOwner();
            }
        }

        after LLLLLLL705: insertDuplicate(byte, BIN, LN, LogManager, INList, CursorImpl, LockResult, boolean) {
            dupRoot.releaseLatch();
            //original(dupRoot);
        }

        after LLLLLLL706_1: maybeSplitDuplicateRoot(BIN, int) {

            curRoot.releaseLatch();

        }

        after LLLLLLL707: maybeSplitDuplicateRoot(BIN, int) {
            newRoot.latch();
            //original(newRoot);
        }

        after LLLLLLL708_1: createDuplicateTree(byte, LogManager, INList, LN, CursorImpl) {

            catch (DatabaseException e) {
                dupBin.releaseLatchIfOwner();
                dupRoot.releaseLatchIfOwner();
                throw e;
            }
        }

        after LLLLLLL709: createDuplicateTree(byte, LogManager, INList, LN, CursorImpl) {
            dupBin.latch();
            //original(dupBin);
        }

        after LLLLLLL710: createDuplicateTree(byte, LogManager, INList, LN, CursorImpl) {
            dupRoot.latch();
            //original(dupRoot);
        }

        after LLLLLLL711_1: findBinForInsert(byte, LogManager, INList, CursorImpl) {

            if (rootLatchIsHeld) {
                rootLatch.release();
            }
        }


        after LLLLLLL712: findBinForInsert(byte , LogManager , INList , CursorImpl ) {
            bin.releaseLatch();
            //original(bin);
        }

        after LLLLLLL713: accountForSubtreeRemoval(INList , IN , UtilizationTracker ) {
            inList.latchMajor();
        }


        after LLLLLLL713_1: accountForSubtreeRemoval(INList , IN , UtilizationTracker ) {
            inList.releaseMajorLatch();

        }

        after LLLLLLL714: validateDelete(int ) {
            rootLatch.acquireShared();
            try {
                //original(inMemoryList);
            } finally {
                rootLatch.release();
            }
        }

        after LLLLLLL715: validateDelete(int ) {
            rootLatch.acquireShared();
        }
        after LLLLLLL715_1: validateDelete(int ) {

            //	try {    //original(index);	} finally {
            rootLatch.release();
            //}
        }

        after LLLLLLL728: withRootLatchedShared(WithRootLatched ) {
            rootLatch.acquireExclusive();
            //original();
        }

        after LLLLLLL729: withRootLatchedShared(WithRootLatched ) {
            rootLatch.acquireShared();
            //original();
        }

        after LLLLLLL730: deleteDup(byte, byte, UtilizationTracker) {
            assert in .isLatchOwner();
            //original(in);
        }

        after LLLLLLL731: searchDupTreeForDBIN(TreeLocation, byte, DIN, LN, boolean, boolean, boolean, boolean, boolean) {
            location.bin.releaseLatch();
            //original(location);
        }

        after LLLLLLL732: getNextBinInternal(boolean, BIN, boolean) {
            assert(LatchSupport.countLatchesHeld() == 1): LatchSupport.latchesHeldToString();
            //original();
        }

        after LLLLLLL733: getNextBinInternal(boolean, BIN, boolean) {
            assert(LatchSupport.countLatchesHeld() == 0): LatchSupport.latchesHeldToString();
            //original();
        }

        after LLLLLLL734: getNextBinInternal(boolean, BIN, boolean) {
            next.releaseLatch();
            //original(next);
        }

        after LLLLLLL735(IN  ) {
            nextIN.latch();
            assert(LatchSupport.countLatchesHeld() == 2): LatchSupport.latchesHeldToString();
            //original(nextIN);
        }

        after LLLLLLL736: getNextBinInternal(boolean, BIN, boolean) {
            parent.releaseLatch();
            //original(parent);
        }

        after LLLLLLL737: getNextBinInternal(boolean, BIN, boolean) {
            parent.releaseLatch();
            assert LatchSupport.countLatchesHeld() == 1: LatchSupport.latchesHeldToString();
            //original(parent);
        }

        after LLLLLLL738: searchSubTreeUntilSplit(IN, byte, long, boolean) {
            parent.releaseLatch();
            //original(parent);
        }

        after LLLLLLL739: searchSubTreeUntilSplit(IN, byte, long, boolean) {
            child.releaseLatch();
            parent.releaseLatch();
            //original(parent, child);
        }

        after LLLLLLL740: searchSubTreeUntilSplit(IN, byte, long, boolean) {
            child.releaseLatch();
            //original(child);
        }

        after LLLLLLL741: insert(LN, byte, boolean, CursorImpl, LockResult) {
            assert bin.isLatchOwner();
            //original(bin);
        }

        after LLLLLLL742: insertDuplicate(byte, BIN, LN, LogManager, INList, CursorImpl, LockResult, boolean) {
            dupBin.releaseLatch();
            //original(dupBin);
        }

        after LLLLLLL743: insertDuplicate(byte, BIN, LN, LogManager, INList, CursorImpl, LockResult, boolean) {
            cursor.releaseBIN();
            //original(cursor);
        }

        after LLLLLLL744: insertDuplicate(byte, BIN, LN, LogManager, INList, CursorImpl, LockResult, boolean) {
            dupRoot.latch();
            //original(dupRoot);
        }

        after LLLLLLL745: insertDuplicate(byte, BIN, LN, LogManager, INList, CursorImpl, LockResult, boolean) {
            cursor.releaseBIN();
            //original(cursor);
        }

        after LLLLLLL746: insertDuplicate(byte, BIN, LN, LogManager, INList, CursorImpl, LockResult, boolean) {
            cursor.latchBIN();
            //original(cursor);
        }

        after LLLLLLL747: createDuplicateTree(byte, LogManager, INList, LN, CursorImpl) {
            dupBin.releaseLatch();
            //original(dupBin);
        }

        after LLLLLLL748: findBinForInsert(byte, LogManager, INList, CursorImpl) {
            rootLatch.acquireShared();
            //original();
        }

        after LLLLLLL749: findBinForInsert(byte, LogManager, INList, CursorImpl) {
            rootLatch.release();
            //original();
        }

        after LLLLLLL750: findBinForInsert(byte, LogManager, INList, CursorImpl) {
            bin.latch();
            //original(bin);
        }

        after LLLLLLL751: findBinForInsert(byte, LogManager, INList, CursorImpl) {
            rootLatch.release();
            rootLatch.acquireExclusive();
            //original();
        }

        after LLLLLLL752: findBinForInsert(byte, LogManager, INList, CursorImpl) {
            rootLatch.release();
            //original();
        }

        after LLLLLLL753: findBinForInsert(byte, LogManager, INList, CursorImpl) {
            rootLatch.release();
            //original();
        }

    }
\00after LLLLLLL485: LogBufferPool(FileManager , EnvironmentImpl ) {
	bufferPoolLatch = LatchSupport.makeLatch(DEBUG_NAME + "_FullLatch", envImpl);
	//original(envImpl);
    }

    /** 
     * Initialize the pool at construction time and when the cache is resized. This method is called after the memory budget has been calculated.
     */
    after reset(DbConfigManager )  {
	//original(configManager);
	bufferPoolLatch.release();
    }

    after LLLLLLL486: reset(DbConfigManager ) {
	bufferPoolLatch.acquire();
	//original();
    }

    after LLLLLLL487_1:writeBufferToFile(int ) {

//	try { //original(bufferSize, latchedBuffer);} finally {
	    if (latchedBuffer != null) {
		latchedBuffer.release();
	    }
//	}
    }

    after LLLLLLL488:writeBufferToFile(int ) {

	currentWriteBuffer.latchForWrite();
	//original();
    }

    protected LogBuffer hook489: getReadBuffer(long ) {
	bufferPoolLatch.acquire();
//	try {foundBuffer = //original(lsn, foundBuffer);} finally {	    bufferPoolLatch.releaseIfOwner();}
	//return foundBuffer;
    }

    protected LogBuffer Labe1: getReadBuffer(long ) {

	    bufferPoolLatch.releaseIfOwner();
	//return foundBuffer;
    }


    after LLLLLLL490: writeBufferToFile(int ) { 
	bufferPoolLatch.release();
	//original();
    }

    after LLLLLLL491: writeBufferToFile(int ) { 
	bufferPoolLatch.acquire();
	//original();
    }

    after LLLLLLL492: writeBufferToFile(int ) { 
	latchedBuffer.release();
	//original(latchedBuffer);
    }

    after LLLLLLL493: writeBufferToFile(int ) { 
//	try {	    //original(nextToUse);} finally {
	    bufferPoolLatch.releaseIfOwner();
	//}
    }

    after LLLLLLL494: writeBufferToFile(int ) { 
	latchedBuffer.release();
	//original(latchedBuffer);
    }

    after LLLLLLL495: writeBufferToFile(int ) { 

	bufferPoolLatch.acquire();
	//original();
    }

}
\00after LLLLLLL178: deleteFileSummary(Long ) {

	cursor.releaseBINs();
//	original(cursor);
    }

    after LLLLLLL179: getObsoleteDetail(Long , PackedOffsets , boolean ){
	if (cursor != null) {
	    cursor.releaseBINs();
	    cursor.close();
	}
	//original(cursor);
    }

    after LLLLLLL180_1: verifyLsnIsObsolete(long ) {
	//try {	    //original(lsn, entry, db, bin);} finally {
	    if (bin != null) {
		bin.releaseLatch();
	    }
	//}
    }

}
\00after LLLLLLL502: LogManager(EnvironmentImpl , boolean ){

	logWriteLatch = LatchSupport.makeLatch(DEBUG_NAME, envImpl);
	//original(envImpl);
    }

    after LLLLLLL503: logInternal(LoggableObject , boolean , boolean ,	 boolean , long , boolean , ByteBuffer , UtilizationTracker ) {
      useLogBuffer.latchForWrite();
	}

    after LLLLLLL503_1: logInternal(LoggableObject , boolean , boolean ,	 boolean , long , boolean , ByteBuffer , UtilizationTracker ) {
//try {	    usedTemporaryBuffer = original(marshalledBuffer, entrySize, currentLsn, usedTemporaryBuffer, useLogBuffer);} finally {
	    useLogBuffer.release();
	//}
//	return usedTemporaryBuffer;
    }

}
\00after LLLLLLL585: replayOneIN(INFileReader , DatabaseImpl , boolean ) {
	in.latch();
	//original(in);
    }

    after LLLLLLL586_1: undoLNs(RecoveryInfo , Set ) {
//	try {	    //original(info, reader, location, ln, logLsn, abortLsn, abortKnownDeleted, db);	} finally {
	    if (location.bin != null) {
		location.bin.releaseLatchIfOwner();
	    }
	//}
    }

    after LLLLLLL587:  replaceOrInsert(DatabaseImpl , IN , long , long , boolean ) {
	inFromLog.releaseLatchIfOwner();
	assert (LatchSupport.countLatchesHeld() == 0) : LatchSupport.latchesHeldToString() + "LSN = "
		+ DbLsn.toString(logLsn) + " inFromLog = " + inFromLog.getNodeId();
	//original(inFromLog, logLsn);
    }

    after LLLLLLL588:replayINDelete(DatabaseImpl , long , boolean , byte , byte , long ) {
	if (result.parent != null) {
	    result.parent.releaseLatch();
	}
	//original(result);
    }

    after LLLLLLL589: replaceOrInsertDuplicateRoot(DatabaseImpl , DIN , long ) {
	if (parent != null) {
	    parent.releaseLatch();
	}
	//original(parent);
    }

    after LLLLLLL590: replaceOrInsertChild(DatabaseImpl , IN , long , long , List , boolean )  {
	if (result.parent != null) {
	    result.parent.releaseLatch();
	}
	//original(result);
    }

    after LLLLLLL591: redo(DatabaseImpl , TreeLocation , LN , byte , byte , long , RecoveryInfo ) {
	if (location.bin != null) {
	    location.bin.releaseLatchIfOwner();
	}
	//original(location);
    }

    after LLLLLLL592: undo(Level , DatabaseImpl , TreeLocation , LN , byte ,byte , long , long , boolean , RecoveryInfo ,  boolean ) {
//(TreeLocation location, long logLsn, long abortLsn, boolean replaced,    DIN duplicateRoot) throws DatabaseException {
	duplicateRoot.latch();
  }

    after LLLLLLL592_1: undo (Level , DatabaseImpl , TreeLocation , LN , byte ,byte , long , long , boolean , RecoveryInfo ,  boolean ) {
	//try {replaced = //original(location, logLsn, abortLsn, replaced, duplicateRoot);} finally {
	    duplicateRoot.releaseLatch();//}return replaced;
    }


}
\00after LLLLLLL770: LockManager(EnvironmentImpl ) {
	lockTableLatches = new Latch[nLockTables];
	//original();
    }

    after LLLLLLL771: LockManager(EnvironmentImpl ) {

	lockTableLatches[i] = LatchSupport.makeLatch("Lock Table " + i, envImpl);
	//original(envImpl, i);
    }

    after LLLLLLL772: lock(long , Locker , LockType , long , boolean ,   DatabaseImpl ) {

	assert checkNoLatchesHeld(nonBlockingRequest) : LatchSupport.countLatchesHeld()
		+ " latches held while trying to lock, lock table =" + LatchSupport.latchesHeldToString();
//	original(nonBlockingRequest);
    }

		after LLLLLLL773:dumpToString()
		{
			lockTableLatches[i].acquire();
		}

    after LLLLLLL773_1:
		{
	    lockTableLatches[i].release();
    }

}
\00after LLLLLLL822:TxnManager(EnvironmentImpl ){
		if (EnvironmentImpl.getFairLatches()) {
			  lockManager = new LatchedLockManager(env);
		} else 
    
}

    /** 
     * Called when txn is created.
     */
    before  registerTxn(Txn ) {

	allTxnLatch.acquire();
	//original(txn);

    }

    after registerTxn(Txn )  {
	//original(txn);
	allTxnLatch.release();
    }


    /** 
     * Called when txn ends.
     */
  before unRegisterTxn(Txn , boolean ) {
	allTxnLatch.acquire();
	}

	after unRegisterTxn(Txn , boolean ) {
	//try {	    //original(txn, isCommit);	} finally {
	    allTxnLatch.release();
	//}
    }

  after LLLLLLL823:getFirstActiveLsn(){
		allTxnLatch.acquire();
    }

  after LLLLLLL823_1:getFirstActiveLsn(){
		allTxnLatch.release();
	
    }

}
\00after LLLLLLL755:TreeIterator(Tree )   {
	if (nextBin != null) {
	    nextBin.releaseLatch();
	}
	//original();
    }

    after LLLLLLL756: hasNext()  {
	if (nextBin != null) {
	    nextBin.latch();
	}
	//original();
    }

    after LLLLLLL757: hasNext() {
	try {
	    if (nextBin != null) {
		nextBin.releaseLatch();
	    }
	} catch (LatchNotHeldException e) {
	}
	//original();
    }

    after LLLLLLL758: next()   {
	nextBin.latch();
	//original();
    }

    after LLLLLLL759: next() {
	try {
	    if (nextBin != null) {
		nextBin.releaseLatch();
	    }
	} catch (LatchNotHeldException e) {
	}
	//original();
    }

}
\00after LLLLLLL352: fetchLSN(long ) {
	in.latch();
  }

 after LLLLLLL352_1: fetchLSN(long ) {

	    in.releaseLatch();
 }

}
\00after LLLLLLL479:  LogBuffer(int , EnvironmentImpl ) {

	readLatch = LatchSupport.makeLatch(DEBUG_NAME, env);
	////original(env);
    }

  before reinit(){
	readLatch.acquire();
	//original();
	//readLatch.release();
    }  


  after reinit(){
	readLatch.release();
    }

    /** 
     * This LSN has been written to the log.
     */
    before registerLsn(long )  {
	readLatch.acquire();
    }


    after registerLsn(long )  {
	    readLatch.release();
	
    }


    /** 
     * Support for reading a log entry out of a still-in-memory log
     * @return true if this buffer holds the entry at this LSN. The buffer willbe latched for read. Returns false if LSN is not here, and releases the read latch.
     */
  before containsLsn(long )  {
	readLatch.acquire();
	//return //original(lsn);
    }

    after LLLLLLL480: containsLsn(long ) {
	readLatch.release();
	//original();
    }

}
\00after LLLLLLL134_1:findINInTree(Tree , DatabaseImpl , IN , long ) {
	    if ((result != null) && (result.exactParentFound)) {
					result.parent.releaseLatch();
	    }
	
    }

     after LLLLLLL136: processIN(IN , DatabaseImpl , long ) {
	inInTree.releaseLatch();
//	original(inInTree);
    }

}
\00after LLLLLLL135: execute() {
        if (parentDIN != null) {
          parentDIN.releaseLatchIfOwner();
        }
        if (bin != null) {
          bin.releaseLatchIfOwner();
        }
        //original();
      }
    }
}
\00after execute() {
  //      original();
        env=_this.envHandle.getEnvironmentImpl();
        env.getTriggerLatch().release();
      }
    }
   static class Database_acquireTriggerListWriteLock {
      before execute() {
        env=_this.envHandle.getEnvironmentImpl();
        env.getTriggerLatch().acquireExclusive();
        //original();
      }
    }
}
\00after LLLLLLL250: execute() {
        assert _this.checkAlreadyLatched(alreadyLatched) : _this.dumpToString(true);
        //original();
      }

      after LLLLLLL251: execute() {
        if (!alreadyLatched) {
          _this.latchDBIN();
        }
   else {
          alreadyLatched=false;
        }
        //original();
      }
      after LLLLLLL252: execute() {
        _this.releaseDBIN();
        //original();
      }
      after LLLLLLL253: execute() {
        assert LatchSupport.countLatchesHeld() == 0;
        //original();
      }
      after LLLLLLL254: execute() {

        assert (LatchSupport.countLatchesHeld() == 0);
        _this.dupBinToBeRemoved.latch();
        //original();
      }
      after LLLLLLL255: execute() {
        _this.dupBinToBeRemoved.releaseLatch();
        //original();
      }
      after LLLLLLL256: execute() {

        alreadyLatched=true;
        //original();
      }
      after LLLLLLL257: execute() {
        assert LatchSupport.countLatchesHeld() == 0;
        //original();
      }
    }
   static class CursorImpl_lockNextKeyForInsert {
     
      after LLLLLLL249:execute(){
        latched=false;
        //original();
      }
    }

   static class CursorImpl_fetchCurrent {


      after LLLLLLL259:execute(){
        assert _this.targetBin.isLatchOwner();
        //original();
      }


      after LLLLLLL261:execute(){
        _this.targetBin.releaseLatchIfOwner();
        //original();
      }
      after LLLLLLL262:execute(){
        duplicateRoot.latch();
        _this.targetBin.releaseLatch();
        //original();
      }

        try {
          around_proceed://original();
        }
   catch (      DatabaseException DE) {
          _this.releaseBINs();
          throw DE;
        }
      }
    }
}
\00after LLLLLLL181:  execute() {

        cursor.releaseBIN();
        //original();
      }
      after LLLLLLL182:  execute() {

        cursor.latchBIN();
        //original();
      }
      after LLLLLLL183:  execute() {

        cursor.releaseBIN();
        //original();
      }
      after LLLLLLL184:  execute() {

        cursor.latchBIN();
        //original();
      }
      after LLLLLLL185:  execute() {
        cursor.releaseBINs();
        //original();
      }
    }
}
\00after LLLLLLL528_1: execute(){
          inMemINs.releaseMajorLatchIfHeld();
        
      }
     after LLLLLLL529:execute(){
        inMemINs.latchMajor();
        //original();
      }

      afterLabel530_1: execute(){

          in.releaseLatch();
        
      }
    }
}
\00after LLLLLLL722: execute()   {
        isRootLatched=false;
        //original();
      }

      after LLLLLLL723:execute()   {
        if (origParent.isDbRoot()) {
          _this.rootLatch.acquireExclusive();
          isRootLatched=true;
        }
        origParent.latch();
        //original();
      }

      after LLLLLLL724:execute()    {
        child.latch();
        //original();
      }

      after LLLLLLL725: execute()    {
        child.releaseLatch();
        //original();
      }

      after LLLLLLL726:execute()    {
        assert isRootLatched;
        //original();
      }

      after LLLLLLL727: execute()   {
        if (!success) {
          if (child != null) {
            child.releaseLatchIfOwner();
          }
          origParent.releaseLatchIfOwner();
        }
        if (nodeLadder.size() > 0) {
          iter=nodeLadder.listIterator(nodeLadder.size());
          while (iter.hasPrevious()) {
            info2=(SplitInfo)iter.previous();
            info2.child.releaseLatchIfOwner();
          }
        }
        if (isRootLatched) {
          _this.rootLatch.release();
        }
        //original();
      }
    }

   static class Tree_searchSplitsAllowed {
      after LLLLLLL716_1:execute()   {
          if (rootLatched) {
            _this.rootLatch.release();
          }
        
      }
      after LLLLLLL717:execute()    {
        _this.rootLatch.acquireShared();
        rootLatched=true;
        rootLatchedExclusive=false;
        //original();
      }

      after LLLLLLL718:execute()    {
        rootIN.latch();
        //original();
      }

      after LLLLLLL719:execute()    {
        rootLatched=true;
        _this.rootLatch.acquireExclusive();
        //original();
      }

      after LLLLLLL720:execute()    {
        _this.splitRoot();
        _this.rootLatch.release();
        rootLatched=false;
        //original();
      }

      after LLLLLLL721:execute()    {
        b=!rootLatchedExclusive;
        if (b) {
          rootIN=null;
          _this.rootLatch.release();
          _this.rootLatch.acquireExclusive();
          rootLatchedExclusive=true;
        }
        //original();
      }
    }


  inner class RootChildReference {

      after LLLLLLL666: fetchTarget(    DatabaseImpl ,    IN )   {
        if (getTarget() == null && !rootLatch.isWriteLockedByCurrentThread()) {
          rootLatch.release();
          rootLatch.acquireExclusive();
        }
        //original();
      }
      after LLLLLLL667: setTarget(    Node ){
        assert rootLatch.isWriteLockedByCurrentThread();
        //original();
      }
      after LLLLLLL668: clearTarget(){
        assert rootLatch.isWriteLockedByCurrentThread();
        //original();
      }
      after LLLLLLL669: setLsn(    long ){
        assert rootLatch.isWriteLockedByCurrentThread();
        //original();
      }
    }
}





\00after LLLLLLL630: execute() {
//     try {          //original();}//  finally {
          newSibling.releaseLatch();
    //    }
      }

      after LLLLLLL631: execute() {
        newSibling.latch();
        //original();
      }
    }

   static class IN_isValidForDelete {
      
    after LLLLLLL634:execute() {
        needToLatch=!_this.isLatchOwner();
		}
   after LLLLLLL634_1:execute() {
//        try {          //original();} finally {
          if (needToLatch) {
            _this.releaseLatchIfOwner();
          }
  //      }
      }

      after LLLLLLL635: execute() {
        if (needToLatch) {
          _this.latch();
        }
        //original();
      }
    }

   static class IN_validateSubtreeBeforeDelete {
      
		after LLLLLLL628: execute() {
        needToLatch=!_this.isLatchOwner();    
				//try {          //original();//      }
    }

  after LLLLLLL628_1: execute() {
          if (needToLatch) {
            _this.releaseLatchIfOwner();
          }
      }

      after LLLLLLL629: execute() {
        if (needToLatch) {
          _this.latch();
        }
        //original();
      }
   }

   static class IN_verify {
      before execute()  {
        unlatchThis=false;
        //original();
      }
      after LLLLLLL632: execute() {
        if (!_this.isLatchOwner()) {
          _this.latch();
          unlatchThis=true;
        }
        //original();
      }
      after LLLLLLL633: execute() {
        if (unlatchThis) {
          _this.releaseLatch();
        }
        //original();
      }
    }
}
\00after LLLLLLL356:execute(){
        inList.latchMajor();
        //original();
      }
      after LLLLLLL357:execute() {
        inList.latchMinorAndDumpAddedINs();
        //original();
      }
      after LLLLLLL358: execute(){
        inList.releaseMajorLatch();
        //original();
      }
    }
}
\00after execute_Latches_DatabaseImpl_preload: execute() {
        PreloadStats result=original();
        assert LatchSupport.countLatchesHeld() == 0;
        //return result;
      }
    }
}
\00after LLLLLLL343_1: execute() {
   if (addToMajor) {
    _this.releaseMajorLatchIfHeld();
   }
  }
  after LLLLLLL344: execute() {
   enteredWithLatchHeld = _this.majorLatch.isOwner();
   //original();
  }
  after LLLLLLL345: execute() {
   if (enteredWithLatchHeld) {
    addToMajor = false;
   } else {
    if (!(_this.majorLatch.acquireNoWait())) {
     addToMajor = false;
    }
   }

   if (addToMajor) //original(); // because its only one line 
  }
    //...
  after LLLLLLL345_1: execute() {
     else {
      _this.minorLatch.acquire();
      try {
       _this.addAndSetMemory(_this.addedINs, in );
      } finally {
       _this.minorLatch.release();
      }
      if (!enteredWithLatchHeld) {
       if (_this.majorLatch.acquireNoWait()) {
        try {
         _this.latchMinorAndDumpAddedINs();
        } finally {
         _this.releaseMajorLatch();
        }
       }
      }
     }
    }


  }
 }
\00after LLLLLLL847: execute(){
        if (readOnly) {
          envConfig.setConfigParam(EnvironmentParams.JE_LOGGING_DBLOG.getName(),"false");
          envConfig.setReadOnly(true);
        }
        //original(); 
      }
    }
}
\00after LLLLLLL853: makeUtilityEnvironment(File, boolean){
	config.setConfigParam(EnvironmentParams.JE_LOGGING_DBLOG.getName(), "false");
	//original(config);
    }

}
\00after LLLLLLL519: execute(){
        if ((wakeupPeriod == 0) && (bytePeriod == 0)) {
          throw new IllegalArgumentException(EnvironmentParams.CHECKPOINTER_BYTES_INTERVAL.getName() + " and " + EnvironmentParams.CHECKPOINTER_WAKEUP_INTERVAL.getName()+ " cannot both be 0. ");
        }
     //   original();
      }
    }
}
\00after LLLLLLL116: execute() { //  Label129 introduced in LookAHEADCache FileProcessor_inner.ump ; after LLLLLLL129: execute() {
        if (Cleaner.DO_CRITICAL_EVICTION) {
          _this.env.getEvictor().doCriticalEviction();
        }
       // original();
      }
    }
}
\00after LLLLLLL117:execute(){ Label117 introduced in LookAHEADCache FilePocessor.ump
        _this.nLNQueueHitsThisRun++;
        _this.nLNsCleanedThisRun++;
     //   original();
      }
    }
}
\00after LLLLLLL313:execute(){ // Label313 introduced in CheckLeaks_Environment.ump
        statsConfig=new StatsConfig();
        statsConfig.setFast(false);
        lockStat=_this.lockStat(statsConfig);
        if (lockStat.getNTotalLocks() != 0) {
          clean=false;
          System.out.println("Problem: " + lockStat.getNTotalLocks() + " locks left");
          _this.txnManager.getLockManager().dump();
        }
        txnStat=_this.txnStat(statsConfig);
        if (txnStat.getNActive() != 0) {
          clean=false;
          System.out.println("Problem: " + txnStat.getNActive() + " txns left");
          active=txnStat.getActiveTxns();
          if (active != null) {
            for (int i=0; i < active.length; i+=1) {
              System.out.println(active[i]);
            }
          }
        }
  //      original();
      }
    }
}
\00after a node has been selected, to check that it is still evictable.
     */
    public boolean isEvictable() {
			if (isEvictionProhibited()) {
					return false;
			}
			if (hasNonLNChildren()) {
					return false;
			}
			return true;
    }

    /** 
     * Returns the eviction type for this IN, for use by the evictor.  Uses the internal isEvictionProhibited and getChildEvictionType methods that may be overridden by subclasses.
     * @return MAY_EVICT_LNS if evictable LNs may be stripped; otherwise,MAY_EVICT_NODE if the node itself may be evicted; otherwise, MAY_NOT_EVICT.
     */
    public int getEvictionType() {
			if (isEvictionProhibited()) {
					return MAY_NOT_EVICT;
			} else {
					return getChildEvictionType();
			}
				}

    /** 
     * Returns whether the node is not evictable, irrespective of the status of the children nodes.
     */
    boolean isEvictionProhibited() {
			return isDbRoot();
    }

    /** 
     * Returns the eviction type based on the status of child nodes, irrespective of isEvictionProhibited.
     */
    int getChildEvictionType() {
			return hasResidentChildren() ? MAY_NOT_EVICT : MAY_EVICT_NODE;
    }

}
\00after cloneCursor(boolean , CursorImpl ) {
	//CursorImpl result = original(addCursor, usePosition);
	if (allowEviction) {

     Label203:	YYY   // this.hook203();
	}
	return result;
    }

}
\00after LLLLLLL334:requestShutdownDaemons() {
			if (evictor != null) {
					evictor.requestShutdown();
			}
//			original();
    }

    /** 
     * Ask all daemon threads to shut down.
     */
    after shutdownDaemons() {
			//original();
			shutdownEvictor();
    }

}
\00after LLLLLLL187: getObsoleteDetail(Long , PackedOffsets , boolean ){
	    cursor.evict();
//	original(cursor);
    }

    after LLLLLLL188: insertFileSummary(FileSummaryLN , long , int ) { // <<     private synchronized void insertFileSummary(...)
	    cursor.evict();
//	original(cursor);
    }

    after LLLLLLL189: insertFileSummary(FileSummaryLN , long , int ) { // <<     private synchronized void insertFileSummary(...)
	    cursor.setAllowEviction(false);
//	original(cursor);
    }

   after LLLLLLL190: verifyFileSummaryDatabase() {
	   cursor.evict();
//	original(cursor);
    }

}
\00after LLLLLLL596: buildTree() {
			env.invokeEvictor();
			//original();
    }
  
    after LLLLLLL597:undoLNs(RecoveryInfo , Set )
    {
      env.invokeEvictor();
      			//original();
    }
    
    after LLLLLLL598:redoLNs(RecoveryInfo , Set ){
      env.invokeEvictor();
    }

}
\00after a complete pass over the major INList. Releasing the latch is important because it provides an opportunity for to add the minor INList to the major INList.
     * @return the number of bytes evicted, or zero if no progress was made.
     */
    long evictBatch(String source, long requiredEvictBytes) throws DatabaseException {
	return new Evictor_evictBatch(this, source, requiredEvictBytes).execute();
    }

    /** 
     * Return true if eviction should happen.
     */
    boolean isRunnable(String source) throws DatabaseException {
	return new Evictor_isRunnable(this, source).execute();
    }

    /** 
     * Select a single node to evict.
     */
    private IN selectIN(INList inList, ScanIterator scanIter) throws DatabaseException {
			IN target = null;
			long targetGeneration = Long.MAX_VALUE;
			int targetLevel = Integer.MAX_VALUE;
			boolean targetDirty = true;
			boolean envIsReadOnly = envImpl.isReadOnly();
			int scanned = 0;
			boolean wrapped = false;
			while (scanned < nodesPerScan) {
					if (scanIter.hasNext()) {
				IN in = scanIter.next();
				nNodesScannedThisRun++;
				DatabaseImpl db = in.getDatabase();
				boolean b = db == null;
        Label387:				//b = this.hook387(db, b);
				if (b) {
						String inInfo = " IN type=" + in.getLogType() + " id=" + in.getNodeId() + " not expected on INList";
						String errMsg = (db == null) ? inInfo
							: "Database " + db.getDebugName() + " id=" + db.getId() + inInfo;
						throw new DatabaseException(errMsg);
				}
				boolean b2 = false;
        Label386:				//b2 = this.hook386(db, b2);
				if (b2) {
						continue;
				}
				if (db.getId().equals(DbTree.ID_DB_ID)) {
						continue;
				}
				if (envIsReadOnly && (target != null) && in.getDirty()) {
						continue;
				}
				int evictType = in.getEvictionType();
				if (evictType == IN.MAY_NOT_EVICT) {
						continue;
				}
				if (evictByLruOnly) {
						if (targetGeneration > in.getGeneration()) {
					targetGeneration = in.getGeneration();
					target = in;
						}
				} else {
						int level = normalizeLevel(in, evictType);
						if (targetLevel != level) {
					if (targetLevel > level) {
							targetLevel = level;
							targetDirty = in.getDirty();
							targetGeneration = in.getGeneration();
							target = in;
					}
						} else if (targetDirty != in.getDirty()) {
					if (targetDirty) {
							targetDirty = false;
							targetGeneration = in.getGeneration();
							target = in;
					}
						} else {
					if (targetGeneration > in.getGeneration()) {
							targetGeneration = in.getGeneration();
							target = in;
					}
						}
				}
				scanned++;
					} else {
				if (wrapped) {
						break;
				} else {
						nextNode = inList.first();
						scanIter.reset(nextNode);
						wrapped = true;
				}
					}
			}

      Label380:			//this.hook380(target);
			return target;
		}

    /** 
     * Normalize the tree level of the given IN. Is public for unit testing. A BIN containing evictable LNs is given level 0, so it will be stripped first. For non-duplicate and DBMAP trees, the high order bits are cleared to make their levels correspond; that way, all bottom level nodes (BINs and DBINs) are given the same eviction priority. Note that BINs in a duplicate tree are assigned the same level as BINs in a non-duplicate tree. This isn't always optimimal, but is the best we can do considering that BINs in duplicate trees may contain a mix of LNs and DINs.
     */
    public int normalizeLevel(IN in, int evictType) {
	int level = in.getLevel() & IN.LEVEL_MASK;
	if (level == 1 && evictType == IN.MAY_EVICT_LNS) {
	    level = 0;
	}
	return level;
    }

    /** 
     * Strip or evict this node.
     * @return number of bytes evicted.
     */
    private long evict(INList inList, IN target, ScanIterator scanIter) throws DatabaseException {
	try {	
	boolean envIsReadOnly = envImpl.isReadOnly();
			long evictedBytes = 0;
			if (target.latchNoWait(false)) {
					//evictedBytes = this.hook374(inList, target, scanIter, envIsReadOnly, evictedBytes);
         Label374:
				if (target instanceof BIN) {

              Label385:							//this.hook385(target);
							evictedBytes = ((BIN) target).evictLNs();

              Label383:							//this.hook383(evictedBytes);
					}
					if (evictedBytes == 0 && target.isEvictable()) {
							Tree tree = target.getDatabase().getTree();
							SearchResult result = tree.getParentINForChildIN(target, true, false);
							if (result.exactParentFound) {
						evictedBytes = evictIN(target, result.parent, result.index, inList, scanIter, envIsReadOnly);
							}
					}
} finally {
Label374_1: ;//		// end hook374
}

			}
			return evictedBytes;
    }

    /** 
     * Evict an IN. Dirty nodes are logged before they're evicted. inlist is latched with the major latch by the caller.
     */
    private long evictIN(IN child, IN parent, int index, INList inlist, ScanIterator scanIter, boolean envIsReadOnly)
	    throws DatabaseException {
try{
			long evictBytes = 0;
			Label375:			//evictBytes = this.hook375(child, parent, index, inlist, scanIter, envIsReadOnly, evictBytes);
      Label378:			//this.hook378(parent);
			long oldGenerationCount = child.getGeneration();
			IN renewedChild = (IN) parent.getTarget(index);
			if ((renewedChild != null) && (renewedChild.getGeneration() <= oldGenerationCount)
				&& renewedChild.latchNoWait(false)) {
          Label379:					//evictBytes = this.hook379(parent, index, inlist, scanIter, envIsReadOnly, evictBytes, renewedChild);
					if (renewedChild.isEvictable()) {
							long renewedChildLsn = DbLsn.NULL_LSN;
							boolean newChildLsn = false;
							if (renewedChild.getDirty()) {
						if (!envIsReadOnly) {
								boolean logProvisional = (envImpl.getCheckpointer() != null
									&& (renewedChild.getLevel() < envImpl.getCheckpointer().getHighestFlushLevel()));
								renewedChildLsn = renewedChild.log(logManager, false, logProvisional, true, parent);
								newChildLsn = true;
						}
							} else {
						renewedChildLsn = parent.getLsn(index);
							}
							if (renewedChildLsn != DbLsn.NULL_LSN) {
						scanIter.mark();
						inlist.removeLatchAlreadyHeld(renewedChild);
						scanIter.resetToMark();
            Label389: 						//evictBytes = this.hook389(evictBytes, renewedChild);
						if (newChildLsn) {
								parent.updateEntry(index, null, renewedChildLsn);
						} else {
								parent.updateEntry(index, (Node) null);
						}

            Label384:						//this.hook384();
							}
					}
					// end of hook379
			}
}finally {
Label375_1: ;//		// end hook375
}

			return evictBytes;
    }

    /** 
     * Used by unit tests.
     */
    IN getNextNode() {
	return nextNode;
    }

    public void setRunnableHook(TestHook hook) {
	runnableHook = hook;
    }

 //   protected void hook373(EnvironmentImpl envImpl) throws DatabaseException {
  //  }

//    protected long hook374(INList inList, IN target, ScanIterator scanIter, boolean envIsReadOnly, long evictedBytes)
//	    throws DatabaseException {
	
//	return evictedBytes;
  //  }

  //  protected long hook375(IN child, IN parent, int index, INList inlist, ScanIterator scanIter, boolean envIsReadOnly,
	//    long evictBytes) throws DatabaseException {

//	return evictBytes;
  //  }

 //   protected void hook378(IN parent) throws DatabaseException {
  //  }

 //   protected long hook379(IN parent, int index, INList inlist, ScanIterator scanIter, boolean envIsReadOnly,
//	    long evictBytes, IN renewedChild) throws DatabaseException {

//	return evictBytes;
//    }

//    protected void hook380(IN target) throws DatabaseException {
//    }

//    protected void hook383(long evictedBytes) throws DatabaseException {
//    }

//    protected void hook384() throws DatabaseException {
//    }

//    protected void hook385(IN target) throws DatabaseException {
//    }

//    protected boolean hook386(DatabaseImpl db, boolean b2) throws DatabaseException {
//	return b2;
 //   }

//    protected boolean hook387(DatabaseImpl db, boolean b) throws DatabaseException {
//	return b;
//    }

//    protected long hook389(long evictBytes, IN renewedChild) throws DatabaseException {
//	return evictBytes;
//    }

}
\00after LLLLLLL306: createDb(Locker , String , DatabaseConfig , Database , boolean ){ // >>  public synchronized DatabaseImpl createDb (...)
			idCursor.setAllowEviction(allowEviction);
			//original(allowEviction, idCursor);
    }

     after LLLLLLL307: createDb(Locker , String , DatabaseConfig , Database , boolean ){ // >>  public synchronized DatabaseImpl createDb (...)
			nameCursor.setAllowEviction(allowEviction);
			//original(allowEviction, nameCursor);
    }

     after LLLLLLL308: getDb(Locker , String , Database , boolean ) {

			nameCursor.setAllowEviction(allowEviction);
			//original(allowEviction, nameCursor);
    }

     after LLLLLLL309:getDb(DatabaseId , long , boolean , String ){

			idCursor.setAllowEviction(allowEviction);
			//original(allowEviction, idCursor);
    }

     after LLLLLLL310:getDb(DatabaseId , long , boolean , String ){
			idCursor.setAllowEviction(allowEviction);
    }

}
\00after LLLLLLL191: execute(){
        cursor.evict();
        //original();
      }
    }
}
\00after LLLLLLL844:execute(){
        if (doAction == EVICT) {
          preload(env,dbName);
        }
        //original();
      }
      after LLLLLLL845: execute() {
        if (doAction == EVICT) {
          envConfig.setConfigParam(EnvironmentParams.ENV_RUN_EVICTOR.getName(),"false");
          envConfig.setConfigParam(EnvironmentParams.EVICTOR_CRITICAL_PERCENTAGE.getName(),"1000");
        }
        //original();
      }
      after LLLLLLL846: execute(){
        if (action.equalsIgnoreCase("evict")) {
          doAction=EVICT;
        }
  
      }
    }
}
\00after LLLLLLL350:execute() {
        newCriticalThreshold=(newMaxMemory * _this.envImpl.getConfigManager().getInt(EnvironmentParams.EVICTOR_CRITICAL_PERCENTAGE)) / 100;
        //original();
      }
    }
}
\00after LLLLLLL555: undo(Level , DatabaseImpl , TreeLocation , LN , byte ,
	    byte , long , long , boolean , RecoveryInfo , boolean ){ //>>> reuse LoggingRecovery_RecoveryManager.ump

	trace(traceLevel, db, TRACE_LN_UNDO, success, lnFromLog, logLsn, location.bin, found, replaced, false,	location.childLsn, abortLsn, location.index);
	//original(traceLevel, db, location, lnFromLog, logLsn, abortLsn, found, replaced, success);
    }

}
\00after LLLLLLL848:execute(){
        envConfig.setConfigParam(EnvironmentParams.JE_LOGGING_CONSOLE.getName(),"true");
        //original(); //@Abdulaziz aaa
      }
    }
}
\00after LLLLLLL854: makeUtilityEnvironment(File , boolean){
	config.setConfigParam(EnvironmentParams.JE_LOGGING_CONSOLE.getName(), "true");
//	original(config);
    }

}
\00after LLLLLLL177: insertFileSummary(FileSummaryLN , long , int ){
//synchronized void insertFileSummary(FileSummaryLN ln, long fileNum, int sequence)
	    env.getLogger().log(Level.SEVERE, "Cleaner duplicate key sequence file=0x" + Long.toHexString(fileNum)
		    + " sequence=0x" + Long.toHexString(sequence));
	}
	//original(fileNum, sequence, status);
    }

}
\00after LLLLLLL760: addWaiterToEndOfList(LockInfo , MemoryBudget , int ){
			mb.updateLockMemoryUsage(MemoryBudget.LOCKINFO_OVERHEAD, lockTableIndex);
      //	original(mb, lockTableIndex);
    }

    after LLLLLLL761:addWaiterToHeadOfList(LockInfo , MemoryBudget , int ) {
			mb.updateLockMemoryUsage(MemoryBudget.LOCKINFO_OVERHEAD, lockTableIndex);
			//original(mb, lockTableIndex);
    }

    after LLLLLLL762:flushWaiter(Locker , MemoryBudget , int ){
			mb.updateLockMemoryUsage(REMOVE_LOCKINFO_OVERHEAD, lockTableIndex);
			//original(mb, lockTableIndex);
    }

    after LLLLLLL763:flushWaiter(Locker , MemoryBudget , int ){
			mb.updateLockMemoryUsage(REMOVE_LOCKINFO_OVERHEAD, lockTableIndex);
			//original(mb, lockTableIndex);
    }

    after LLLLLLL764:addOwner(LockInfo , MemoryBudget , int ) {
			mb.updateLockMemoryUsage(MemoryBudget.LOCKINFO_OVERHEAD, lockTableIndex);
      //			original(mb, lockTableIndex);
    }

    after LLLLLLL765: flushOwner(LockInfo , MemoryBudget , int ){
			if (removed) {
					mb.updateLockMemoryUsage(REMOVE_LOCKINFO_OVERHEAD, lockTableIndex);
			}
      //			original(mb, lockTableIndex, removed);
		}

    after LLLLLLL766:flushOwner(Locker , MemoryBudget , int ) {
		if (flushedInfo != null) {
			  mb.updateLockMemoryUsage(REMOVE_LOCKINFO_OVERHEAD, lockTableIndex);
			}
     //		original(mb, lockTableIndex, flushedInfo);
		}

    after LLLLLLL767:release(Locker , MemoryBudget, int ){
			mb.updateLockMemoryUsage(REMOVE_LOCKINFO_OVERHEAD, lockTableIndex);
		//			original(mb, lockTableIndex);
    }

    after LLLLLLL768:transfer(Locker , Locker , MemoryBudget , int ) {
			mb.updateLockMemoryUsage(0 - (numRemovedLockInfos * MemoryBudget.LOCKINFO_OVERHEAD), lockTableIndex);
		//			original(mb, lockTableIndex, numRemovedLockInfos);
    }

}
\00after -records");
		    }
		    try {
			records = Long.parseLong(val);
		    } catch (NumberFormatException e) {
			usage(val + " is not a number");
		    }
		    if (records <= 0) {
			usage(val + " is not a positive integer");
		    }
		} else if (name.equals("-key")) {
		    if (val == null) {
			usage("No value after -key");
		    }
		    try {
			keySize = Integer.parseInt(val);
		    } catch (NumberFormatException e) {
			usage(val + " is not a number");
		    }
		    if (keySize <= 0) {
			usage(val + " is not a positive integer");
		    }
		} else if (name.equals("-data")) {
		    if (val == null) {
			usage("No value after -data");
		    }
		    try {
			dataSize = Integer.parseInt(val);
		    } catch (NumberFormatException e) {
			usage(val + " is not a number");
		    }
		    if (dataSize <= 0) {
			usage(val + " is not a positive integer");
		    }
		} else if (name.equals("-nodemax")) {
		    if (val == null) {
			usage("No value after -nodemax");
		    }
		    try {
			nodeMax = Integer.parseInt(val);
		    } catch (NumberFormatException e) {
			usage(val + " is not a number");
		    }
		    if (nodeMax <= 0) {
			usage(val + " is not a positive integer");
		    }
		} else if (name.equals("-density")) {
		    if (val == null) {
			usage("No value after -density");
		    }
		    try {
			density = Integer.parseInt(val);
		    } catch (NumberFormatException e) {
			usage(val + " is not a number");
		    }
		    if (density < 1 || density > 100) {
			usage(val + " is not betwen 1 and 100");
		    }
		} else if (name.equals("-overhead")) {
		    if (val == null) {
			usage("No value after -overhead");
		    }
		    try {
			overhead = Long.parseLong(val);
		    } catch (NumberFormatException e) {
			usage(val + " is not a number");
		    }
		    if (overhead < 0) {
			usage(val + " is not a non-negative integer");
		    }
		} else if (name.equals("-measure")) {
		    if (val == null) {
			usage("No value after -measure");
		    }
		    measureDir = new File(val);
		} else if (name.equals("-measurerandom")) {
		    measureRandom = true;
		} else {
		    usage("Unknown arg: " + name);
		}
	    }
	    if (records == 0) {
		usage("-records not specified");
	    }
	    if (keySize == 0) {
		usage("-key not specified");
	    }
	    printCacheSizes(System.out, records, keySize, dataSize, nodeMax, density, overhead);
	    if (measureDir != null) {
		measure(System.out, measureDir, records, keySize, dataSize, nodeMax, measureRandom);
	    }
	} catch (Throwable e) {
	    e.printStackTrace(System.out);
	}
    }

    private static void usage(String msg) {
	if (msg != null) {
	    System.out.println(msg);
	}
	System.out.println("usage:" + "\njava " + CmdUtil.getJavaCommand(DbCacheSize.class) + "\n   -records <count>"
		+ "\n      # Total records (key/data pairs); required" + "\n   -key <bytes> "
		+ "\n      # Average key bytes per record; required" + "\n  [-data <bytes>]"
		+ "\n      # Average data bytes per record; if omitted no leaf"
		+ "\n      # node sizes are included in the output" + "\n  [-nodemax <entries>]"
		+ "\n      # Number of entries per Btree node; default: 128" + "\n  [-density <percentage>]"
		+ "\n      # Percentage of node entries occupied; default: 80" + "\n  [-overhead <bytes>]"
		+ "\n      # Overhead of non-Btree objects (log buffers, locks,"
		+ "\n      # etc); default: 10% of total cache size" + "\n  [-measure <environmentHomeDirectory>]"
		+ "\n      # An empty directory used to write a database to find"
		+ "\n      # the actual cache size; default: do not measure" + "\n  [-measurerandom"
		+ "\n      # With -measure insert randomly generated keys;"
		+ "\n      # default: insert sequential keys");
	System.exit(2);
    }

    private static void printCacheSizes(PrintStream out, long records, int keySize, int dataSize, int nodeMax,
	    int density, long overhead) {
	out.println("Inputs:" + " records=" + records + " keySize=" + keySize + " dataSize=" + dataSize + " nodeMax="
		+ nodeMax + " density=" + density + '%' + " overhead=" + ((overhead > 0) ? overhead : 10) + "%");
	int nodeAvg = (nodeMax * density) / 100;
	long nBinEntries = (records * nodeMax) / nodeAvg;
	long nBinNodes = (nBinEntries + nodeMax - 1) / nodeMax;
	long nInNodes = 0;
	int nLevels = 1;
	for (long n = nBinNodes; n > 0; n /= nodeMax) {
	    nInNodes += n;
	    nLevels += 1;
	}
	long minInSize = nInNodes * calcInSize(nodeMax, nodeAvg, keySize, true);
	long maxInSize = nInNodes * calcInSize(nodeMax, nodeAvg, keySize, false);
	long lnSize = 0;
	if (dataSize > 0) {
	    lnSize = records * calcLnSize(dataSize);
	}
	out.println();
	out.println(HEADER);
	out.println(line(minInSize, overhead, "Minimum, internal nodes only"));
	out.println(line(maxInSize, overhead, "Maximum, internal nodes only"));
	if (dataSize > 0) {
	    out.println(line(minInSize + lnSize, overhead, "Minimum, internal nodes and leaf nodes"));
	    out.println(line(maxInSize + lnSize, overhead, "Maximum, internal nodes and leaf nodes"));
	} else {
	    out.println("\nTo get leaf node sizing specify -data");
	}
	out.println("\nBtree levels: " + nLevels);
    }

    private static int calcInSize(int nodeMax, int nodeAvg, int keySize, boolean lsnCompression) {
	int size = MemoryBudget.IN_FIXED_OVERHEAD;
	size += MemoryBudget.byteArraySize(nodeMax) + (nodeMax * (2 * MemoryBudget.ARRAY_ITEM_OVERHEAD));
	if (lsnCompression) {
	    size += MemoryBudget.byteArraySize(nodeMax * 2);
	} else {
	    size += MemoryBudget.BYTE_ARRAY_OVERHEAD + (nodeMax * MemoryBudget.LONG_OVERHEAD);
	}
	size += (nodeAvg + 1) * MemoryBudget.byteArraySize(keySize);
	return size;
    }

    private static int calcLnSize(int dataSize) {
	return MemoryBudget.LN_OVERHEAD + MemoryBudget.byteArraySize(dataSize);
    }

    private static String line(long btreeSize, long overhead, String comment) {
	long cacheSize;
	if (overhead == 0) {
	    cacheSize = (100 * btreeSize) / 90;
	} else {
	    cacheSize = btreeSize + overhead;
	}
	StringBuffer buf = new StringBuffer(100);
	column(buf, INT_FORMAT.format(cacheSize));
	column(buf, INT_FORMAT.format(btreeSize));
	column(buf, comment);
	return buf.toString();
    }

    private static void column(StringBuffer buf, String str) {
	int start = buf.length();
	while (buf.length() - start + str.length() < COLUMN_WIDTH) {
	    buf.append(' ');
	}
	buf.append(str);
	for (int i = 0; i < COLUMN_SEPARATOR; i += 1) {
	    buf.append(' ');
	}
    }

    private static void measure(PrintStream out, File dir, long records, int keySize, int dataSize, int nodeMax,
	    boolean randomKeys) throws DatabaseException {
			String[] fileNames = dir.list();
			if (fileNames != null && fileNames.length > 0) {
					usage("Directory is not empty: " + dir);
			}
			Environment env = openEnvironment(dir, true);
			Database db = openDatabase(env, nodeMax, true);
			try {
					out.println("\nMeasuring with cache size: " + INT_FORMAT.format(env.getConfig().getCacheSize()));
					insertRecords(out, env, db, records, keySize, dataSize, randomKeys);

				  Label832:				 // hook832(out, env);
					db.close();
					env.close();
					env = openEnvironment(dir, false);
					db = openDatabase(env, nodeMax, false);
					out.println("\nPreloading with cache size: " + INT_FORMAT.format(env.getConfig().getCacheSize()));
					preloadRecords(out, db);
				  Label831:					//hook831(out, env);
			} finally {
					try {
				db.close();
				env.close();
					} catch (Exception e) {
				out.println("During close: " + e);
					}
			}
    }

    private static Environment openEnvironment(File dir, boolean allowCreate) throws DatabaseException {
	EnvironmentConfig envConfig = new EnvironmentConfig();
	envConfig.setAllowCreate(allowCreate);
	envConfig.setCachePercent(90);
	return new Environment(dir, envConfig);
    }

    private static Database openDatabase(Environment env, int nodeMax, boolean allowCreate) throws DatabaseException {
	DatabaseConfig dbConfig = new DatabaseConfig();
	dbConfig.setAllowCreate(allowCreate);
	dbConfig.setNodeMaxEntries(nodeMax);
	return env.openDatabase(null, "foo", dbConfig);
    }

    private static void insertRecords(PrintStream out, Environment env, Database db, long records, int keySize,
	    int dataSize, boolean randomKeys) throws DatabaseException {
	new DbCacheSize_insertRecords(out, env, db, records, keySize, dataSize, randomKeys).execute();
    }

    private static void preloadRecords(final PrintStream out, final Database db) throws DatabaseException {
	Thread thread = new Thread() {
	    public void run() {
		while (true) {
		    try {
			out.print(".");
			out.flush();
			Thread.sleep(5 * 1000);
		    } catch (InterruptedException e) {
			break;
		    }
		}
	    }
	};
	thread.start();
	db.preload(0);
	thread.interrupt();
	try {
	    thread.join();
	} catch (InterruptedException e) {
	    e.printStackTrace(out);
	}
    }

  //  protected static void hook831(PrintStream out, Environment env) throws DatabaseException {
  //  }

  //  protected static void hook832(PrintStream out, Environment env) throws DatabaseException {
  //  }

}
\00after LLLLLLL351: MemoryBudget(EnvironmentImpl , DbConfigManager ) {
			inOverhead = IN.computeOverhead(configManager);
			binOverhead = BIN.computeOverhead(configManager);
			dinOverhead = DIN.computeOverhead(configManager);
			dbinOverhead = DBIN.computeOverhead(configManager);
			//original(configManager);
		}

}
\00after LLLLLLL809:init(EnvironmentImpl , TransactionConfig ){
	    updateMemoryUsage(MemoryBudget.TXN_OVERHEAD);
	    //original();
    }

    after LLLLLLL810:addReadLock(Lock){
			delta += READ_LOCK_OVERHEAD;
			updateMemoryUsage(delta);
			//original(delta);
    }

  after LLLLLLL811:addReadLock(Lock){
			delta = MemoryBudget.HASHSET_OVERHEAD;
			//return original(delta);
    }

    after LLLLLLL812:removeLock(long, Lock ){
			updateMemoryUsage(0 - READ_LOCK_OVERHEAD);
//			original();
    }

  after LLLLLLL813:removeLock(long, Lock ){
			updateMemoryUsage(0 - WRITE_LOCK_OVERHEAD);
//			original();
    }

    after LLLLLLL814:moveWriteToReadLock(long , Lock )  {
	    updateMemoryUsage(0 - WRITE_LOCK_OVERHEAD);
	    //original();
    }

}
\00after LLLLLLL346: removeLatchAlreadyHeld(IN ) {
		if (updateMemoryUsage) {
			  envImpl.getMemoryBudget().updateTreeMemoryUsage(in.getAccumulatedDelta() - in.getInMemorySize());
			  in.setInListResident(false);
		}
//		original(in);
		  }

    /** 
     * Clear the entire list during recovery and at shutdown.
     */
    after clear(){
//		original();
		if (updateMemoryUsage) {
			  envImpl.getMemoryBudget().refreshTreeMemoryUsage(0);
			}
	  }

}
\00after LLLLLLL779: LockManager(EnvironmentImpl ){
			nLockTables = configMgr.getInt(EnvironmentParams.N_LOCK_TABLES);
			//original(configMgr);
    }

   after LLLLLLL780: attemptLockInternal(Long, Locker , LockType , boolean , int ) {
			memoryBudget.updateLockMemoryUsage(TOTAL_LOCK_OVERHEAD, lockTableIndex);
			//original(lockTableIndex);
    }

    after LLLLLLL781:releaseAndFindNotifyTargetsInternal(long , Lock , Locker , boolean , int ) {
			memoryBudget.updateLockMemoryUsage(REMOVE_TOTAL_LOCK_OVERHEAD, lockTableIndex);
			//original(lockTableIndex);
    }

}
\00after LLLLLLL355:processLSN(long , LogEntryType ) {
	if (envImpl.getMemoryBudget().getCacheMemoryUsage() > maxBytes) {
	    throw DatabaseImpl.memoryExceededPreloadException;
	}
//	original();
    }

}
\00after LLLLLLL610: setKnownDeleted(int ){
	updateMemorySize(getTarget(index), null);
	//original(index);
    }

}
\00after LLLLLLL335:EnvironmentImpl(File , EnvironmentConfig ){
    memoryBudget.initCacheMemoryUsage();
   //	original();
    }

}
\00after LLLLLLL828:unRegisterTxn(Txn , boolean ) {
      env.getMemoryBudget().updateMiscMemoryUsage(txn.getAccumulatedDelta() - txn.getInMemorySize());
//      original(txn);
    }


   after LLLLLLL829: registerXATxn(Xid , Txn , boolean ) {
      env.getMemoryBudget().updateMiscMemoryUsage(MemoryBudget.HASHMAP_ENTRY_OVERHEAD);
      //original();
    }

  after LLLLLLL830:unRegisterXATxn(Xid , boolean )
  {
      env.getMemoryBudget().updateMiscMemoryUsage(0 - MemoryBudget.HASHMAP_ENTRY_OVERHEAD);
      //original();
  }

}
\00after LLLLLLL168:reset() {
			if (memSize > 0) {
					updateMemoryBudget(0 - memSize);
			}
			//original();
				}

    after LLLLLLL169: addTrackedSummary(TrackedFileSummary ){
		updateMemoryBudget(-MemoryBudget.TFS_LIST_SEGMENT_OVERHEAD);
	//	original();
    }

}
\00after LLLLLLL613: fetchTarget(DatabaseImpl , IN){
    if (in != null) fetchTarget(DatabaseImpl, IN){
      in.updateMemorySize(null, target);
	  }
      //	original(in);
  }

}
\00after init(DatabaseImpl , byte , int , int ) {
     // original(db, identifierKey, initialCapacity, level);
      inListResident = false;
    }

  after LLLLLLL637: postFetchInit(DatabaseImpl , long ) {
    initMemorySize();
//	original();
    }

    /** 
     * Initialize a node read in during recovery.
     */
  after postRecoveryInit(DatabaseImpl, long ) {
	//original(db, sourceLsn);
	  initMemorySize();
    }

    after LLLLLLL638:fetchTarget(int){
      updateMemorySize(null, node);
//	original(node);
    }

    /** 
     * Update the idx'th entry of this node. This flavor is used when the target LN is being modified, by an operation like a delete or update. We don't have to check whether the LSN has been nulled or not, because we know an LSN existed before. Also, the modification of the target is done in the caller, so instead of passing in the old and new nodes, we pass in the old and new node sizes.
     */
  before updateEntry(int , long , long , long ) {
	updateMemorySize(oldLNSize, newLNSize);
	//original(idx, lsn, oldLNSize, newLNSize);
    }

    /** 
     * Add self and children to this in-memory IN list. Called by recovery, can run with no latching.
     */
  before rebuildINList(INList ){
	initMemorySize();
	//original(inList);
    }

}
\00after LLLLLLL283:delete(){
      newLNSize = ln.getMemorySizeIncludedByParent();
    }

    after LLLLLLL284:delete(){
      oldLNSize = ln.getMemorySizeIncludedByParent();
    }

    after LLLLLLL285:delete(){
      newLNSize = ln.getMemorySizeIncludedByParent();
    }

    after LLLLLLL286:delete(){
      oldLNSize = ln.getMemorySizeIncludedByParent();
    }

}
\00after execute()  {
        //boolean result=original();
        mb.updateTreeMemoryUsage(memoryChange);
       // return result;
      }
      after LLLLLLL360: execute() {
        memoryChange=0;
        mb=_this.envImpl.getMemoryBudget();
        //original();
      }
      after LLLLLLL361:execute() {
        memoryChange+=(thisIN.getAccumulatedDelta() - thisIN.getInMemorySize());
        thisIN.setInListResident(false);
        //original();
      }
      after LLLLLLL362:execute() {

        mb.updateTreeMemoryUsage(memoryChange);
       // original();
      }
    }
}
\00after LLLLLLL553:execute(){
        totalSize=0;
        mb=_this.envImpl.getMemoryBudget();
        //original();
      }
      
      after LLLLLLL554: execute(){
        mb.refreshTreeMemoryUsage(totalSize);
        //   original();
      }

      after LLLLLLL530:execute(){
        totalSize=mb.accumulateNewUsage(in,totalSize);
        //original();
      }
    }
  @MethodObject static class Checkpointer_doCheckpoint {

      after LLLLLLL548:execute(){
        dirtyMapMemSize=0;
        //original();
      }

      after LLLLLLL549:execute(){
        mb.updateMiscMemoryUsage(0 - dirtyMapMemSize);
        //original();
      }

      after LLLLLLL550:execute(){
        mb.updateMiscMemoryUsage(totalSize);
        //original();
      }
      after LLLLLLL551:execute(){
        totalSize=0;
        //original();
      }
      after LLLLLLL552:execute(){
        size=nodeSet.size() * MemoryBudget.CHECKPOINT_REFERENCE_SIZE;
        totalSize+=size;
        dirtyMapMemSize+=size;
        //original();
      }
    }
}
\00after execute(){
        //original();
        if (_this.updateMemoryUsage) {
          mb=_this.envImpl.getMemoryBudget();
          mb.updateTreeMemoryUsage(in.getInMemorySize());
          in.setInListResident(true);
        }
      }
    }
}
\00after LLLLLLL194: execute()  {
        oldMemorySize=_this.fileSummaryMap.size() * MemoryBudget.UTILIZATION_PROFILE_ENTRY;
        //original();
      }
      after LLLLLLL195: execute() {
        newMemorySize=_this.fileSummaryMap.size() * MemoryBudget.UTILIZATION_PROFILE_ENTRY;
        mb=_this.env.getMemoryBudget();
        mb.updateMiscMemoryUsage(newMemorySize - oldMemorySize);
        //original();
      }
    }
  @MethodObject static class UtilizationProfile_removeFile {
      after LLLLLLL192: execute () {
        mb=_this.env.getMemoryBudget();
        mb.updateMiscMemoryUsage(0 - MemoryBudget.UTILIZATION_PROFILE_ENTRY);
        //original();
      }
    }
  @MethodObject static class UtilizationProfile_putFileSummary {
      after LLLLLLL193: execute() {
        mb=_this.env.getMemoryBudget();
        mb.updateMiscMemoryUsage(MemoryBudget.UTILIZATION_PROFILE_ENTRY);
        //original();
      }
    }
  @MethodObject static class UtilizationProfile_clearCache {
      before execute(){
        memorySize=_this.fileSummaryMap.size() * MemoryBudget.UTILIZATION_PROFILE_ENTRY;
        mb=_this.env.getMemoryBudget();
        mb.updateMiscMemoryUsage(0 - memorySize);
        //original();
      }
    }
}
\00after execute(){
      //  original();
        overrideArch=System.getProperty(FORCE_JVM_ARCH);
        try {
          if (overrideArch == null) {
            arch=System.getProperty(JVM_ARCH_PROPERTY);
            if (arch != null) {
              is64=Integer.parseInt(arch) == 64;
            }
          }
   else {
            is64=Integer.parseInt(overrideArch) == 64;
          }
        }
   catch (      NumberFormatException NFE) {
          NFE.printStackTrace(System.err);
        }
        if (is64) {
          if (isJVM14) {
            RE=new RuntimeException("1.4 based 64 bit JVM not supported");
            RE.printStackTrace(System.err);
            throw RE;
          }
          LONG_OVERHEAD=LONG_OVERHEAD_64;
          BYTE_ARRAY_OVERHEAD=BYTE_ARRAY_OVERHEAD_64;
          OBJECT_OVERHEAD=OBJECT_OVERHEAD_64;
          ARRAY_ITEM_OVERHEAD=ARRAY_ITEM_OVERHEAD_64;
          HASHMAP_OVERHEAD=HASHMAP_OVERHEAD_64;
          HASHMAP_ENTRY_OVERHEAD=HASHMAP_ENTRY_OVERHEAD_64;
          HASHSET_OVERHEAD=HASHSET_OVERHEAD_64;
          HASHSET_ENTRY_OVERHEAD=HASHSET_ENTRY_OVERHEAD_64;
          TWOHASHMAPS_OVERHEAD=TWOHASHMAPS_OVERHEAD_64;
          TREEMAP_OVERHEAD=TREEMAP_OVERHEAD_64;
          TREEMAP_ENTRY_OVERHEAD=TREEMAP_ENTRY_OVERHEAD_64;
          LN_OVERHEAD=LN_OVERHEAD_64;
          DUPCOUNTLN_OVERHEAD=DUPCOUNTLN_OVERHEAD_64;
          BIN_FIXED_OVERHEAD=BIN_FIXED_OVERHEAD_64_15;
          DIN_FIXED_OVERHEAD=DIN_FIXED_OVERHEAD_64_15;
          DBIN_FIXED_OVERHEAD=DBIN_FIXED_OVERHEAD_64_15;
          IN_FIXED_OVERHEAD=IN_FIXED_OVERHEAD_64_15;
          TXN_OVERHEAD=TXN_OVERHEAD_64_15;
          CHECKPOINT_REFERENCE_SIZE=CHECKPOINT_REFERENCE_SIZE_64_15;
          KEY_OVERHEAD=KEY_OVERHEAD_64;
          LOCK_OVERHEAD=LOCK_OVERHEAD_64;
          LOCKINFO_OVERHEAD=LOCKINFO_OVERHEAD_64;
          UTILIZATION_PROFILE_ENTRY=UTILIZATION_PROFILE_ENTRY_64;
          TFS_LIST_INITIAL_OVERHEAD=TFS_LIST_INITIAL_OVERHEAD_64;
          TFS_LIST_SEGMENT_OVERHEAD=TFS_LIST_SEGMENT_OVERHEAD_64;
          LN_INFO_OVERHEAD=LN_INFO_OVERHEAD_64;
          LONG_LIST_PER_ITEM_OVERHEAD=LONG_LIST_PER_ITEM_OVERHEAD_64;
        }
   else {
          LONG_OVERHEAD=LONG_OVERHEAD_32;
          BYTE_ARRAY_OVERHEAD=BYTE_ARRAY_OVERHEAD_32;
          OBJECT_OVERHEAD=OBJECT_OVERHEAD_32;
          ARRAY_ITEM_OVERHEAD=ARRAY_ITEM_OVERHEAD_32;
          HASHMAP_OVERHEAD=HASHMAP_OVERHEAD_32;
          HASHMAP_ENTRY_OVERHEAD=HASHMAP_ENTRY_OVERHEAD_32;
          HASHSET_OVERHEAD=HASHSET_OVERHEAD_32;
          HASHSET_ENTRY_OVERHEAD=HASHSET_ENTRY_OVERHEAD_32;
          TWOHASHMAPS_OVERHEAD=TWOHASHMAPS_OVERHEAD_32;
          TREEMAP_OVERHEAD=TREEMAP_OVERHEAD_32;
          TREEMAP_ENTRY_OVERHEAD=TREEMAP_ENTRY_OVERHEAD_32;
          LN_OVERHEAD=LN_OVERHEAD_32;
          DUPCOUNTLN_OVERHEAD=DUPCOUNTLN_OVERHEAD_32;
          if (isJVM14) {
            BIN_FIXED_OVERHEAD=BIN_FIXED_OVERHEAD_32_14;
            DIN_FIXED_OVERHEAD=DIN_FIXED_OVERHEAD_32_14;
            DBIN_FIXED_OVERHEAD=DBIN_FIXED_OVERHEAD_32_14;
            IN_FIXED_OVERHEAD=IN_FIXED_OVERHEAD_32_14;
            TXN_OVERHEAD=TXN_OVERHEAD_32_14;
            CHECKPOINT_REFERENCE_SIZE=CHECKPOINT_REFERENCE_SIZE_32_14;
          }
   else {
            BIN_FIXED_OVERHEAD=BIN_FIXED_OVERHEAD_32_15;
            DIN_FIXED_OVERHEAD=DIN_FIXED_OVERHEAD_32_15;
            DBIN_FIXED_OVERHEAD=DBIN_FIXED_OVERHEAD_32_15;
            IN_FIXED_OVERHEAD=IN_FIXED_OVERHEAD_32_15;
            TXN_OVERHEAD=TXN_OVERHEAD_32_15;
            CHECKPOINT_REFERENCE_SIZE=CHECKPOINT_REFERENCE_SIZE_32_15;
          }
          KEY_OVERHEAD=KEY_OVERHEAD_32;
          LOCK_OVERHEAD=LOCK_OVERHEAD_32;
          LOCKINFO_OVERHEAD=LOCKINFO_OVERHEAD_32;
          UTILIZATION_PROFILE_ENTRY=UTILIZATION_PROFILE_ENTRY_32;
          TFS_LIST_INITIAL_OVERHEAD=TFS_LIST_INITIAL_OVERHEAD_32;
          TFS_LIST_SEGMENT_OVERHEAD=TFS_LIST_SEGMENT_OVERHEAD_32;
          LN_INFO_OVERHEAD=LN_INFO_OVERHEAD_32;
          LONG_LIST_PER_ITEM_OVERHEAD=LONG_LIST_PER_ITEM_OVERHEAD_32;
        }
      }
    }
  @MethodObject static class MemoryBudget_reset {
      after execute() {
        //original();
        _this.trackerBudget=true ? newTrackerBudget : newMaxMemory;
        _this.cacheBudget=newMaxMemory - newLogBufferBudget;
        _this.nLockTables=configManager.getInt(EnvironmentParams.N_LOCK_TABLES);
        _this.lockMemoryUsage=new long[_this.nLockTables];
      }
    }
}
\00after LLLLLLL290: execute() { 

        cacheBudget=_this.envImpl.getMemoryBudget().getCacheBudget();
        if (maxBytes == 0) {
          maxBytes=cacheBudget;
        }
   			else 
      			if (maxBytes > cacheBudget) {
						throw new IllegalArgumentException("maxBytes parameter to Database.preload() was specified as " + maxBytes + " bytes \nbut the cache is only "+ cacheBudget+ " bytes.");
						}

        //original();
      }
    }
}
\00after LLLLLLL815:execute() {
        delta=0;
        //original();
      }
      after LLLLLLL816:execute() {
        _this.updateMemoryUsage(delta);
        //original();
      }
      after LLLLLLL817:execute() {
        delta+=_this.WRITE_LOCK_OVERHEAD;
        //original();
      }
      after LLLLLLL818:execute() {
        delta+=MemoryBudget.TWOHASHMAPS_OVERHEAD;
        //original();
      }
      after LLLLLLL819:execute() {
        delta-=_this.READ_LOCK_OVERHEAD;
       // original();
      }
    }
}
\00after LLLLLLL650: execute() {
        newSize=_this.computeMemorySize();
        _this.updateMemorySize(oldMemorySize,newSize);
        //original();
      }
    }
  @MethodObject static class IN_deleteEntry {
      after LLLLLLL648:execute(){
        _this.updateMemorySize(oldLSNArraySize,_this.computeLsnOverhead());
        //original();
      }
      after LLLLLLL649: execute(){
        _this.updateMemorySize(_this.getEntryInMemorySize(index),0);
        oldLSNArraySize=_this.computeLsnOverhead();
        //original();
      }
    }
  @MethodObject static class IN_trackProvisionalObsolete {
      after execute(){
        //original();
        if (memDelta != 0) {
          _this.changeMemorySize(memDelta);
        }
      }
      after LLLLLLL651:execute() {
        child.changeMemorySize(0 - childMemDelta);
        memDelta+=childMemDelta;
        //original();
      }
      after LLLLLLL652: execute() {
        childMemDelta=child.provisionalObsolete.size() * MemoryBudget.LONG_LIST_PER_ITEM_OVERHEAD;
        //original();
      }
       after LLLLLLL653: execute() {
        memDelta+=MemoryBudget.LONG_LIST_PER_ITEM_OVERHEAD;
        //original();
      }
      after LLLLLLL654: execute(){
        memDelta+=MemoryBudget.LONG_LIST_PER_ITEM_OVERHEAD;
       // original();
      }
    }
  @MethodObject static class IN_insertEntry1 {
      after LLLLLLL645:execute() {
        _this.updateMemorySize(0,_this.getEntryInMemorySize(index));
        //original();
      }
      after LLLLLLL646:execute() {
        _this.changeMemorySize(_this.computeLsnOverhead() - oldSize);
        //original();
      }
      after LLLLLLL647:execute() {
        oldSize=_this.computeLsnOverhead();
        //original();
      }
    }
  @MethodObject static class IN_updateEntryCompareKey {
      before execute(){
        oldSize=_this.getEntryInMemorySize(idx);
        //original();
      }
      after LLLLLLL644: execute() {
        newSize=_this.getEntryInMemorySize(idx);
        _this.updateMemorySize(oldSize,newSize);
        //original();
      }
    }
  @MethodObject static class IN_updateEntry {
      before execute(){
        oldSize=_this.getEntryInMemorySize(idx);
        //original();
      }

   after execute(){
        newSize=_this.getEntryInMemorySize(idx);
        _this.updateMemorySize(oldSize,newSize);
      }
    }
  @MethodObject static class IN_setLsn {
      before execute(){
        oldSize=_this.computeLsnOverhead();
        //original();
      }
      after LLLLLLL639: execute(){
        _this.changeMemorySize(_this.computeLsnOverhead() - oldSize);
        //original();
      }
    }
  @MethodObject static class IN_updateEntry2 {
      before execute(){
        oldSize=_this.getEntryInMemorySize(idx);
        //original();
      }
      after LLLLLLL642: execute() {
        newSize=_this.getEntryInMemorySize(idx);
        _this.updateMemorySize(oldSize,newSize);
       // original();
      }
    }
  @MethodObject static class IN_flushProvisionalObsolete {
      after LLLLLLL655: execute() {
        _this.changeMemorySize(0 - memDelta);
        //original();
      }
      after LLLLLLL656: execute() {
        memDelta=_this.provisionalObsolete.size() * MemoryBudget.LONG_LIST_PER_ITEM_OVERHEAD;
        //original();
      }
    }
  @MethodObject static class IN_updateEntry3 {
      before execute(){
        oldSize=_this.getEntryInMemorySize(idx);
        //original();
      }
      after LLLLLLL643: execute() {
        newSize=_this.getEntryInMemorySize(idx);
        _this.updateMemorySize(oldSize,newSize);
        //original();
      }
    }
  @MethodObject static class IN_setEntry {
      before execute(){
        oldSize=_this.getEntryInMemorySize(idx);
        //original();
      }
      after LLLLLLL640: execute() {
        newSize=_this.getEntryInMemorySize(idx);
        _this.updateMemorySize(oldSize,newSize);
        //original();
      }
      after LLLLLLL641: execute() {
        oldSize=0;
        //original();
      }
    }
}
\00after execute(){
        //original();
        newSize=_this.getEntryInMemorySize(_this.dupCountLNRef.getKey(),_this.dupCountLNRef.getTarget());
        _this.updateMemorySize(oldSize,newSize);
      }
      after LLLLLLL614:execute(){
        oldSize=_this.getEntryInMemorySize(_this.dupCountLNRef.getKey(),_this.dupCountLNRef.getTarget());
        //original();
      }
    }
  @MethodObject static class DIN_updateDupCountLN {
      before execute(){
        oldSize=_this.getEntryInMemorySize(_this.dupCountLNRef.getKey(),_this.dupCountLNRef.getTarget());
      }

      after execute(){
        newSize=_this.getEntryInMemorySize(_this.dupCountLNRef.getKey(),_this.dupCountLNRef.getTarget());
        _this.updateMemorySize(oldSize,newSize);
      }
    }
}
\00after execute(){
        //original();
        if (adjustMem != 0) {
          _this.updateMemoryBudget(adjustMem);
        }
      }
      after LLLLLLL170: execute() {
        adjustMem=0;
        //original();
      }
      after LLLLLLL171: execute() {
        adjustMem+=MemoryBudget.TFS_LIST_INITIAL_OVERHEAD;
        //original();
      }
      after LLLLLLL172: execute() {
        adjustMem+=MemoryBudget.TFS_LIST_SEGMENT_OVERHEAD;
        //original();
      }
    }
}
\00after LLLLLLL161:execute(){
        adjustMem=(2 * readBufferSize) + obsoleteOffsets.getLogSize();
        budget=_this.env.getMemoryBudget();
        {
          //this.hook118();
          Label118:  YYY
          budget.updateMiscMemoryUsage(adjustMem);
        }
        //original();
      }

      after LLLLLLL162:execute() {
        budget.updateMiscMemoryUsage(0 - adjustMem);
        //original();
      }
    }
}
\00after LLLLLLL774: (EnvironmentImpl ) {
			nRequests = 0;
			nWaits = 0;
			//original();
    }

    before attemptLockInternal(Long , Locker , LockType , boolean , int ) {
			nRequests++;
			//return original(nodeId, locker, type, nonBlockingRequest, lockTableIndex);
    }

    after LLLLLLL775: attemptLockInternal(Long , Locker , LockType , boolean , int ) {
			nWaits++;
			//original();
    }

    after LLLLLLL776: dumpLockTableInternal(LockStats , int ) {
			stats.accumulateNTotalLocks(lockTable.size());
			//original(stats, lockTable);
    }

    after LLLLLLL777: dumpLockTableInternal(LockStats , int ) {
			stats.setNWaiters(stats.getNWaiters() + lock.nWaiters());
			stats.setNOwners(stats.getNOwners() + lock.nOwners());
			//original(stats, lock);
    }

    after LLLLLLL778: dumpLockTableInternal(LockStats , int ) {

			if (info.getLockType().isWriteLock()) {
					stats.setNWriteLocks(stats.getNWriteLocks() + 1);
			} else {
					stats.setNReadLocks(stats.getNReadLocks() + 1);
			}
			//original(stats, info);
				}

}
\00after LLLLLLL96:deleteSafeToDeleteFiles() {

			nCleanerDeletions++;
			//original();
    }

    /** 
     * Update the lowUtilizationFiles and mustBeCleanedFiles fields with new read-only collections, and update the backlog file count.
     */
    after updateReadOnlyFileCollections() {
			//original();
			nBacklogFiles = fileSelector.getBacklog();
    }

    after LLLLLLL97: processPendingLN(LN , DatabaseImpl , byte , byte, TreeLocation ){
				nPendingLNsProcessed++;
				//original();
    }

    after LLLLLLL98: processPendingLN(LN , DatabaseImpl , byte , byte, TreeLocation ){
			nLNsDead++;
			//original();
    }

    after LLLLLLL99: processPendingLN(LN , DatabaseImpl , byte , byte, TreeLocation ){
			nPendingLNsLocked++;
			//original();
    }

    after LLLLLLL100: processPendingLN(LN , DatabaseImpl , byte , byte, TreeLocation ){

			nLNsDead++;
			//original();
    }

    after LLLLLLL101: shouldMigrateLN(boolean , boolean , boolean , boolean , long ) {
				nMarkedLNsProcessed++;
			//original();
    }

    after LLLLLLL102: shouldMigrateLN(boolean , boolean , boolean , boolean , long ) {
			nToBeCleanedLNsProcessed++;
			//original();
    }

    after LLLLLLL103: shouldMigrateLN(boolean , boolean , boolean , boolean , long ) {
			nClusterLNsProcessed++;
			//original();
    }

    after LLLLLLL104:  migrateLN(DatabaseImpl , long , BIN , int , boolean , boolean , long , String ) {
			nLNsMigrated++;
			//original();
    }

    after LLLLLLL105: migrateLN(DatabaseImpl , long, BIN , int , boolean , boolean , long , String )  {

			if (wasCleaned) {
					nLNsDead++;
			}
			//original(wasCleaned);
    }

    after LLLLLLL106: migrateLN(DatabaseImpl , long, BIN , int , boolean , boolean , long , String )  {
			if (wasCleaned) {
					nLNsLocked++;
			}
			//original(wasCleaned);
    }

    after LLLLLLL107: migrateLN(DatabaseImpl , long, BIN , int , boolean , boolean , long , String )  {
			if (wasCleaned) {
					nLNsDead++;
			}
			//original(wasCleaned);
    }

    after LLLLLLL108: migrateLN(DatabaseImpl , long, BIN , int , boolean , boolean , long , String )  {

			if (wasCleaned) {
					nLNsDead++;
			}
			//original(wasCleaned);
    }

    after LLLLLLL109: migrateDupCountLN(DatabaseImpl , long , DIN , ChildReference , boolean , boolean , long , String ) { 

	nLNsMigrated++;
	//original();
    }

    after LLLLLLL110: migrateDupCountLN(DatabaseImpl , long , DIN , ChildReference , boolean , boolean , long , String ) { 

			if (wasCleaned) {
					nLNsLocked++;
			}
			//original(wasCleaned);
    }

    after LLLLLLL111:migrateDupCountLN(DatabaseImpl , long , DIN , ChildReference , boolean ,	boolean , long , String ){
			if (wasCleaned) {
					nLNsDead++;
			}
			//original(wasCleaned);
				}

}
\00after LLLLLLL83: get(Transaction, int ){
			nGets += 1;
			if (cached) {
					nCachedGets += 1;
			}
			//original(cached);
    }

}
\00after LLLLLLL509: logInternal(LoggableObject , boolean , boolean , boolean , long , boolean , ByteBuffer ,UtilizationTracker ){
			nTempBufferWrites++;
		//	original();
    }

}
\00after LLLLLLL496: getReadBuffer(long ) {
			nCacheMiss++;
			//original();
    }

}
\00after LLLLLLL535: PreloadProcessor(EnvironmentImpl , long , long , PreloadStats ) {
			this.stats = stats;
			//original(stats);
		}

    after LLLLLLL354: processLSN(long , LogEntryType ) {

		if (childType.equals(LogEntryType.LOG_DUPCOUNTLN_TRANSACTIONAL)
			|| childType.equals(LogEntryType.LOG_DUPCOUNTLN)) {
			  stats.nDupCountLNsLoaded++;
		} else if (childType.equals(LogEntryType.LOG_LN_TRANSACTIONAL) || childType.equals(LogEntryType.LOG_LN)) {
			  stats.nLNsLoaded++;
		} else if (childType.equals(LogEntryType.LOG_DBIN)) {
			  stats.nDBINsLoaded++;
		} else if (childType.equals(LogEntryType.LOG_BIN)) {
			  stats.nBINsLoaded++;
		} else if (childType.equals(LogEntryType.LOG_DIN)) {
			  stats.nDINsLoaded++;
		} else if (childType.equals(LogEntryType.LOG_IN)) {
			  stats.nINsLoaded++;
		}
		//original(childType);
    }

}
\00after LLLLLLL824: TxnManager(EnvironmentImpl ) {
        numCommits = 0;
        numAborts = 0;
        numXAPrepares = 0;
        numXACommits = 0;
        numXAAborts = 0;
        //original();
    }

    after LLLLLLL825: unRegisterTxn(Txn , boolean ){

        if (isCommit) {
            numCommits++;
        } else {
            numAborts++;
        }
        //original(isCommit);
    }

    after LLLLLLL826: registerXATxn(Xid , Txn , boolean ) {
        if (isPrepare) {
            numXAPrepares++;
        }
        //original(isPrepare);
    }

    after LLLLLLL827: unRegisterXATxn(Xid , boolean ){
        if (isCommit) {
            numXACommits++;
        } else {
            numXAAborts++;
        }
         //        original(isCommit);
    }

}
\00after LLLLLLL138: doClean(boolean , boolean , boolean ) {
			resetPerRunCounters();
			//original();
    }

    after LLLLLLL139: doClean(boolean , boolean , boolean ) {
			traceMsg += " begins backlog=" + cleaner.nBacklogFiles;
			//return original(traceMsg);
    }

    after LLLLLLL140: doClean(boolean , boolean , boolean ) {
			accumulatePerRunCounters();
			//original();
    }

   after LLLLLLL141: doClean(boolean , boolean , boolean ) {
			traceMsg += " nEntriesRead=" + nEntriesReadThisRun + " nINsObsolete=" + nINsObsoleteThisRun + " nINsCleaned="
				+ nINsCleanedThisRun + " nINsDead=" + nINsDeadThisRun + " nINsMigrated=" + nINsMigratedThisRun
				+ " nLNsObsolete=" + nLNsObsoleteThisRun + " nLNsCleaned=" + nLNsCleanedThisRun + " nLNsDead="
				+ nLNsDeadThisRun + " nLNsMigrated=" + nLNsMigratedThisRun + " nLNsMarked=" + nLNsMarkedThisRun
				+ " nLNQueueHits=" + nLNQueueHitsThisRun + " nLNsLocked=" + nLNsLockedThisRun;
			//return original(traceMsg);
    }

    after LLLLLLL142: processFoundLN(LNInfo , long , long , BIN , int , DIN ) { 
			nLNsLockedThisRun++;
			//original();
    }

    after LLLLLLL143: processFoundLN(LNInfo , long , long , BIN , int , DIN ) { 
			nLNsDeadThisRun++;
			//original();
    }

    after LLLLLLL144: processFoundLN(LNInfo , long , long , BIN , int , DIN ) { 
			nLNsMarkedThisRun++;
			//original();
    }

    after LLLLLLL125: processIN(IN , DatabaseImpl , long) {
			nINsCleanedThisRun++;
			//original(inClone, db, lsn, obsolete, dirtied, completed);
    }

    after LLLLLLL151: processIN(IN , DatabaseImpl , long) {
			nINsDeadThisRun++;
			//original();
    }

    after LLLLLLL152: processIN(IN , DatabaseImpl , long) {
			nINsDeadThisRun++;
			//original();
    }

    after LLLLLLL153: processIN(IN , DatabaseImpl , long) {
			nINsMigratedThisRun++;
			//original();
    }

}
\00after LLLLLLL531: Checkpointer(EnvironmentImpl , long , String ) { 
			nCheckpoints = 0;
			//	original();
    }

    after LLLLLLL532: flushIN(CheckpointReference , Map , int , boolean , boolean, long ) {
			nFullINFlushThisRun++;
			nFullINFlush++;
			//original();
    }

    after LLLLLLL533: logTargetAndUpdateParent(IN , IN , int , boolean , long ,	boolean ) { 
			nFullINFlushThisRun++;
			nFullINFlush++;
			if (target instanceof BIN) {
					nFullBINFlush++;
			}
			//original(target);
    }

    after LLLLLLL537: logTargetAndUpdateParent(IN , IN , int , boolean , long ,	boolean ) { 
			nDeltaINFlushThisRun++;
			nDeltaINFlush++;
			//original();
    }

}
\00after LLLLLLL275: execute() {
        treeStatsAccumulator=_this.getTreeStatsAccumulator();
        if (treeStatsAccumulator != null) {
          Label200:   YYY //this.hook200();
          if (_this.index < 0) {
          throw new ReturnObject(OperationStatus.NOTFOUND);
        }
        duplicateRoot=(DIN)_this.bin.fetchTarget(_this.index);
        Label201: YYY //this.hook201();
        dcl=duplicateRoot.getDupCountLN();
        	if (dcl != null) {
        	  dcl.accumulateStats(treeStatsAccumulator);
        	}
        }
        //original();
      }
    }
}
\00after LLLLLLL534:execute() {
        _this.nCheckpoints++;
        //original();
      }
      after LLLLLLL535:execute() {
        _this.resetPerRunCounters();
        //original();
      }
      after LLLLLLL536: execute() {
        _this.lastCheckpointStart=checkpointStart;
        //original();
      }
    }
}
\00after LLLLLLL148: execute() {
            _this.nLNsDeadThisRun++;
            //original();
        }
        after LLLLLLL149: execute() {
            _this.nLNsDeadThisRun++;
            //original();
        }
        after LLLLLLL150: execute() {
            _this.nLNsDeadThisRun++;
            //original();
        }
    }
     static class FileProcessor_processFile {
        after LLLLLLL145: execute(){
            _this.nEntriesReadThisRun = reader.getNumRead();
            _this.nRepeatIteratorReadsThisRun = reader.getNRepeatIteratorReads();
            //original();
        }
        after LLLLLLL146:execute(){
            _this.cleaner.nEntriesRead += 1;
            //original();
        }
        after LLLLLLL147: execute() {
            if (isLN) {
                _this.nLNsObsoleteThisRun++;
            } else if (isIN) {
                _this.nINsObsoleteThisRun++;
            }
            //original();
        }
    }
}
\00after LLLLLLL508:execute(){
        _this.nRepeatFaultReads++;
        //original();
      }
    }
}
\00after LLLLLLL838: execute(){

        if (doAction == DBSTATS) {
          dbConfig=new DatabaseConfig();
          dbConfig.setReadOnly(true);
          DbInternal.setUseExistingConfig(dbConfig,true);
          db=env.openDatabase(null,dbName,dbConfig);
          try {
            System.out.println(db.getStats(new StatsConfig()));
          }
    finally {
            db.close();
          }
        }
        //original();
      }
      after LLLLLLL839: execute(){
        if (action.equalsIgnoreCase("dbstats")) {
          doAction=DBSTATS;
        }

      }
    }
}
\00after LLLLLLL481:  LogBuffer(int , EnvironmentImpl ) {
			buffer = ByteBuffer.allocateDirect(capacity);
			//original(capacity);
    }

}
\00after LLLLLLL455:execute(){ 

        channel=file.getChannel();
        //original();
      }
      after LLLLLLL445:execute(){ 
        totalBytesWritten=channel.write(data,destOffset);
        //original();
      }
    }
   static class FileManager_readFromFile {
      before execute()  {
        channel=file.getChannel();
        //original();
      }
      after LLLLLLL446: execute()  {

        channel.read(readBuffer,offset);
        //original();
      }
    }
}
\00after LLLLLLL294: truncate(Locker , String , boolean ){
	nameCursor.releaseBIN();
//	original(nameCursor);
    }

    after LLLLLLL295: truncate(Locker , DatabaseImpl , boolean ){
	nameCursor.releaseBIN();
//	original(nameCursor);
    }

}
\00after LLLLLLL40:  execute() {

        triggerLock=false;
        //original();
      }
      after LLLLLLL41:  execute() {
        triggerLock=true;
        //original();
      }
      after LLLLLLL42: execute()  {
        if (triggerLock) {
          _this.releaseTriggerListReadLock();
        }
        //original();
      }
    }
}
\00after LLLLLLL439_1:remove(long ){
	    evictTarget.release();

    }

    after LLLLLLL440_1:clear(){
//	try {	    original(iter, fileHandle);} finally {
	    fileHandle.release();
//	}
    }


    after LLLLLLL441:add(Long, FileHandle ){
	evictTarget.release();
	//original(evictTarget);
    }

    after LLLLLLL442: remove(long ){
	evictTarget.latch();
	//original(evictTarget);
    }

    after LLLLLLL443: clear(){
	fileHandle.latch();
	//original(fileHandle);
    }

}
\00after LLLLLLL504: HEADER_CONTENT_BYTES() {
        r -= CHECKSUM_BYTES;
        //return original(r);
    }

    after LLLLLLL505:LogManager(EnvironmentImpl , boolean ) {

        doChecksumOnRead = configManager.getBoolean(EnvironmentParams.LOG_CHECKSUM_READ);
        //original(configManager);
    }

}
\00after LLLLLLL472: FileReader(EnvironmentImpl , int , boolean , long , Long ,	 long , long ) {
        if (doValidateChecksum) {
            cksumValidator = new ChecksumValidator();
        }
        ////original();
    }

    after LLLLLLL473:  FileReader(EnvironmentImpl , int , boolean , long , Long ,	 long , long ) {
        this.doValidateChecksum = env.getLogManager().getChecksumOnRead();
        //original(env);
    }

}
\00after LLLLLLL593: readINsAndTrackIds(long )
		{
			reader.setAlwaysValidateChecksum(true);
			//original(reader);
    }

}
\00after LLLLLLL474: execute() {
        if (doValidate) {
          _this.validateChecksum(dataBuffer);
        }
        //original();
      }
      after LLLLLLL475: execute() {
        collectData|=doValidate;
        if (doValidate) {
          _this.startChecksum(dataBuffer);
        }
        //original();
      }
      after LLLLLLL476: execute() {
        doValidate=_this.doValidateChecksum && (isTargetEntry || _this.alwaysValidateChecksum);
        //original();
      }
    }
}
\00after LLLLLLL137: execute() {

        reader.setAlwaysValidateChecksum(true);
        //original();
      }
    }
}
\00after LLLLLLL506: execute() {
        if (_this.doChecksumOnRead) {
          validator.update(_this.envImpl,entryBuffer,itemSize,false);
          validator.validate(_this.envImpl,storedChecksum,lsn);
        }
        //original();
      }
      after LLLLLLL507: execute() {
        validator=null;
        storedChecksum=LogUtils.getUnsignedInt(entryBuffer);
        if (_this.doChecksumOnRead) {
          validator=new ChecksumValidator();
          validator.update(_this.envImpl,entryBuffer,_this.HEADER_CONTENT_BYTES(),false);
        }
        //original();
      }
    }
}
\00after LLLLLLL501:log(LoggableObject , boolean , boolean , boolean , boolean , long ){
	if (fsyncRequired) {
	    fileManager.groupSync();
	}
	//original(fsyncRequired);
    }

}
\00after LLLLLLL452:FileManager(EnvironmentImpl , File , boolean ){
	syncManager = new FSyncManager(envImpl);
	//original(envImpl);
    }

}
\00after LLLLLLL467:FileManager(EnvironmentImpl , File , boolean)  {
	lockEnvironment(readOnly, false);
//	original(readOnly);
//
    }

    /** 
     * Clear the file lock.
     */
  before close() { 
	if (envLock != null) {
	    envLock.release();
	}
	if (exclLock != null) {
	    exclLock.release();
	}
	if (channel != null) {
	    channel.close();
	}
	if (lockFile != null) {
	    lockFile.close();
	}
    }

}
\00after LLLLLLL115:deleteSafeToDeleteFiles(){
	    if (!env.getFileManager().lockEnvironment(false, true)) {
	    //bellow label introduced in EnvironmentLocking_Cleaner.ump
      Label87:  YYY
	    throw new ReturnVoid();
	    }
    }

  after LLLLLLL_115_1:deleteSafeToDeleteFiles()
  {
    env.getFileManager().releaseExclusiveLock();
  }
  
//	try {
//	    original(safeFiles);
//	} finally {
	//    env.getFileManager().releaseExclusiveLock();
	//}
   // }

    

}
\00after LLLLLLL39: execute(){

        Tracer.trace(Level.FINEST,_this.envHandle.getEnvironmentImpl(),"Database.truncate" + ": txnId=" + ((txn == null) ? "null" : Long.toString(txn.getId())));
       // original();
      }
    }
}
\00after LLLLLLL43: truncate(Transaction , boolean ){
	databaseImpl.checkIsDeleted("truncate");
	//original();
    }

}
\00after LLLLLLL445: execute(){

        if (_this.chunkedNIOSize > 0) {
          useData=data.duplicate();
          origLimit=useData.limit();
          useData.limit(useData.position());
          while (useData.limit() < origLimit) {
            useData.limit((int)(Math.min(useData.limit() + _this.chunkedNIOSize,origLimit)));
            bytesWritten=channel.write(useData,destOffset);
            destOffset+=bytesWritten;
            totalBytesWritten+=bytesWritten;
          }
        }
//   else {
 //         original();
  //      }
      }
    }
   static class FileManager_readFromFile {
      around Label446-Label446_1: execute(){
        if (_this.chunkedNIOSize > 0) {
          readLength=readBuffer.limit();
          currentPosition=offset;
          while (readBuffer.position() < readLength) {
            readBuffer.limit((int)(Math.min(readBuffer.limit() + _this.chunkedNIOSize,readLength)));
            bytesRead1=channel.read(readBuffer,currentPosition);
            if (bytesRead1 < 1)           break;
            currentPosition+=bytesRead1;
          }
        }
   else {
          around_proceed: //original();
        }
      }
    }
}
\00after LLLLLLL276: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,	LockType , boolean , boolean ){
		if (DEBUG) {
			  verifyCursor(bin);
		}
		//original();
    }

    after LLLLLLL277: getNextWithKeyChangeStatus(DatabaseEntry , DatabaseEntry ,	LockType , boolean , boolean ){
			if (DEBUG) {
					verifyCursor(dupBin);
			}
//			original();
    }

    after LLLLLLL278: checkCursorState(boolean ) {

			if (DEBUG) {
					if (bin != null) {
				verifyCursor(bin);
					}
					if (dupBin != null) {
				verifyCursor(dupBin);
					}
			}
			//original();
		}

}
\00after LLLLLLL279: execute() {
        if (_this.DEBUG) {
          _this.verifyCursor(_this.dupBin);
        }
        //original();
      }
    }
}
\00after updateTreeMemoryUsage(long ) {
				//original(increment);
				if (getCacheMemoryUsage() > cacheBudget) {
						envImpl.alertEvictor();
				}
    }

    /** 
     * Update the environment wide misc memory count, wake up the evictor if necessary.
     * @param incrementnote that increment may be negative.
     */
    after updateMiscMemoryUsage(long ) {
			//original(increment);
			if (getCacheMemoryUsage() > cacheBudget) {
					envImpl.alertEvictor();
			}
    }

    after updateLockMemoryUsage(long , int ) {
			//original(increment, lockTableIndex);
			if (getCacheMemoryUsage() > cacheBudget) {
					envImpl.alertEvictor();
			}
    }

}
\00after LLLLLLL601: evictLNs() { // Label601 from  Evictor_BIN.ump
	updateMemorySize(removed, 0);
//	original(removed);
    }

    after LLLLLLL602: evictLN(int ) { // Label602 from  Evictor_BIN.ump
	updateMemorySize(removed, 0);
	//original(removed);
    }

}
\00after LLLLLLL349: execute(){

        _this.criticalThreshold=newCriticalThreshold;
        //original();
      }
    }
}
\00after LLLLLLL836: execute(){
        c.setCacheSize(cacheUsage / 2);
        //original();
      }
      after LLLLLLL837: execute(){
        cacheUsage=envImpl.getMemoryBudget().getCacheMemoryUsage();
        //original();
      }
    }
}
\00after execute()  {
       // boolean result=original();
        result=doRun;
        //return result;
      }
      after LLLLLLL388:execute() {
        currentUsage=mb.getCacheMemoryUsage();
        maxMem=mb.getCacheBudget();
        doRun=((currentUsage - maxMem) > 0);
        if (doRun) {
          _this.currentRequiredEvictBytes=_this.evictBytesSetting;
          _this.currentRequiredEvictBytes+=(currentUsage - maxMem);
          if (_this.DEBUG) {
            if (source == _this.SOURCE_CRITICAL) {
              System.out.println("executed: critical runnable");
            }
          }
        }
        if (_this.runnableHook != null) {
          doRun=((Boolean)_this.runnableHook.getHookValue()).booleanValue();
          _this.currentRequiredEvictBytes=maxMem;
        }
        //original();
      }
    }
}
\00after LLLLLLL196: execute() {

        b2&=totalBytes > mb.getTrackerBudget();
        //original();
      }
      after LLLLLLL197: execute() {
        b1&=mem > largestBytes;
        //original();
      }
      after LLLLLLL198: execute() {
        mem=tfs.getMemorySize();
        totalBytes+=mem;
        //original();
      }
      after LLLLLLL199: execute() {
        largestBytes=mem;
        //original();
      }
    }
}
\00after execute(){
        //original();
        if (logger.isLoggable(Level.FINE)) {
          sb=new StringBuffer();
          sb.append(" Commit:id = ").append(id);
          sb.append(" numWriteLocks=").append(numWriteLocks);
          sb.append(" numReadLocks = ").append(numReadLocks);
          Tracer.trace(Level.FINE,envImpl,sb.toString());
        }
      }
    }
}
\00after LLLLLLL510: LogResult(long , boolean , boolean ) {
	this.wakeupCheckpointer = wakeupCheckpointer;
	//original(wakeupCheckpointer);
    }

}
\00after LLLLLLL498: LogManager(EnvironmentImpl , boolean ){
	checkpointMonitor = new CheckpointMonitor(envImpl);
	//original(envImpl);
    }

    after LLLLLLL499:log(LoggableObject , boolean , boolean , boolean ,  boolean , long ){
	if (logResult.wakeupCheckpointer) {
	    checkpointMonitor.activate();
	}
	//original(logResult);
    }

   after LLLLLLL500: logInternal(LoggableObject , boolean , boolean , boolean , long , boolean , ByteBuffer  ,	UtilizationTracker ) {
	wakeupCheckpointer = checkpointMonitor.recordLogWrite(entrySize, item);
	//return //original(item, entrySize, wakeupCheckpointer);
    }

}
\00after LLLLLLL390: verifyCursors() {
	bin.releaseLatch();
    }

}
\00after LLLLLLL94:  envConfigUpdate(DbConfigManager ) {
	lookAheadCacheSize = cm.getInt(EnvironmentParams.CLEANER_LOOK_AHEAD_CACHE_SIZE);
//	original(cm);
    }

}
\00after LLLLLLL132: execute(){

        offset=lookAheadCache.nextOffset();
        info=lookAheadCache.remove(offset);
       // original();
      }

      after LLLLLLL133:execute(){

        if (!isDupCountLN) {
          for (int i=0; i < bin.getNEntries(); i+=1) {
            lsn=bin.getLsn(i);
            if (i != index && !bin.isEntryKnownDeleted(i) && !bin.isEntryPendingDeleted(i) && DbLsn.getFileNumber(lsn) == fileNum.longValue()) {
              myOffset=new Long(DbLsn.getFileOffset(lsn));
              myInfo=lookAheadCache.remove(myOffset);
              if (myInfo != null) {
                Label117: YYY //this.hook117();
                _this.processFoundLN(myInfo,lsn,lsn,bin,i,null);
              }
            }
          }
        }
        //original();
      }
    }
   static class FileProcessor_processFile {


      after LLLLLLL127:  execute(){
        lookAheadCache=new LookAheadCache(lookAheadCacheSize);
//        original();
      }


      after LLLLLLL128:  execute(){
        lookAheadCacheSize=_this.cleaner.lookAheadCacheSize;
        //original();
      }

      after LLLLLLL129: execute() {
        while (!lookAheadCache.isEmpty()) {
          Label116: YYY //this.hook116();
          _this.processLN(fileNum,location,null,null,lookAheadCache,dbCache);
        }
        //original();
      }
      after LLLLLLL130: execute() {

        lookAheadCache.add(aLsn,aLninfo);
        if (lookAheadCache.isFull()) {
          //original();
        }
      }
      after LLLLLLL131: execute() {
        p=lookAheadCache;
        //original();
      }
    }
}
\00after LLLLLLL545:Checkpointer(EnvironmentImpl , long , String ){

			timeInterval = waitTime;
//			original(waitTime);
    }

}
\00after LLLLLLL329:execute(){

        checkpointerWakeupTime=Checkpointer.getWakeupPeriod(_this.configManager);
        //original();
      }
    }
}
\00after LLLLLLL540: execute(){
        result+=wakeupPeriod;
        //original();
      }
    }
   static class Checkpointer_isRunnable {

      after LLLLLLL542:execute() {
        if (useTimeInterval != 0) {
          lastUsedLsn=_this.envImpl.getFileManager().getLastUsedLsn();
          if (((System.currentTimeMillis() - _this.lastCheckpointMillis) >= useTimeInterval) && (DbLsn.compareTo(lastUsedLsn,_this.lastCheckpointEnd) != 0)) {
            throw new ReturnBoolean(true);
          }
					 else {
								    throw new ReturnBoolean(false);
								  }
								}
				 else {
						    throw new ReturnBoolean(false); // added from hook542() 
						  }
      }
      
			after LLLLLLL543:execute(){
        if (config.getMinutes() != 0) {
          useTimeInterval=config.getMinutes() * 60 * 1000;
				}
				 else {
						     Label544: YYY   useTimeInterval=_this.timeInterval; // from hook544() 
				}
      }

      //protected void hook544() throws DatabaseException {
      //  useTimeInterval=_this.timeInterval;
       // original();
      //}
    }
}
\00after LLLLLLL114:execute() {
        if (_this.DO_CRITICAL_EVICTION) {
          Label86:      YYY   
          //this.hook86();
        }
       
      }
    }
}
\00after LLLLLLL556: recover(boolean ){

	env.enableDebugLoggingToDbLog();
	//original();
    }

    /** 
     * Find the end of the log, initialize the FileManager. While we're perusing the log, return the last checkpoint LSN if we happen to see it.
     */
  after findEndOfLog(boolean ) {

	//original(readOnly);
	env.enableDebugLoggingToDbLog();
    }

}
\00after LLLLLLL166:add(Long , LNInfo ) {
	usedMem += info.getMemorySize();
	usedMem += MemoryBudget.TREEMAP_ENTRY_OVERHEAD - 1;
	//original(info);
    }

    after LLLLLLL167: remove(Long ) {
	usedMem--;
	usedMem -= info.getMemorySize();
	usedMem -= MemoryBudget.TREEMAP_ENTRY_OVERHEAD + 1;
	//original(info);
    }

}




\00after LLLLLLL124:processFoundLN(LNInfo , long , long , BIN , int , DIN ){
			cleaner.trace(cleaner.detailedTraceLevel, Cleaner.CLEAN_LN, ln, logLsn, completed, obsolete, migrated);
			//original(logLsn, ln, obsolete, migrated, completed);
    }

    after LLLLLLL125_1:processIN(IN , DatabaseImpl, long ) {
			finally {
	    cleaner.trace(cleaner.detailedTraceLevel, Cleaner.CLEAN_IN, inClone, lsn, completed, obsolete, dirtied);
    	}
  }

}
\00after LLLLLLL90: envConfigUpdate(DbConfigManager ) {
	detailedTraceLevel = Tracer.parseLevel(env, EnvironmentParams.JE_LOGGING_LEVEL_CLEANER);
	//original();
    }

    after LLLLLLL91: processPendingLN(LN , DatabaseImpl , byte , byte , TreeLocation ) {
	trace(detailedTraceLevel, CLEAN_PENDING_LN, ln, DbLsn.NULL_LSN, completed, obsolete, false);
	//original(ln, obsolete, completed);
    }

    after LLLLLLL92: migrateLN(DatabaseImpl , long , BIN , int , boolean , boolean , long , String ){
	trace(detailedTraceLevel, cleanAction, ln, lsn, completed, obsolete, migrated);
	//original(lsn, cleanAction, obsolete, migrated, completed, ln);
    }

    after LLLLLLL93: igrateDupCountLN(DatabaseImpl , long , DIN , ChildReference , boolean , boolean , long , String ) {
	trace(detailedTraceLevel, cleanAction, ln, lsn, completed, obsolete, migrated);
	//original(lsn, cleanAction, obsolete, migrated, completed, ln);
    }

}
\00after LLLLLLL126: execute() {
        if (processedHere) {
          _this.cleaner.trace(_this.cleaner.detailedTraceLevel,Cleaner.CLEAN_LN,ln,logLsn,completed,obsolete,false);
        }
       // original();
      }
    }
}
\00after LLLLLLL609:compress(BINReference , boolean ){
			db.getDbEnvironment().addToCompressorQueue(binRef, false);
			//original(binRef, db);
    }

}
\00after operationEnd(boolean ) {
		//	original(operationOK);
			synchronized (this) {
					if ((deleteInfo != null) && (deleteInfo.size() > 0)) {
				envImpl.addToCompressorQueue(deleteInfo.values(), false);
				deleteInfo.clear();
	    }
	}
    }

}
\00after LLLLLLL594: redo(DatabaseImpl , TreeLocation , LN , byte , byte , long ,RecoveryInfo ) {
			if (deletedKey != null) {
					db.getDbEnvironment().addToCompressorQueue(location.bin, new Key(deletedKey), false);
			}
			//original(db, location, deletedKey);
    }

    after LLLLLLL595: undo(Level , DatabaseImpl , TreeLocation , LN , byte , byte , long , long , boolean , RecoveryInfo , boolean ) {
			db.getDbEnvironment().addToCompressorQueue(location.bin, new Key(deletedKey), false);
			//original(db, location, deletedKey);
    }

}
\00after LLLLLLL330:runOrPauseDaemons(DbConfigManager ) {
			inCompressor.runOrPause(mgr.getBoolean(EnvironmentParams.ENV_RUN_INCOMPRESSOR));
			//original(mgr);
    }

    after LLLLLLL331: requestShutdownDaemons() {
			if (inCompressor != null) {
					inCompressor.requestShutdown();
			}
			//original();
    }

    /** 
     * Ask all daemon threads to shut down.
     */
    before shutdownDaemons() {
			shutdownINCompressor();
			//original();
    }

}
\00after LLLLLLL527: logTargetAndUpdateParent(IN , IN , int , boolean , long , boolean )
    {
			envImpl.lazyCompress(target);
			//original(target, parent, allowDeltas, checkpointStart, logProvisionally, newLsn, mustLogParent);
    }

}
\00after LLLLLLL281: delete() {
			locker.addDeleteInfo(dupBin, new Key(lnKey));
			//original(lnKey);
    }

    after LLLLLLL282: delete() {
			locker.addDeleteInfo(bin, new Key(lnKey));
			//original(lnKey);
    }

}
\00after LLLLLLL803:commit(byte ){
			if ((deleteInfo != null) && deleteInfo.size() > 0) {
					envImpl.addToCompressorQueue(deleteInfo.values(), false);
					deleteInfo.clear();
			}
			//original();
    }

    after LLLLLLL804:abortInternal(boolean , boolean ){
			deleteInfo = null;
			//original();
    }

}
\00after LLLLLLL754:deleteDupSubtree(byte , BIN , int ) {
			if (bin.getNEntries() == 0) {
					database.getDbEnvironment().addToCompressorQueue(bin, null, false);
			}
			//original(bin);
    }

}
\00after LLLLLLL332: execute(){
        compressorWakeupInterval=PropUtil.microsToMillis(_this.configManager.getLong(EnvironmentParams.COMPRESSOR_WAKEUP_INTERVAL));
        _this.inCompressor=new INCompressor(_this,compressorWakeupInterval,"INCompressor");
       // original();
      }
    }
}
\00after LLLLLLL840:  execute(){
        if (doAction == COMPRESS) {
          env.compress();
        }
        //original();
      }
      after LLLLLLL841:  execute(){
        if (action.equalsIgnoreCase("compress")) {
          doAction=COMPRESS;
        }
 //  else {
   //       original();
     //   }
      }
    }
}
\00after LLLLLLL636: execute(){
        if (deletedEntrySeen) {
          _this.databaseImpl.getDbEnvironment().addToCompressorQueue(binRef,false);
        }
//        original();
      }
    }
}
\00after LLLLLLL280: execute() {
        envImpl=_this.database.getDbEnvironment();
        envImpl.addToCompressorQueue(_this.targetBin,new Key(_this.targetBin.getKey(_this.targetIndex)),false);
        //original();
      }
    }
}
\00after LLLLLLL517: execute() { // label introduced in loggingFinestCheckpointer_inner
        sb.append("size interval=").append(useBytesInterval);
  //      original();
      }
    }
}
\00after LLLLLLL_84:Sequence(Database, Transaction, DatabaseEntry, SequenceConfig)
  {
	  logger = db.getEnvironment().getEnvironmentImpl().getLogger();
  }


}
\00after LLLLLLL36:init(Database, DatabaseImpl, Locker, boolean ,CursorConfig){
	this.logger = dbImpl.getDbEnvironment().getLogger();
	//original(dbImpl);
    }

    /** 
     * Copy constructor.
     */
    Cursor(Cursor cursor, boolean samePosition) throws DatabaseException {
	logger = dbImpl.getDbEnvironment().getLogger();
    }

}
\00after LLLLLLL336:EnvironmentImpl(File, EnvironmentConfig){
	envLogger = initLogger(envHome);
//	original(envHome);
    }

    after LLLLLLL337:doClose(boolean) {
	closeLogger();
//	original();
    }

}
\00after LLLLLLL855: makeUtilityEnvironment {
	config.setConfigParam(EnvironmentParams.JE_LOGGING_LEVEL.getName(), "SEVERE");
	//original(config);
    }

//Label885

}
\00after LLLLLLL359:walkInternal() {
		if (setDbState) {
			  dbImpl.finishedINListHarvest();
		}
		//original();
	}

}
\00after LLLLLLL805:commit(byte ){
			cleanupDatabaseImpls(true);
		//	original();
    }

    after LLLLLLL806:commit(byte ){
			setDeletedDatabaseState(true);
			//original();
    }

    after LLLLLLL807:abortInternal(boolean , boolean ){
			cleanupDatabaseImpls(false);
			//original();
		}

    after LLLLLLL808:abortInternal(boolean , boolean ){
			setDeletedDatabaseState(false);
			//original();
    }

}
\00after LLLLLLL112:processPendingLN(LN , DatabaseImpl , byte  , byte , TreeLocation ){
			c = c || db.isDeleted();
			//return original(db, c);
    }

  after LLLLLLL113:processPendingLN(LN , DatabaseImpl , byte , byte , TreeLocation ){
			addPendingDB(db);
			//original(db);
    }

}
\00after LLLLLLL55:preload(long ){
			databaseImpl.checkIsDeleted("preload");
			//original();
    }

    after LLLLLLL56:preload(long , long ){
			databaseImpl.checkIsDeleted("preload");
			//original();
    }

    after LLLLLLL57: preload(PreloadConfig ) {
			databaseImpl.checkIsDeleted("preload");
			//original();
    }

}
\00after LLLLLLL546: flushDirtyNodes(SortedMap , boolean , boolean , boolean ,	 long) {

			if (!(targetRef.db.isDeleted())) {
					flushIN(targetRef, dirtyMap, currentLevel.intValue(), logProvisionally, allowDeltas, checkpointStart);
			}
			//original(dirtyMap, allowDeltas, checkpointStart, currentLevel, logProvisionally, targetRef);
    }

}
\00after a commit of Environment.removeDatabase - purge the deleted database after a commit of Environment.truncateDatabase - purge the newly created database after an abort of Environment.truncateDatabase
     */
    public void deleteAndReleaseINs() throws DatabaseException {
			startDeleteProcessing();
			releaseDeletedINs();
				}

				public void releaseDeletedINs() throws DatabaseException {
			if (pendingDeletedHook != null) {
					pendingDeletedHook.doHook();
			}
			try {
					long rootLsn = tree.getRootLsn();
					if (rootLsn == DbLsn.NULL_LSN) {
				envImpl.getDbMapTree().deleteMapLN(id);
					} else {
				UtilizationTracker snapshot = new UtilizationTracker(envImpl);
				snapshot.countObsoleteNodeInexact(rootLsn, LogEntryType.LOG_IN);
				ObsoleteProcessor obsoleteProcessor = new ObsoleteProcessor(snapshot);
				SortedLSNTreeWalker walker = new SortedLSNTreeWalker(this, true, true, rootLsn, obsoleteProcessor);
				envImpl.getDbMapTree().deleteMapLN(id);
				walker.walk();
				envImpl.getUtilizationProfile().countAndLogSummaries(snapshot.getTrackedFiles());
					}
			} finally {
					deleteState = DELETED;
			}
    }

    public void checkIsDeleted(String operation) throws DatabaseException {
	if (isDeleted()) {
	    throw new DatabaseException("Attempt to " + operation + " a deleted database");
	}
    }

    after LLLLLLL288:DatabaseImpl(String , DatabaseId , EnvironmentImpl , DatabaseConfig ){
			deleteState = NOT_DELETED;
			//original();
    }

    after LLLLLLL289:DatabaseImpl() {
			deleteState = NOT_DELETED;
			//original();
    }

}
\00after LLLLLLL163:FileSelector() {
			pendingDBs = new HashSet();
			//original();
    }

    after LLLLLLL164:getFilesAtCheckpointStart() { 
			anyPendingDuringCheckpoint |= !pendingDBs.isEmpty();
			//original();
    }

    after LLLLLLL165:updateProcessedFiles(){
			b &= pendingDBs.isEmpty();
			//return original(b);
    }

}
\00after LLLLLLL186:verifyLsnIsObsolete(long ){
			b |= db.isDeleted();
			//return original(db, b);
    }

}
\00after LLLLLLL59: openDb(Transaction , Database , String , DatabaseConfig , boolean ){
				if (database != null && !database.isDeleted())
						databaseExists = true;
//			return original(database, databaseExists);
		  }

}
\00after LLLLLLL159:processIN(IN , DatabaseImpl, long ){
			b |= db.isDeleted();
//	return original(db, b);
    }

    after LLLLLLL160:processIN(IN , DatabaseImpl, long ){
			cleaner.addPendingDB(db);
			//original(db);
    }

}
\00after execute() {
        //original();
        pendingDBs=_this.fileSelector.getPendingDBs();
        if (pendingDBs != null) {
          for (int i=0; i < pendingDBs.length; i+=1) {
            dbId2=pendingDBs[i];
            db2=dbMapTree.getDb(dbId2,_this.lockTimeout);
            if (db2 == null || db2.isDeleteFinished()) {
              _this.fileSelector.removePendingDB(dbId2);
            }
          }
        }
      }
    }
}
\00after LLLLLLL842:execute() { 
        if (doAction == REMOVEDB) {
          removeAndClean(env,dbName);}
       // original();
      }

      after LLLLLLL843:execute() { 
        if (action.equalsIgnoreCase("removedb")) {
          doAction=REMOVEDB;
        }
			// else { usage(); System.exit(1); } or// original();
				   //}
      }
    }
}
\00after LLLLLLL157: execute() {
        b|=db.isDeleted();
        //original();
      }
			
			after LLLLLLL158: execute() {
        _this.cleaner.addPendingDB(db);
        //original();
      }
    }
   
   static class FileProcessor_processFile {
      after LLLLLLL154: execute() { 
        checkPendingDbSet=new HashSet();
        //original();
      }
      after LLLLLLL155: execute() { 
        for (Iterator i=checkPendingDbSet.iterator(); i.hasNext(); ) {
          dbId=(DatabaseId)i.next();
          db=dbMapTree.getDb(dbId,_this.cleaner.lockTimeout,dbCache);
          _this.cleaner.addPendingDB(db);
        }
        //original();
      }
      after LLLLLLL156: execute() { 
        dbId1=reader.getDatabaseId();
        if (dbId1 != null) {
          checkPendingDbSet.add(dbId1);
        }
        //original();
      }
    }
}
\00after reinit(){
	//original();
	rewriteAllowed = false;
    }

}
\00after LLLLLLL465:writeLogBuffer(LogBuffer) { 
	assert fullBuffer.getRewriteAllowed() || (DbLsn.getFileOffset(firstLsn) >= file.length()
		|| file.length() == firstLogEntryOffset()) : "FileManager would overwrite non-empty file 0x"
			+ Long.toHexString(DbLsn.getFileNumber(firstLsn)) + " lsnOffset=0x"
			+ Long.toHexString(DbLsn.getFileOffset(firstLsn)) + " fileLength=0x"
			+ Long.toHexString(file.length());
//	original(fullBuffer, firstLsn, file);
    }

// ---
    after LLLLLLL466:writeLogBuffer(LogBuffer) {
	try {
	      if (IO_EXCEPTION_TESTING) {
          throw new IOException("generated for testing");
	      }
	    writeToFile(file, data, DbLsn.getFileOffset(firstLsn));
	    } 
catch (IOException IOE2) {
	    fullBuffer.setRewriteAllowed();
	    throw new DatabaseException(IOE2);
	}
	if (false) //THIS IS DEAD CODE
//	    original(fullBuffer, firstLsn, file, data, IOE);
	throw new DatabaseException(IOE);
    }

}
\00after LLLLLLL516:trace(EnvironmentImpl , String , boolean ) {
	sb.append(" nFullINFlushThisRun=").append(nFullINFlushThisRun);
	sb.append(" nDeltaINFlushThisRun=").append(nDeltaINFlushThisRun);
//	original(sb);
    }

}
\00after LLLLLLL615: verifyMemorySize(){
	Tracer.trace(Level.INFO, databaseImpl.getDbEnvironment(), msg);
	//original(msg);
    }

}
\00after LLLLLLL530: execute(){
        if (in.getDirty()) {
          level=new Integer(in.getLevel());
          if (newDirtyMap.containsKey(level)) {
            dirtySet=(Set)newDirtyMap.get(level);
          }
          else {
            dirtySet=new HashSet();
            newDirtyMap.put(level,dirtySet);
          }
          dirtySet.add(new CheckpointReference(in.getDatabase(),in.getNodeId(),in.containsDuplicates(),in.isDbRoot(),in.getMainTreeKey(),in.getDupTreeKey()));
        }
      }
*/
      //protected void hook553() throws DatabaseException {
      //}
     // protected void hook554() throws DatabaseException {
     // }
    }
}

/*
    protected void hook44() throws DatabaseException {
    }

    protected void hook45(Transaction txn, DatabaseEntry key) throws DatabaseException {
    }

    protected void hook46(Transaction txn, CursorConfig cursorConfig) throws DatabaseException {
    }

    protected void hook47(Transaction txn, DatabaseEntry key) throws DatabaseException {
    }

    protected void hook48(Transaction txn, DatabaseEntry key, LockMode lockMode) throws DatabaseException {
    }

    protected void hook49(Transaction txn, DatabaseEntry key, DatabaseEntry data, LockMode lockMode)
	    throws DatabaseException {
    }

    protected void hook50(Transaction txn, DatabaseEntry key, DatabaseEntry data) throws DatabaseException {
    }

    protected void hook51(Transaction txn, DatabaseEntry key, DatabaseEntry data) throws DatabaseException {
    }

    protected void hook52(Transaction txn, DatabaseEntry key, DatabaseEntry data) throws DatabaseException {
    }
*/

/*    protected void hook53(List list) throws DatabaseException {
			try {
				for (int i = 0; i < triggerList.size(); i += 1) {
						Object obj = triggerList.get(i);
						if (obj instanceof SecondaryTrigger) {
					list.add(((SecondaryTrigger) obj).getDb());
						}
				}
			}
		  finally {
Label53:   ;//
			}

    }
*/
  /*  protected void hook54(Locker locker, DatabaseEntry priKey, DatabaseEntry oldData, DatabaseEntry newData)
	    throws DatabaseException {
	for (int i = 0; i < triggerList.size(); i += 1) {
	    DatabaseTrigger trigger = (DatabaseTrigger) triggerList.get(i);
	    trigger.databaseUpdated(this, locker, priKey, oldData, newData);
	}
    }
*/

//    protected void hook55() throws DatabaseException {
//    }

//    protected void hook56() throws DatabaseException {
//    }

 //   protected void hook57() throws DatabaseException {
  //  }

}
\00after the key is matched to perform the data portion of the match. We may be matching just against an LN, or doing further searching into the dup tree. See searchAndPosition for more details.
     */
    private int searchAndPositionBoth(boolean containsDuplicates, Node n, DatabaseEntry matchData, boolean exactSearch,
        LockType lockType, long oldLsn) throws DatabaseException {
        assert assertCursorState(false): dumpToString(true);
        boolean found = false;
        boolean exact = false;
        assert(matchData != null);
        byte[] data = Key.makeKey(matchData);
        if (containsDuplicates) {
            DIN duplicateRoot = (DIN) n;
            Label236: //this.hook236(duplicateRoot);
            dupBin = (DBIN) database.getTree().searchSubTree(duplicateRoot, data, Tree.SearchType.NORMAL, -1, null,
                true);
            if (dupBin != null) {
                addCursor(dupBin);
                dupIndex = dupBin.findEntry(data, true, exactSearch);
                if (dupIndex >= 0) {
                    if ((dupIndex & IN.EXACT_MATCH) != 0) {
                        exact = true;
                    }
                    dupIndex &= ~IN.EXACT_MATCH;
                    found = true;
                } else {
                    dupIndex = -1;
                    found = !exactSearch;
                }
            }
        } else {
            LN ln = (LN) n;
            LockResult lockResult = lockLN(ln, lockType);
            ln = lockResult.getLN();
            if (ln == null) {
                found = !exactSearch;
            } else {
                dupBin = null;
                dupIndex = -1;
                int cmp = Key.compareKeys(ln.getData(), data, database.getDuplicateComparator());
                if (cmp == 0 || (cmp <= 0 && !exactSearch)) {
                    if (cmp == 0) {
                        exact = true;
                    }
                    found = true;
                } else {
                    index--;
                    found = !exactSearch;
                }
            }
        }
        return (found ? FOUND : 0) | (exact ? EXACT_DATA : 0);
    }

    private OperationStatus fetchCurrent(DatabaseEntry foundKey, DatabaseEntry foundData, LockType lockType,
        boolean first) throws DatabaseException {
        return new CursorImpl_fetchCurrent(this, foundKey, foundData, lockType, first).execute();
    }

    /** 
     * Locks the given LN's node ID; a deleted LN will not be locked or returned. Attempts to use a non-blocking lock to avoid unlatching/relatching. Retries if necessary, to handle the case where the LN is changed while the BIN is unlatched. Preconditions: The target BIN must be latched. When positioned in a dup tree, the BIN may be latched on entry also and if so it will be latched on exit. Postconditions: The target BIN is latched. When positioned in a dup tree, the BIN will be latched if it was latched on entry or a blocking lock was needed. Therefore, when positioned in a dup tree, releaseDBIN should be called.
     * @param lnthe LN to be locked.
     * @param lockTypethe type of lock requested.
     * @return the LockResult containing the LN that was locked, or containing anull LN if the LN was deleted or cleaned. If the LN is deleted, a lock will not be held.
     */
    private LockResult lockLN(LN ln, LockType lockType) throws DatabaseException {
        LockResult lockResult = lockLNDeletedAllowed(ln, lockType);
        ln = lockResult.getLN();
        if (ln != null) {
            setTargetBin();
            if (targetBin.isEntryKnownDeleted(targetIndex) || ln.isDeleted()) {
                revertLock(ln.getNodeId(), lockResult.getLockGrant());
                lockResult.setLN(null);
            }
        }
        return lockResult;
    }

    /** 
     * Locks the given LN's node ID; a deleted LN will be locked and returned. Attempts to use a non-blocking lock to avoid unlatching/relatching. Retries if necessary, to handle the case where the LN is changed while the BIN is unlatched. Preconditions: The target BIN must be latched. When positioned in a dup tree, the BIN may be latched on entry also and if so it will be latched on exit. Postconditions: The target BIN is latched. When positioned in a dup tree, the BIN will be latched if it was latched on entry or a blocking lock was needed. Therefore, when positioned in a dup tree, releaseDBIN should be called.
     * @param lnthe LN to be locked.
     * @param lockTypethe type of lock requested.
     * @return the LockResult containing the LN that was locked, or containing anull LN if the LN was cleaned.
     */
    public LockResult lockLNDeletedAllowed(LN ln, LockType lockType) throws DatabaseException {
        LockResult lockResult;
        if (lockType == LockType.NONE) {
            lockResult = new LockResult(LockGrantType.NONE_NEEDED, null);
            lockResult.setLN(ln);
            return lockResult;
        }
        if (locker.getDefaultNoWait()) {
            lockResult = locker.lock(ln.getNodeId(), lockType, true, database);
        } else {
            lockResult = locker.nonBlockingLock(ln.getNodeId(), lockType, database);
        }
        if (lockResult.getLockGrant() != LockGrantType.DENIED) {
            lockResult.setLN(ln);
            return lockResult;
        }
        while (true) {
            long nodeId = ln.getNodeId();
            Label238: //this.hook238();
            lockResult = locker.lock(nodeId, lockType, false, database);
            Label237: //this.hook237();
            setTargetBin();
            ln = (LN) targetBin.fetchTarget(targetIndex);
            if (ln != null && nodeId != ln.getNodeId()) {
                revertLock(nodeId, lockResult.getLockGrant());
                continue;
            } else {
                lockResult.setLN(ln);
                return lockResult;
            }
        }
    }

    /** 
     * Locks the DupCountLN for the given duplicate root. Attempts to use a non-blocking lock to avoid unlatching/relatching. Preconditions: The dupRoot, BIN and DBIN are latched. Postconditions: The dupRoot, BIN and DBIN are latched. Note that the dupRoot may change during locking and should be refetched if needed.
     * @param dupRootthe duplicate root containing the DupCountLN to be locked.
     * @param lockTypethe type of lock requested.
     * @return the LockResult containing the LN that was locked.
     */
    public LockResult lockDupCountLN(DIN dupRoot, LockType lockType) throws DatabaseException {
        DupCountLN ln = dupRoot.getDupCountLN();
        LockResult lockResult;
        if (locker.getDefaultNoWait()) {
            lockResult = locker.lock(ln.getNodeId(), lockType, true, database);
        } else {
            lockResult = locker.nonBlockingLock(ln.getNodeId(), lockType, database);
        }
        if (lockResult.getLockGrant() == LockGrantType.DENIED) {
            Label241: //this.hook241(dupRoot);
            lockResult = locker.lock(ln.getNodeId(), lockType, false, database);
            Label240: //this.hook240();
            dupRoot = (DIN) bin.fetchTarget(index);
            Label239: //this.hook239(dupRoot);
            ln = dupRoot.getDupCountLN();
        }
        lockResult.setLN(ln);
        return lockResult;
    }

    /** 
     * Fetch, latch and return the DIN root of the duplicate tree at the cursor position. Preconditions: The BIN must be latched and the current BIN entry must contain a DIN. Postconditions: The BIN and DIN will be latched. The DBIN will remain latched if isDBINLatched is true.
     * @param isDBINLatchedis true if the DBIN is currently latched.
     */
    public DIN getLatchedDupRoot(boolean isDBINLatched) throws DatabaseException {
        assert bin != null;
        Label243: //this.hook243();
        assert index >= 0;
        DIN dupRoot = (DIN) bin.fetchTarget(index);
        Label242: //this.hook242(isDBINLatched, dupRoot);
        return dupRoot;
    }

    /** 
     * Helper to return a Data DBT from a BIN.
     */
    private void setDbt(DatabaseEntry data, byte[] bytes) {
        if (bytes != null) {
            boolean partial = data.getPartial();
            int off = partial ? data.getPartialOffset() : 0;
            int len = partial ? data.getPartialLength() : bytes.length;
            if (off + len > bytes.length) {
                len = (off > bytes.length) ? 0 : bytes.length - off;
            }
            byte[] newdata = null;
            if (len == 0) {
                newdata = LogUtils.ZERO_LENGTH_BYTE_ARRAY;
            } else {
                newdata = new byte[len];
                System.arraycopy(bytes, off, newdata, 0, len);
            }
            data.setData(newdata);
            data.setOffset(0);
            data.setSize(len);
        } else {
            data.setData(null);
            data.setOffset(0);
            data.setSize(0);
        }
    }

    /** 
     * Calls checkCursorState and returns false is an exception is thrown.
     */
    private boolean assertCursorState(boolean mustBeInitialized) {
        try {
            checkCursorState(mustBeInitialized);
            return true;
        } catch (DatabaseException e) {
            return false;
        }
    }

    /** 
     * Check that the cursor is open and optionally if it is initialized.
     */
    public void checkCursorState(boolean mustBeInitialized) throws DatabaseException {
        if (status == CURSOR_INITIALIZED) {
            Label278: //this.hook278();
                return;
        }
        else if (status == CURSOR_NOT_INITIALIZED) {
            if (mustBeInitialized) {
                throw new DatabaseException("Cursor Not Initialized.");
            }
        } else if (status == CURSOR_CLOSED) {
            throw new DatabaseException("Cursor has been closed.");
        } else {
            throw new DatabaseException("Unknown cursor status: " + status);
        }
    }

    /** 
     * Return this lock to its prior status. If the lock was just obtained, release it. If it was promoted, demote it.
     */
    private void revertLock(LN ln, LockResult lockResult) throws DatabaseException {
        revertLock(ln.getNodeId(), lockResult.getLockGrant());
    }

    /** 
     * Return this lock to its prior status. If the lock was just obtained, release it. If it was promoted, demote it.
     */
    private void revertLock(long nodeId, LockGrantType lockStatus) throws DatabaseException {
        if ((lockStatus == LockGrantType.NEW) || (lockStatus == LockGrantType.WAIT_NEW)) {
            locker.releaseLock(nodeId);
        } else if ((lockStatus == LockGrantType.PROMOTION) || (lockStatus == LockGrantType.WAIT_PROMOTION)) {
            locker.demoteLock(nodeId);
        }
    }

    /** 
     * Locks the logical EOF node for the database.
     */
    public void lockEofNode(LockType lockType) throws DatabaseException {
        locker.lock(database.getEofNodeId(), lockType, false, database);
    }

    /** 
     * @throws RunRecoveryExceptionif the underlying environment is invalid.
     */
    public void checkEnv() throws RunRecoveryException {
        database.getDbEnvironment().checkIfInvalid();
    }

    public CursorImpl getLockerPrev() {
        return lockerPrev;
    }

    public CursorImpl getLockerNext() {
        return lockerNext;
    }

    public void setLockerPrev(CursorImpl p) {
        lockerPrev = p;
    }

    public void setLockerNext(CursorImpl n) {
        lockerNext = n;
    }

    /** 
     * Dump the cursor for debugging purposes. Dump the bin and dbin that the cursor refers to if verbose is true.
     */
    public void dump(boolean verbose) {
        System.out.println(dumpToString(verbose));
    }

    /** 
     * dump the cursor for debugging purposes.
     */
    public void dump() {
        System.out.println(dumpToString(true));
    }

    private String statusToString(byte status) {
        switch (status) {
            case CURSOR_NOT_INITIALIZED:
                return "CURSOR_NOT_INITIALIZED";
            case CURSOR_INITIALIZED:
                return "CURSOR_INITIALIZED";
            case CURSOR_CLOSED:
                return "CURSOR_CLOSED";
            default:
                return "UNKNOWN (" + Byte.toString(status) + ")";
        }
    }

    public String dumpToString(boolean verbose) {
        StringBuffer sb = new StringBuffer();
        sb.append("<Cursor idx=\"").append(index).append("\"");
        if (dupBin != null) {
            sb.append(" dupIdx=\"").append(dupIndex).append("\"");
        }
        sb.append(" status=\"").append(statusToString(status)).append("\"");
        sb.append(">\n");
        if (verbose) {
            sb.append((bin == null) ? "" : bin.dumpString(2, true));
            sb.append((dupBin == null) ? "" : dupBin.dumpString(2, true));
        }
        sb.append("\n</Cursor>");
        return sb.toString();
    }

    public void setTestHook(TestHook hook) {
        testHook = hook;
    }
/*
    protected void hook204(LN ln, long oldLsn, long newLsn) throws DatabaseException {}

    protected void hook205(LN ln, long oldLsn, long newLsn) throws DatabaseException {}

    protected void hook206() throws DatabaseException, CloneNotSupportedException {}

    protected void hook207() throws DatabaseException {}

    protected void hook208(BIN bin) {}

    protected void hook209(BIN abin) throws DatabaseException {}

    protected void hook210(DBIN abin) throws DatabaseException {}

    protected void hook211() throws DatabaseException {}
*/
   /* protected void hook212(LockType lockType) throws DatabaseException {
        if (bin.getNEntries() <= index) {
            throw new ReturnInt(0);
        }
        Node n = bin.fetchTarget(index);
        if (n != null && n.containsDuplicates()) {
            DIN dupRoot = (DIN) n;
            Label265: //this.hook265(dupRoot);
            DupCountLN dupCountLN = (DupCountLN) dupRoot.getDupCountLNRef().fetchTarget(database, dupRoot);
            Label264: ////this.hook264(dupRoot);
            if (lockType != LockType.NONE) {
                locker.lock(dupCountLN.getNodeId(), lockType, false, database);
            }
            throw new ReturnInt(dupCountLN.getDupCount());
        } else {
            throw new ReturnInt(1);
        }
    }
*/

    protected void hook213(boolean isDup, LN ln, LockResult lockResult, LockResult dclLockResult, DIN dupRoot)
    throws DatabaseException {
        //
        //	isDup = (dupBin != null);
        //	if (isDup) {
        //	    dupRoot = getLatchedDupRoot(true);
        //	    dclLockResult = lockDupCountLN(dupRoot, LockType.WRITE);
        //	    dupRoot = (DIN) bin.getTarget(index);
        //	    this.hook267();
        //	}
        //	setTargetBin();
        //	long oldLsn = targetBin.getLsn(targetIndex);
        //	byte[] lnKey = targetBin.getKey(targetIndex);
        //	lockResult.setAbortLsn(oldLsn, targetBin.isEntryKnownDeleted(targetIndex));
        //	long oldLNSize = 0;
        // //	oldLNSize = this.hook284(ln, oldLNSize);
        //  Label284____:
        //	long newLsn = ln.delete(database, lnKey, dupKey, oldLsn, locker);
        //	long newLNSize = 0;
        //  //	newLNSize = this.hook283(ln, newLNSize);
        //  Label283____:
        //	targetBin.updateEntry(targetIndex, newLsn, oldLNSize, newLNSize);
        //	targetBin.setPendingDeleted(targetIndex);
        //	this.hook266();
        //	if (isDup) {
        //	    dupRoot.incrementDuplicateCount(dclLockResult, dupKey, locker, false);
        //	    this.hook268(dupRoot);
        //	    dupRoot = null;
        //	    this.hook281(lnKey);
        //	} else {
        //	    this.hook282(lnKey);
        //	}
        //	this.hook204(ln, oldLsn, newLsn);
    }

 //   protected void hook214() throws DatabaseException {}

   // protected void hook215() throws DatabaseException {}

    //protected void hook216() throws DatabaseException {}

    //protected void hook217() throws DatabaseException {}

/*    protected void hook218(DatabaseEntry data, DatabaseEntry foundKey, DatabaseEntry foundData, boolean isDup)
    throws DatabaseException {
        LN ln = (LN) targetBin.fetchTarget(targetIndex);
        byte[] lnKey = targetBin.getKey(targetIndex);
        Comparator userComparisonFcn = targetBin.getKeyComparator();
        if (targetBin.isEntryKnownDeleted(targetIndex) || ln == null) {
            Label270: //this.hook270();
            throw new ReturnObject(OperationStatus.NOTFOUND);
        }
        LockResult lockResult = lockLN(ln, LockType.WRITE);
        ln = lockResult.getLN();
        if (ln == null) {
            Label271: //this.hook271();
            throw new ReturnObject(OperationStatus.NOTFOUND);
        }
        byte[] foundDataBytes;
        byte[] foundKeyBytes;
        isDup = setTargetBin();
        if (isDup) {
            foundDataBytes = lnKey;
            foundKeyBytes = targetBin.getDupKey();
        } else {
            foundDataBytes = ln.getData();
            foundKeyBytes = lnKey;
        }
        byte[] newData;
        if (data.getPartial()) {
            int dlen = data.getPartialLength();
            int doff = data.getPartialOffset();
            int origlen = (foundDataBytes != null) ? foundDataBytes.length : 0;
            int oldlen = (doff + dlen > origlen) ? doff + dlen : origlen;
            int len = oldlen - dlen + data.getSize();
            if (len == 0) {
                newData = LogUtils.ZERO_LENGTH_BYTE_ARRAY;
            } else {
                newData = new byte[len];
            }
            int pos = 0;
            int slicelen = (doff < origlen) ? doff : origlen;
            if (slicelen > 0)
                System.arraycopy(foundDataBytes, 0, newData, pos, slicelen);
            pos += doff;
            slicelen = data.getSize();
            System.arraycopy(data.getData(), data.getOffset(), newData, pos, slicelen);
            pos += slicelen;
            slicelen = origlen - (doff + dlen);
            if (slicelen > 0)
                System.arraycopy(foundDataBytes, doff + dlen, newData, pos, slicelen);
        } else {
            int len = data.getSize();
            if (len == 0) {
                newData = LogUtils.ZERO_LENGTH_BYTE_ARRAY;
            } else {
                newData = new byte[len];
            }
            System.arraycopy(data.getData(), data.getOffset(), newData, 0, len);
        }
        if (database.getSortedDuplicates()) {
            boolean keysEqual = false;
            if (foundDataBytes != null) {
                keysEqual = Key.compareKeys(foundDataBytes, newData, userComparisonFcn) == 0;
            }
            if (!keysEqual) {
                revertLock(ln, lockResult);
                throw new DatabaseException("Can't replace a duplicate with different data.");
            }
        }
        if (foundData != null) {
            setDbt(foundData, foundDataBytes);
        }
        if (foundKey != null) {
            setDbt(foundKey, foundKeyBytes);
        }
        long oldLsn = targetBin.getLsn(targetIndex);
        lockResult.setAbortLsn(oldLsn, targetBin.isEntryKnownDeleted(targetIndex));
        long oldLNSize = 0;
        //	oldLNSize = this.hook286(ln, oldLNSize);
        Lable286:
            byte[] newKey = (isDup ? targetBin.getDupKey() : lnKey);
        long newLsn = ln.modify(newData, database, newKey, oldLsn, locker);
        long newLNSize = 0;
        //newLNSize = this.hook285(ln, newLNSize);
        Label285:
            targetBin.updateEntry(targetIndex, newLsn, oldLNSize, newLNSize);
        this.hook269();
        this.hook205(ln, oldLsn, newLsn);
        status = CURSOR_INITIALIZED;
        throw new ReturnObject(OperationStatus.SUCCESS);
    }
*/
  //  protected void hook219() throws DatabaseException {}

    //protected void hook220() throws DatabaseException {}

   // protected void hook221(DatabaseEntry foundKey, DatabaseEntry foundData, LockType lockType, boolean first)
    //throws DatabaseException {
 //       throw new ReturnObject(fetchCurrent(foundKey, foundData, lockType, first));
    //}

//    protected void hook222() throws DatabaseException {}

/*    protected void hook223(LockType lockType) throws DatabaseException {
        assert assertCursorState(true): dumpToString(true);
        Label272: //this.hook272();
        if (bin == null) {
            throw new ReturnObject(null);
        }
        LN ln = null;
        if (!bin.isEntryKnownDeleted(index)) {
            ln = (LN) bin.fetchTarget(index);
        }
        if (ln == null) {
            Label273: //this.hook273();
            throw new ReturnObject(null);
        }
        addCursor(bin);
        LockResult lockResult = lockLN(ln, lockType);
        ln = lockResult.getLN();
        throw new ReturnObject(ln);
    }
*/
  //  protected void hook224(boolean alreadyLatched) throws DatabaseException {}

   // protected boolean hook225(boolean alreadyLatched) throws DatabaseException {
  //      return alreadyLatched;
//    }

//    protected boolean hook226(boolean alreadyLatched) throws DatabaseException {
        return alreadyLatched;
  //  }

    protected void hook227() throws DatabaseException {}

    protected void hook228() throws DatabaseException {}

    protected void hook229() throws DatabaseException {}

    protected void hook230(boolean alreadyLatched) throws DatabaseException {}

    protected void hook231() throws DatabaseException {}

    protected void hook232() throws DatabaseException {}

    protected void hook233() throws DatabaseException {}

  /*  protected void hook234(boolean first, DIN duplicateRoot, IN in , boolean found) throws DatabaseException {
        if (duplicateRoot == null) {
            removeCursorBIN();
            if (first) { in = database.getTree().getFirstNode();
            } else { in = database.getTree().getLastNode();
            }
            if ( in != null) {
                assert( in instanceof BIN);
                dupBin = null;
                dupIndex = -1;
                bin = (BIN) in ;
                index = (first ? 0 : (bin.getNEntries() - 1));
                addCursor(bin);
                TreeWalkerStatsAccumulator treeStatsAccumulator = getTreeStatsAccumulator();
                if (bin.getNEntries() == 0) {
                    found = true;
                } else {
                    Node n = null;
                    if (! in .isEntryKnownDeleted(index)) {
                        n = in .fetchTarget(index);
                    }
                    if (n != null && n.containsDuplicates()) {
                        DIN dupRoot = (DIN) n;
                        Label274: //this.hook274( in , dupRoot); in = null;
                        found = positionFirstOrLast(first, dupRoot);
                    } else {
                        if (treeStatsAccumulator != null) {
                            if (n == null || ((LN) n).isDeleted()) {
                                treeStatsAccumulator.incrementDeletedLNCount();
                            } else {
                                treeStatsAccumulator.incrementLNCount();
                            }
                        }
                        found = true;
                    }
                }
            }
        } else {
            removeCursorDBIN();
            if (first) { in = database.getTree().getFirstNode(duplicateRoot);
            } else { in = database.getTree().getLastNode(duplicateRoot);
            }
            if ( in != null) {
                assert( in instanceof DBIN);
                dupBin = (DBIN) in ;
                dupIndex = (first ? 0 : (dupBin.getNEntries() - 1));
                addCursor(dupBin);
                found = true;
            }
        }
        status = CURSOR_INITIALIZED;
        throw new ReturnBoolean(found);
    }
*/
    protected void hook235(DatabaseEntry matchKey, DatabaseEntry matchData, SearchMode searchMode, LockType lockType,
        boolean foundSomething, boolean foundExactKey, boolean foundExactData, boolean foundLast,
        boolean exactSearch, BINBoundary binBoundary) throws DatabaseException {
        byte[] key = Key.makeKey(matchKey);
        bin = (BIN) database.getTree().search(key, Tree.SearchType.NORMAL, -1, binBoundary, true);
        if (bin != null) {
            addCursor(bin);
            index = bin.findEntry(key, true, exactSearch);
            foundSomething = !exactSearch;
            dupBin = null;
            dupIndex = -1;
            boolean containsDuplicates = false;
            if (index >= 0) {
                if ((index & IN.EXACT_MATCH) != 0) {
                    foundExactKey = true;
                    index &= ~IN.EXACT_MATCH;
                }
                Node n = null;
                if (!bin.isEntryKnownDeleted(index)) {
                    n = bin.fetchTarget(index);
                }
                if (n != null) {
                    containsDuplicates = n.containsDuplicates();
                    if (searchMode.isDataSearch()) {
                        if (foundExactKey) {
                            int searchResult = searchAndPositionBoth(containsDuplicates, n, matchData, exactSearch,
                                lockType, bin.getLsn(index));
                            foundSomething = (searchResult & FOUND) != 0;
                            foundExactData = (searchResult & EXACT_DATA) != 0;
                        }
                    } else {
                        foundSomething = true;
                        if (!containsDuplicates && exactSearch) {
                            LN ln = (LN) n;
                            LockResult lockResult = lockLN(ln, lockType);
                            ln = lockResult.getLN();
                            if (ln == null) {
                                foundSomething = false;
                            }
                        }
                    }
                }
                foundLast = (searchMode == SearchMode.SET_RANGE && foundSomething && !containsDuplicates &&
                    binBoundary.isLastBin && index == bin.getNEntries() - 1);
            }
        }
        status = CURSOR_INITIALIZED;
        throw new ReturnInt((foundSomething ? FOUND : 0) | (foundExactKey ? EXACT_KEY : 0) |
            (foundExactData ? EXACT_DATA : 0) | (foundLast ? FOUND_LAST : 0));
    }

   /* protected void hook236(DIN duplicateRoot) throws DatabaseException {}

    protected void hook237() throws DatabaseException {}

    protected void hook238() throws DatabaseException {}

    protected void hook239(DIN dupRoot) throws DatabaseException {}

    protected void hook240() throws DatabaseException {}

    protected void hook241(DIN dupRoot) throws DatabaseException {}

    protected void hook242(boolean isDBINLatched, DIN dupRoot) throws DatabaseException {}

    protected void hook243() throws DatabaseException {}

    protected void hook264(DIN dupRoot) throws DatabaseException {}

    protected void hook265(DIN dupRoot) throws DatabaseException {}

    protected void hook266() throws DatabaseException {}

    protected void hook267() throws DatabaseException {}

    protected void hook268(DIN dupRoot) throws DatabaseException {}

    protected void hook269() throws DatabaseException {}

    protected void hook270() throws DatabaseException {}

    protected void hook271() throws DatabaseException {}

    protected void hook272() throws DatabaseException {}

    protected void hook273() throws DatabaseException {}

    protected void hook274(IN in , DIN dupRoot) throws DatabaseException {}

    //protected void hook276() throws DatabaseException {
    //}

    protected void hook277() throws DatabaseException {}

    protected void hook278() throws DatabaseException {}

    //protected void hook281(byte[] lnKey) throws DatabaseException {
    //}

    //protected void hook282(byte[] lnKey) throws DatabaseException {
    //}

    protected long hook283(LN ln, long newLNSize) throws DatabaseException {
        return newLNSize;
    }

    protected long hook284(LN ln, long oldLNSize) throws DatabaseException {
        return oldLNSize;
    }

    protected long hook285(LN ln, long newLNSize) throws DatabaseException {
        return newLNSize;
    }

    protected long hook286(LN ln, long oldLNSize) throws DatabaseException {
        return oldLNSize;
    }
*/
}
\00after logging, while under the logging latch.
     */
    public void postLogWork(long justLoggedLsn) throws DatabaseException;

    /** 
     * Return true if this item can be marshalled outside the log write latch.
     */
    public boolean marshallOutsideWriteLatch();

    /** 
     * Returns true if this item should be counted as obsoleted when logged. This currently applies to deleted LNs only.
     */
    public boolean countAsObsoleteWhenLogged();

}
\00after the firstWaiter goes away and leaves that field null, so as to leave the list ordered.
     */
    private void addWaiterToEndOfList(LockInfo waiter, MemoryBudget mb, int lockTableIndex) {
		if (waiterList == null) {
			  if (firstWaiter == null) {
			firstWaiter = waiter;
			  } else {
			waiterList = new ArrayList();
			waiterList.add(waiter);
			  }
		} else {
			  waiterList.add(waiter);
		}
		//this.hook760(mb, lockTableIndex);
    Label760:
    }

    /** 
     * Add this waiter to the front of the list.
     */
    private void addWaiterToHeadOfList(LockInfo waiter, MemoryBudget mb, int lockTableIndex) {
	if (firstWaiter != null) {
	    if (waiterList == null) {
		waiterList = new ArrayList();
	    }
	    waiterList.add(0, firstWaiter);
	}
	firstWaiter = waiter;
	Label761:
//this.hook761(mb, lockTableIndex);
    }

    /** 
     * Get a list of waiters for debugging and error messages.
     */
    List getWaitersListClone() {
	List dumpWaiters = new ArrayList();
	if (firstWaiter != null) {
	    dumpWaiters.add(firstWaiter);
	}
	if (waiterList != null) {
	    dumpWaiters.addAll(waiterList);
	}
	return dumpWaiters;
    }

    /** 
     * Remove this locker from the waiter list.
     */
    void flushWaiter(Locker locker, MemoryBudget mb, int lockTableIndex) {
	if ((firstWaiter != null) && (firstWaiter.getLocker() == locker)) {
	    firstWaiter = null;
	   // this.hook762(mb, lockTableIndex);
Label762:

	} else if (waiterList != null) {
	    Iterator iter = waiterList.iterator();
	    while (iter.hasNext()) {
		LockInfo info = (LockInfo) iter.next();
		if (info.getLocker() == locker) {
		    iter.remove();
		    //this.hook763(mb, lockTableIndex);
        Label763:
		    return;
		}
	    }
	}
    }

    private void addOwner(LockInfo newLock, MemoryBudget mb, int lockTableIndex) {
	if (firstOwner == null) {
	    firstOwner = newLock;
	} else {
	    if (ownerSet == null) {
		ownerSet = new HashSet();
	    }
	    ownerSet.add(newLock);
	}
	//this.hook764(mb, lockTableIndex);
  Label764:
    }

    /** 
     * Get a new Set of the owners.
     */
    Set getOwnersClone() {
	Set owners;
	if (ownerSet != null) {
	    owners = new HashSet(ownerSet);
	} else {
	    owners = new HashSet();
	}
	if (firstOwner != null) {
	    owners.add(firstOwner);
	}
	return owners;
    }

    /** 
     * Remove this LockInfo from the owner set.
     */
    private boolean flushOwner(LockInfo oldOwner, MemoryBudget mb, int lockTableIndex) {
	boolean removed = false;
	if (oldOwner != null) {
	    if (firstOwner == oldOwner) {
		firstOwner = null;
		return true;
	    }
	    if (ownerSet != null) {
		removed = ownerSet.remove(oldOwner);
	    }
	}
  //	this.hook765(mb, lockTableIndex, removed);
  Label765:
	return removed;
    }

    /** 
     * Remove this locker from the owner set.
     */
    private LockInfo flushOwner(Locker locker, MemoryBudget mb, int lockTableIndex) {
	LockInfo flushedInfo = null;
	if ((firstOwner != null) && (firstOwner.getLocker() == locker)) {
	    flushedInfo = firstOwner;
	    firstOwner = null;
	} else if (ownerSet != null) {
	    Iterator iter = ownerSet.iterator();
	    while (iter.hasNext()) {
		LockInfo o = (LockInfo) iter.next();
		if (o.getLocker() == locker) {
		    iter.remove();
		    flushedInfo = o;
		}
	    }
	}
//	this.hook766(mb, lockTableIndex, flushedInfo);
  Label766:
	return flushedInfo;
    }

    /** 
     * Returns the owner LockInfo for a locker, or null if locker is not an owner.
     */
    private LockInfo getOwnerLockInfo(Locker locker) {
	if ((firstOwner != null) && (firstOwner.getLocker() == locker)) {
	    return firstOwner;
	}
	if (ownerSet != null) {
	    Iterator iter = ownerSet.iterator();
	    while (iter.hasNext()) {
		LockInfo o = (LockInfo) iter.next();
		if (o.getLocker() == locker) {
		    return o;
		}
	    }
	}
	return null;
    }

    /** 
     * Return true if locker is an owner of this Lock for lockType, false otherwise. This method is only used by unit tests.
     */
    boolean isOwner(Locker locker, LockType lockType) {
	LockInfo o = getOwnerLockInfo(locker);
	if (o != null) {
	    LockType ownedLockType = o.getLockType();
	    if (lockType == ownedLockType) {
		return true;
	    }
	    LockUpgrade upgrade = ownedLockType.getUpgrade(lockType);
	    if (!upgrade.getPromotion()) {
		return true;
	    }
	}
	return false;
    }

    /** 
     * Return true if locker is an owner of this Lock and this is a write lock.
     */
    boolean isOwnedWriteLock(Locker locker) {
	LockInfo o = getOwnerLockInfo(locker);
	return o != null && o.getLockType().isWriteLock();
    }

    /** 
     * Return true if locker is a waiter on this Lock. This method is only used by unit tests.
     */
    boolean isWaiter(Locker locker) {
	if (firstWaiter != null) {
	    if (firstWaiter.getLocker() == locker) {
		return true;
	    }
	}
	if (waiterList != null) {
	    Iterator iter = waiterList.iterator();
	    while (iter.hasNext()) {
		LockInfo info = (LockInfo) iter.next();
		if (info.getLocker() == locker) {
		    return true;
		}
	    }
	}
	return false;
    }

    int nWaiters() {
	int count = 0;
	if (firstWaiter != null) {
	    count++;
	}
	if (waiterList != null) {
	    count += waiterList.size();
	}
	return count;
    }

    int nOwners() {
	int count = 0;
	if (firstOwner != null) {
	    count++;
	}
	if (ownerSet != null) {
	    count += ownerSet.size();
	}
	return count;
    }

    /** 
     * Attempts to acquire the lock and returns the LockGrantType. Assumes we hold the lockTableLatch when entering this method.
     */
    LockGrantType lock(LockType requestType, Locker locker, boolean nonBlockingRequest, MemoryBudget mb,
	    int lockTableIndex) {
	assert validateRequest(locker);
	LockInfo newLock = new LockInfo(locker, requestType);
	LockGrantType grant = tryLock(newLock, nWaiters() == 0, mb, lockTableIndex);
	if (grant == LockGrantType.WAIT_NEW || grant == LockGrantType.WAIT_PROMOTION
		|| grant == LockGrantType.WAIT_RESTART) {
	    if (requestType.getCausesRestart() && grant != LockGrantType.WAIT_RESTART) {
		LockInfo waiter = null;
		Iterator iter = null;
		if (waiterList != null) {
		    iter = waiterList.iterator();
		}
		if (firstWaiter != null) {
		    waiter = firstWaiter;
		} else if ((iter != null) && (iter.hasNext())) {
		    waiter = (LockInfo) iter.next();
		}
		while (waiter != null) {
		    Locker waiterLocker = waiter.getLocker();
		    LockType waiterType = waiter.getLockType();
		    if (waiterType != LockType.RESTART && locker != waiterLocker
			    && !locker.sharesLocksWith(waiterLocker)) {
			LockConflict conflict = waiterType.getConflict(requestType);
			if (conflict.getRestart()) {
			    grant = LockGrantType.WAIT_RESTART;
			    break;
			}
		    }
		    if ((iter != null) && (iter.hasNext())) {
			waiter = (LockInfo) iter.next();
		    } else {
			waiter = null;
		    }
		}
	    }
	    if (nonBlockingRequest) {
		grant = LockGrantType.DENIED;
	    } else {
		if (grant == LockGrantType.WAIT_PROMOTION) {
		    addWaiterToHeadOfList(newLock, mb, lockTableIndex);
		} else {
		    assert grant == LockGrantType.WAIT_NEW || grant == LockGrantType.WAIT_RESTART;
		    if (grant == LockGrantType.WAIT_RESTART) {
			newLock.setLockType(LockType.RESTART);
		    }
		    addWaiterToEndOfList(newLock, mb, lockTableIndex);
		}
	    }
	}
	return grant;
    }

    /** 
     * Releases a lock and moves the next waiter(s) to the owners.
     * @returnnull if we were not the owner,a non-empty set if owners should be notified after releasing, an empty set if no notification is required.
     */
    Set release(Locker locker, MemoryBudget mb, int lockTableIndex) {
	LockInfo removedLock = flushOwner(locker, mb, lockTableIndex);
	if (removedLock == null) {
	    return null;
	}
	Set lockersToNotify = Collections.EMPTY_SET;
	if (nWaiters() == 0) {
	    return lockersToNotify;
	}
	LockInfo waiter = null;
	Iterator iter = null;
	boolean isFirstWaiter = false;
	if (waiterList != null) {
	    iter = waiterList.iterator();
	}
	if (firstWaiter != null) {
	    waiter = firstWaiter;
	    isFirstWaiter = true;
	} else if ((iter != null) && (iter.hasNext())) {
	    waiter = (LockInfo) iter.next();
	}
	while (waiter != null) {
	    LockType waiterType = waiter.getLockType();
	    Locker waiterLocker = waiter.getLocker();
	    LockGrantType grant;
	    if (waiterType == LockType.RESTART) {
		grant = rangeInsertConflict(waiterLocker) ? LockGrantType.WAIT_NEW : LockGrantType.NEW;
	    } else {
		grant = tryLock(waiter, true, mb, lockTableIndex);
	    }
	    if (grant == LockGrantType.NEW || grant == LockGrantType.EXISTING || grant == LockGrantType.PROMOTION) {
		if (isFirstWaiter) {
		    firstWaiter = null;
		} else {
		    iter.remove();
		}
		if (lockersToNotify == Collections.EMPTY_SET) {
		    lockersToNotify = new HashSet();
		}
		lockersToNotify.add(waiterLocker);
		//this.hook767(mb, lockTableIndex);
    Label767:
	    } else {
		assert grant == LockGrantType.WAIT_NEW || grant == LockGrantType.WAIT_PROMOTION
			|| grant == LockGrantType.WAIT_RESTART;
		break;
	    }
	    if ((iter != null) && (iter.hasNext())) {
		waiter = (LockInfo) iter.next();
		isFirstWaiter = false;
	    } else {
		waiter = null;
	    }
	}
	return lockersToNotify;
    }

    /** 
     * Called from lock() to try locking a new request, and from release() to try locking a waiting request.
     * @param newLock is the lock that is requested.
     * @param firstWaiterInLine determines whether to grant the lock when aNEW lock can be granted, but other non-conflicting owners exist; for example, when a new READ lock is requested but READ locks are held by other owners.  This parameter should be true if the requestor is the first waiter in line (or if there are no waiters), and false otherwise.
     * @param mb is the current memory budget.
     * @return LockGrantType.EXISTING, NEW, PROMOTION, WAIT_RESTART, WAIT_NEWor WAIT_PROMOTION.
     */
    private LockGrantType tryLock(LockInfo newLock, boolean firstWaiterInLine, MemoryBudget mb, int lockTableIndex) {
	if (nOwners() == 0) {
	    addOwner(newLock, mb, lockTableIndex);
	    return LockGrantType.NEW;
	}
	Locker locker = newLock.getLocker();
	LockType requestType = newLock.getLockType();
	LockUpgrade upgrade = null;
	LockInfo lockToUpgrade = null;
	boolean ownerExists = false;
	boolean ownerConflicts = false;
	LockInfo owner = null;
	Iterator iter = null;
	if (ownerSet != null) {
	    iter = ownerSet.iterator();
	}
	if (firstOwner != null) {
	    owner = firstOwner;
	} else if ((iter != null) && (iter.hasNext())) {
	    owner = (LockInfo) iter.next();
	}
	while (owner != null) {
	    Locker ownerLocker = owner.getLocker();
	    LockType ownerType = owner.getLockType();
	    if (locker == ownerLocker) {
		assert (upgrade == null);
		upgrade = ownerType.getUpgrade(requestType);
		if (upgrade.getUpgrade() == null) {
		    return LockGrantType.EXISTING;
		} else {
		    lockToUpgrade = owner;
		}
	    } else {
		if (!locker.sharesLocksWith(ownerLocker)) {
		    LockConflict conflict = ownerType.getConflict(requestType);
		    if (conflict.getRestart()) {
			return LockGrantType.WAIT_RESTART;
		    } else {
			if (!conflict.getAllowed()) {
			    ownerConflicts = true;
			}
			ownerExists = true;
		    }
		}
	    }
	    if ((iter != null) && (iter.hasNext())) {
		owner = (LockInfo) iter.next();
	    } else {
		owner = null;
	    }
	}
	if (upgrade != null) {
	    LockType upgradeType = upgrade.getUpgrade();
	    assert upgradeType != null;
	    if (!ownerConflicts) {
		lockToUpgrade.setLockType(upgradeType);
		return upgrade.getPromotion() ? LockGrantType.PROMOTION : LockGrantType.EXISTING;
	    } else {
		return LockGrantType.WAIT_PROMOTION;
	    }
	} else {
	    if (!ownerConflicts && (!ownerExists || firstWaiterInLine)) {
		addOwner(newLock, mb, lockTableIndex);
		return LockGrantType.NEW;
	    } else {
		return LockGrantType.WAIT_NEW;
	    }
	}
    }

    /** 
     * Called from release() when a RESTART request is waiting to determine if any RANGE_INSERT owners exist.  We can't call tryLock for a RESTART lock because it must never be granted.
     */
    private boolean rangeInsertConflict(Locker waiterLocker) {
	LockInfo owner = null;
	Iterator iter = null;
	if (ownerSet != null) {
	    iter = ownerSet.iterator();
	}
	if (firstOwner != null) {
	    owner = firstOwner;
	} else if ((iter != null) && (iter.hasNext())) {
	    owner = (LockInfo) iter.next();
	}
	while (owner != null) {
	    Locker ownerLocker = owner.getLocker();
	    if (ownerLocker != waiterLocker && !ownerLocker.sharesLocksWith(waiterLocker)
		    && owner.getLockType() == LockType.RANGE_INSERT) {
		return true;
	    }
	    if ((iter != null) && (iter.hasNext())) {
		owner = (LockInfo) iter.next();
	    } else {
		owner = null;
	    }
	}
	return false;
    }

    /** 
     * Downgrade a write lock to a read lock.
     */
    void demote(Locker locker) {
	LockInfo owner = getOwnerLockInfo(locker);
	if (owner != null) {
	    LockType type = owner.getLockType();
	    if (type.isWriteLock()) {
		owner.setLockType((type == LockType.RANGE_WRITE) ? LockType.RANGE_READ : LockType.READ);
	    }
	}
    }

    /** 
     * Transfer a lock from one transaction to another. Make sure that this destination locker is only present as a single reader or writer.
     */
    LockType transfer(Locker currentLocker, Locker destLocker, MemoryBudget mb, int lockTableIndex)
	    throws DatabaseException {
	LockType lockType = null;
	int numRemovedLockInfos = 0;
	if (firstOwner != null) {
	    if (firstOwner.getLocker() == destLocker) {
		firstOwner = null;
		numRemovedLockInfos++;
	    } else if (firstOwner.getLocker() == currentLocker) {
		lockType = setNewLocker(firstOwner, destLocker);
	    }
	}
	if (ownerSet != null) {
	    Iterator iter = ownerSet.iterator();
	    while (iter.hasNext()) {
		LockInfo owner = (LockInfo) iter.next();
		if (owner.getLocker() == destLocker) {
		    iter.remove();
		    numRemovedLockInfos++;
		} else if (owner.getLocker() == currentLocker) {
		    lockType = setNewLocker(owner, destLocker);
		}
	    }
	}
	if ((firstWaiter != null) && (firstWaiter.getLocker() == destLocker)) {
	    firstWaiter = null;
	    numRemovedLockInfos++;
	}
	if (waiterList != null) {
	    Iterator iter = waiterList.iterator();
	    while (iter.hasNext()) {
		LockInfo info = (LockInfo) iter.next();
		if (info.getLocker() == destLocker) {
		    iter.remove();
		    numRemovedLockInfos++;
		}
	    }
	}
	//this.hook768(mb, lockTableIndex, numRemovedLockInfos);
  Label768:
	return lockType;
    }

    private LockType setNewLocker(LockInfo owner, Locker destLocker) throws DatabaseException {
	owner.setLocker(destLocker);
	destLocker.addLock(nodeId, this, owner.getLockType(), LockGrantType.NEW);
	return owner.getLockType();
    }

    /** 
     * Transfer a lock from one transaction to many others. Only really needed for case where a write handle lock is being transferred to multiple read handles.
     */
    LockType transferMultiple(Locker currentLocker, Locker[] destLockers, MemoryBudget mb, int lockTableIndex)
	    throws DatabaseException {
	LockType lockType = null;
	LockInfo oldOwner = null;
	if (destLockers.length == 1) {
	    return transfer(currentLocker, destLockers[0], mb, lockTableIndex);
	} else {
	    if (firstOwner != null) {
		for (int i = 0; i < destLockers.length; i++) {
		    if (firstOwner.getLocker() == destLockers[i]) {
			firstOwner = null;
			break;
		    }
		}
	    }
	    if (ownerSet != null) {
		Iterator ownersIter = ownerSet.iterator();
		while (ownersIter.hasNext()) {
		    LockInfo o = (LockInfo) ownersIter.next();
		    for (int i = 0; i < destLockers.length; i++) {
			if (o.getLocker() == destLockers[i]) {
			    ownersIter.remove();
			    break;
			}
		    }
		}
	    }
	    if (firstOwner != null) {
		oldOwner = cloneLockInfo(firstOwner, currentLocker, destLockers, mb, lockTableIndex);
	    }
	    if ((ownerSet != null) && (oldOwner == null)) {
		Iterator ownersIter = ownerSet.iterator();
		while (ownersIter.hasNext()) {
		    LockInfo o = (LockInfo) ownersIter.next();
		    oldOwner = cloneLockInfo(o, currentLocker, destLockers, mb, lockTableIndex);
		    if (oldOwner != null) {
			break;
		    }
		}
	    }
	    if (firstWaiter != null) {
		for (int i = 0; i < destLockers.length; i++) {
		    if (firstWaiter.getLocker() == destLockers[i]) {
			firstWaiter = null;
			break;
		    }
		}
	    }
	    if (waiterList != null) {
		Iterator iter = waiterList.iterator();
		while (iter.hasNext()) {
		    LockInfo o = (LockInfo) iter.next();
		    for (int i = 0; i < destLockers.length; i++) {
			if (o.getLocker() == destLockers[i]) {
			    iter.remove();
			    break;
			}
		    }
		}
	    }
	}
	boolean removed = flushOwner(oldOwner, mb, lockTableIndex);
	assert removed;
	return lockType;
    }

    /** 
     * If oldOwner is the current owner, clone it and transform it into a dest locker.
     */
    private LockInfo cloneLockInfo(LockInfo oldOwner, Locker currentLocker, Locker[] destLockers, MemoryBudget mb,
	    int lockTableIndex) throws DatabaseException {
	if (oldOwner.getLocker() == currentLocker) {
	    try {
		LockType lockType = oldOwner.getLockType();
		for (int i = 0; i < destLockers.length; i++) {
		    LockInfo clonedLockInfo = (LockInfo) oldOwner.clone();
		    clonedLockInfo.setLocker(destLockers[i]);
		    destLockers[i].addLock(nodeId, this, lockType, LockGrantType.NEW);
		    addOwner(clonedLockInfo, mb, lockTableIndex);
		}
		return oldOwner;
	    } catch (CloneNotSupportedException e) {
		throw new DatabaseException(e);
	    }
	} else {
	    return null;
	}
    }

    /** 
     * Return the locker that has a write ownership on this lock. If no write owner exists, return null.
     */
    Locker getWriteOwnerLocker() {
	LockInfo owner = null;
	Iterator iter = null;
	if (ownerSet != null) {
	    iter = ownerSet.iterator();
	}
	if (firstOwner != null) {
	    owner = firstOwner;
	} else if ((iter != null) && (iter.hasNext())) {
	    owner = (LockInfo) iter.next();
	}
	while (owner != null) {
	    if (owner.getLockType().isWriteLock()) {
		return owner.getLocker();
	    }
	    if ((iter != null) && (iter.hasNext())) {
		owner = (LockInfo) iter.next();
	    } else {
		owner = null;
	    }
	}
	return null;
    }

    /** 
     * Debugging aid, validation before a lock request.
     */
    private boolean validateRequest(Locker locker) {
	if (firstWaiter != null) {
	    if (firstWaiter.getLocker() == locker) {
		assert false : "locker " + locker + " is already on waiters list.";
	    }
	}
	if (waiterList != null) {
	    Iterator iter = waiterList.iterator();
	    while (iter.hasNext()) {
		LockInfo o = (LockInfo) iter.next();
		if (o.getLocker() == locker) {
		    assert false : "locker " + locker + " is already on waiters list.";
		}
	    }
	}
	return true;
    }

    /** 
     * Debug dumper.
     */
    public String toString() {
	StringBuffer sb = new StringBuffer();
	sb.append(" NodeId:").append(nodeId);
	sb.append(" Owners:");
	if (nOwners() == 0) {
	    sb.append(" (none)");
	} else {
	    if (firstOwner != null) {
		sb.append(firstOwner);
	    }
	    if (ownerSet != null) {
		Iterator iter = ownerSet.iterator();
		while (iter.hasNext()) {
		    LockInfo info = (LockInfo) iter.next();
		    sb.append(info);
		}
	    }
	}
	sb.append(" Waiters:");
	if (nWaiters() == 0) {
	    sb.append(" (none)");
	} else {
	    sb.append(getWaitersListClone());
	}
	return sb.toString();
    }

    protected void hook760(MemoryBudget mb, int lockTableIndex) {
    }

    protected void hook761(MemoryBudget mb, int lockTableIndex) {
    }

    protected void hook762(MemoryBudget mb, int lockTableIndex) {
    }

    protected void hook763(MemoryBudget mb, int lockTableIndex) {
    }

    protected void hook764(MemoryBudget mb, int lockTableIndex) {
    }

    protected void hook765(MemoryBudget mb, int lockTableIndex, boolean removed) {
    }

    protected void hook766(MemoryBudget mb, int lockTableIndex, LockInfo flushedInfo) {
    }

    protected void hook767(MemoryBudget mb, int lockTableIndex) {
    }

    protected void hook768(MemoryBudget mb, int lockTableIndex, int numRemovedLockInfos) throws DatabaseException {
    }

}
\00after it has been deleted by the cleaner.
     */
    void removeFile(Long fileNum) throws DatabaseException {
	new UtilizationProfile_removeFile(this, fileNum).execute();
    }

    /** 
     * For the LN at the cursor position deletes all LNs for the file.  This method performs eviction and is not synchronized.
     */
    private void deleteFileSummary(Long fileNum) throws DatabaseException {
	Locker locker = null;
	CursorImpl cursor = null;
	try {
	    locker = new BasicLocker(env);
	    cursor = new CursorImpl(fileSummaryDb, locker);
	    DatabaseEntry keyEntry = new DatabaseEntry();
	    DatabaseEntry dataEntry = new DatabaseEntry();
	    long fileNumVal = fileNum.longValue();
	    if (!getFirstFSLN(cursor, fileNumVal, keyEntry, dataEntry, LockType.WRITE)) {
		return;
	    }
	    OperationStatus status = OperationStatus.SUCCESS;
	    while (status == OperationStatus.SUCCESS) {
		Label173:           ;  //this.hook173();
		FileSummaryLN ln = (FileSummaryLN) cursor.getCurrentLN(LockType.NONE);
		if (ln != null) {
		    if (fileNumVal != ln.getFileNumber(keyEntry.getData())) {
			break;
		    }
		    TrackedFileSummary tfs = tracker.getTrackedFile(fileNumVal);
		    if (tfs != null) {
			ln.setTrackedSummary(tfs);
		    }
		    cursor.delete();
		}
		status = cursor.getNext(keyEntry, dataEntry, LockType.WRITE, true, false);
	    }
	} finally {
	    if (cursor != null) {
		Label178:           ;  //this.hook178(cursor);
		cursor.close();
	    }
	    if (locker != null) {
		locker.operationEnd();
	    }
	}
    }

    /** 
     * Updates and stores the FileSummary for a given tracked file, if flushing of the summary is allowed.
     */
    public void flushFileSummary(TrackedFileSummary tfs) throws DatabaseException {
	if (tfs.getAllowFlush()) {
	    putFileSummary(tfs);
	}
    }

    /** 
     * Updates and stores the FileSummary for a given tracked file.  This method is synchronized and may not perform eviction.
     */
    private synchronized PackedOffsets putFileSummary(TrackedFileSummary tfs) throws DatabaseException {
	return new UtilizationProfile_putFileSummary(this, tfs).execute();
    }

    /** 
     * Returns the stored/packed obsolete offsets and the tracked obsolete offsets for the given file.  The tracked summary object returned can be used to test for obsolete offsets that are being added during cleaning by other threads participating in lazy migration.  The caller must call TrackedFileSummary.setAllowFlush(true) when cleaning is complete. This method performs eviction and is not synchronized.
     * @param logUpdate if true, log any updates to the utilization profile. Iffalse, only retrieve the new information.
     */
    TrackedFileSummary getObsoleteDetail(Long fileNum, PackedOffsets packedOffsets, boolean logUpdate)
	    throws DatabaseException {
	if (!env.getCleaner().trackDetail) {
	    return null;
	}
	assert cachePopulated;
	long fileNumVal = fileNum.longValue();
	List list = new ArrayList();
	TrackedFileSummary tfs = env.getLogManager().getUnflushableTrackedSummary(fileNumVal);
	Locker locker = null;
	CursorImpl cursor = null;
	try {
	    locker = new BasicLocker(env);
	    cursor = new CursorImpl(fileSummaryDb, locker);
	    DatabaseEntry keyEntry = new DatabaseEntry();
	    DatabaseEntry dataEntry = new DatabaseEntry();
	    OperationStatus status = OperationStatus.SUCCESS;
	    if (!getFirstFSLN(cursor, fileNumVal, keyEntry, dataEntry, LockType.NONE)) {
		status = OperationStatus.NOTFOUND;
	    }
	    while (status == OperationStatus.SUCCESS) {
		Label174:           ;  //this.hook174();
		FileSummaryLN ln = (FileSummaryLN) cursor.getCurrentLN(LockType.NONE);
		if (ln != null) {
		    if (fileNumVal != ln.getFileNumber(keyEntry.getData())) {
			break;
		    }
		    PackedOffsets offsets = ln.getObsoleteOffsets();
		    if (offsets != null) {
			list.add(offsets.toArray());
		    }
		    //           ;  //this.hook187(cursor);
        Label187:
		}
		status = cursor.getNext(keyEntry, dataEntry, LockType.NONE, true, false);
	    }
	} finally {
	    Label179:           ;  //this.hook179(cursor);
	    if (locker != null) {
		locker.operationEnd();
	    }
	}
	if (!tfs.isEmpty()) {
	    PackedOffsets offsets = null;
	    if (logUpdate) {
		offsets = putFileSummary(tfs);
		if (offsets != null) {
		    list.add(offsets.toArray());
		}
	    } else {
		long[] offsetList = tfs.getObsoleteOffsets();
		if (offsetList != null) {
		    list.add(offsetList);
		}
	    }
	}
	int size = 0;
	for (int i = 0; i < list.size(); i += 1) {
	    long[] a = (long[]) list.get(i);
	    size += a.length;
	}
	long[] offsets = new long[size];
	int index = 0;
	for (int i = 0; i < list.size(); i += 1) {
	    long[] a = (long[]) list.get(i);
	    System.arraycopy(a, 0, offsets, index, a.length);
	    index += a.length;
	}
	assert index == offsets.length;
	packedOffsets.pack(offsets);
	return tfs;
    }

    /** 
     * Populate the profile for file selection.  This method performs eviction and is not synchronized.  It must be called before recovery is complete so that synchronization is unnecessary.  It must be called before the recovery checkpoint so that the checkpoint can flush file summary information.
     */
    public boolean populateCache() throws DatabaseException {
	return new UtilizationProfile_populateCache(this).execute();
    }

    /** 
     * Positions at the most recent LN for the given file number.
     */
    private boolean getFirstFSLN(CursorImpl cursor, long fileNum, DatabaseEntry keyEntry, DatabaseEntry dataEntry,
	    LockType lockType) throws DatabaseException {
	byte[] keyBytes = FileSummaryLN.makePartialKey(fileNum);
	keyEntry.setData(keyBytes);
	int result = cursor.searchAndPosition(keyEntry, dataEntry, SearchMode.SET_RANGE, lockType);
	if ((result & CursorImpl.FOUND) == 0) {
	    return false;
	}
	boolean exactKeyMatch = ((result & CursorImpl.EXACT_KEY) != 0);
	if (exactKeyMatch
		&& cursor.getCurrentAlreadyLatched(keyEntry, dataEntry, lockType, true) != OperationStatus.KEYEMPTY) {
	    return true;
	}
	OperationStatus status = cursor.getNext(keyEntry, dataEntry, lockType, true, !exactKeyMatch);
	return status == OperationStatus.SUCCESS;
    }

    /** 
     * If the file summary db is already open, return, otherwise attempt to open it.  If the environment is read-only and the database doesn't exist, return false.  If the environment is read-write the database will be created if it doesn't exist.
     */
    private boolean openFileSummaryDatabase() throws DatabaseException {
	if (fileSummaryDb != null) {
	    return true;
	}
	DbTree dbTree = env.getDbMapTree();
	Locker autoTxn = null;
	boolean operationOk = false;
	try {
	    autoTxn = new AutoTxn(env, new TransactionConfig());
	    DatabaseImpl db = dbTree.getDb(autoTxn, DbTree.UTILIZATION_DB_NAME, null, true);
	    if (db == null) {
		if (env.isReadOnly()) {
		    return false;
		}
		db = dbTree.createDb(autoTxn, DbTree.UTILIZATION_DB_NAME, new DatabaseConfig(), null, true);
	    }
	    fileSummaryDb = db;
	    operationOk = true;
	    return true;
	} finally {
	    if (autoTxn != null) {
		autoTxn.operationEnd(operationOk);
	    }
	}
    }

    /** 
     * Insert the given LN with the given key values.  This method is synchronized and may not perform eviction.
     */
    private synchronized void insertFileSummary(FileSummaryLN ln, long fileNum, int sequence) throws DatabaseException {
	byte[] keyBytes = FileSummaryLN.makeFullKey(fileNum, sequence);
	Locker locker = null;
	CursorImpl cursor = null;
	try {
	    locker = new BasicLocker(env);
	    cursor = new CursorImpl(fileSummaryDb, locker);
	    //           ;  //this.hook189(cursor);
      Label189:
	    OperationStatus status = cursor.putLN(keyBytes, ln, false);
	    Label177:           ;  //this.hook177(fileNum, sequence, status);
	    //           ;  //this.hook188(cursor);
      Label188:
	} finally {
	    if (cursor != null) {
		cursor.close();
	    }
	    if (locker != null) {
		locker.operationEnd();
	    }
	}
    }

    /** 
     * Checks that all FSLN offsets are indeed obsolete.  Assumes that the system is quiesent (does not lock LNs).  This method is not synchronized (because it doesn't access fileSummaryMap) and eviction is allowed.
     * @return true if no verification failures.
     */
    public boolean verifyFileSummaryDatabase() throws DatabaseException {
	DatabaseEntry key = new DatabaseEntry();
	DatabaseEntry data = new DatabaseEntry();
	openFileSummaryDatabase();
	Locker locker = null;
	CursorImpl cursor = null;
	boolean ok = true;
	try {
	    locker = new BasicLocker(env);
	    cursor = new CursorImpl(fileSummaryDb, locker);
	    if (cursor.positionFirstOrLast(true, null)) {
		OperationStatus status = cursor.getCurrentAlreadyLatched(key, data, LockType.NONE, true);
		while (status == OperationStatus.SUCCESS) {
		    Label175:           ;  //this.hook175();
		    FileSummaryLN ln = (FileSummaryLN) cursor.getCurrentLN(LockType.NONE);
		    if (ln != null) {
			long fileNumVal = ln.getFileNumber(key.getData());
			PackedOffsets offsets = ln.getObsoleteOffsets();
			if (offsets != null) {
			    long[] vals = offsets.toArray();
			    for (int i = 0; i < vals.length; i++) {
				long lsn = DbLsn.makeLsn(fileNumVal, vals[i]);
				if (!verifyLsnIsObsolete(lsn)) {
				    ok = false;
				}
			    }
			}
			//           ;  //this.hook190(cursor);
      Label190:
			status = cursor.getNext(key, data, LockType.NONE, true, false);
		    }
		}
	    }
	} finally {
	    if (cursor != null) {
		cursor.close();
	    }
	    if (locker != null) {
		locker.operationEnd();
	    }
	}
	return ok;
    }

    private boolean verifyLsnIsObsolete(long lsn) throws DatabaseException {
	try {
	    Object o = env.getLogManager().getLogEntry(lsn);
	    if (!(o instanceof LNLogEntry)) {
		return true;
	    }
	    LNLogEntry entry = (LNLogEntry) o;
	    if (entry.getLN().isDeleted()) {
		return true;
	    }
	    DatabaseId dbId = entry.getDbId();
	    DatabaseImpl db = env.getDbMapTree().getDb(dbId);
	    boolean b = db == null;
	    Label186: //b =            ;  //this.hook186(db, b);
	    if (b) {
		return true;
	    }
	    BIN bin = null;
	    Label180:           ;  //this.hook180(lsn, entry, db, bin);
			Tree tree = db.getTree();
			TreeLocation location = new TreeLocation();
			boolean parentFound = tree.getParentBINForChildLN(location, entry.getKey(), entry.getDupKey(), entry.getLN(), false, true, false, false);
			bin = location.bin;
			int index = location.index;
			if (!parentFound) {
					throw new ReturnBoolean(true);
			}
			if (bin.isEntryKnownDeleted(index)) {
					throw new ReturnBoolean(true);
			}
			if (bin.getLsn(index) != lsn) {
					throw new ReturnBoolean(true);
			}
			System.err.println("lsn " + DbLsn.getNoFormatString(lsn) + " was found in tree.");
			throw new ReturnBoolean(false);
//end hook180

	    throw ReturnHack.returnBoolean;
	} catch (ReturnBoolean r) {
	    Label180_1:           ;
	    return r.value;
	}
    }

  /*  protected void hook173() throws DatabaseException {
    }

    protected void hook174() throws DatabaseException {
    }

    protected void hook175() throws DatabaseException {
    }

    protected void hook177(long fileNum, int sequence, OperationStatus status) throws DatabaseException {
    }

    protected void hook178(CursorImpl cursor) throws DatabaseException {
    }

    protected void hook179(CursorImpl cursor) throws DatabaseException {
    }

    protected void hook180(long lsn, LNLogEntry entry, DatabaseImpl db, BIN bin) throws DatabaseException {
	Tree tree = db.getTree();
	TreeLocation location = new TreeLocation();
	boolean parentFound = tree.getParentBINForChildLN(location, entry.getKey(), entry.getDupKey(), entry.getLN(),
		false, true, false, false);
	bin = location.bin;
	int index = location.index;
	if (!parentFound) {
	    throw new ReturnBoolean(true);
	}
	if (bin.isEntryKnownDeleted(index)) {
	    throw new ReturnBoolean(true);
	}
	if (bin.getLsn(index) != lsn) {
	    throw new ReturnBoolean(true);
	}
	System.err.println("lsn " + DbLsn.getNoFormatString(lsn) + " was found in tree.");
	throw new ReturnBoolean(false);
    }
*/
//    protected boolean hook186(DatabaseImpl db, boolean b) throws DatabaseException {
//	return b;
//    }

  //  protected void hook187(CursorImpl cursor) throws DatabaseException {
  //  }

//    protected void hook188(CursorImpl cursor) throws DatabaseException {
//    }

    protected void hook189(CursorImpl cursor) throws DatabaseException {
    }

    protected void hook190(CursorImpl cursor) throws DatabaseException {
    }

}
\00after corruption is at LSN:\n   "
		    + DbLsn.toString(nextGoodRecordPostCorruption));
	}
	startLsn = lastUsedLsn;
	initStartingPosition(nextAvailableLsn, null);
	if (switchedFiles) {
	    currentEntryPrevOffset = 0;
	}
	return true;
    }

    /** 
     * @return true if this reader should process this entry, or just skipover it.
     */
    protected boolean isTargetEntry(byte logEntryTypeNumber, byte logEntryTypeVersion) {
	if (targetEntryTypes.size() == 0) {
	    return true;
	} else {
	    return targetEntryTypes.contains(new Byte(logEntryTypeNumber));
	}
    }

}
\00after we process pending LNs. If we do this too seldom, the pending LN queue may grow large, and it isn't budgeted memory. If we process it too often, we will repeatedly request a non-blocking lock for the same locked node.
     */
    private static final int PROCESS_PENDING_EVERY_N_LNS = 100;

    /** 
     * Whether to prohibit BINDeltas for a BIN that is fetched by the cleaner. The theory is that when fetching a BIN during cleaning we normally expect that the BIN will be evicted soon, and a delta during checkpoint would be wasted. However, this does not take into account use of the BIN by the application after fetching; the BIN could become hot and then deltas may be profitable. To be safe we currently allow deltas when fetching.
     */
    private static final boolean PROHIBIT_DELTAS_WHEN_FETCHING = false;

    private static final boolean DEBUG_TRACING = false;

    private EnvironmentImpl env;

    private Cleaner cleaner;

    private FileSelector fileSelector;

    private UtilizationProfile profile;

    FileProcessor(String name, EnvironmentImpl env, Cleaner cleaner, UtilizationProfile profile,
        FileSelector fileSelector) {
        super(0, name, env);
        this.env = env;
        this.cleaner = cleaner;
        this.fileSelector = fileSelector;
        this.profile = profile;
    }

    public void clearEnv() {
        env = null;
        cleaner = null;
        fileSelector = null;
        profile = null;
    }

    /** 
     * Return the number of retries when a deadlock exception occurs.
     */
    protected int nDeadlockRetries() throws DatabaseException {
        return cleaner.nDeadlockRetries;
    }

    /** 
     * Cleaner doesn't have a work queue so just throw an exception if it's ever called.
     */
    public void addToQueue(Object o) throws DatabaseException {
        throw new DatabaseException("Cleaner.addToQueue should never be called.");
    }

    /** 
     * Activates the cleaner. Is normally called when je.cleaner.byteInterval bytes are written to the log.
     */
    public void onWakeup() throws DatabaseException {
        doClean(true, true, false);
    }

    /** 
     * Cleans selected files and returns the number of files cleaned. May be called by the daemon thread or programatically.
     * @param invokedFromDaemoncurrently has no effect.
     * @param cleanMultipleFilesis true to clean until we're under budget, or false to cleanat most one file.
     * @param forceCleaningis true to clean even if we're not under the utilizationthreshold.
     * @return the number of files cleaned, not including files cleanedunsuccessfully.
     */
    public synchronized int doClean(boolean invokedFromDaemon, boolean cleanMultipleFiles, boolean forceCleaning)
    throws DatabaseException {
        if (env.isClosed()) {
            return 0;
        }
        int nOriginalLogFiles = profile.getNumberOfFiles();
        int nFilesCleaned = 0;
        while (true) {
            if (nFilesCleaned >= nOriginalLogFiles) {
                break;
            }
            if (env.isClosing()) {
                break;
            }
            cleaner.processPending();
            cleaner.deleteSafeToDeleteFiles();
            boolean needLowUtilizationSet = cleaner.clusterResident || cleaner.clusterAll;
            Long fileNum = fileSelector.selectFileForCleaning(profile, forceCleaning, needLowUtilizationSet,
                cleaner.maxBatchFiles);
            cleaner.updateReadOnlyFileCollections();
            if (fileNum == null) {
                break;
            }
            Label138: //this.hook138();
                boolean finished = false;
            long fileNumValue = fileNum.longValue();
            int runId = ++cleaner.nCleanerRuns;
            try {
                String traceMsg = "CleanerRun " + runId + " on file 0x" + Long.toHexString(fileNumValue);
                Label139: //traceMsg = this.hook139(traceMsg);
                Label121: //this.hook121(traceMsg);
                if (DEBUG_TRACING) {
                    System.out.println("\n" + traceMsg);
                }
                if (processFile(fileNum)) {
                    fileSelector.addCleanedFile(fileNum);
                    nFilesCleaned += 1;
                    Label140: //this.hook140();
                        finished = true;
                }
            } catch (IOException IOE) {
                Label122: //this.hook122(IOE);
                throw new DatabaseException(IOE);
            } finally {
                if (!finished) {
                    fileSelector.putBackFileForCleaning(fileNum);
                }
                String traceMsg = "CleanerRun " + runId + " on file 0x" + Long.toHexString(fileNumValue) +
                    " invokedFromDaemon=" + invokedFromDaemon + " finished=" + finished;
                Label141: //traceMsg = this.hook141(traceMsg);
                Label123:    //this.hook123(traceMsg);
                if (DEBUG_TRACING) {
                    System.out.println("\n" + traceMsg);
                }
            }
            if (!cleanMultipleFiles) {
                break;
            }
        }
        return nFilesCleaned;
    }

    /** 
     * Process all log entries in the given file. Note that we check for obsolete entries using the active TFS (TrackedFileSummary) for a file while it is being processed, and we prohibit flushing (eviction) of that offset information until file processing is complete. An entry could become obsolete because: 1- normal application activity deletes or updates the entry, 2- proactive migration migrates the entry before we process it, or 3- if trackDetail is false. However, checking the TFS is expensive if it has many entries, because we perform a linear search. There is a tradeoff between the cost of the TFS lookup and its benefit, which is to avoid a tree search if the entry is obsolete. Note that many more lookups for non-obsolete entries than obsolete entries will typically be done. In spite of that we check the tracked summary to avoid the situation where eviction does proactive migration, and evicts a BIN that is very soon afterward fetched during cleaning.
     * @return false if we aborted file processing because the environment isbeing closed.
     */
    private boolean processFile(Long fileNum) throws DatabaseException, IOException {
        return new FileProcessor_processFile(this, fileNum).execute();
    }

    /** 
     * Processes the first LN in the look ahead cache and removes it from the cache. While the BIN is latched, look through the BIN for other LNs in the cache; if any match, process them to avoid a tree search later.
     * @param info
     * @param offset
     */
    private void processLN(Long fileNum, TreeLocation location, Long offset, LNInfo info, Object lookAheadCachep,
        Map dbCache) throws DatabaseException {
        new FileProcessor_processLN(this, fileNum, location, offset, info, lookAheadCachep, dbCache).execute();
    }

    /** 
     * Processes an LN that was found in the tree. Lock the LN's node ID and then set the entry's MIGRATE flag if the LSN of the LN log entry is the active LSN in the tree.
     * @param infoidentifies the LN log entry.
     * @param logLsnis the LSN of the log entry.
     * @param treeLsnis the LSN found in the tree.
     * @param binis the BIN found in the tree; is latched on method entry andexit.
     * @param indexis the BIN index found in the tree.
     * @param parentDINis non-null for a DupCountLN only; if non-null, is latched onmethod entry and exit.
     */
    private void processFoundLN(LNInfo info, long logLsn, long treeLsn, BIN bin, int index, DIN parentDIN)
    throws DatabaseException {
        LN ln = info.getLN();
        byte[] key = info.getKey();
        byte[] dupKey = info.getDupKey();
        DatabaseImpl db = bin.getDatabase();
        boolean isDupCountLN = parentDIN != null;
        boolean obsolete = false;
        boolean migrated = false;
        boolean lockDenied = false;
        boolean completed = false;
        long nodeId = ln.getNodeId();
        BasicLocker locker = null;
        try {
            Tree tree = db.getTree();
            assert tree != null;
            if (treeLsn != logLsn) {
                locker = new BasicLocker(env);
                LockResult lockRet = locker.nonBlockingLock(nodeId, LockType.READ, db);
                if (lockRet.getLockGrant() == LockGrantType.DENIED) {
                    Label142: //this.hook142();
                        lockDenied = true;
                }
                else {
                    Label143: //this.hook143();
                        obsolete = true;
                }
            }
            if (!obsolete && !lockDenied) {
                if (isDupCountLN) {
                    ChildReference dclRef = parentDIN.getDupCountLNRef();
                    dclRef.setMigrate(true);
                    parentDIN.setDirty(true);
                    if (treeLsn == logLsn && dclRef.getTarget() == null) {
                        ln.postFetchInit(db, logLsn);
                        parentDIN.updateDupCountLN(ln);
                    }
                } else {
                    bin.setMigrate(index, true);
                    bin.setDirty(true);
                    if (treeLsn == logLsn && bin.getTarget(index) == null) {
                        ln.postFetchInit(db, logLsn);
                        bin.updateEntry(index, ln);
                    }
                    if (PROHIBIT_DELTAS_WHEN_FETCHING && bin.getGeneration() == 0) {
                        bin.setProhibitNextDelta();
                    }
                    bin.setGeneration();
                }
                Label144: //this.hook144();
                    migrated = true;
            }
            completed = true;
        } finally {
            if (locker != null) {
                locker.operationEnd();
            }
            if (completed && lockDenied) {
                fileSelector.addPendingLN(ln, db.getId(), key, dupKey);
            }
            Label124: //this.hook124(logLsn, ln, obsolete, migrated, completed);
        }
    }

    /** 
     * If an IN is still in use in the in-memory tree, dirty it. The checkpoint invoked at the end of the cleaning run will end up rewriting it.
     */
    private void processIN(IN inClone, DatabaseImpl db, long lsn) throws DatabaseException {
        try {
            boolean obsolete = false;
            boolean dirtied = false;
            boolean completed = false;
            //this.hook125(inClone, db, lsn, obsolete, dirtied, completed);
            Label125:
                boolean b = db == null;
            //b = this.hook159(db, b);
            Label159:
                if (b) {
                    //this.hook160(db);
                    Label160: Label151: //this.hook151();
                        obsolete = true;
                    completed = true;
                    throw new ReturnVoid();
                }
            Tree tree = db.getTree();
            assert tree != null;
            IN inInTree = findINInTree(tree, db, inClone, lsn);
            if (inInTree == null) {
                Label152: //this.hook152();
                    obsolete = true;
            }
            else {
                Label153: //this.hook153();
                    inInTree.setDirty(true);
                inInTree.setProhibitNextDelta();
                Label136: //this.hook136(inInTree);
                    dirtied = true;
            }
            completed = true;
            //End of hook125
        } catch (ReturnVoid r) {
            return;
        }
Label125_1: ;//;

    }

    /** 
     * Given a clone of an IN that has been taken out of the log, try to find it in the tree and verify that it is the current one in the log. Returns the node in the tree if it is found and it is current re: LSN's. Otherwise returns null if the clone is not found in the tree or it's not the latest version. Caller is responsible for unlatching the returned IN.
     */
    private IN findINInTree(Tree tree, DatabaseImpl db, IN inClone, long lsn) throws DatabaseException {
        try {
            if (inClone.isDbRoot()) {
                IN rootIN = isRoot(tree, db, inClone, lsn);
                if (rootIN == null) {
                    return null;
                } else {
                    return rootIN;
                }
            }
            inClone.latch(Cleaner.UPDATE_GENERATION);
            SearchResult result = null;
            Label134: //this.hook134(tree, db, inClone, lsn, result);
            result = tree.getParentINForChildIN(inClone, true, Cleaner.UPDATE_GENERATION, inClone.getLevel(), null);
            if (!result.exactParentFound) {
                throw new ReturnObject(null);
            }
            int compareVal = DbLsn.compareTo(result.parent.getLsn(result.index), lsn);
            if (compareVal > 0) {
                throw new ReturnObject(null);
            } 
            else {
                IN in ;
                if (compareVal == 0) { in = (IN) result.parent.getTarget(result.index);
                    if ( in == null) { in = inClone; in .postFetchInit(db, lsn);
                        result.parent.updateEntry(result.index, in );
                    }
                } else { in = (IN) result.parent.fetchTarget(result.index);
                } in.latch(Cleaner.UPDATE_GENERATION);
                throw new ReturnObject( in );
            }
            Label134_1: ;//
            //End of hook134

            throw ReturnHack.returnObject;
        } catch (ReturnObject r) {
            return (IN) r.value;
        }
    }

    /** 
     * Check if the cloned IN is the same node as the root in tree. Return the real root if it is, null otherwise. If non-null is returned, the returned IN (the root) is latched -- caller is responsible for unlatching it.
     */
    private IN isRoot(Tree tree, DatabaseImpl db, IN inClone, long lsn) throws DatabaseException {
        RootDoWork rdw = new RootDoWork(db, inClone, lsn);
        return tree.withRootLatchedShared(rdw);
    }

    /** 
     * XXX: Was this intended to override Thread.toString()? If so it no longer does, because we separated Thread from DaemonThread.
     */
    public String toString() {
        StringBuffer sb = new StringBuffer();
        sb.append("<Cleaner name=\"").append(name).append("\"/>");
        return sb.toString();
    }

    //protected void hook121(String traceMsg) throws DatabaseException, IOException {}

   // protected void hook122(IOException IOE) throws DatabaseException {}

  //  protected void hook123(String traceMsg) throws DatabaseException {}

   // protected void hook124(long logLsn, LN ln, boolean obsolete, boolean migrated, boolean completed)
   // throws DatabaseException {}

    //   protected void hook125(IN inClone, DatabaseImpl db, long lsn, boolean obsolete, boolean dirtied, boolean completed)
    //   throws DatabaseException {
    //    }

    /*
        protected void hook134(Tree tree, DatabaseImpl db, IN inClone, long lsn, SearchResult result)
    	    throws DatabaseException {

    	result = tree.getParentINForChildIN(inClone, true, Cleaner.UPDATE_GENERATION, inClone.getLevel(), null);
    	if (!result.exactParentFound) {
    	    throw new ReturnObject(null);
    	}
    	int compareVal = DbLsn.compareTo(result.parent.getLsn(result.index), lsn);
    	if (compareVal > 0) {
    	    throw new ReturnObject(null);
    	} else {
    	    IN in;
    	    if (compareVal == 0) {
    		in = (IN) result.parent.getTarget(result.index);
    		if (in == null) {
    		    in = inClone;
    		    in.postFetchInit(db, lsn);
    		    result.parent.updateEntry(result.index, in);
    		}
    	    } else {
    		in = (IN) result.parent.fetchTarget(result.index);
    	    }
    	    in.latch(Cleaner.UPDATE_GENERATION);
    	    throw new ReturnObject(in);
    	}
        }
    */
    // protected void hook136(IN inInTree) throws DatabaseException {
    // }

    //  protected void hook138() throws DatabaseException {
    //  }

    protected String hook139(String traceMsg) throws DatabaseException, IOException {
        return traceMsg;
    }

    protected void hook140() throws DatabaseException, IOException {}

    protected String hook141(String traceMsg) throws DatabaseException {
        return traceMsg;
    }

    //  protected void hook142() throws DatabaseException {
    //  }

    //    protected void hook143() throws DatabaseException {
    //    }

    //    protected void hook144() throws DatabaseException {
    //    }

    //  protected void hook151() throws DatabaseException {
    //  }

    //  protected void hook152() throws DatabaseException {
    // }

    // protected void hook153() throws DatabaseException {
    // }

    protected boolean hook159(DatabaseImpl db, boolean b) throws DatabaseException {
        return b;
    }

    protected void hook160(DatabaseImpl db) throws DatabaseException {}

}
\00after the memory budget has been calculated.
     */
    void reset(DbConfigManager configManager) throws DatabaseException {
	if (runInMemory && bufferPool != null) {
	    return;
	}
	int numBuffers = configManager.getInt(EnvironmentParams.NUM_LOG_BUFFERS);
	long logBufferBudget = envImpl.getMemoryBudget().getLogBufferBudget();
	int newBufferSize = (int) logBufferBudget / numBuffers;
	LinkedList newPool = new LinkedList();
	if (runInMemory) {
	    numBuffers = 1;
	}
	for (int i = 0; i < numBuffers; i++) {
	    newPool.add(new LogBuffer(newBufferSize, envImpl));
	}
	Label486: //this.hook486();
	bufferPool = newPool;
	logBufferSize = newBufferSize;
    }

    /** 
     * Get a log buffer for writing sizeNeeded bytes. If currentWriteBuffer is too small or too full, flush currentWriteBuffer and get a new one. Called within the log write latch.
     * @return a buffer that can hold sizeNeeded bytes.
     */
    LogBuffer getWriteBuffer(int sizeNeeded, boolean flippedFile) throws IOException, DatabaseException {
	if ((!currentWriteBuffer.hasRoom(sizeNeeded)) || flippedFile) {
	    writeBufferToFile(sizeNeeded);
	}
	if (flippedFile) {
	    if (!runInMemory) {
		fileManager.syncLogEndAndFinishFile();
	    }
	}
	return currentWriteBuffer;
    }

    /** 
     * Write the contents of the currentWriteBuffer to disk.  Leave this buffer in memory to be available to would be readers.  Set up a new currentWriteBuffer. Assumes the log write latch is held.
     * @param sizeNeeded is the size of the next object we need to write tothe log. May be 0 if this is called on behalf of LogManager.flush().
     */
    void writeBufferToFile(int sizeNeeded) throws IOException, DatabaseException {
	int bufferSize = ((logBufferSize > sizeNeeded) ? logBufferSize : sizeNeeded);
	Label488: //this.hook488();
	LogBuffer latchedBuffer = currentWriteBuffer;
	Label487: //this.hook487(bufferSize, latchedBuffer);
  ByteBuffer currentByteBuffer = currentWriteBuffer.getDataBuffer();
	int savePosition = currentByteBuffer.position();
	int saveLimit = currentByteBuffer.limit();
	currentByteBuffer.flip();
	if (runInMemory) {
	    Label492: //this.hook492(latchedBuffer);
	    latchedBuffer = null;
	    Label491: //this.hook491();
	    currentWriteBuffer = new LogBuffer(bufferSize, envImpl);
	    bufferPool.add(currentWriteBuffer);
	    Label490: //this.hook490();
	} else {
	    try {
		fileManager.writeLogBuffer(currentWriteBuffer);
		currentWriteBuffer.getDataBuffer().rewind();
		Label494: //this.hook494(latchedBuffer);
		latchedBuffer = null;
		LogBuffer nextToUse = null;
		Label493: //this.hook493(nextToUse);
		Label495: //this.hook495();
		Iterator iter = bufferPool.iterator();
		nextToUse = (LogBuffer) iter.next();
		boolean done = bufferPool.remove(nextToUse);
		assert done;
		nextToUse.reinit();
		bufferPool.add(nextToUse);
		currentWriteBuffer = nextToUse;
//End of hook 493
	    } catch (DatabaseException DE) {
		currentByteBuffer.position(savePosition);
		currentByteBuffer.limit(saveLimit);
		throw DE;
	    }
	}
Label487_1:
// End of hook487
    }

    /** 
     * A loggable object has been freshly marshalled into the write log buffer. 1. Update buffer so it knows what LSNs it contains. 2. If this object requires a flush, write this buffer out to the  backing file. Assumes log write latch is held.
     */
    void writeCompleted(long lsn, boolean flushRequired) throws DatabaseException, IOException {
	currentWriteBuffer.registerLsn(lsn);
	if (flushRequired) {
	    writeBufferToFile(0);
	}
    }

    /** 
     * Find a buffer that holds this LSN.
     * @return the buffer that contains this LSN, latched and ready toread, or return null.
     */
    LogBuffer getReadBuffer(long lsn) throws DatabaseException {
			LogBuffer foundBuffer = null;
			Label489: //	foundBuffer = this.hook489(lsn, foundBuffer);
			Iterator iter = bufferPool.iterator();
			while (iter.hasNext()) {
					LogBuffer l = (LogBuffer) iter.next();
					if (l.containsLsn(lsn)) {
				foundBuffer = l;
				break;
					}
			}
			if (foundBuffer == null && currentWriteBuffer.containsLsn(lsn)) {
					foundBuffer = currentWriteBuffer;
			}
			if (foundBuffer == null) {
					Label496: //this.hook496();
			}
Label489_1:
	//End of hook489
		if (foundBuffer == null) {
			  return null;
		} else {
			  return foundBuffer;
		}
    }

    protected void hook485(EnvironmentImpl envImpl) throws DatabaseException {
    }

    protected void hook486() throws DatabaseException {
    }

  /*  protected void hook487(int bufferSize, LogBuffer latchedBuffer) throws IOException, DatabaseException {
	ByteBuffer currentByteBuffer = currentWriteBuffer.getDataBuffer();
	int savePosition = currentByteBuffer.position();
	int saveLimit = currentByteBuffer.limit();
	currentByteBuffer.flip();
	if (runInMemory) {
	    this.hook492(latchedBuffer);
	    latchedBuffer = null;
	    this.hook491();
	    currentWriteBuffer = new LogBuffer(bufferSize, envImpl);
	    bufferPool.add(currentWriteBuffer);
	    this.hook490();
	} else {
	    try {
		fileManager.writeLogBuffer(currentWriteBuffer);
		currentWriteBuffer.getDataBuffer().rewind();
		this.hook494(latchedBuffer);
		latchedBuffer = null;
		LogBuffer nextToUse = null;
		this.hook493(nextToUse);
	    } catch (DatabaseException DE) {
		currentByteBuffer.position(savePosition);
		currentByteBuffer.limit(saveLimit);
		throw DE;
	    }
	}
    }
*/

    protected void hook488() throws IOException, DatabaseException {
    }

//    protected LogBuffer hook489(long lsn, LogBuffer foundBuffer) throws DatabaseException {
//		Iterator iter = bufferPool.iterator();
//		while (iter.hasNext()) {
//			  LogBuffer l = (LogBuffer) iter.next();
//			  if (l.containsLsn(lsn)) {
//			foundBuffer = l;
//			break;
//			  }
//		}
//		if (foundBuffer == null && currentWriteBuffer.containsLsn(lsn)) {
//			  foundBuffer = currentWriteBuffer;
//		}
//		if (foundBuffer == null) {
//			  Label496: //this.hook496();
//		}
//		return foundBuffer;
//   }

  //  protected void hook490() throws IOException, DatabaseException {
   // }

  //  protected void hook491() throws IOException, DatabaseException {
//    }

    //protected void hook492(LogBuffer latchedBuffer) throws IOException, DatabaseException {
    //}
/*
    protected void hook493(LogBuffer nextToUse) throws IOException, DatabaseException {
	Label495: //this.hook495();
	Iterator iter = bufferPool.iterator();
	nextToUse = (LogBuffer) iter.next();
	boolean done = bufferPool.remove(nextToUse);
	assert done;
	nextToUse.reinit();
	bufferPool.add(nextToUse);
	currentWriteBuffer = nextToUse;
    }
*/
    //protected void hook494(LogBuffer latchedBuffer) throws IOException, DatabaseException {
    //}

    //protected void hook495() throws IOException, DatabaseException {
    //}

   // protected void hook496() throws DatabaseException {
    //}

}
\00after assignment, so no synchronization is needed.
     */
    Set mustBeCleanedFiles = Collections.EMPTY_SET;

    /** 
     * All files that are below the minUtilization threshold. Used to perform clustering migration. Is read-only after assignment, so no synchronization is needed.
     */
    Set lowUtilizationFiles = Collections.EMPTY_SET;

    private String name;

    private EnvironmentImpl env;

    private UtilizationProfile profile;

    private UtilizationTracker tracker;

    private FileSelector fileSelector;

    private FileProcessor[] threads;

    private Object deleteFileLock;

    public Cleaner(EnvironmentImpl env, String name) throws DatabaseException {
	this.env = env;
	this.name = name;
	tracker = new UtilizationTracker(env, this);
	profile = new UtilizationProfile(env, tracker);
	fileSelector = new FileSelector();
	threads = new FileProcessor[0];
	deleteFileLock = new Object();
	trackDetail = env.getConfigManager().getBoolean(EnvironmentParams.CLEANER_TRACK_DETAIL);
	envConfigUpdate(env.getConfigManager());
	env.addConfigObserver(this);
    }

    /** 
     * Process notifications of mutable property changes.
     */
    public void envConfigUpdate(DbConfigManager cm) throws DatabaseException {
	lockTimeout = PropUtil.microsToMillis(cm.getLong(EnvironmentParams.CLEANER_LOCK_TIMEOUT));
	readBufferSize = cm.getInt(EnvironmentParams.CLEANER_READ_SIZE);
	if (readBufferSize <= 0) {
	    readBufferSize = cm.getInt(EnvironmentParams.LOG_ITERATOR_READ_SIZE);
	}
	Label94: //this.hook94(cm);
	nDeadlockRetries = cm.getInt(EnvironmentParams.CLEANER_DEADLOCK_RETRY);
	expunge = cm.getBoolean(EnvironmentParams.CLEANER_REMOVE);
	clusterResident = cm.getBoolean(EnvironmentParams.CLEANER_CLUSTER);
	clusterAll = cm.getBoolean(EnvironmentParams.CLEANER_CLUSTER_ALL);
	maxBatchFiles = cm.getInt(EnvironmentParams.CLEANER_MAX_BATCH_FILES);
	Label90: //this.hook90();
	if (clusterResident && clusterAll) {
	    throw new IllegalArgumentException("Both " + EnvironmentParams.CLEANER_CLUSTER + " and "
		    + EnvironmentParams.CLEANER_CLUSTER_ALL + " may not be set to true.");
	}
	int nThreads = cm.getInt(EnvironmentParams.CLEANER_THREADS);
	assert nThreads > 0;
	if (nThreads != threads.length) {
	    for (int i = nThreads; i < threads.length; i += 1) {
		if (threads[i] != null) {
		    threads[i].shutdown();
		    threads[i] = null;
		}
	    }
	    FileProcessor[] newThreads = new FileProcessor[nThreads];
	    for (int i = 0; i < nThreads && i < threads.length; i += 1) {
		newThreads[i] = threads[i];
	    }
	    threads = newThreads;
	    for (int i = 0; i < nThreads; i += 1) {
		if (threads[i] == null) {
		    threads[i] = new FileProcessor(name + '-' + (i + 1), env, this, profile, fileSelector);
		}
	    }
	}
	cleanerBytesInterval = cm.getLong(EnvironmentParams.CLEANER_BYTES_INTERVAL);
	if (cleanerBytesInterval == 0) {
	    cleanerBytesInterval = cm.getLong(EnvironmentParams.LOG_FILE_MAX) / 4;
	}
    }

    public UtilizationTracker getUtilizationTracker() {
	return tracker;
    }

    public UtilizationProfile getUtilizationProfile() {
	return profile;
    }

    public void wakeup() {
	for (int i = 0; i < threads.length; i += 1) {
	    if (threads[i] != null) {
		threads[i].wakeup();
	    }
	}
    }

    private boolean areThreadsRunning() {
	for (int i = 0; i < threads.length; i += 1) {
	    if (threads[i] != null) {
		return threads[i].isRunning();
	    }
	}
	return false;
    }

    /** 
     * Cleans selected files and returns the number of files cleaned. This method is not invoked by a deamon thread, it is programatically.
     * @param cleanMultipleFilesis true to clean until we're under budget, or false to cleanat most one file.
     * @param forceCleaningis true to clean even if we're not under the utilizationthreshold.
     * @return the number of files cleaned, not including files cleanedunsuccessfully.
     */
    public int doClean(boolean cleanMultipleFiles, boolean forceCleaning) throws DatabaseException {
	FileProcessor processor = new FileProcessor("", env, this, profile, fileSelector);
	return processor.doClean(false, cleanMultipleFiles, forceCleaning);
    }

    /** 
     * Deletes all files that are safe-to-delete, if an exclusive lock on the environment can be obtained.
     */
    void deleteSafeToDeleteFiles() throws DatabaseException {
	try {
	    synchronized (deleteFileLock) {
		Set safeFiles = fileSelector.copySafeToDeleteFiles();
		if (safeFiles == null) {
		    return;
		}
		env.checkIfInvalid();
		if (env.mayNotWrite()) {
		    return;
		}
		//this.hook115(safeFiles);
		Label115:
for (Iterator i = safeFiles.iterator(); i.hasNext();) {
	    Long fileNum = (Long) i.next();
	    long fileNumValue = fileNum.longValue();
	    boolean deleted = false;
	    try {
		if (expunge) {
		    env.getFileManager().deleteFile(fileNumValue);
		} else {
		    env.getFileManager().renameFile(fileNumValue, FileManager.DEL_SUFFIX);
		}
		deleted = true;
	    } catch (DatabaseException e) {
		traceFileNotDeleted(e, fileNumValue);
	    } catch (IOException e) {
		traceFileNotDeleted(e, fileNumValue);
	    }
	    if (deleted) {
		this.hook88(fileNumValue);
		try {
		    profile.removeFile(fileNum);
		} finally {
		    fileSelector.removeDeletedFile(fileNum);
		}
	    }
	    Label96:
	}
//
Label_115_1:
	    }
	} catch (ReturnVoid r) {
	    return;
	}
    }

    private void traceFileNotDeleted(Exception e, long fileNum) {
    }

    /** 
     * Returns a copy of the cleaned and processed files at the time a checkpoint starts. <p> If non-null is returned, the checkpoint should flush an extra level, and addCheckpointedFiles() should be called when the checkpoint is complete. </p>
     */
    public Set[] getFilesAtCheckpointStart() throws DatabaseException {
	processPending();
	return fileSelector.getFilesAtCheckpointStart();
    }

    /** 
     * When a checkpoint is complete, update the files that were returned at the beginning of the checkpoint.
     */
    public void updateFilesAtCheckpointEnd(Set[] files) throws DatabaseException {
	fileSelector.updateFilesAtCheckpointEnd(files);
	deleteSafeToDeleteFiles();
    }

    /** 
     * Update the lowUtilizationFiles and mustBeCleanedFiles fields with new read-only collections, and update the backlog file count.
     */
    public void updateReadOnlyFileCollections() {
	mustBeCleanedFiles = fileSelector.getMustBeCleanedFiles();
	lowUtilizationFiles = fileSelector.getLowUtilizationFiles();
    }

    /** 
     * If any LNs are pending, process them. This method should be called often enough to prevent the pending LN set from growing too large.
     */
    void processPending() throws DatabaseException {
	new Cleaner_processPending(this).execute();
    }

    /** 
     * Processes a pending LN, getting the lock first to ensure that the overhead of retries is mimimal.
     */
    private void processPendingLN(LN ln, DatabaseImpl db, byte[] key, byte[] dupKey, TreeLocation location)
	    throws DatabaseException {
	boolean parentFound = false;
	boolean processedHere = true;
	boolean lockDenied = false;
	boolean obsolete = false;
	boolean completed = false;
	BasicLocker locker = null;
	BIN bin = null;
	DIN parentDIN = null;
	try {
	    Label97: //this.hook97();
	    boolean c = db == null;
	    //c = this.hook112(db, c);
      Label112:
	    if (c) {
		//this.hook113(db);
    Label113:
		Label98: //this.hook98();
		obsolete = true;
		completed = true;
		return;
	    }
	    Tree tree = db.getTree();
	    assert tree != null;
	    locker = new BasicLocker(env);
	    LockResult lockRet = locker.nonBlockingLock(ln.getNodeId(), LockType.READ, db);
	    if (lockRet.getLockGrant() == LockGrantType.DENIED) {
		Label99: //this.hook99();
		lockDenied = true;
		completed = true;
		return;
	    }
	    parentFound = tree.getParentBINForChildLN(location, key, dupKey, ln, false, true, true, UPDATE_GENERATION);
	    bin = location.bin;
	    int index = location.index;
	    if (!parentFound) {
		Label100: //this.hook100();
		obsolete = true;
		completed = true;
		return;
	    }
	    if (ln.containsDuplicates()) {
		parentDIN = (DIN) bin.fetchTarget(index);
		parentDIN.latch(UPDATE_GENERATION);
		ChildReference dclRef = parentDIN.getDupCountLNRef();
		processedHere = false;
		migrateDupCountLN(db, dclRef.getLsn(), parentDIN, dclRef, true, true, ln.getNodeId(), CLEAN_PENDING_LN);
	    } else {
		processedHere = false;
		migrateLN(db, bin.getLsn(index), bin, index, true, true, ln.getNodeId(), CLEAN_PENDING_LN);
	    }
	    completed = true;
	} catch (DatabaseException DBE) {
	    DBE.printStackTrace();
	    this.hook89(DBE);
	    throw DBE;
	} finally {
	    Label95: //this.hook95(bin, parentDIN);
	    if (locker != null) {
		locker.operationEnd();
	    }
	    if (processedHere) {
		if (completed && !lockDenied) {
		    fileSelector.removePendingLN(ln.getNodeId());
		}
		Label91: //this.hook91(ln, obsolete, completed);
	    }
	}
    }

    /** 
     * This method should be called just before logging a BIN. LNs will be migrated if the MIGRATE flag is set, or if they are in a file to be cleaned, or if the LNs qualify according to the rules for cluster and clusterAll. <p> On return this method guarantees that no MIGRATE flag will be set on any child entry. If this method is *not* called before logging a BIN, then the addPendingLNs method must be called. </p>
     * @param binis the latched BIN. The latch will not be released by thismethod.
     * @param proactiveMigrationperform proactive migration if needed; this is false during asplit, to reduce the delay in the user operation.
     */
    public void lazyMigrateLNs(final BIN bin, boolean proactiveMigration) throws DatabaseException {
	DatabaseImpl db = bin.getDatabase();
	boolean isBinInDupDb = db.getSortedDuplicates() && !bin.containsDuplicates();
	Integer[] sortedIndices = null;
	int nSortedIndices = 0;
	int nEntries = bin.getNEntries();
	for (int index = 0; index < nEntries; index += 1) {
	    boolean migrateFlag = bin.getMigrate(index);
	    boolean isResident = (bin.getTarget(index) != null);
	    long childLsn = bin.getLsn(index);
	    if (shouldMigrateLN(migrateFlag, isResident, proactiveMigration, isBinInDupDb, childLsn)) {
		if (isResident) {
		    migrateLN(db, childLsn, bin, index, migrateFlag, false, 0, CLEAN_MIGRATE_LN);
		} else {
		    if (sortedIndices == null) {
			sortedIndices = new Integer[nEntries];
		    }
		    sortedIndices[nSortedIndices++] = new Integer(index);
		}
	    }
	}
	if (sortedIndices != null) {
	    Arrays.sort(sortedIndices, 0, nSortedIndices, new Comparator() {
		public int compare(Object o1, Object o2) {
		    int i1 = ((Integer) o1).intValue();
		    int i2 = ((Integer) o2).intValue();
		    return DbLsn.compareTo(bin.getLsn(i1), bin.getLsn(i2));
		}
	    });
	    for (int i = 0; i < nSortedIndices; i += 1) {
		int index = sortedIndices[i].intValue();
		long childLsn = bin.getLsn(index);
		boolean migrateFlag = bin.getMigrate(index);
		migrateLN(db, childLsn, bin, index, migrateFlag, false, 0, CLEAN_MIGRATE_LN);
	    }
	}
    }

    /** 
     * This method should be called just before logging a root DIN. The DupCountLN will be migrated if the MIGRATE flag is set, or if it is in a file to be cleaned, or if the LN qualifies according to the rules for cluster and clusterAll. <p> On return this method guarantees that the MIGRATE flag will not be set on the child entry. If this method is *not* called before logging a root DIN, then the addPendingDupCountLN method must be called. </p>
     * @param dinis the latched DIN. The latch will not be released by thismethod.
     * @param dclRefis the reference to the DupCountLN.
     * @param proactiveMigrationperform proactive migration if needed; this is false during asplit, to reduce the delay in the user operation.
     */
    public void lazyMigrateDupCountLN(DIN din, ChildReference dclRef, boolean proactiveMigration)
	    throws DatabaseException {
	DatabaseImpl db = din.getDatabase();
	boolean migrateFlag = dclRef.getMigrate();
	boolean isResident = (dclRef.getTarget() != null);
	boolean isBinInDupDb = false;
	long childLsn = dclRef.getLsn();
	if (shouldMigrateLN(migrateFlag, isResident, proactiveMigration, isBinInDupDb, childLsn)) {
	    migrateDupCountLN(db, childLsn, din, dclRef, migrateFlag, false, 0, CLEAN_MIGRATE_LN);
	}
    }

    /** 
     * Returns whether an LN entry should be migrated. Updates stats.
     * @param migrateFlagis whether the MIGRATE flag is set on the entry.
     * @param isResidentis whether the LN is currently resident.
     * @param proactiveMigrationperform proactive migration if needed; this is false during asplit, to reduce the delay in the user operation.
     * @param isBinInDupDbis whether this is a BIN entry in a database with duplicatesenabled.
     * @param childLsnis the LSN of the LN.
     * @return whether to migrate the LN.
     */
    private boolean shouldMigrateLN(boolean migrateFlag, boolean isResident, boolean proactiveMigration,
	    boolean isBinInDupDb, long childLsn) {
	boolean doMigration = false;
	if (migrateFlag) {
	    doMigration = true;
	    Label101: //this.hook101();
	} else if (!proactiveMigration || isBinInDupDb || env.isClosing()) {
	} else {
	    Long fileNum = new Long(DbLsn.getFileNumber(childLsn));
	    if ((PROACTIVE_MIGRATION || isResident) && mustBeCleanedFiles.contains(fileNum)) {
		doMigration = true;
		Label102: //this.hook102();
	    } else if ((clusterAll || (clusterResident && isResident)) && lowUtilizationFiles.contains(fileNum)) {
		doMigration = true;
		Label103: //this.hook103();
	    }
	}
	return doMigration;
    }

    /** 
     * Migrate an LN in the given BIN entry, if it is not obsolete. The BIN is latched on entry to this method and is left latched when it returns.
     */
    private void migrateLN(DatabaseImpl db, long lsn, BIN bin, int index, boolean wasCleaned, boolean isPending,
	    long lockedPendingNodeId, String cleanAction) throws DatabaseException {
			boolean obsolete = false;
			boolean migrated = false;
			boolean lockDenied = false;
			boolean completed = false;
			boolean clearTarget = false;
			BasicLocker locker = null;
			LN ln = null;
			try {
					if (!bin.isEntryKnownDeleted(index)) {
				ln = (LN) bin.getTarget(index);
				if (ln == null) {
						ln = (LN) bin.fetchTarget(index);
						clearTarget = !db.getId().equals(DbTree.ID_DB_ID);
				}
					}
					if (ln == null) {
				Label105: //this.hook105(wasCleaned);
				obsolete = true;
				completed = true;
				return;
					}
					if (lockedPendingNodeId != ln.getNodeId()) {
				locker = new BasicLocker(env);
				LockResult lockRet = locker.nonBlockingLock(ln.getNodeId(), LockType.READ, db);
				if (lockRet.getLockGrant() == LockGrantType.DENIED) {
						Label106: //this.hook106(wasCleaned);
						lockDenied = true;
						completed = true;
						return;
				}
					}
					if (ln.isDeleted()) {
				bin.setKnownDeletedLeaveTarget(index);
				Label107: //this.hook107(wasCleaned);
				obsolete = true;
				completed = true;
				return;
					}
					if (bin.getMigrate(index)) {
				Long fileNum = new Long(DbLsn.getFileNumber(lsn));
				if (!fileSelector.isFileCleaningInProgress(fileNum)) {
						obsolete = true;
						completed = true;
						Label108: //this.hook108(wasCleaned);
						return;
				}
					}
					byte[] key = getLNMainKey(bin, index);
					long newLNLsn = ln.log(env, db.getId(), key, lsn, locker);
					bin.updateEntry(index, newLNLsn);
					Label104: //this.hook104();
					migrated = true;
					completed = true;
					return;
			} finally {
					if (isPending) {
				if (completed && !lockDenied) {
						fileSelector.removePendingLN(lockedPendingNodeId);
				}
					} else {
				if (bin.getMigrate(index) && (!completed || lockDenied)) {
						byte[] key = getLNMainKey(bin, index);
						byte[] dupKey = getLNDupKey(bin, index, ln);
						fileSelector.addPendingLN(ln, db.getId(), key, dupKey);
						if (!areThreadsRunning()) {
					env.getUtilizationTracker().activateCleaner();
						}
						clearTarget = false;
				}
					}
					bin.setMigrate(index, false);
					if (clearTarget) {
				bin.updateEntry(index, null);
					}
					if (locker != null) {
				locker.operationEnd();
					}
					Label92: //this.hook92(lsn, cleanAction, obsolete, migrated, completed, ln);
			}
    }

    /** 
     * Migrate the DupCountLN for the given DIN. The DIN is latched on entry to this method and is left latched when it returns.
     */
    private void migrateDupCountLN(DatabaseImpl db, long lsn, DIN parentDIN, ChildReference dclRef, boolean wasCleaned,
	    boolean isPending, long lockedPendingNodeId, String cleanAction) throws DatabaseException {
			boolean obsolete = false;
			boolean migrated = false;
			boolean lockDenied = false;
			boolean completed = false;
			boolean clearTarget = false;
			BasicLocker locker = null;
			LN ln = null;
			try {
					ln = (LN) dclRef.getTarget();
					if (ln == null) {
				ln = (LN) dclRef.fetchTarget(db, parentDIN);
				assert ln != null;
				clearTarget = !db.getId().equals(DbTree.ID_DB_ID);
					}
					if (lockedPendingNodeId != ln.getNodeId()) {
				locker = new BasicLocker(env);
				LockResult lockRet = locker.nonBlockingLock(ln.getNodeId(), LockType.READ, db);
				if (lockRet.getLockGrant() == LockGrantType.DENIED) {
						Label110: //this.hook110(wasCleaned);
						lockDenied = true;
						completed = true;
						return;
				}
					}
					Long fileNum = new Long(DbLsn.getFileNumber(lsn));
					if (!fileSelector.isFileCleaningInProgress(fileNum)) {
				obsolete = true;
				completed = true;
				Label111: //this.hook111(wasCleaned);
				return;
					}
					byte[] key = parentDIN.getDupKey();
					long newLNLsn = ln.log(env, db.getId(), key, lsn, locker);
					parentDIN.updateDupCountLNRef(newLNLsn);
					Label109: //this.hook109();
					migrated = true;
					completed = true;
					return;
			} finally {
					if (isPending) {
				if (completed && !lockDenied) {
						fileSelector.removePendingLN(lockedPendingNodeId);
				}
					} else {
				if (dclRef.getMigrate() && (!completed || lockDenied)) {
						byte[] key = parentDIN.getDupKey();
						byte[] dupKey = null;
						fileSelector.addPendingLN(ln, db.getId(), key, dupKey);
						if (!areThreadsRunning()) {
					env.getUtilizationTracker().activateCleaner();
						}
						clearTarget = false;
				}
					}
					dclRef.setMigrate(false);
					if (clearTarget) {
				parentDIN.updateDupCountLN(null);
					}
					if (locker != null) {
				locker.operationEnd();
					}
					Label93: //this.hook93(lsn, cleanAction, obsolete, migrated, completed, ln);
			}
		}

    /** 
     * Returns the main key for a given BIN entry.
     */
    private byte[] getLNMainKey(BIN bin, int index) throws DatabaseException {
	if (bin.containsDuplicates()) {
	    return bin.getDupKey();
	} else {
	    return bin.getKey(index);
	}
    }

    /** 
     * Returns the duplicate key for a given BIN entry.
     */
    private byte[] getLNDupKey(BIN bin, int index, LN ln) throws DatabaseException {
	DatabaseImpl db = bin.getDatabase();
	if (!db.getSortedDuplicates() || ln.containsDuplicates()) {
	    return null;
	} else if (bin.containsDuplicates()) {
	    return bin.getKey(index);
	} else {
	    return ln.getData();
	}
    }

    protected void hook88(long fileNumValue) throws DatabaseException {
    }

    protected void hook89(DatabaseException DBE) throws DatabaseException {
    }

  /*  protected void hook90() throws DatabaseException {
    }

    protected void hook91(LN ln, boolean obsolete, boolean completed) throws DatabaseException {
    }

    protected void hook92(long lsn, String cleanAction, boolean obsolete, boolean migrated, boolean completed, LN ln)
	    throws DatabaseException {
    }

    protected void hook93(long lsn, String cleanAction, boolean obsolete, boolean migrated, boolean completed, LN ln)
	    throws DatabaseException {
    }
*/
   // protected void hook94(DbConfigManager cm) throws DatabaseException {
   // }
/*
    protected void hook95(BIN bin, DIN parentDIN) throws DatabaseException {
    }

    protected void hook96() throws DatabaseException {
    }

    protected void hook97() throws DatabaseException {
    }

    protected void hook98() throws DatabaseException {
    }

    protected void hook99() throws DatabaseException {
    }

    protected void hook100() throws DatabaseException {
    }

    protected void hook101() {
    }

    protected void hook102() {
    }

    protected void hook103() {
    }

    protected void hook104() throws DatabaseException {
    }

    protected void hook105(boolean wasCleaned) throws DatabaseException {
    }

    protected void hook106(boolean wasCleaned) throws DatabaseException {
    }

    protected void hook107(boolean wasCleaned) throws DatabaseException {
    }

    protected void hook108(boolean wasCleaned) throws DatabaseException {
    }

    protected void hook109() throws DatabaseException {
    }

    protected void hook110(boolean wasCleaned) throws DatabaseException {
    }

    protected void hook111(boolean wasCleaned) throws DatabaseException {
    }
*/
  //  protected boolean hook112(DatabaseImpl db, boolean c) throws DatabaseException {
	//return c;
  //  }

 //   protected void hook113(DatabaseImpl db) throws DatabaseException {
 //   }

//    protected void hook115(Set safeFiles) throws DatabaseException {
//
//	for (Iterator i = safeFiles.iterator(); i.hasNext();) {
//	    Long fileNum = (Long) i.next();
//	    long fileNumValue = fileNum.longValue();
//	    boolean deleted = false;
//	    try {
//		if (expunge) {
//		    env.getFileManager().deleteFile(fileNumValue);
//		} else {
//		    env.getFileManager().renameFile(fileNumValue, FileManager.DEL_SUFFIX);
//		}
//		deleted = true;
//	    } catch (DatabaseException e) {
//		traceFileNotDeleted(e, fileNumValue);
//	    } catch (IOException e) {
//		traceFileNotDeleted(e, fileNumValue);
//	    }
//	    if (deleted) {
//		this.hook88(fileNumValue);
//		try {
//		    profile.removeFile(fileNum);
//		} finally {
//		    fileSelector.removeDeletedFile(fileNum);
//		}
//	    }
//	    this.hook96();
//	}
//    }

}
\00afterwards.
     */
    private ObjectStreamClass getClassFormat(byte[] classID, DatabaseEntry data)
	    throws DatabaseException, ClassNotFoundException {
	BigInteger classIDObj = new BigInteger(classID);
	ObjectStreamClass classFormat = (ObjectStreamClass) formatMap.get(classIDObj);
	if (classFormat == null) {
	    byte[] keyBytes = new byte[classID.length + 1];
	    keyBytes[0] = REC_CLASS_FORMAT;
	    System.arraycopy(classID, 0, keyBytes, 1, classID.length);
	    DatabaseEntry key = new DatabaseEntry(keyBytes);
	    OperationStatus status = db.get(null, key, data, LockMode.DEFAULT);
	    if (status != OperationStatus.SUCCESS) {
		throw new ClassNotFoundException("Catalog class ID not found");
	    }
	    try {
		ObjectInputStream ois = new ObjectInputStream(
			new ByteArrayInputStream(data.getData(), data.getOffset(), data.getSize()));
		classFormat = (ObjectStreamClass) ois.readObject();
	    } catch (IOException e) {
		throw new RuntimeExceptionWrapper(e);
	    }
	    formatMap.put(classIDObj, classFormat);
	}
	return classFormat;
    }

    /** 
     * Get the ClassInfo for a given class name, adding it and its ObjectStreamClass to the database if they are not already present, and caching both of them using the class info and class format maps. When a class is first loaded from the database, the stored ObjectStreamClass is compared to the current ObjectStreamClass loaded by the Java class loader; if they are different, a new class ID is assigned for the current format.
     */
    private ClassInfo getClassInfo(ObjectStreamClass classFormat) throws DatabaseException, ClassNotFoundException {
	String className = classFormat.getName();
	ClassInfo classInfo = (ClassInfo) classMap.get(className);
	if (classInfo != null) {
	    return classInfo;
	} else {
	    char[] nameChars = className.toCharArray();
	    byte[] keyBytes = new byte[1 + UtfOps.getByteLength(nameChars)];
	    keyBytes[0] = REC_CLASS_INFO;
	    UtfOps.charsToBytes(nameChars, 0, keyBytes, 1, nameChars.length);
	    DatabaseEntry key = new DatabaseEntry(keyBytes);
	    DatabaseEntry data = new DatabaseEntry();
	    OperationStatus status = db.get(null, key, data, LockMode.DEFAULT);
	    if (status != OperationStatus.SUCCESS) {
		classInfo = putClassInfo(new ClassInfo(), className, key, classFormat);
	    } else {
		classInfo = new ClassInfo(data);
		DatabaseEntry formatData = new DatabaseEntry();
		ObjectStreamClass storedClassFormat = getClassFormat(classInfo.getClassID(), formatData);
		if (!areClassFormatsEqual(storedClassFormat, getBytes(formatData), classFormat)) {
		    classInfo = putClassInfo(classInfo, className, key, classFormat);
		}
		classInfo.setClassFormat(classFormat);
		classMap.put(className, classInfo);
	    }
	}
	return classInfo;
    }

    /** 
     * Assign a new class ID (increment the current ID record), write the ObjectStreamClass record for this new ID, and update the ClassInfo record with the new ID also. The ClassInfo passed as an argument is the one to be updated.
     */
    private ClassInfo putClassInfo(ClassInfo classInfo, String className, DatabaseEntry classKey,
	    ObjectStreamClass classFormat) throws DatabaseException, ClassNotFoundException {
	CursorConfig cursorConfig = null;
	if (cdbMode) {
	    cursorConfig = new CursorConfig();
	    DbCompat.setWriteCursor(cursorConfig, true);
	}
	Cursor cursor = null;
	try {
	    cursor = db.openCursor(null, cursorConfig);
	    DatabaseEntry key = new DatabaseEntry(LAST_CLASS_ID_KEY);
	    DatabaseEntry data = new DatabaseEntry();
	    OperationStatus status = cursor.getSearchKey(key, data, writeLockMode);
	    if (status != OperationStatus.SUCCESS) {
		throw new IllegalStateException("Class ID not initialized");
	    }
	    byte[] idBytes = getBytes(data);
	    idBytes = incrementID(idBytes);
	    data.setData(idBytes);
	    cursor.put(key, data);
	    byte[] keyBytes = new byte[1 + idBytes.length];
	    keyBytes[0] = REC_CLASS_FORMAT;
	    System.arraycopy(idBytes, 0, keyBytes, 1, idBytes.length);
	    key.setData(keyBytes);
	    ByteArrayOutputStream baos = new ByteArrayOutputStream();
	    ObjectOutputStream oos;
	    try {
		oos = new ObjectOutputStream(baos);
		oos.writeObject(classFormat);
	    } catch (IOException e) {
		throw new RuntimeExceptionWrapper(e);
	    }
	    data.setData(baos.toByteArray());
	    cursor.put(key, data);
	    classInfo.setClassID(idBytes);
	    classInfo.toDbt(data);
	    cursor.put(classKey, data);
	    classInfo.setClassFormat(classFormat);
	    classMap.put(className, classInfo);
	    formatMap.put(new BigInteger(idBytes), classFormat);
	    return classInfo;
	} finally {
	    if (cursor != null) {
		cursor.close();
	    }
	    hook_commitTransaction();
	}
    }

    private void hook_commitTransaction() throws DatabaseException {
    }

    private static byte[] incrementID(byte[] key) {
	BigInteger id = new BigInteger(key);
	id = id.add(BigInteger.valueOf(1));
	return id.toByteArray();
    }

    /** 
     * Return whether two class formats are equal. This determines whether a new class format is needed for an object being serialized. Formats must be identical in all respects, or a new format is needed.
     */
    private static boolean areClassFormatsEqual(ObjectStreamClass format1, byte[] format1Bytes,
	    ObjectStreamClass format2) {
	try {
	    if (format1Bytes == null) {
		format1Bytes = getObjectBytes(format1);
	    }
	    byte[] format2Bytes = getObjectBytes(format2);
	    return java.util.Arrays.equals(format2Bytes, format1Bytes);
	} catch (IOException e) {
	    return false;
	}
    }

    private static byte[] getBytes(DatabaseEntry dbt) {
	byte[] b = dbt.getData();
	if (b == null) {
	    return null;
	}
	if (dbt.getOffset() == 0 && b.length == dbt.getSize()) {
	    return b;
	}
	int len = dbt.getSize();
	if (len == 0) {
	    return ZERO_LENGTH_BYTE_ARRAY;
	} else {
	    byte[] t = new byte[len];
	    System.arraycopy(b, dbt.getOffset(), t, 0, t.length);
	    return t;
	}
    }

    private static byte[] getObjectBytes(Object o) throws IOException {
	ByteArrayOutputStream baos = new ByteArrayOutputStream();
	ObjectOutputStream oos = new ObjectOutputStream(baos);
	oos.writeObject(o);
	return baos.toByteArray();
    }

}
\00after the memory budget has been calculated.
     */
    public void resetPool(DbConfigManager configManager) throws DatabaseException {
	logBufferPool.reset(configManager);
    }

    /** 
     * Log this single object and force a write of the log files.
     * @param itemobject to be logged
     * @param fsyncRequiredif true, log files should also be fsynced.
     * @return LSN of the new log entry
     */
    public long logForceFlush(LoggableObject item, boolean fsyncRequired) throws DatabaseException {
	return log(item, false, true, fsyncRequired, false, DbLsn.NULL_LSN);
    }

    /** 
     * Log this single object and force a flip of the log files.
     * @param itemobject to be logged
     * @param fsyncRequiredif true, log files should also be fsynced.
     * @return LSN of the new log entry
     */
    public long logForceFlip(LoggableObject item) throws DatabaseException {
	return log(item, false, true, false, true, DbLsn.NULL_LSN);
    }

    /** 
     * Write a log entry.
     * @return LSN of the new log entry
     */
    public long log(LoggableObject item) throws DatabaseException {
	return log(item, false, false, false, false, DbLsn.NULL_LSN);
    }

    /** 
     * Write a log entry.
     * @return LSN of the new log entry
     */
    public long log(LoggableObject item, boolean isProvisional, long oldNodeLsn) throws DatabaseException {
	return log(item, isProvisional, false, false, false, oldNodeLsn);
    }

    /** 
     * Write a log entry.
     * @param itemis the item to be logged.
     * @param isProvisionaltrue if this entry should not be read during recovery.
     * @param flushRequiredif true, write the log to the file after adding the item. i.e.call java.nio.channel.FileChannel.write().
     * @param fsyncRequiredif true, fsync the last file after adding the item.
     * @param oldNodeLsnis the previous version of the node to be counted as obsolete,or null if the item is not a node or has no old LSN.
     * @return LSN of the new log entry
     */
    private long log(LoggableObject item, boolean isProvisional, boolean flushRequired, boolean fsyncRequired,
	    boolean forceNewLogFile, long oldNodeLsn) throws DatabaseException {
	if (readOnly) {
	    return DbLsn.NULL_LSN;
	}
	boolean marshallOutsideLatch = item.marshallOutsideWriteLatch();
	ByteBuffer marshalledBuffer = null;
	UtilizationTracker tracker = envImpl.getUtilizationTracker();
	LogResult logResult = null;
	try {
	    if (marshallOutsideLatch) {
		int itemSize = item.getLogSize();
		int entrySize = itemSize + HEADER_BYTES;
		marshalledBuffer = marshallIntoBuffer(item, itemSize, isProvisional, entrySize);
	    }
	    logResult = logItem(item, isProvisional, flushRequired, forceNewLogFile, oldNodeLsn, marshallOutsideLatch,
		    marshalledBuffer, tracker);
	} catch (BufferOverflowException e) {
	    throw new RunRecoveryException(envImpl, e);
	} catch (IOException e) {
	    throw new DatabaseException(Tracer.getStackTrace(e), e);
	}
	Label501: //this.hook501(fsyncRequired);
	Label499: //this.hook499(logResult);
	if (logResult.wakeupCleaner) {
	    tracker.activateCleaner();
	}
	return logResult.currentLsn;
    }

    abstract protected LogResult logItem(LoggableObject item, boolean isProvisional, boolean flushRequired,
	    boolean forceNewLogFile, long oldNodeLsn, boolean marshallOutsideLatch, ByteBuffer marshalledBuffer,
	    UtilizationTracker tracker) throws IOException, DatabaseException;

    /** 
     * Called within the log write critical section.
     */
    protected LogResult logInternal(LoggableObject item, boolean isProvisional, boolean flushRequired,
	    boolean forceNewLogFile, long oldNodeLsn, boolean marshallOutsideLatch, ByteBuffer marshalledBuffer,
	    UtilizationTracker tracker) throws IOException, DatabaseException {
			LogEntryType entryType = item.getLogType();
			if (oldNodeLsn != DbLsn.NULL_LSN) {
					tracker.countObsoleteNode(oldNodeLsn, entryType);
			}
			int entrySize;
			if (marshallOutsideLatch) {
					entrySize = marshalledBuffer.limit();
			} else {
					entrySize = item.getLogSize() + HEADER_BYTES;
			}
			if (forceNewLogFile) {
					fileManager.forceNewLogFile();
			}
			boolean flippedFile = fileManager.bumpLsn(entrySize);
			long currentLsn = DbLsn.NULL_LSN;
			boolean wakeupCleaner = false;
			boolean usedTemporaryBuffer = false;
			try {
					currentLsn = fileManager.getLastUsedLsn();
					wakeupCleaner = tracker.countNewLogEntry(currentLsn, entryType, entrySize);
					if (item.countAsObsoleteWhenLogged()) {
				tracker.countObsoleteNodeInexact(currentLsn, entryType);
					}
					if (!marshallOutsideLatch) {
				marshalledBuffer = marshallIntoBuffer(item, entrySize - HEADER_BYTES, isProvisional, entrySize);
					}
					if (entrySize != marshalledBuffer.limit()) {
				throw new DatabaseException(
					"Logged item entrySize= " + entrySize + " but marshalledSize=" + marshalledBuffer.limit()
						+ " type=" + entryType + " currentLsn=" + DbLsn.getNoFormatString(currentLsn));
					}
					LogBuffer useLogBuffer = logBufferPool.getWriteBuffer(entrySize, flippedFile);
					marshalledBuffer = addPrevOffsetAndChecksum(marshalledBuffer, fileManager.getPrevEntryOffset(), entrySize);
					Label503: //usedTemporaryBuffer = this.hook503(marshalledBuffer, entrySize, currentLsn, usedTemporaryBuffer,useLogBuffer);
					ByteBuffer useBuffer = useLogBuffer.getDataBuffer();
					if (useBuffer.capacity() - useBuffer.position() < entrySize) {
							fileManager.writeLogBuffer(new LogBuffer(marshalledBuffer, currentLsn));
							usedTemporaryBuffer = true;
							assert useBuffer.position() == 0;
							Label509: //this.hook509();
					} else {
							useBuffer.put(marshalledBuffer);
					}
					Label503_1:
				 //End hook503
			} catch (Exception e) {
					fileManager.restoreLastPosition();
					if (e instanceof DatabaseException) {
				throw (DatabaseException) e;
					} else if (e instanceof IOException) {
				throw (IOException) e;
					} else {
				throw new DatabaseException(e);
					}
			}
			if (!usedTemporaryBuffer) {
					logBufferPool.writeCompleted(currentLsn, flushRequired);
			}
			item.postLogWork(currentLsn);
			boolean wakeupCheckpointer = false;
			Label500: //wakeupCheckpointer = this.hook500(item, entrySize, wakeupCheckpointer);
			return new LogResult(currentLsn, wakeupCheckpointer, wakeupCleaner);
    }

    /** 
     * Serialize a loggable object into this buffer.
     */
    private ByteBuffer marshallIntoBuffer(LoggableObject item, int itemSize, boolean isProvisional, int entrySize)
	    throws DatabaseException {
	ByteBuffer destBuffer = ByteBuffer.allocate(entrySize);
	destBuffer.position(CHECKSUM_BYTES);
	writeHeader(destBuffer, item.getLogType(), itemSize, isProvisional);
	item.writeToLog(destBuffer);
	destBuffer.flip();
	return destBuffer;
    }

    private ByteBuffer addPrevOffsetAndChecksum(ByteBuffer destBuffer, long lastOffset, int entrySize) {
	Checksum checksum = Adler32.makeChecksum();
	destBuffer.position(HEADER_PREV_OFFSET);
	LogUtils.writeUnsignedInt(destBuffer, lastOffset);
	checksum.update(destBuffer.array(), CHECKSUM_BYTES, (entrySize - CHECKSUM_BYTES));
	destBuffer.position(0);
	LogUtils.writeUnsignedInt(destBuffer, checksum.getValue());
	destBuffer.position(0);
	return destBuffer;
    }

    /** 
     * Serialize a loggable object into this buffer. Return it ready for a copy.
     */
    ByteBuffer putIntoBuffer(LoggableObject item, int itemSize, long prevLogEntryOffset, boolean isProvisional,
	    int entrySize) throws DatabaseException {
	ByteBuffer destBuffer = marshallIntoBuffer(item, itemSize, isProvisional, entrySize);
	return addPrevOffsetAndChecksum(destBuffer, 0, entrySize);
    }

    /** 
     * Helper to write the common entry header.
     * @param destBufferdestination
     * @param itemobject being logged
     * @param itemSizeWe could ask the item for this, but are passing it as aparameter for efficiency, because it's already available
     */
    private void writeHeader(ByteBuffer destBuffer, LogEntryType itemType, int itemSize, boolean isProvisional) {
	byte typeNum = itemType.getTypeNum();
	destBuffer.put(typeNum);
	byte version = itemType.getVersion();
	if (isProvisional)
	    version = LogEntryType.setProvisional(version);
	destBuffer.put(version);
	destBuffer.position(HEADER_SIZE_OFFSET);
	LogUtils.writeInt(destBuffer, itemSize);
    }

    /** 
     * Instantiate all the objects in the log entry at this LSN.
     * @param lsnlocation of entry in log.
     * @return log entry that embodies all the objects in the log entry.
     */
    public LogEntry getLogEntry(long lsn) throws DatabaseException {
	envImpl.checkIfInvalid();
	LogSource logSource = getLogSource(lsn);
	return getLogEntryFromLogSource(lsn, logSource);
    }

    LogEntry getLogEntry(long lsn, RandomAccessFile file) throws DatabaseException {
	return getLogEntryFromLogSource(lsn, new FileSource(file, readBufferSize, fileManager));
    }

    /** 
     * Instantiate all the objects in the log entry at this LSN. This will release the log source at the first opportunity.
     * @param lsnlocation of entry in log
     * @return log entry that embodies all the objects in the log entry
     */
    private LogEntry getLogEntryFromLogSource(long lsn, LogSource logSource) throws DatabaseException {
	return new LogManager_getLogEntryFromLogSource(this, lsn, logSource).execute();
    }

    /** 
     * Fault in the first object in the log entry log entry at this LSN.
     * @param lsnlocation of object in log
     * @return the object in the log
     */
    public Object get(long lsn) throws DatabaseException {
	LogEntry entry = getLogEntry(lsn);
	return entry.getMainItem();
    }

    /** 
     * Find the LSN, whether in a file or still in the log buffers.
     */
    private LogSource getLogSource(long lsn) throws DatabaseException {
	LogBuffer logBuffer = logBufferPool.getReadBuffer(lsn);
	if (logBuffer == null) {
	    try {
		return new FileHandleSource(fileManager.getFileHandle(DbLsn.getFileNumber(lsn)), readBufferSize,
			fileManager);
	    } catch (LogFileNotFoundException e) {
		throw new LogFileNotFoundException(DbLsn.getNoFormatString(lsn) + ' ' + e.getMessage());
	    }
	} else {
	    return logBuffer;
	}
    }

    /** 
     * Flush all log entries, fsync the log file.
     */
    public void flush() throws DatabaseException {
	if (readOnly) {
	    return;
	}
	flushInternal();
	fileManager.syncLogEnd();
    }

    abstract protected void flushInternal() throws LogException, DatabaseException;

    /** 
     * Returns a tracked summary for the given file which will not be flushed. Used for watching changes that occur while a file is being cleaned.
     */
    abstract public TrackedFileSummary getUnflushableTrackedSummary(long file) throws DatabaseException;

    protected TrackedFileSummary getUnflushableTrackedSummaryInternal(long file) throws DatabaseException {
	return envImpl.getUtilizationTracker().getUnflushableTrackedSummary(file);
    }

    /** 
     * Count node as obsolete under the log write latch. This is done here because the log write latch is managed here, and all utilization counting must be performed under the log write latch.
     */
    abstract public void countObsoleteNode(long lsn, LogEntryType type) throws DatabaseException;

    protected void countObsoleteNodeInternal(UtilizationTracker tracker, long lsn, LogEntryType type)
	    throws DatabaseException {
	tracker.countObsoleteNode(lsn, type);
    }

    /** 
     * Counts file summary info under the log write latch.
     */
    abstract public void countObsoleteNodes(TrackedFileSummary[] summaries) throws DatabaseException;

    protected void countObsoleteNodesInternal(UtilizationTracker tracker, TrackedFileSummary[] summaries)
	    throws DatabaseException {
	for (int i = 0; i < summaries.length; i += 1) {
	    TrackedFileSummary summary = summaries[i];
	    tracker.addSummary(summary.getFileNumber(), summary);
	}
    }

    /** 
     * Counts the given obsolete IN LSNs under the log write latch.
     */
    abstract public void countObsoleteINs(List lsnList) throws DatabaseException;

    protected void countObsoleteINsInternal(List lsnList) throws DatabaseException {
	UtilizationTracker tracker = envImpl.getUtilizationTracker();
	for (int i = 0; i < lsnList.size(); i += 1) {
	    Long offset = (Long) lsnList.get(i);
	    tracker.countObsoleteNode(offset.longValue(), LogEntryType.LOG_IN);
	}
    }

    public void setReadHook(TestHook hook) {
	readHook = hook;
    }

  //  protected void hook498(EnvironmentImpl envImpl) throws DatabaseException { }

//    protected void hook499(LogResult logResult) throws DatabaseException { }
    
//protected boolean hook500(LoggableObject item, int entrySize, boolean wakeupCheckpointer)
//	    throws IOException, DatabaseException {	return wakeupCheckpointer; }

//    protected void hook501(boolean fsyncRequired) throws DatabaseException { }

    protected void hook502(EnvironmentImpl envImpl) throws DatabaseException {
    }

//    protected boolean hook503(ByteBuffer marshalledBuffer, int entrySize, long currentLsn, boolean usedTemporaryBuffer,
//	    LogBuffer useLogBuffer) throws IOException, DatabaseException, Exception {
//	ByteBuffer useBuffer = useLogBuffer.getDataBuffer();
//	if (useBuffer.capacity() - useBuffer.position() < entrySize) {
//	    fileManager.writeLogBuffer(new LogBuffer(marshalledBuffer, currentLsn));
//	    usedTemporaryBuffer = true;
//	    assert useBuffer.position() == 0;
//	    Label509: //this.hook509();
//	} else {
//	    useBuffer.put(marshalledBuffer);
//	}
//	return usedTemporaryBuffer;
//    }

//    protected static int hook504(int r) {
//	return r;
 //   }

    protected void hook505(DbConfigManager configManager) throws DatabaseException {
    }

//    protected void hook509() throws IOException, DatabaseException, Exception {
//    }

}
\00after the change, or null if the recordhas been deleted.
     */
    void updateSecondary(Locker locker, Cursor cursor, DatabaseEntry priKey, DatabaseEntry oldData,
	    DatabaseEntry newData) throws DatabaseException {
	if (secondaryConfig.getImmutableSecondaryKey() && oldData != null && newData != null) {
	    return;
	}
	SecondaryKeyCreator keyCreator = secondaryConfig.getKeyCreator();
	if (keyCreator != null) {
	    assert secondaryConfig.getMultiKeyCreator() == null;
	    DatabaseEntry oldSecKey = null;
	    if (oldData != null) {
		oldSecKey = new DatabaseEntry();
		if (!keyCreator.createSecondaryKey(this, priKey, oldData, oldSecKey)) {
		    oldSecKey = null;
		}
	    }
	    DatabaseEntry newSecKey = null;
	    if (newData != null) {
		newSecKey = new DatabaseEntry();
		if (!keyCreator.createSecondaryKey(this, priKey, newData, newSecKey)) {
		    newSecKey = null;
		}
	    }
	    if ((oldSecKey != null && !oldSecKey.equals(newSecKey))
		    || (newSecKey != null && !newSecKey.equals(oldSecKey))) {
		boolean localCursor = (cursor == null);
		if (localCursor) {
		    cursor = new Cursor(this, locker, null);
		}
		try {
		    if (oldSecKey != null) {
			deleteKey(cursor, priKey, oldSecKey);
		    }
		    if (newSecKey != null) {
			insertKey(locker, cursor, priKey, newSecKey);
		    }
		} finally {
		    if (localCursor && cursor != null) {
			cursor.close();
		    }
		}
	    }
	} else {
	    SecondaryMultiKeyCreator multiKeyCreator = secondaryConfig.getMultiKeyCreator();
	    assert multiKeyCreator != null;
	    Set oldKeys = Collections.EMPTY_SET;
	    Set newKeys = Collections.EMPTY_SET;
	    if (oldData != null) {
		oldKeys = new HashSet();
		multiKeyCreator.createSecondaryKeys(this, priKey, oldData, oldKeys);
	    }
	    if (newData != null) {
		newKeys = new HashSet();
		multiKeyCreator.createSecondaryKeys(this, priKey, newData, newKeys);
	    }
	    if (!oldKeys.equals(newKeys)) {
		boolean localCursor = (cursor == null);
		if (localCursor) {
		    cursor = new Cursor(this, locker, null);
		}
		try {
		    Set oldKeysCopy = oldKeys;
		    if (oldKeys != Collections.EMPTY_SET) {
			oldKeysCopy = new HashSet(oldKeys);
			oldKeys.removeAll(newKeys);
			for (Iterator i = oldKeys.iterator(); i.hasNext();) {
			    DatabaseEntry oldKey = (DatabaseEntry) i.next();
			    deleteKey(cursor, priKey, oldKey);
			}
		    }
		    if (newKeys != Collections.EMPTY_SET) {
			newKeys.removeAll(oldKeysCopy);
			for (Iterator i = newKeys.iterator(); i.hasNext();) {
			    DatabaseEntry newKey = (DatabaseEntry) i.next();
			    insertKey(locker, cursor, priKey, newKey);
			}
		    }
		} finally {
		    if (localCursor && cursor != null) {
			cursor.close();
		    }
		}
	    }
	}
    }

    /** 
     * Deletes an old secondary key.
     */
    private void deleteKey(Cursor cursor, DatabaseEntry priKey, DatabaseEntry oldSecKey) throws DatabaseException {
	OperationStatus status = cursor.search(oldSecKey, priKey, LockMode.RMW, SearchMode.BOTH);
	if (status == OperationStatus.SUCCESS) {
	    cursor.deleteInternal();
	} else {
	    throw new DatabaseException("Secondary " + getDebugName() + " is corrupt: the primary record contains a key"
		    + " that is not present in the secondary");
	}
    }

    /** 
     * Inserts a new secondary key.
     */
    private void insertKey(Locker locker, Cursor cursor, DatabaseEntry priKey, DatabaseEntry newSecKey)
	    throws DatabaseException {
	Database foreignDb = secondaryConfig.getForeignKeyDatabase();
	if (foreignDb != null) {
	    Cursor foreignCursor = null;
	    try {
		foreignCursor = new Cursor(foreignDb, locker, null);
		DatabaseEntry tmpData = new DatabaseEntry();
		OperationStatus status = foreignCursor.search(newSecKey, tmpData, LockMode.DEFAULT, SearchMode.SET);
		if (status != OperationStatus.SUCCESS) {
		    throw new DatabaseException("Secondary " + getDebugName() + " foreign key not allowed: it is not"
			    + " present in the foreign database");
		}
	    } finally {
		if (foreignCursor != null) {
		    foreignCursor.close();
		}
	    }
	}
	OperationStatus status;
	if (configuration.getSortedDuplicates()) {
	    status = cursor.putInternal(newSecKey, priKey, PutMode.NODUP);
	} else {
	    status = cursor.putInternal(newSecKey, priKey, PutMode.NOOVERWRITE);
	}
	if (status != OperationStatus.SUCCESS) {
	    throw new DatabaseException("Could not insert secondary key in " + getDebugName() + ' ' + status);
	}
    }

    /** 
     * Called by the ForeignKeyTrigger when a record in the foreign database is deleted.
     * @param secKey is the primary key of the foreign database, which is thesecondary key (ordinary key) of this secondary database.
     */
    void onForeignKeyDelete(Locker locker, DatabaseEntry secKey) throws DatabaseException {
	ForeignKeyDeleteAction deleteAction = secondaryConfig.getForeignKeyDeleteAction();
	LockMode lockMode = (deleteAction == ForeignKeyDeleteAction.ABORT) ? LockMode.DEFAULT : LockMode.RMW;
	DatabaseEntry priKey = new DatabaseEntry();
	Cursor cursor = null;
	OperationStatus status;
	try {
	    cursor = new Cursor(this, locker, null);
	    status = cursor.search(secKey, priKey, lockMode, SearchMode.SET);
	    while (status == OperationStatus.SUCCESS) {
		if (deleteAction == ForeignKeyDeleteAction.ABORT) {
		    throw new DatabaseException("Secondary " + getDebugName()
			    + " refers to a foreign key that has been deleted" + " (ForeignKeyDeleteAction.ABORT)");
		} else if (deleteAction == ForeignKeyDeleteAction.CASCADE) {
		    Cursor priCursor = null;
		    try {
			DatabaseEntry data = new DatabaseEntry();
			priCursor = new Cursor(primaryDb, locker, null);
			status = priCursor.search(priKey, data, LockMode.RMW, SearchMode.SET);
			if (status == OperationStatus.SUCCESS) {
			    priCursor.delete();
			} else {
			    throw secondaryCorruptException();
			}
		    } finally {
			if (priCursor != null) {
			    priCursor.close();
			}
		    }
		} else if (deleteAction == ForeignKeyDeleteAction.NULLIFY) {
		    Cursor priCursor = null;
		    try {
			DatabaseEntry data = new DatabaseEntry();
			priCursor = new Cursor(primaryDb, locker, null);
			status = priCursor.search(priKey, data, LockMode.RMW, SearchMode.SET);
			if (status == OperationStatus.SUCCESS) {
			    ForeignMultiKeyNullifier multiNullifier = secondaryConfig.getForeignMultiKeyNullifier();
			    if (multiNullifier != null) {
				if (multiNullifier.nullifyForeignKey(this, priKey, data, secKey)) {
				    priCursor.putCurrent(data);
				}
			    } else {
				ForeignKeyNullifier nullifier = secondaryConfig.getForeignKeyNullifier();
				if (nullifier.nullifyForeignKey(this, data)) {
				    priCursor.putCurrent(data);
				}
			    }
			} else {
			    throw secondaryCorruptException();
			}
		    } finally {
			if (priCursor != null) {
			    priCursor.close();
			}
		    }
		} else {
		    throw new IllegalStateException();
		}
		status = cursor.retrieveNext(secKey, priKey, LockMode.DEFAULT, GetMode.NEXT_DUP);
	    }
	} finally {
	    if (cursor != null) {
		cursor.close();
	    }
	}
    }

    DatabaseException secondaryCorruptException() throws DatabaseException {
	throw new DatabaseException(
		"Secondary " + getDebugName() + " is corrupt: it refers" + " to a missing key in the primary database");
    }

    static UnsupportedOperationException notAllowedException() {
	throw new UnsupportedOperationException("Operation not allowed on a secondary");
    }

  //  protected void hook79(Transaction txn, DatabaseEntry key) throws DatabaseException {    }

//    protected void hook80(Transaction txn, DatabaseEntry key, LockMode lockMode) throws DatabaseException {    }

//    protected void hook81(Transaction txn, DatabaseEntry key, DatabaseEntry data, LockMode lockMode)	    throws DatabaseException {    }

}
\00after the FileSummaryLN is written to the log during checkpoint. <p> We keep the active file summary in the tracked file list, but we remove older files to prevent unbounded growth of the list. </p> <p> Must be called under the log write latch. </p>
     */
    void resetFile(TrackedFileSummary file) {
	if (file.getFileNumber() < activeFile && file.getAllowFlush()) {
	    files.remove(file);
	    takeSnapshot();
	}
    }

    /** 
     * Takes a snapshot of the tracked file list. <p> Must be called under the log write latch. </p>
     */
    private void takeSnapshot() {
	TrackedFileSummary[] a = new TrackedFileSummary[files.size()];
	files.toArray(a);
	snapshot = a;
    }

    /** 
     * Returns whether an object reference is in an array.
     */
    private boolean inArray(Object o, Object[] a) {
	for (int i = 0; i < a.length; i += 1) {
	    if (a[i] == o) {
		return true;
	    }
	}
	return false;
    }

}
\00after successful cleaning.\n"
		    + "# If false, the cleaner changes log file extensions to .DEL\n"
		    + "# instead of deleting them. The default is set to true.");

    public static final IntConfigParam CLEANER_MIN_FILES_TO_DELETE = new IntConfigParam("je.cleaner.minFilesToDelete",
	    new Integer(1), new Integer(1000000), new Integer(5), false, "# (deprecated, no longer used");

    public static final IntConfigParam CLEANER_RETRIES = new IntConfigParam("je.cleaner.retries", new Integer(0),
	    new Integer(1000), new Integer(10), false, "# (deprecated, no longer used");

    public static final IntConfigParam CLEANER_RESTART_RETRIES = new IntConfigParam("je.cleaner.restartRetries",
	    new Integer(0), new Integer(1000), new Integer(5), false, "# (deprecated, no longer used");

    public static final IntConfigParam CLEANER_MIN_AGE = new IntConfigParam("je.cleaner.minAge", new Integer(1),
	    new Integer(1000), new Integer(2), true,
	    "# The minimum age of a file (number of files between it and the\n"
		    + "# active file) to qualify it for cleaning under any conditions.\n"
		    + "# The default is set to 2.");

    public static final BooleanConfigParam CLEANER_CLUSTER = new BooleanConfigParam("je.cleaner.cluster", false, true,
	    "# *** Experimental and may be removed in a future release. ***\n"
		    + "# If true, eviction and checkpointing will cluster records by key\n"
		    + "# value, migrating them from low utilization files if they are\n" + "# resident.\n"
		    + "# The cluster and clusterAll properties may not both be set to true.");

    public static final BooleanConfigParam CLEANER_CLUSTER_ALL = new BooleanConfigParam("je.cleaner.clusterAll", false,
	    true,
	    "# *** Experimental and may be removed in a future release. ***\n"
		    + "# If true, eviction and checkpointing will cluster records by key\n"
		    + "# value, migrating them from low utilization files whether or not\n" + "# they are resident.\n"
		    + "# The cluster and clusterAll properties may not both be set to true.");

    public static final IntConfigParam CLEANER_MAX_BATCH_FILES = new IntConfigParam("je.cleaner.maxBatchFiles",
	    new Integer(0), new Integer(100000), new Integer(0), true,
	    "# The maximum number of log files in the cleaner's backlog, or\n"
		    + "# zero if there is no limit.  Changing this property can impact the\n"
		    + "# performance of some out-of-memory applications.");

    public static final IntConfigParam CLEANER_READ_SIZE = new IntConfigParam("je.cleaner.readSize", new Integer(128),
	    null, new Integer(0), true, "# The read buffer size for cleaning.  If zero (the default), then\n"
		    + "# je.log.iteratorReadSize value is used.");

    public static final BooleanConfigParam CLEANER_TRACK_DETAIL = new BooleanConfigParam("je.cleaner.trackDetail", true,
	    false, "# If true, the cleaner tracks and stores detailed information that\n"
		    + "# is used to decrease the cost of cleaning.");

    public static final IntConfigParam CLEANER_DETAIL_MAX_MEMORY_PERCENTAGE = new IntConfigParam(
	    "je.cleaner.detailMaxMemoryPercentage", new Integer(1), new Integer(90), new Integer(2), true,
	    "# Tracking of detailed cleaning information will use no more than\n"
		    + "# this percentage of the cache.  The default value is two percent.\n"
		    + "# This setting is only used if je.cleaner.trackDetail=true.");

    public static final BooleanConfigParam CLEANER_RMW_FIX = new BooleanConfigParam("je.cleaner.rmwFix", true, false,
	    "# If true, detail information is discarded that was added by earlier\n"
		    + "# versions of JE if it may be invalid.  This may be set to false\n"
		    + "# for increased performance, but only if LockMode.RMW was never used.");

    public static final ConfigParam CLEANER_FORCE_CLEAN_FILES = new ConfigParam("je.cleaner.forceCleanFiles", "", false,
	    "# Specifies a list of files or file ranges to force clean.  This is\n"
		    + "# intended for use in forcing the cleaning of a large number of log\n"
		    + "# files.  File numbers are in hex and are comma separated or hyphen\n"
		    + "# separated to specify ranges, e.g.: '9,a,b-d' will clean 5 files.");

    public static final IntConfigParam CLEANER_THREADS = new IntConfigParam("je.cleaner.threads", new Integer(1), null,
	    new Integer(1), true,
	    "# The number of threads allocated by the cleaner for log file\n"
		    + "# processing.  If the cleaner backlog becomes large, increase this\n"
		    + "# value.  The default is set to 1.");

    public static final IntConfigParam N_LOCK_TABLES = new IntConfigParam("je.lock.nLockTables", new Integer(1),
	    new Integer(32767), new Integer(1), false,
	    "# Number of Lock Tables.  Set this to a value other than 1 when\n"
		    + "# an application has multiple threads performing concurrent JE\n"
		    + "# operations.  It should be set to a prime number, and in general\n"
		    + "# not higher than the number of application threads performing JE\n" + "# operations.");

    public static final LongConfigParam LOCK_TIMEOUT = new LongConfigParam("je.lock.timeout", new Long(0),
	    new Long(4294967296L), new Long(500000L), false, "# The lock timeout in microseconds.");

    public static final LongConfigParam TXN_TIMEOUT = new LongConfigParam("je.txn.timeout", new Long(0),
	    new Long(4294967296L), new Long(0), false,
	    "# The transaction timeout, in microseconds. A value of 0 means no limit.");

    public static final BooleanConfigParam TXN_SERIALIZABLE_ISOLATION = new BooleanConfigParam(
	    "je.txn.serializableIsolation", false, false,
	    "# Transactions have the Serializable (Degree 3) isolation level.  The\n"
		    + "# default is false, which implies the Repeatable Read isolation level.");

    public static final BooleanConfigParam TXN_DEADLOCK_STACK_TRACE = new BooleanConfigParam(
	    "je.txn.deadlockStackTrace", false, true,
	    "# Set this parameter to true to add stacktrace information to deadlock\n"
		    + "# (lock timeout) exception messages.  The stack trace will show where\n"
		    + "# each lock was taken.  The default is false, and true should only be\n"
		    + "# used during debugging because of the added memory/processing cost.\n"
		    + "# This parameter is 'static' across all environments.");

    public static final BooleanConfigParam TXN_DUMPLOCKS = new BooleanConfigParam("je.txn.dumpLocks", false, true,
	    "# Dump the lock table when a lock timeout is encountered, for\n" + "# debugging assistance.");

    public static void main(String argv[]) {
	if (argv.length != 1) {
	    throw new IllegalArgumentException("Usage: EnvironmentParams " + "<samplePropertyFile>");
	}
	try {
	    FileWriter exampleFile = new FileWriter(new File(argv[0]));
	    TreeSet paramNames = new TreeSet(SUPPORTED_PARAMS.keySet());
	    Iterator iter = paramNames.iterator();
	    exampleFile.write("####################################################\n"
		    + "# Example Berkeley DB, Java Edition property file\n"
		    + "# Each parameter is set to its default value\n"
		    + "####################################################\n\n");
	    while (iter.hasNext()) {
		String paramName = (String) iter.next();
		ConfigParam param = (ConfigParam) SUPPORTED_PARAMS.get(paramName);
		exampleFile.write(param.getDescription() + "\n");
		String extraDesc = param.getExtraDescription();
		if (extraDesc != null) {
		    exampleFile.write(extraDesc + "\n");
		}
		exampleFile.write("#" + param.getName() + "=" + param.getDefault() + "\n# (mutable at run time: "
			+ param.isMutable() + ")\n\n");
	    }
	    exampleFile.close();
	} catch (Exception e) {
	    e.printStackTrace();
	    System.exit(-1);
	}
    }

    static void addSupportedParam(ConfigParam param) {
	SUPPORTED_PARAMS.put(param.getName(), param);
    }

}
\00after currentFileNum.
     * @param currentFileNumthe file we're at right now. Note that it may not exist, ifit's been cleaned and renamed.
     * @param forwardif true, we want the next larger file, if false we want theprevious file
     * @return null if there is no following file, or if filenum doesn't exist
     */
    public Long getFollowingFileNum(long currentFileNum, boolean forward) {
        String[] names = listFiles(JE_SUFFIXES);
        String searchName = getFileName(currentFileNum, JE_SUFFIX);
        int foundIdx = Arrays.binarySearch(names, searchName);
        boolean foundTarget = false;
        if (foundIdx >= 0) {
            if (forward) {
                foundIdx++;
            } else {
                foundIdx--;
            }
        } else {
            foundIdx = Math.abs(foundIdx + 1);
            if (!forward) {
                foundIdx--;
            }
        }
        if (forward && (foundIdx < names.length)) {
            foundTarget = true;
        } else if (!forward && (foundIdx > -1)) {
            foundTarget = true;
        }
        if (foundTarget) {
            return getNumFromName(names[foundIdx]);
        } else {
            return null;
        }
    }

    /** 
     * @return true if there are any files at all.
     */
    public boolean filesExist() {
        String[] names = listFiles(JE_SUFFIXES);
        return (names.length != 0);
    }

    /** 
     * Get the first or last file number in the set of je files.
     * @param firstif true, get the first file, else get the last file
     * @return the file number or null if no files exist
     */
    private Long getFileNum(boolean first) {
        String[] names = listFiles(JE_SUFFIXES);
        if (names.length == 0) {
            return null;
        } else {
            int index = 0;
            if (!first) {
                index = names.length - 1;
            }
            return getNumFromName(names[index]);
        }
    }

    /** 
     * Get the file number from a file name.
     * @param thefile name
     * @return the file number
     */
    private Long getNumFromName(String fileName) {
        String fileNumber = fileName.substring(0, fileName.indexOf("."));
        return new Long(Long.parseLong(fileNumber, 16));
    }

    /** 
     * Find je files. Return names sorted in ascending fashion.
     * @param suffixwhich type of file we're looking for
     * @return array of file names
     */
    String[] listFiles(String[] suffixes) {
        String[] fileNames = dbEnvHome.list(new JEFileFilter(suffixes));
        Arrays.sort(fileNames);
        return fileNames;
    }

    /** 
     * Find je files, flavor for unit test support.
     * @param suffixwhich type of file we're looking for
     * @return array of file names
     */
    public static String[] listFiles(File envDirFile, String[] suffixes) {
        String[] fileNames = envDirFile.list(new JEFileFilter(suffixes));
        Arrays.sort(fileNames);
        return fileNames;
    }

    /** 
     * @return the full file name and path for the nth je file.
     */
    String[] getFullFileNames(long fileNum) {
        if (includeDeletedFiles) {
            int nSuffixes = JE_AND_DEL_SUFFIXES.length;
            String[] ret = new String[nSuffixes];
            for (int i = 0; i < nSuffixes; i++) {
                ret[i] = getFullFileName(getFileName(fileNum, JE_AND_DEL_SUFFIXES[i]));
            }
            return ret;
        } else {
            return new String[] {
                getFullFileName(getFileName(fileNum, JE_SUFFIX))
            };
        }
    }

    /** 
     * @return the full file name and path for the given file number and suffix.
     */
    public String getFullFileName(long fileNum, String suffix) {
        return getFullFileName(getFileName(fileNum, suffix));
    }

    /** 
     * @return the full file name and path for this file name.
     */
    private String getFullFileName(String fileName) {
        return dbEnvHome + File.separator + fileName;
    }

    /** 
     * @return the file name for the nth file.
     */
    public static String getFileName(long fileNum, String suffix) {
        return (HexFormatter.formatLong(fileNum).substring(10) + suffix);
    }

    /** 
     * Rename this file to NNNNNNNN.suffix. If that file already exists, try NNNNNNNN.suffix.1, etc. Used for deleting files or moving corrupt files aside.
     * @param fileNumthe file we want to move
     * @param newSuffixthe new file suffix
     */
    public void renameFile(long fileNum, String newSuffix) throws DatabaseException, IOException {
        int repeatNum = 0;
        boolean renamed = false;
        while (!renamed) {
            String generation = "";
            if (repeatNum > 0) {
                generation = "." + repeatNum;
            }
            String newName = getFullFileName(getFileName(fileNum, newSuffix) + generation);
            File targetFile = new File(newName);
            if (targetFile.exists()) {
                repeatNum++;
            } else {
                String oldFileName = getFullFileNames(fileNum)[0];
                Label458:
                    //this.hook458(fileNum);
                    File oldFile = new File(oldFileName);
                if (oldFile.renameTo(targetFile)) {
                    renamed = true;
                } else {
                    throw new LogException("Couldn't rename " + oldFileName + " to " + newName);
                }
            }
        }
    }

    /** 
     * Delete log file NNNNNNNN.
     * @param fileNumthe file we want to move
     */
    public void deleteFile(long fileNum) throws DatabaseException, IOException {
        String fileName = getFullFileNames(fileNum)[0];
        Label459:
            //this.hook459(fileNum);
            File file = new File(fileName);
        boolean done = file.delete();
        if (!done) {
            throw new LogException("Couldn't delete " + file);
        }
    }

    /** 
     * Return a read only file handle that corresponds the this file number. Retrieve it from the cache or open it anew and validate the file header. This method takes a latch on this file, so that the file descriptor will be held in the cache as long as it's in use. When the user is done with the file, the latch must be released.
     * @param fileNumwhich file
     * @return the file handle for the existing or newly created file
     */
    FileHandle getFileHandle(long fileNum) throws LogException, DatabaseException {
        try {
            Long fileId = new Long(fileNum);
            FileHandle fileHandle = null;
            // Start of hook460
            Label460:
                Label463:
                Label450:
                Label462:
                fileHandle = makeFileHandle(fileNum, FileMode.READ_MODE);
            Label464:
                Label453:
                if (fileHandle.getFile() == null) {
                    Label454: ;//
                }
            else {
                throw new ReturnObject(fileHandle);
            }
            Label460_1:
            // End of hook460
            throw ReturnHack.returnObject;
        } catch (ReturnObject r) {
            return (FileHandle) r.value;
        }
    }

    private FileHandle makeFileHandle(long fileNum, FileMode mode) throws DatabaseException {
        String[] fileNames = getFullFileNames(fileNum);
        RandomAccessFile newFile = null;
        String fileName = null;
        try {
            FileNotFoundException FNFE = null;
            for (int i = 0; i < fileNames.length; i++) {
                fileName = fileNames[i];
                try {
                    newFile = new RandomAccessFile(fileName, mode.getModeValue());
                    break;
                } catch (FileNotFoundException e) {
                    if (FNFE == null) {
                        FNFE = e;
                    }
                }
            }
            if (newFile == null) {
                throw FNFE;
            }
            boolean oldHeaderVersion = false;
            if (newFile.length() == 0) {
                if (mode == FileMode.READWRITE_MODE) {
                    long lastLsn = DbLsn.longToLsn((Long) perFileLastUsedLsn.remove(new Long(fileNum - 1)));
                    long headerPrevOffset = 0;
                    if (lastLsn != DbLsn.NULL_LSN) {
                        headerPrevOffset = DbLsn.getFileOffset(lastLsn);
                    }
                    FileHeader fileHeader = new FileHeader(fileNum, headerPrevOffset);
                    writeFileHeader(newFile, fileName, fileHeader);
                }
            } else {
                oldHeaderVersion = readAndValidateFileHeader(newFile, fileName, fileNum);
            }
            return new FileHandle(newFile, fileName, envImpl, oldHeaderVersion);
        } catch (FileNotFoundException e) {
            throw new LogFileNotFoundException("Couldn't open file " + fileName + ": " + e.getMessage());
        } catch (DbChecksumException e) {
            closeFileInErrorCase(newFile);
            throw new DbChecksumException(envImpl, "Couldn't open file " + fileName, e);
        } catch (Throwable t) {
            closeFileInErrorCase(newFile);
            throw new DatabaseException("Couldn't open file " + fileName + ": " + t, t);
        }
    }

    /** 
     * Close this file and eat any exceptions. Used in catch clauses.
     */
    private void closeFileInErrorCase(RandomAccessFile file) {
        try {
            if (file != null) {
                file.close();
            }
        } catch (IOException e) {}
    }

    /** 
     * Read the given je log file and validate the header.
     * @throws DatabaseExceptionif the file header isn't valid
     * @return whether the file header has an old version number.
     */
    private boolean readAndValidateFileHeader(RandomAccessFile file, String fileName, long fileNum)
    throws DatabaseException, IOException {
        LogManager logManager = envImpl.getLogManager();
        LogEntry headerEntry = logManager.getLogEntry(DbLsn.makeLsn(fileNum, 0), file);
        FileHeader header = (FileHeader) headerEntry.getMainItem();
        return header.validate(fileName, fileNum);
    }

    /** 
     * Write a proper file header to the given file.
     */
    private void writeFileHeader(RandomAccessFile file, String fileName, FileHeader header)
    throws DatabaseException, IOException {
        envImpl.checkIfInvalid();
        if (envImpl.mayNotWrite()) {
            return;
        }
        int headerSize = header.getLogSize();
        int entrySize = headerSize + LogManager.HEADER_BYTES;
        ByteBuffer headerBuf = envImpl.getLogManager().putIntoBuffer(header, headerSize, 0, false, entrySize);
        if (++writeCount >= stopOnWriteCount) {
            Runtime.getRuntime().halt(0xff);
        }
        int bytesWritten;
        try {
            if (RUNRECOVERY_EXCEPTION_TESTING) {
                generateRunRecoveryException(file, headerBuf, 0);
            }
            bytesWritten = writeToFile(file, headerBuf, 0);
        } catch (ClosedChannelException e) {
            throw new RunRecoveryException(envImpl, "Channel closed, may be due to thread interrupt", e);
        } catch (IOException e) {
            throw new RunRecoveryException(envImpl, "IOException caught: " + e);
        }
        if (bytesWritten != entrySize) {
            throw new LogException("File " + fileName + " was created with an incomplete header. Only " + bytesWritten +
                " bytes were written.");
        }
    }

    /** 
     * @return the prevOffset field stored in the file header.
     */
    long getFileHeaderPrevOffset(long fileNum) throws IOException, DatabaseException {
        LogEntry headerEntry = envImpl.getLogManager().getLogEntry(DbLsn.makeLsn(fileNum, 0));
        FileHeader header = (FileHeader) headerEntry.getMainItem();
        return header.getLastEntryInPrevFileOffset();
    }

    /** 
     * @return the file offset of the last LSN that was used. For constructingthe headers of log entries. If the last LSN that was used was in a previous file, or this is the very first LSN of the whole system, return 0.
     */
    long getPrevEntryOffset() {
        return prevOffset;
    }

    /** 
     * Increase the current log position by "size" bytes. Move the prevOffset pointer along.
     * @param sizeis an unsigned int
     * @return true if we flipped to the next log file.
     */
    boolean bumpLsn(long size) {
        saveLastPosition();
        boolean flippedFiles = false;
        if (forceNewFile || (DbLsn.getFileOffset(nextAvailableLsn) + size) > maxFileSize) {
            forceNewFile = false;
            currentFileNum++;
            if (lastUsedLsn != DbLsn.NULL_LSN) {
                perFileLastUsedLsn.put(new Long(DbLsn.getFileNumber(lastUsedLsn)), new Long(lastUsedLsn));
            }
            prevOffset = 0;
            lastUsedLsn = DbLsn.makeLsn(currentFileNum, firstLogEntryOffset());
            flippedFiles = true;
        } else {
            if (lastUsedLsn == DbLsn.NULL_LSN) {
                prevOffset = 0;
            } else {
                prevOffset = DbLsn.getFileOffset(lastUsedLsn);
            }
            lastUsedLsn = nextAvailableLsn;
        }
        nextAvailableLsn = DbLsn.makeLsn(DbLsn.getFileNumber(lastUsedLsn), (DbLsn.getFileOffset(lastUsedLsn) + size));
        return flippedFiles;
    }

    /** 
     * Write out a log buffer to the file.
     * @param fullBufferbuffer to write
     */
    void writeLogBuffer(LogBuffer fullBuffer) throws DatabaseException {
        envImpl.checkIfInvalid();
        if (envImpl.mayNotWrite()) {
            return;
        }
        long firstLsn = fullBuffer.getFirstLsn();
        if (firstLsn != DbLsn.NULL_LSN) {
            RandomAccessFile file = endOfLog.getWritableFile(DbLsn.getFileNumber(firstLsn));
            ByteBuffer data = fullBuffer.getDataBuffer();
            if (++writeCount >= stopOnWriteCount) {
                Runtime.getRuntime().halt(0xff);
            }
            try {
                //	this.hook465(fullBuffer, firstLsn, file);
                Label465: if (IO_EXCEPTION_TESTING) {
                    throw new IOException("generated for testing");
                }
                if (RUNRECOVERY_EXCEPTION_TESTING) {
                    generateRunRecoveryException(file, data, DbLsn.getFileOffset(firstLsn));
                }
                writeToFile(file, data, DbLsn.getFileOffset(firstLsn));
            }
            catch (ClosedChannelException e) {
                throw new RunRecoveryException(envImpl, "File closed, may be due to thread interrupt", e);
            } catch (IOException IOE) {
                abortCommittedTxns(data);
                Label466:
                    //	this.hook466(fullBuffer, firstLsn, file, data, IOE); 
            }
            assert EnvironmentImpl.maybeForceYield();
        }
    }

    /** 
     * Write a buffer to a file at a given offset, using NIO if so configured.
     */
    private int writeToFile(RandomAccessFile file, ByteBuffer data, long destOffset)
    throws IOException, DatabaseException {
        return new FileManager_writeToFile(this, file, data, destOffset).execute();
    }

    /** 
     * Read a buffer from a file at a given offset, using NIO if so configured.
     */
    void readFromFile(RandomAccessFile file, ByteBuffer readBuffer, long offset) throws IOException {
        new FileManager_readFromFile(this, file, readBuffer, offset).execute();
    }

    private void abortCommittedTxns(ByteBuffer data) {
        final byte commitType = LogEntryType.LOG_TXN_COMMIT.getTypeNum();
        final byte abortType = LogEntryType.LOG_TXN_ABORT.getTypeNum();
        //	this.hook461(data);
        Label461:
            while (data.remaining() > 0) {
                int recStartPos = data.position();
                data.position(recStartPos + LogManager.HEADER_ENTRY_TYPE_OFFSET);
                int typePos = data.position();
                byte entryType = data.get();
                boolean recomputeChecksum = false;
                if (entryType == commitType) {
                    data.position(typePos);
                    data.put(abortType);
                    recomputeChecksum = true;
                }
                byte version = data.get();
                data.position(data.position() + LogManager.PREV_BYTES);
                int itemSize = LogUtils.readInt(data);
                int itemDataStartPos = data.position();
                if (recomputeChecksum) {
                    Checksum checksum = Adler32.makeChecksum();
                    data.position(recStartPos);
                    int nChecksumBytes = itemSize + (LogManager.HEADER_BYTES - LogManager.CHECKSUM_BYTES);
                    byte[] checksumBytes = new byte[nChecksumBytes];
                    System.arraycopy(data.array(), recStartPos + LogManager.CHECKSUM_BYTES, checksumBytes, 0,
                        nChecksumBytes);
                    checksum.update(checksumBytes, 0, nChecksumBytes);
                    LogUtils.writeUnsignedInt(data, checksum.getValue());
                }
                data.position(itemDataStartPos + itemSize);
            }
        data.position(0);
    }

    /** 
     * FSync the end of the log.
     */
    void syncLogEnd() throws DatabaseException {
        try {
            endOfLog.force();
        } catch (IOException e) {
            throw new DatabaseException(e);
        }
    }

    /** 
     * Sync the end of the log, close off this log file. Should only be called under the log write latch.
     */
    void syncLogEndAndFinishFile() throws DatabaseException, IOException {
        if (syncAtFileEnd) {
            syncLogEnd();
        }
        endOfLog.close();
    }

    /** 
     * Close all file handles and empty the cache.
     */
    public void clear() throws IOException, DatabaseException {
        endOfLog.close();
    }

    /** 
     * Clear the file lock.
     */
    public void close() throws IOException, DatabaseException {}

    /** 
     * Ensure that if the environment home dir is on readonly media or in a readonly directory that the environment has been opened for readonly access.
     * @return true if the environment home dir is readonly.
     */
    private boolean checkEnvHomePermissions(boolean readOnly) throws DatabaseException {
        boolean envDirIsReadOnly = !dbEnvHome.canWrite();
        if (envDirIsReadOnly && !readOnly) {
            throw new DatabaseException("The Environment directory " + dbEnvHome + " is not writable, but the " +
                "Environment was opened for read-write access.");
        }
        return envDirIsReadOnly;
    }

    /** 
     * Truncate a log at this position. Used by recovery to a timestamp utilities and by recovery to set the end-of-log position. <p> This method forces a new log file to be written next, if the last file (the file truncated to) has an old version in its header. This ensures that when the log is opened by an old version of JE, a version incompatibility will be detected. [#11243] </p>
     */
    public void truncateLog(long fileNum, long offset) throws IOException, DatabaseException {
        FileHandle handle = makeFileHandle(fileNum, FileMode.READWRITE_MODE);
        RandomAccessFile file = handle.getFile();
        try {
            file.getChannel().truncate(offset);
        } finally {
            file.close();
        }
        if (handle.isOldHeaderVersion()) {
            forceNewFile = true;
        }
    }

    /** 
     * Set the flag that causes a new file to be written before the next write.
     */
    void forceNewLogFile() {
        forceNewFile = true;
    }

    /** 
     * @return the size in bytes of the file header log entry.
     */
    public static int firstLogEntryOffset() {
        return FileHeader.entrySize() + LogManager.HEADER_BYTES;
    }

    /** 
     * Return the next available LSN in the log. Note that this is unsynchronized, so is only valid as an approximation of log size.
     */
    public long getNextLsn() {
        return nextAvailableLsn;
    }

    /** 
     * Return the last allocated LSN in the log. Note that this is unsynchronized, so if it is called outside the log write latch it is only valid as an approximation of log size.
     */
    public long getLastUsedLsn() {
        return lastUsedLsn;
    }

    private void generateRunRecoveryException(RandomAccessFile file, ByteBuffer data, long destOffset)
    throws DatabaseException, IOException {
        if (runRecoveryExceptionThrown) {
            try {
                throw new Exception("Write after RunRecoveryException");
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        runRecoveryExceptionCounter += 1;
        if (runRecoveryExceptionCounter >= RUNRECOVERY_EXCEPTION_MAX) {
            runRecoveryExceptionCounter = 0;
        }
        if (runRecoveryExceptionRandom == null) {
            runRecoveryExceptionRandom = new Random(System.currentTimeMillis());
        }
        if (runRecoveryExceptionCounter == runRecoveryExceptionRandom.nextInt(RUNRECOVERY_EXCEPTION_MAX)) {
            int len = runRecoveryExceptionRandom.nextInt(data.remaining());
            if (len > 0) {
                byte[] a = new byte[len];
                data.get(a, 0, len);
                ByteBuffer buf = ByteBuffer.wrap(a);
                writeToFile(file, buf, destOffset);
            }
            runRecoveryExceptionThrown = true;
            throw new RunRecoveryException(envImpl, "Randomly generated for testing");
        }
    }

    protected void hook449(EnvironmentImpl envImpl) throws DatabaseException {}

    protected FileHandle hook450(long fileNum, Long fileId, FileHandle fileHandle)
    throws LogException, DatabaseException {
        fileHandle = this.hook462(fileNum, fileId, fileHandle);
        return fileHandle;
    }

    //protected void hook452(EnvironmentImpl envImpl) throws DatabaseException {}

    //protected void hook453(FileHandle fileHandle) throws LogException, DatabaseException {
    //}

//    protected void hook454(FileHandle fileHandle) throws LogException, DatabaseException {}

    //   protected void hook456(DbConfigManager configManager) throws DatabaseException {
    //   }

    protected void hook457(DbConfigManager configManager) throws DatabaseException {}

    protected void hook458(long fileNum) throws DatabaseException, IOException {}

    protected void hook459(long fileNum) throws DatabaseException, IOException {}

    //protected void hook460(long fileNum, Long fileId, FileHandle fileHandle) throws LogException, DatabaseException {
    //fileHandle = this.hook463(fileNum, fileId, fileHandle);
    //this.hook453(fileHandle);
    //if (fileHandle.getFile() == null) {
    //   this.hook454(fileHandle);
    //} else {
    //   throw new ReturnObject(fileHandle);
    //	}
    //  }

    //  protected void hook461(ByteBuffer data) {
    //   }

    //   protected FileHandle hook462(long fileNum, Long fileId, FileHandle fileHandle)
    //    throws LogException, DatabaseException {
    //fileHandle = makeFileHandle(fileNum, FileMode.READ_MODE);
    //this.hook464(fileId, fileHandle);
    //return fileHandle;
    // }

    // protected FileHandle hook463(long fileNum, Long fileId, FileHandle fileHandle)
    //  throws LogException, DatabaseException {
    //fileHandle = this.hook450(fileNum, fileId, fileHandle);
    //return fileHandle;
    // }

    //protected void hook464(Long fileId, FileHandle fileHandle) throws LogException, DatabaseException {
    //}

    protected void hook465(LogBuffer fullBuffer, long firstLsn, RandomAccessFile file)
    throws DatabaseException, ClosedChannelException, IOException {}

    //   protected void hook466(LogBuffer fullBuffer, long firstLsn, RandomAccessFile file, ByteBuffer data, IOException IOE)
    //    throws DatabaseException {
    //throw new DatabaseException(IOE);
    // }

    protected void hook467(boolean readOnly) throws DatabaseException {}

}
\00after the change, or null if the recordhas been deleted.
     */
    void databaseUpdated(Database db, Locker locker, DatabaseEntry priKey, DatabaseEntry oldData, DatabaseEntry newData)
	    throws DatabaseException;

}\00after reading an instance from the log. The envImpl field must be set before calling this method.
     */
    private void initDefaultSettings() throws DatabaseException {
	DbConfigManager configMgr = envImpl.getConfigManager();
	binDeltaPercent = configMgr.getInt(EnvironmentParams.BIN_DELTA_PERCENT);
	binMaxDeltas = configMgr.getInt(EnvironmentParams.BIN_MAX_DELTAS);
	if (maxMainTreeEntriesPerNode == 0) {
	    maxMainTreeEntriesPerNode = configMgr.getInt(EnvironmentParams.NODE_MAX);
	}
	if (maxDupTreeEntriesPerNode == 0) {
	    maxDupTreeEntriesPerNode = configMgr.getInt(EnvironmentParams.NODE_MAX_DUPTREE);
	}
    }

    /** 
     * Clone. For now just pass off to the super class for a field-by-field copy.
     */
    public Object clone() throws CloneNotSupportedException {
	return super.clone();
    }

    /** 
     * @return the database tree.
     */
    public Tree getTree() {
	return tree;
    }

    void setTree(Tree tree) {
	this.tree = tree;
    }

    /** 
     * @return the database id.
     */
    public DatabaseId getId() {
	return id;
    }

    void setId(DatabaseId id) {
	this.id = id;
    }

    public long getEofNodeId() {
	return eofNodeId;
    }

    /** 
     * @return true if this database is transactional.
     */
    public boolean isTransactional() {
	return transactional;
    }

    /** 
     * Sets the transactional property for the first opened handle.
     */
    public void setTransactional(boolean transactional) {
	this.transactional = transactional;
    }

    /** 
     * @return true if duplicates are allowed in this database.
     */
    public boolean getSortedDuplicates() {
	return duplicatesAllowed;
    }

    public int getNodeMaxEntries() {
	return maxMainTreeEntriesPerNode;
    }

    public int getNodeMaxDupTreeEntries() {
	return maxDupTreeEntriesPerNode;
    }

    /** 
     * Set the duplicate comparison function for this database.
     * @param duplicateComparator -The Duplicate Comparison function.
     */
    public void setDuplicateComparator(Comparator duplicateComparator) {
	this.duplicateComparator = duplicateComparator;
    }

    /** 
     * Set the btree comparison function for this database.
     * @param btreeComparator -The btree Comparison function.
     */
    public void setBtreeComparator(Comparator btreeComparator) {
	this.btreeComparator = btreeComparator;
    }

    /** 
     * @return the btree Comparator object.
     */
    public Comparator getBtreeComparator() {
	return btreeComparator;
    }

    /** 
     * @return the duplicate Comparator object.
     */
    public Comparator getDuplicateComparator() {
	return duplicateComparator;
    }

    /** 
     * Set the db environment during recovery, after instantiating the database from the log
     */
    public void setEnvironmentImpl(EnvironmentImpl envImpl) throws DatabaseException {
	this.envImpl = envImpl;
	initDefaultSettings();
	tree.setDatabase(this);
    }

    /** 
     * @return the database environment.
     */
    public EnvironmentImpl getDbEnvironment() {
	return envImpl;
    }

    /** 
     * Returns whether one or more handles are open.
     */
    public boolean hasOpenHandles() {
	return referringHandles.size() > 0;
    }

    /** 
     * Add a referring handle
     */
    public void addReferringHandle(Database db) {
	referringHandles.add(db);
    }

    /** 
     * Decrement the reference count.
     */
    public void removeReferringHandle(Database db) {
	referringHandles.remove(db);
    }

    /** 
     * @return the referring handle count.
     */
    synchronized int getReferringHandleCount() {
	return referringHandles.size();
    }

    /** 
     * For this secondary database return the primary that it is associated with, or null if not associated with any primary. Note that not all handles need be associated with a primary.
     */
    public Database findPrimaryDatabase() throws DatabaseException {
	for (Iterator i = referringHandles.iterator(); i.hasNext();) {
	    Object obj = i.next();
	    if (obj instanceof SecondaryDatabase) {
		return ((SecondaryDatabase) obj).getPrimaryDatabase();
	    }
	}
	return null;
    }

    public String getName() throws DatabaseException {
	return envImpl.getDbMapTree().getDbName(id);
    }

    /** 
     * Return the count of nodes in the database. Used for truncate, perhaps should be made available through other means? Database should be quiescent.
     */
    long countRecords() throws DatabaseException {
	LNCounter lnCounter = new LNCounter();
	SortedLSNTreeWalker walker = new SortedLSNTreeWalker(this, false, false, tree.getRootLsn(), lnCounter);
	walker.walk();
	return lnCounter.getCount();
    }

    private boolean walkDatabaseTree(TreeWalkerStatsAccumulator statsAcc, PrintStream out, boolean verbose)
	    throws DatabaseException {
	boolean ok = true;
	Locker locker = new ThreadLocker(envImpl);
	Cursor cursor = null;
	CursorImpl impl = null;
	try {
	    EnvironmentImpl.incThreadLocalReferenceCount();
	    cursor = DbInternal.newCursor(this, locker, null);
	    impl = DbInternal.getCursorImpl(cursor);
	    tree.setTreeStatsAccumulator(statsAcc);
	    impl.setTreeStatsAccumulator(statsAcc);
	    DatabaseEntry foundData = new DatabaseEntry();
	    DatabaseEntry key = new DatabaseEntry();
	    OperationStatus status = DbInternal.position(cursor, key, foundData, LockMode.READ_UNCOMMITTED, true);
	    while (status == OperationStatus.SUCCESS) {
		try {
		    status = DbInternal.retrieveNext(cursor, key, foundData, LockMode.READ_UNCOMMITTED, GetMode.NEXT);
		} catch (DatabaseException DBE) {
		    ok = false;
		    if (DbInternal.advanceCursor(cursor, key, foundData)) {
			if (verbose) {
			    out.println("Error encountered (continuing):");
			    out.println(DBE);
			    printErrorRecord(out, key, foundData);
			}
		    } else {
			throw DBE;
		    }
		}
	    }
	} finally {
	    if (impl != null) {
		impl.setTreeStatsAccumulator(null);
	    }
	    tree.setTreeStatsAccumulator(null);
	    EnvironmentImpl.decThreadLocalReferenceCount();
	    if (cursor != null) {
		cursor.close();
	    }
	}
	return ok;
    }

    /** 
     * Prints the key and data, if available, for a BIN entry that could not be read/verified. Uses the same format as DbDump and prints both the hex and printable versions of the entries.
     */
    private void printErrorRecord(PrintStream out, DatabaseEntry key, DatabaseEntry data) {
	byte[] bytes = key.getData();
	StringBuffer sb = new StringBuffer("Error Key ");
	if (bytes == null) {
	    sb.append("UNKNOWN");
	} else {
	    CmdUtil.formatEntry(sb, bytes, false);
	    sb.append(' ');
	    CmdUtil.formatEntry(sb, bytes, true);
	}
	out.println(sb);
	bytes = data.getData();
	sb = new StringBuffer("Error Data ");
	if (bytes == null) {
	    sb.append("UNKNOWN");
	} else {
	    CmdUtil.formatEntry(sb, bytes, false);
	    sb.append(' ');
	    CmdUtil.formatEntry(sb, bytes, true);
	}
	out.println(sb);
    }

    /** 
     * Preload the cache, using up to maxBytes bytes or maxMillsecs msec.
     */
    public PreloadStats preload(PreloadConfig config) throws DatabaseException {
	return new DatabaseImpl_preload(this, config).execute();
    }

    public String dumpString(int nSpaces) {
	StringBuffer sb = new StringBuffer();
	sb.append(TreeUtils.indent(nSpaces));
	sb.append("<database id=\"");
	sb.append(id.toString());
	sb.append("\"");
	if (btreeComparator != null) {
	    sb.append(" btc=\"");
	    sb.append(serializeComparator(btreeComparator));
	    sb.append("\"");
	}
	if (duplicateComparator != null) {
	    sb.append(" dupc=\"");
	    sb.append(serializeComparator(duplicateComparator));
	    sb.append("\"");
	}
	sb.append("/>");
	return sb.toString();
    }

    /** 
     * @see LogWritable#getLogSize
     */
    public int getLogSize() {
	return id.getLogSize() + tree.getLogSize() + LogUtils.getBooleanLogSize()
		+ LogUtils.getStringLogSize(serializeComparator(btreeComparator))
		+ LogUtils.getStringLogSize(serializeComparator(duplicateComparator)) + (LogUtils.getIntLogSize() * 2);
    }

    /** 
     * @see LogWritable#writeToLog
     */
    public void writeToLog(ByteBuffer logBuffer) {
	id.writeToLog(logBuffer);
	tree.writeToLog(logBuffer);
	LogUtils.writeBoolean(logBuffer, duplicatesAllowed);
	LogUtils.writeString(logBuffer, serializeComparator(btreeComparator));
	LogUtils.writeString(logBuffer, serializeComparator(duplicateComparator));
	LogUtils.writeInt(logBuffer, maxMainTreeEntriesPerNode);
	LogUtils.writeInt(logBuffer, maxDupTreeEntriesPerNode);
    }

    /** 
     * @see LogReadable#readFromLog
     */
    public void readFromLog(ByteBuffer itemBuffer, byte entryTypeVersion) throws LogException {
	id.readFromLog(itemBuffer, entryTypeVersion);
	tree.readFromLog(itemBuffer, entryTypeVersion);
	duplicatesAllowed = LogUtils.readBoolean(itemBuffer);
	btreeComparatorName = LogUtils.readString(itemBuffer);
	duplicateComparatorName = LogUtils.readString(itemBuffer);
	try {
	    if (!EnvironmentImpl.getNoComparators()) {
		if (btreeComparatorName.length() != 0) {
		    Class btreeComparatorClass = Class.forName(btreeComparatorName);
		    btreeComparator = instantiateComparator(btreeComparatorClass, "Btree");
		}
		if (duplicateComparatorName.length() != 0) {
		    Class duplicateComparatorClass = Class.forName(duplicateComparatorName);
		    duplicateComparator = instantiateComparator(duplicateComparatorClass, "Duplicate");
		}
	    }
	} catch (ClassNotFoundException CNFE) {
	    throw new LogException("couldn't instantiate class comparator", CNFE);
	}
	if (entryTypeVersion >= 1) {
	    maxMainTreeEntriesPerNode = LogUtils.readInt(itemBuffer);
	    maxDupTreeEntriesPerNode = LogUtils.readInt(itemBuffer);
	}
    }

    /** 
     * @see LogReadable#dumpLog
     */
    public void dumpLog(StringBuffer sb, boolean verbose) {
	sb.append("<database>");
	id.dumpLog(sb, verbose);
	tree.dumpLog(sb, verbose);
	sb.append("<dupsort v=\"").append(duplicatesAllowed);
	sb.append("\"/>");
	sb.append("<btcf name=\"");
	sb.append(btreeComparatorName);
	sb.append("\"/>");
	sb.append("<dupcf name=\"");
	sb.append(duplicateComparatorName);
	sb.append("\"/>");
	sb.append("</database>");
    }

    /** 
     * @see LogReadable#logEntryIsTransactional
     */
    public boolean logEntryIsTransactional() {
	return false;
    }

    /** 
     * @see LogReadable#getTransactionId
     */
    public long getTransactionId() {
	return 0;
    }

    /** 
     * Used both to write to the log and to validate a comparator when set in DatabaseConfig.
     */
    public static String serializeComparator(Comparator comparator) {
	if (comparator != null) {
	    return comparator.getClass().getName();
	} else {
	    return "";
	}
    }

    /** 
     * Used both to read from the log and to validate a comparator when set in DatabaseConfig.
     */
    public static Comparator instantiateComparator(Class comparator, String comparatorType) throws LogException {
	if (comparator == null) {
	    return null;
	}
	try {
	    return (Comparator) comparator.newInstance();
	} catch (InstantiationException IE) {
	    throw new LogException("Exception while trying to load " + comparatorType + " Comparator class: " + IE);
	} catch (IllegalAccessException IAE) {
	    throw new LogException("Exception while trying to load " + comparatorType + " Comparator class: " + IAE);
	}
    }

    public int getBinDeltaPercent() {
	return binDeltaPercent;
    }

    public int getBinMaxDeltas() {
	return binMaxDeltas;
    }

    //protected void hook288() throws DatabaseException {
    //}

    //protected void hook289() throws DatabaseException {
    //}

}
\00after recovery.
     * @return RecoveryInfo statistics about the recovery process.
     */
    public RecoveryInfo recover(boolean readOnly) throws DatabaseException {
			info = new RecoveryInfo();
			try {
					FileManager fileManager = env.getFileManager();
					DbConfigManager configManager = env.getConfigManager();
					boolean forceCheckpoint = configManager.getBoolean(EnvironmentParams.ENV_RECOVERY_FORCE_CHECKPOINT);
					if (fileManager.filesExist()) {
				findEndOfLog(readOnly);
				Label559:           ;  //this.hook559();
				findLastCheckpoint();
				env.getLogManager().setLastLsnAtRecovery(fileManager.getLastUsedLsn());
				Label558:           ;  //this.hook558();
				env.readMapTreeFromLog(info.useRootLsn);
				buildTree();
					} else {
				Label556:           ;  //this.hook556();
				Label560:           ;  //this.hook560();
				env.logMapTreeRoot();
				forceCheckpoint = true;
					}
					if (preparedTxns.size() > 0) {
				Label573:           ;  //this.hook573();
				preparedTxns = null;
					}
					if (DbInternal.getCreateUP(env.getConfigManager().getEnvironmentConfig())) {
				env.getUtilizationProfile().populateCache();
					}
					if (!readOnly && (env.getLogManager().getLastLsnAtRecovery() != info.checkpointEndLsn || forceCheckpoint)) {
				CheckpointConfig config = new CheckpointConfig();
				config.setForce(true);
				config.setMinimizeRecoveryTime(true);
				env.invokeCheckpoint(config, false, "recovery");
					}
			} catch (IOException e) {
					Label575:           ;  //this.hook575(e);
					throw new RecoveryException(env, "Couldn't recover: " + e.getMessage(), e);
			} finally {
					Tracer.trace(Level.CONFIG, env, "Recovery finished: " + info);
			}
			return info;
    }

    /** 
     * Find the end of the log, initialize the FileManager. While we're perusing the log, return the last checkpoint LSN if we happen to see it.
     */
    private void findEndOfLog(boolean readOnly) throws IOException, DatabaseException {
			LastFileReader reader = new LastFileReader(env, readBufferSize);
			while (reader.readNextEntry()) {
					LogEntryType type = reader.getEntryType();
					if (LogEntryType.LOG_CKPT_END.equals(type)) {
				info.checkpointEndLsn = reader.getLastLsn();
				info.partialCheckpointStartLsn = DbLsn.NULL_LSN;
					} else if (LogEntryType.LOG_CKPT_START.equals(type)) {
				if (info.partialCheckpointStartLsn == DbLsn.NULL_LSN) {
						info.partialCheckpointStartLsn = reader.getLastLsn();
				}
					}
			}
			assert (reader.getLastValidLsn() != reader.getEndOfLog()) : "lastUsed="
				+ DbLsn.getNoFormatString(reader.getLastValidLsn()) + " end="
				+ DbLsn.getNoFormatString(reader.getEndOfLog());
			if (!readOnly) {
					reader.setEndOfFile();
			}
			info.lastUsedLsn = reader.getLastValidLsn();
			info.nextAvailableLsn = reader.getEndOfLog();
			info.nRepeatIteratorReads += reader.getNRepeatIteratorReads();
			env.getFileManager().setLastPosition(info.nextAvailableLsn, info.lastUsedLsn, reader.getPrevOffset());
    }

    /** 
     * Find the last checkpoint and establish the firstActiveLsn point, checkpoint start, and checkpoint end.
     */
    private void findLastCheckpoint() throws IOException, DatabaseException {
	if (info.checkpointEndLsn == DbLsn.NULL_LSN) {
	    CheckpointFileReader searcher = new CheckpointFileReader(env, readBufferSize, false, info.lastUsedLsn,
		    DbLsn.NULL_LSN, info.nextAvailableLsn);
	    while (searcher.readNextEntry()) {
		if (searcher.isCheckpointEnd()) {
		    info.checkpointEndLsn = searcher.getLastLsn();
		    break;
		} else if (searcher.isCheckpointStart()) {
		    info.partialCheckpointStartLsn = searcher.getLastLsn();
		} else if (searcher.isRoot()) {
		    if (info.useRootLsn == DbLsn.NULL_LSN) {
			info.useRootLsn = searcher.getLastLsn();
		    }
		}
	    }
	    info.nRepeatIteratorReads += searcher.getNRepeatIteratorReads();
	}
	if (info.checkpointEndLsn == DbLsn.NULL_LSN) {
	    info.checkpointStartLsn = DbLsn.NULL_LSN;
	    info.firstActiveLsn = DbLsn.NULL_LSN;
	} else {
	    CheckpointEnd checkpointEnd = (CheckpointEnd) (env.getLogManager().get(info.checkpointEndLsn));
	    info.checkpointEnd = checkpointEnd;
	    info.checkpointStartLsn = checkpointEnd.getCheckpointStartLsn();
	    info.firstActiveLsn = checkpointEnd.getFirstActiveLsn();
	    if (checkpointEnd.getRootLsn() != DbLsn.NULL_LSN) {
		info.useRootLsn = checkpointEnd.getRootLsn();
	    }
	    env.getCheckpointer().setCheckpointId(checkpointEnd.getId());
	    env.getCheckpointer().setFirstActiveLsn(checkpointEnd.getFirstActiveLsn());
	}
	if (info.useRootLsn == DbLsn.NULL_LSN) {
	    throw new RecoveryException(env, "This environment's log file has no root. Since the root "
		    + "is the first entry written into a log at environment "
		    + "creation, this should only happen if the initial creation "
		    + "of the environment was never checkpointed or synced. "
		    + "Please move aside the existing log files to allow the " + "creation of a new environment");
	}
    }

    /** 
     * Use the log to recreate an in memory tree.
     */
    private void buildTree() throws IOException, DatabaseException {
	inListClearCounter = 0;
	Label572:           ;  //this.hook572();
	long start = System.currentTimeMillis();
	readINsAndTrackIds(info.checkpointStartLsn);
	long end = System.currentTimeMillis();
	Label571:           ;  //this.hook571(start, end);
	start = System.currentTimeMillis();
	info.numOtherINs += readINs(info.checkpointStartLsn, true, LogEntryType.LOG_BIN_DELTA, null, null, true);
	end = System.currentTimeMillis();
	Label570:           ;  //this.hook570(start, end);
	start = System.currentTimeMillis();
	Set mapLNSet = new HashSet();
	mapLNSet.add(LogEntryType.LOG_MAPLN_TRANSACTIONAL);
	mapLNSet.add(LogEntryType.LOG_TXN_COMMIT);
	mapLNSet.add(LogEntryType.LOG_TXN_ABORT);
	mapLNSet.add(LogEntryType.LOG_TXN_PREPARE);
	undoLNs(info, mapLNSet);
	end = System.currentTimeMillis();
	Label569:           ;  //this.hook569(start, end);
	start = System.currentTimeMillis();
	mapLNSet.add(LogEntryType.LOG_MAPLN);
	redoLNs(info, mapLNSet);
	end = System.currentTimeMillis();
	Label568:           ;  //this.hook568(start, end);
	start = System.currentTimeMillis();
	info.numOtherINs += readINs(info.checkpointStartLsn, false, LogEntryType.LOG_IN, LogEntryType.LOG_BIN,
		LogEntryType.LOG_IN_DELETE_INFO, false);
	end = System.currentTimeMillis();
	Label567:           ;  //this.hook567(start, end);
	start = System.currentTimeMillis();
	info.numBinDeltas = readINs(info.checkpointStartLsn, false, LogEntryType.LOG_BIN_DELTA, null, null, true);
	end = System.currentTimeMillis();
	Label566:           ;  //this.hook566(start, end);
	start = System.currentTimeMillis();
	info.numDuplicateINs += readINs(info.checkpointStartLsn, false, LogEntryType.LOG_DIN, LogEntryType.LOG_DBIN,
		LogEntryType.LOG_IN_DUPDELETE_INFO, true);
	end = System.currentTimeMillis();
	Label565:           ;  //this.hook565(start, end);
	start = System.currentTimeMillis();
	info.numBinDeltas += readINs(info.checkpointStartLsn, false, LogEntryType.LOG_DUP_BIN_DELTA, null, null, true);
	end = System.currentTimeMillis();
	Label564:           ;  //this.hook564(start, end);
	rebuildINList();

  Label596:
	Label563:           ;  //this.hook563();
	start = System.currentTimeMillis();
	Set lnSet = new HashSet();
	lnSet.add(LogEntryType.LOG_LN_TRANSACTIONAL);
	lnSet.add(LogEntryType.LOG_NAMELN_TRANSACTIONAL);
	lnSet.add(LogEntryType.LOG_DEL_DUPLN_TRANSACTIONAL);
	lnSet.add(LogEntryType.LOG_DUPCOUNTLN_TRANSACTIONAL);
	undoLNs(info, lnSet);
	end = System.currentTimeMillis();
	Label562:           ;  //this.hook562(start, end);
	start = System.currentTimeMillis();
	lnSet.add(LogEntryType.LOG_LN);
	lnSet.add(LogEntryType.LOG_NAMELN);
	lnSet.add(LogEntryType.LOG_DEL_DUPLN);
	lnSet.add(LogEntryType.LOG_DUPCOUNTLN);
	lnSet.add(LogEntryType.LOG_FILESUMMARYLN);
	redoLNs(info, lnSet);
	end = System.currentTimeMillis();
	Label561:           ;  //this.hook561(start, end);
    }

    private void readINsAndTrackIds(long rollForwardLsn) throws IOException, DatabaseException {
	INFileReader reader = new INFileReader(env, readBufferSize, rollForwardLsn, info.nextAvailableLsn, true, false,
		info.partialCheckpointStartLsn, fileSummaryLsns);
	reader.addTargetType(LogEntryType.LOG_IN);
	reader.addTargetType(LogEntryType.LOG_BIN);
	reader.addTargetType(LogEntryType.LOG_IN_DELETE_INFO);
	Label593://          ;  //this.hook593(reader);
	try {
	    info.numMapINs = 0;
	    DbTree dbMapTree = env.getDbMapTree();
	    while (reader.readNextEntry()) {
		DatabaseId dbId = reader.getDatabaseId();
		if (dbId.equals(DbTree.ID_DB_ID)) {
		    DatabaseImpl db = dbMapTree.getDb(dbId);
		    replayOneIN(reader, db, false);
		    info.numMapINs++;
		}
	    }
	    info.useMaxNodeId = reader.getMaxNodeId();
	    info.useMaxDbId = reader.getMaxDbId();
	    info.useMaxTxnId = reader.getMaxTxnId();
	    if (info.checkpointEnd != null) {
		if (info.useMaxNodeId < info.checkpointEnd.getLastNodeId()) {
		    info.useMaxNodeId = info.checkpointEnd.getLastNodeId();
		}
		if (info.useMaxDbId < info.checkpointEnd.getLastDbId()) {
		    info.useMaxDbId = info.checkpointEnd.getLastDbId();
		}
		if (info.useMaxTxnId < info.checkpointEnd.getLastTxnId()) {
		    info.useMaxTxnId = info.checkpointEnd.getLastTxnId();
		}
	    }
	    Node.setLastNodeId(info.useMaxNodeId);
	    env.getDbMapTree().setLastDbId(info.useMaxDbId);
	    env.getTxnManager().setLastTxnId(info.useMaxTxnId);
	    info.nRepeatIteratorReads += reader.getNRepeatIteratorReads();
	} catch (Exception e) {
	    traceAndThrowException(reader.getLastLsn(), "readMapIns", e);
	}
    }

    /** 
     * Read INs and process.
     */
    private int readINs(long rollForwardLsn, boolean mapDbOnly, LogEntryType inType1, LogEntryType inType2,
	    LogEntryType inType3, boolean requireExactMatch) throws IOException, DatabaseException {
	INFileReader reader = new INFileReader(env, readBufferSize, rollForwardLsn, info.nextAvailableLsn, false,
		mapDbOnly, info.partialCheckpointStartLsn, fileSummaryLsns);
	if (inType1 != null) {
	    reader.addTargetType(inType1);
	}
	if (inType2 != null) {
	    reader.addTargetType(inType2);
	}
	if (inType3 != null) {
	    reader.addTargetType(inType3);
	}
	int numINsSeen = 0;
	try {
	    DbTree dbMapTree = env.getDbMapTree();
	    while (reader.readNextEntry()) {
		DatabaseId dbId = reader.getDatabaseId();
		boolean isMapDb = dbId.equals(DbTree.ID_DB_ID);
		boolean isTarget = false;
		if (mapDbOnly && isMapDb) {
		    isTarget = true;
		} else if (!mapDbOnly && !isMapDb) {
		    isTarget = true;
		}
		if (isTarget) {
		    DatabaseImpl db = dbMapTree.getDb(dbId);
		    if (db == null) {
		    } else {
			replayOneIN(reader, db, requireExactMatch);
			numINsSeen++;
			inListRebuildDbIds.add(dbId);
		    }
		}
	    }
	    info.nRepeatIteratorReads += reader.getNRepeatIteratorReads();
	    return numINsSeen;
	} catch (Exception e) {
	    traceAndThrowException(reader.getLastLsn(), "readNonMapIns", e);
	    return 0;
	}
    }

    /** 
     * Get an IN from the reader, set its database, and fit into tree.
     */
    private void replayOneIN(INFileReader reader, DatabaseImpl db, boolean requireExactMatch) throws DatabaseException {
	if (reader.isDeleteInfo()) {
	    replayINDelete(db, reader.getDeletedNodeId(), false, reader.getDeletedIdKey(), null, reader.getLastLsn());
	} else if (reader.isDupDeleteInfo()) {
	    replayINDelete(db, reader.getDupDeletedNodeId(), true, reader.getDupDeletedMainKey(),
		    reader.getDupDeletedDupKey(), reader.getLastLsn());
	} else {
	    IN in = reader.getIN();
	    long inLsn = reader.getLsnOfIN();
	    in.postRecoveryInit(db, inLsn);
	    Label585: ;  //this.hook585(in);
	    replaceOrInsert(db, in, reader.getLastLsn(), inLsn, requireExactMatch);
	}
	if ((++inListClearCounter % CLEAR_INCREMENT) == 0) {
	    env.getInMemoryINs().clear();
	}
    }

    /** 
     * Undo all aborted LNs. To do so, walk the log backwards, keeping a collection of committed txns. If we see a log entry that doesn't have a committed txn, undo it.
     */
    private void undoLNs(RecoveryInfo info, Set lnTypes) throws IOException, DatabaseException {
	long firstActiveLsn = info.firstActiveLsn;
	long lastUsedLsn = info.lastUsedLsn;
	long endOfFileLsn = info.nextAvailableLsn;

	LNFileReader reader = new LNFileReader(env, readBufferSize, lastUsedLsn, false, endOfFileLsn, firstActiveLsn,
		null);
	Iterator iter = lnTypes.iterator();
	while (iter.hasNext()) {
	    LogEntryType lnType = (LogEntryType) iter.next();
	    reader.addTargetType(lnType);
	}
	Map countedFileSummaries = new HashMap();
	Set countedAbortLsnNodes = new HashSet();
	DbTree dbMapTree = env.getDbMapTree();
	TreeLocation location = new TreeLocation();
	try {
	    while (reader.readNextEntry()) {
		if (reader.isLN()) {
		    Long txnId = reader.getTxnId();
		    if (txnId != null && !committedTxnIds.contains(txnId)) {
		  //this.hook597();
      Label597:
			LN ln = reader.getLN();
			long logLsn = reader.getLastLsn();
			long abortLsn = reader.getAbortLsn();
			boolean abortKnownDeleted = reader.getAbortKnownDeleted();
			DatabaseId dbId = reader.getDatabaseId();
			DatabaseImpl db = dbMapTree.getDb(dbId);
			if (db != null) {
			    ln.postFetchInit(db, logLsn);
			    Label586:           ;  //this.hook586(info, reader, location, ln, logLsn, abortLsn, abortKnownDeleted, db);
					undo(detailedTraceLevel, db, location, ln, reader.getKey(), reader.getDupTreeKey(), logLsn, abortLsn,		abortKnownDeleted, info, true);
					Label586_1:
			    TxnNodeId txnNodeId = new TxnNodeId(reader.getNodeId(), txnId.longValue());
			    undoUtilizationInfo(ln, logLsn, abortLsn, abortKnownDeleted, txnNodeId,
				    countedFileSummaries, countedAbortLsnNodes);
			    inListRebuildDbIds.add(dbId);
			}
		    }
		} else if (reader.isPrepare()) {
		    long prepareId = reader.getTxnPrepareId();
		    Long prepareIdL = new Long(prepareId);
		    if (!committedTxnIds.contains(prepareIdL) && !abortedTxnIds.contains(prepareIdL)) {
			TransactionConfig txnConf = new TransactionConfig();
			Txn preparedTxn = new Txn(env, txnConf, prepareId);
			preparedTxn.setLockTimeout(0);
			preparedTxns.put(prepareIdL, preparedTxn);
			env.getTxnManager().registerXATxn(reader.getTxnPrepareXid(), preparedTxn, true);
			Label574:           ;  //this.hook574(reader);
		    }
		} else if (reader.isAbort()) {
		    abortedTxnIds.add(new Long(reader.getTxnAbortId()));
		} else {
		    committedTxnIds.add(new Long(reader.getTxnCommitId()));
		}
	    }
	    info.nRepeatIteratorReads += reader.getNRepeatIteratorReads();
	} catch (Exception e) {
	    traceAndThrowException(reader.getLastLsn(), "undoLNs", e);
	}
    }

    /** 
     * Apply all committed LNs.
     * @param rollForwardLsnstart redoing from this point
     * @param lnType1targetted LN
     * @param lnType2targetted LN
     */
    private void redoLNs(RecoveryInfo info, Set lnTypes) throws IOException, DatabaseException {
	long endOfFileLsn = info.nextAvailableLsn;
	long rollForwardLsn = info.checkpointStartLsn;
	LNFileReader reader = new LNFileReader(env, readBufferSize, rollForwardLsn, true, DbLsn.NULL_LSN, endOfFileLsn,
		null);
	Iterator iter = lnTypes.iterator();
	while (iter.hasNext()) {
	    LogEntryType lnType = (LogEntryType) iter.next();
	    reader.addTargetType(lnType);
	}
	Set countedAbortLsnNodes = new HashSet();
	DbTree dbMapTree = env.getDbMapTree();
	TreeLocation location = new TreeLocation();
	try {
	    while (reader.readNextEntry()) {
		if (reader.isLN()) {
		    Long txnId = reader.getTxnId();
		    boolean processThisLN = false;
		    boolean lnIsCommitted = false;
		    boolean lnIsPrepared = false;
		    Txn preparedTxn = null;
		    if (txnId == null) {
			processThisLN = true;
		    } else {
			lnIsCommitted = committedTxnIds.contains(txnId);
			if (!lnIsCommitted) {
			    preparedTxn = (Txn) preparedTxns.get(txnId);
			    lnIsPrepared = preparedTxn != null;
			}
			if (lnIsCommitted || lnIsPrepared) {
			    processThisLN = true;
			}
		    }
		    if (processThisLN) {
      Label598:
			LN ln = reader.getLN();
			DatabaseId dbId = reader.getDatabaseId();
			DatabaseImpl db = dbMapTree.getDb(dbId);
			long logLsn = reader.getLastLsn();
			long treeLsn = DbLsn.NULL_LSN;
			if (db != null) {
			    ln.postFetchInit(db, logLsn);
			    if (preparedTxn != null) {
				preparedTxn.addLogInfo(logLsn);
				preparedTxn.lock(ln.getNodeId(), LockType.WRITE, false, db);
				preparedTxn.setPrepared(true);
			    }
			    treeLsn = redo(db, location, ln, reader.getKey(), reader.getDupTreeKey(), logLsn, info);
			    inListRebuildDbIds.add(dbId);
			}
			TxnNodeId txnNodeId = null;
			if (txnId != null) {
			    txnNodeId = new TxnNodeId(reader.getNodeId(), txnId.longValue());
			}
			redoUtilizationInfo(logLsn, treeLsn, reader.getAbortLsn(), reader.getAbortKnownDeleted(), ln,
				txnNodeId, countedAbortLsnNodes);
		    }
		}
	    }
	    info.nRepeatIteratorReads += reader.getNRepeatIteratorReads();
	} catch (Exception e) {
	    traceAndThrowException(reader.getLastLsn(), "redoLns", e);
	}
    }

    /** 
     * Rebuild the in memory inList with INs that have been made resident by the recovery process.
     */
    private void rebuildINList() throws DatabaseException {
	env.getInMemoryINs().clear();
	env.getDbMapTree().rebuildINListMapDb();
	Iterator iter = inListRebuildDbIds.iterator();
	while (iter.hasNext()) {
	    DatabaseId dbId = (DatabaseId) iter.next();
	    if (!dbId.equals(DbTree.ID_DB_ID)) {
		DatabaseImpl db = env.getDbMapTree().getDb(dbId);
		if (db != null) {
		    db.getTree().rebuildINList();
		}
	    }
	}
    }

    /** 
     * Recover an internal node. If inFromLog is: - not found, insert it in the appropriate location. - if found and there is a physical match (LSNs are the same) do nothing. - if found and there is a logical match (LSNs are different, another version of this IN is in place, replace the found node with the node read from the log only if the log version's LSN is greater. InFromLog should be latched upon entering this method and it will not be latched upon exiting.
     * @param inFromLog -the new node to put in the tree. The identifier key and node id are used to find the existing version of the node.
     * @param logLsn -the location of log entry in in the log.
     * @param inLsnLSN of this in -- may not be the same as the log LSN if thecurrent entry is a BINDelta
     * @param requireExactMatch -true if we won't place this node in the tree unless we find exactly that parent. Used for BINDeltas, where we want to only apply the BINDelta to that exact node.
     */
    private void replaceOrInsert(DatabaseImpl db, IN inFromLog, long logLsn, long inLsn, boolean requireExactMatch)
	    throws DatabaseException {
	List trackingList = null;
	try {
	    if (inFromLog.isRoot()) {
		if (inFromLog.containsDuplicates()) {
		    replaceOrInsertDuplicateRoot(db, (DIN) inFromLog, logLsn);
		} else {
		    replaceOrInsertRoot(db, inFromLog, logLsn);
		}
	    } else {
		trackingList = new ArrayList();
		replaceOrInsertChild(db, inFromLog, logLsn, inLsn, trackingList, requireExactMatch);
	    }
	} catch (Exception e) {
	    String trace = printTrackList(trackingList);
	    Label576:           ;  //this.hook576(db, logLsn, e, trace);
	    throw new DatabaseException("lsnFromLog=" + DbLsn.getNoFormatString(logLsn), e);
	} finally {
	    Label587:           ;  //this.hook587(inFromLog, logLsn);
	}
    }

    /** 
     * Dump a tracking list into a string.
     */
    private String printTrackList(List trackingList) {
	if (trackingList != null) {
	    StringBuffer sb = new StringBuffer();
	    Iterator iter = trackingList.iterator();
	    sb.append("Trace list:");
	    sb.append('\n');
	    while (iter.hasNext()) {
		sb.append((TrackingInfo) iter.next());
		sb.append('\n');
	    }
	    return sb.toString();
	} else {
	    return null;
	}
    }

    /** 
     * Replay an IN delete. Remove an entry from an IN to reflect a reverse split.
     */
    private void replayINDelete(DatabaseImpl db, long nodeId, boolean containsDuplicates, byte[] mainKey, byte[] dupKey,
	    long logLsn) throws DatabaseException {
	boolean found = false;
	boolean deleted = false;
	Tree tree = db.getTree();
	SearchResult result = new SearchResult();
	try {
	    result = db.getTree().getParentINForChildIN(nodeId, containsDuplicates, false, mainKey, dupKey, false,
		    false, -1, null, true);
	    if (result.parent == null) {
		tree.withRootLatchedExclusive(new RootDeleter(tree));
		DbTree dbTree = db.getDbEnvironment().getDbMapTree();
		dbTree.modifyDbRoot(db);
		Label557:           ;  //this.hook557(db);
		deleted = true;
	    } else if (result.exactParentFound) {
		found = true;
		deleted = result.parent.deleteEntry(result.index, false);
	    }
	} finally {
	    Label588:           ;  //this.hook588(result);
	    Label579:           ;  //this.hook579(nodeId, containsDuplicates, logLsn, found, deleted, result);
	}
    }

    /** 
     * If the root of this tree is null, use this IN from the log as a root. Note that we should really also check the LSN of the mapLN, because perhaps the root is null because it's been deleted. However, the replay of all the LNs will end up adjusting the tree correctly. If there is a root, check if this IN is a different LSN and if so, replace it.
     */
    private void replaceOrInsertRoot(DatabaseImpl db, IN inFromLog, long lsn) throws DatabaseException {
	boolean success = true;
	Tree tree = db.getTree();
	RootUpdater rootUpdater = new RootUpdater(tree, inFromLog, lsn);
	try {
	    tree.withRootLatchedExclusive(rootUpdater);
	    if (rootUpdater.updateDone()) {
		EnvironmentImpl env = db.getDbEnvironment();
		env.getDbMapTree().modifyDbRoot(db);
	    }
	} catch (Exception e) {
	    success = false;
	    throw new DatabaseException("lsnFromLog=" + DbLsn.getNoFormatString(lsn), e);
	} finally {
	    Label580:           ;  //this.hook580(db, inFromLog, lsn, success, rootUpdater);
	}
    }

    /** 
     * Recover this root of a duplicate tree.
     */
    private void replaceOrInsertDuplicateRoot(DatabaseImpl db, DIN inFromLog, long lsn) throws DatabaseException {
	boolean found = true;
	boolean inserted = false;
	boolean replaced = false;
	long origLsn = DbLsn.NULL_LSN;
	byte[] mainTreeKey = inFromLog.getMainTreeKey();
	IN parent = null;
	int index = -1;
	boolean success = false;
	try {
	    parent = db.getTree().searchSplitsAllowed(mainTreeKey, -1, true);
	    assert parent instanceof BIN;
	    ChildReference newRef = new ChildReference(inFromLog, mainTreeKey, lsn);
	    index = parent.insertEntry1(newRef);
	    if ((index >= 0 && (index & IN.EXACT_MATCH) != 0)) {
		index &= ~IN.EXACT_MATCH;
		if (parent.isEntryKnownDeleted(index)) {
		    parent.setEntry(index, inFromLog, mainTreeKey, lsn, (byte) 0);
		    replaced = true;
		} else {
		    origLsn = parent.getLsn(index);
		    if (DbLsn.compareTo(origLsn, lsn) < 0) {
			parent.setEntry(index, inFromLog, mainTreeKey, lsn, parent.getState(index));
			replaced = true;
		    }
		}
	    } else {
		found = false;
	    }
	    success = true;
	} finally {
	    Label589:           ;  //this.hook589(parent);
	    Label581:           ;  //this.hook581(db, inFromLog, lsn, found, inserted, replaced, origLsn, parent, index, success);
	}
    }

    private void replaceOrInsertChild(DatabaseImpl db, IN inFromLog, long logLsn, long inLsn, List trackingList,
	    boolean requireExactMatch) throws DatabaseException {
	boolean inserted = false;
	boolean replaced = false;
	long origLsn = DbLsn.NULL_LSN;
	boolean success = false;
	SearchResult result = new SearchResult();
	try {
	    result = db.getTree().getParentINForChildIN(inFromLog, requireExactMatch, false, -1, trackingList);
	    if (result.parent == null) {
		return;
	    }
	    if (result.index >= 0) {
		if (result.parent.getLsn(result.index) == logLsn) {
		} else {
		    if (result.exactParentFound) {
			origLsn = result.parent.getLsn(result.index);
			if (DbLsn.compareTo(origLsn, logLsn) < 0) {
			    result.parent.updateEntry(result.index, inFromLog, inLsn);
			    replaced = true;
			}
		    }
		}
	    }
	    success = true;
	} finally {
	    Label590:           ;  //this.hook590(result);
	    Label582:           ;  //this.hook582(db, inFromLog, logLsn, inserted, replaced, origLsn, success, result);
	}
    }

    /** 
     * Redo a committed LN for recovery. <pre> log LN found  | logLSN &gt; LSN | LN is deleted | action in tree     | in tree      |               | --------------+--------------+---------------+------------------------ Y         |    N         |    n/a        | no action --------------+--------------+---------------+------------------------ Y         |    Y         |     N         | replace w/log LSN --------------+--------------+---------------+------------------------ Y         |    Y         |     Y         | replace w/log LSN, put |              |               | on compressor queue --------------+--------------+---------------+------------------------ N         |    n/a       |     N         | insert into tree --------------+--------------+---------------+------------------------ N         |    n/a       |     Y         | no action --------------+--------------+---------------+------------------------ </pre>
     * @param locationholds state about the search in the tree. Passed in from therecovery manager to reduce objection creation overhead.
     * @param lnFromLog -the new node to put in the tree.
     * @param mainKeyis the key that navigates us through the main tree
     * @param dupTreeKeyis the key that navigates us through the duplicate tree
     * @param logLsnis the LSN from the just-read log entry
     * @param infois a recovery stats object.
     * @return the LSN found in the tree, or null if not found.
     */
    private long redo(DatabaseImpl db, TreeLocation location, LN lnFromLog, byte[] mainKey, byte[] dupKey, long logLsn,
	    RecoveryInfo info) throws DatabaseException {
	boolean found = false;
	boolean replaced = false;
	boolean inserted = false;
	boolean success = false;
	try {
	    location.reset();
	    found = db.getTree().getParentBINForChildLN(location, mainKey, dupKey, lnFromLog, true, false, true, true);
	    if (!found && (location.bin == null)) {
		success = true;
		return DbLsn.NULL_LSN;
	    }
	    if (lnFromLog.containsDuplicates()) {
		if (found) {
		    DIN duplicateRoot = (DIN) location.bin.fetchTarget(location.index);
		    if (DbLsn.compareTo(logLsn, location.childLsn) >= 0) {
			duplicateRoot.updateDupCountLNRefAndNullTarget(logLsn);
		    }
		}
	    } else {
		if (found) {
		    info.lnFound++;
		    if (DbLsn.compareTo(logLsn, location.childLsn) > 0) {
			info.lnReplaced++;
			replaced = true;
			location.bin.updateEntry(location.index, null, logLsn);
		    }
		    if (DbLsn.compareTo(logLsn, location.childLsn) >= 0 && lnFromLog.isDeleted()) {
			location.bin.setKnownDeletedLeaveTarget(location.index);
			byte[] deletedKey = location.bin.containsDuplicates() ? dupKey : mainKey;
			Label594:           ;  //this.hook594(db, location, deletedKey);
		    }
		} else {
		    info.lnNotFound++;
		    if (!lnFromLog.isDeleted()) {
			info.lnInserted++;
			inserted = true;
			boolean insertOk = insertRecovery(db, location, logLsn);
			assert insertOk;
		    }
		}
	    }
	    success = true;
	    return found ? location.childLsn : DbLsn.NULL_LSN;
	} finally {
	    Label591:           ;  //this.hook591(location);
	    Label583:           ;  //this.hook583(db, location, lnFromLog, logLsn, found, replaced, inserted, success);
	}
    }

    /** 
     * Undo the changes to this node. Here are the rules that govern the action taken. <pre> found LN in  | abortLsn is | logLsn ==       | action taken tree      | null        | LSN in tree     | by undo -------------+-------------+---------------------------------------- Y       |     N       |      Y          | replace w/abort LSN ------------ +-------------+-----------------+----------------------- Y       |     Y       |      Y          | remove from tree ------------ +-------------+-----------------+----------------------- Y       |     N/A     |      N          | no action ------------ +-------------+-----------------+----------------------- N       |     N/A     |    N/A          | no action (*) (*) If this key is not present in the tree, this record doesn't reflect the IN state of the tree and this log entry is not applicable. </pre>
     * @param locationholds state about the search in the tree. Passed in from therecovery manager to reduce objection creation overhead.
     * @param lnFromLog -the new node to put in the tree.
     * @param mainKeyis the key that navigates us through the main tree
     * @param dupTreeKeyis the key that navigates us through the duplicate tree
     * @param logLsnis the LSN from the just-read log entry
     * @param abortLsngives us the location of the original version of the node
     * @param infois a recovery stats object.
     */
    public static void undo(Level traceLevel, DatabaseImpl db, TreeLocation location, LN lnFromLog, byte[] mainKey,
	    byte[] dupKey, long logLsn, long abortLsn, boolean abortKnownDeleted, RecoveryInfo info,
	    boolean splitsAllowed) throws DatabaseException {
	boolean found = false;
	boolean replaced = false;
	boolean success = false;
	Label584: //hook584(traceLevel, db, location, lnFromLog, mainKey, dupKey, logLsn, abortLsn, abortKnownDeleted, info, splitsAllowed, found, replaced, success);
	location.reset();
		found = db.getTree().getParentBINForChildLN(location, mainKey, dupKey, lnFromLog, splitsAllowed, true, false,
			true);
		if (lnFromLog.containsDuplicates()) {
			  if (found) {
			DIN duplicateRoot = (DIN) location.bin.fetchTarget(location.index);
			Label592: //replaced = hook592(location, logLsn, abortLsn, replaced, duplicateRoot);
			if (DbLsn.compareTo(logLsn, location.childLsn) == 0) {
							duplicateRoot.updateDupCountLNRefAndNullTarget(abortLsn);
							replaced = true;
					}
			Label592_1:
			//end hook592
			  }
		} else {
			  if (found) {
			if (info != null) {
				  info.lnFound++;
			}
			boolean updateEntry = DbLsn.compareTo(logLsn, location.childLsn) == 0;
			if (updateEntry) {
				  if (abortLsn == DbLsn.NULL_LSN) {
				location.bin.setKnownDeletedLeaveTarget(location.index);
				byte[] deletedKey = location.bin.containsDuplicates() ? dupKey : mainKey;
				Label595: //hook595(db, location, deletedKey);
				  } else {
				if (info != null) {
					  info.lnReplaced++;
				}
				replaced = true;
				location.bin.updateEntry(location.index, null, abortLsn);
				if (abortKnownDeleted) {
					  location.bin.setKnownDeleted(location.index);
				} else {
					  location.bin.clearKnownDeleted(location.index);
				}
				  }
				  location.bin.clearPendingDeleted(location.index);
			}
			  } else {
			if (info != null) {
				  info.lnNotFound++;
			}
			  }
		}
		success = true;
  

// End hook584
    }

    /** 
     * Inserts a LN into the tree for recovery redo processing. In this case, we know we don't have to lock when checking child LNs for deleted status (there can be no other thread running on this tree) and we don't have to log the new entry. (it's in the log already)
     * @param db
     * @param locationthis embodies the parent bin, the index, the key thatrepresents this entry in the bin.
     * @param logLsnLSN of this current ln
     * @param keyto use when creating a new ChildReference object.
     * @return true if LN was inserted, false if it was a duplicate duplicate orif an attempt was made to insert a duplicate when allowDuplicates was false.
     */
    private static boolean insertRecovery(DatabaseImpl db, TreeLocation location, long logLsn)
	    throws DatabaseException {
	ChildReference newLNRef = new ChildReference(null, location.lnKey, logLsn);
	BIN parentBIN = location.bin;
	int entryIndex = parentBIN.insertEntry1(newLNRef);
	if ((entryIndex & IN.INSERT_SUCCESS) == 0) {
	    entryIndex &= ~IN.EXACT_MATCH;
	    boolean canOverwrite = false;
	    if (parentBIN.isEntryKnownDeleted(entryIndex)) {
		canOverwrite = true;
	    } else {
		LN currentLN = (LN) parentBIN.fetchTarget(entryIndex);
		if (currentLN == null || currentLN.isDeleted()) {
		    canOverwrite = true;
		}
		parentBIN.updateEntry(entryIndex, null);
	    }
	    if (canOverwrite) {
		parentBIN.updateEntry(entryIndex, null, logLsn, location.lnKey);
		parentBIN.clearKnownDeleted(entryIndex);
		location.index = entryIndex;
		return true;
	    } else {
		return false;
	    }
	}
	location.index = entryIndex & ~IN.INSERT_SUCCESS;
	return true;
    }

    /** 
     * Update file utilization info during redo.
     */
    private void redoUtilizationInfo(long logLsn, long treeLsn, long abortLsn, boolean abortKnownDeleted, LN ln,
	    TxnNodeId txnNodeId, Set countedAbortLsnNodes) {
	UtilizationTracker tracker = env.getUtilizationTracker();
	if (ln.isDeleted()) {
	    Long logFileNum = new Long(DbLsn.getFileNumber(logLsn));
	    long fileSummaryLsn = DbLsn.longToLsn((Long) fileSummaryLsns.get(logFileNum));
	    int cmpFsLsnToLogLsn = (fileSummaryLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(fileSummaryLsn, logLsn) : -1;
	    if (cmpFsLsnToLogLsn < 0) {
		tracker.countObsoleteNode(logLsn, null);
	    }
	}
	if (treeLsn != DbLsn.NULL_LSN) {
	    int cmpLogLsnToTreeLsn = DbLsn.compareTo(logLsn, treeLsn);
	    if (cmpLogLsnToTreeLsn != 0) {
		long newLsn = (cmpLogLsnToTreeLsn < 0) ? treeLsn : logLsn;
		long oldLsn = (cmpLogLsnToTreeLsn > 0) ? treeLsn : logLsn;
		Long oldLsnFile = new Long(DbLsn.getFileNumber(oldLsn));
		long oldFsLsn = DbLsn.longToLsn((Long) fileSummaryLsns.get(oldLsnFile));
		int cmpOldFsLsnToNewLsn = (oldFsLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(oldFsLsn, newLsn) : -1;
		if (cmpOldFsLsnToNewLsn < 0) {
		    tracker.countObsoleteNode(oldLsn, null);
		}
	    }
	    if (cmpLogLsnToTreeLsn <= 0 && abortLsn != DbLsn.NULL_LSN && !abortKnownDeleted
		    && !countedAbortLsnNodes.contains(txnNodeId)) {
		Long abortFileNum = new Long(DbLsn.getFileNumber(abortLsn));
		long abortFsLsn = DbLsn.longToLsn((Long) fileSummaryLsns.get(abortFileNum));
		int cmpAbortFsLsnToLogLsn = (abortFsLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(abortFsLsn, logLsn) : -1;
		if (cmpAbortFsLsnToLogLsn < 0) {
		    tracker.countObsoleteNodeInexact(abortLsn, null);
		    countedAbortLsnNodes.add(txnNodeId);
		}
	    }
	}
    }

    /** 
     * Update file utilization info during recovery undo (not abort undo).
     */
    private void undoUtilizationInfo(LN ln, long logLsn, long abortLsn, boolean abortKnownDeleted, TxnNodeId txnNodeId,
	    Map countedFileSummaries, Set countedAbortLsnNodes) {
	UtilizationTracker tracker = env.getUtilizationTracker();
	Long logFileNum = new Long(DbLsn.getFileNumber(logLsn));
	long fileSummaryLsn = DbLsn.longToLsn((Long) fileSummaryLsns.get(logFileNum));
	int cmpFsLsnToLogLsn = (fileSummaryLsn != DbLsn.NULL_LSN) ? DbLsn.compareTo(fileSummaryLsn, logLsn) : -1;
	if (cmpFsLsnToLogLsn < 0) {
	    tracker.countObsoleteNode(logLsn, null);
	}
	if (cmpFsLsnToLogLsn > 0) {
	    Long countedFile = (Long) countedFileSummaries.get(txnNodeId);
	    if (countedFile == null || countedFile.longValue() > logFileNum.longValue()) {
		if (!ln.isDeleted()) {
		    tracker.countObsoleteNode(logLsn, null);
		}
		countedFileSummaries.put(txnNodeId, logFileNum);
	    }
	}
    }

    /** 
     * Concoct a header for the recovery pass trace info.
     */
    private String passStartHeader(int passNum) {
	return "Recovery Pass " + passNum + " start: ";
    }

    /** 
     * Concoct a header for the recovery pass trace info.
     */
    private String passEndHeader(int passNum, long start, long end) {
	return "Recovery Pass " + passNum + " end (" + (end - start) + "): ";
    }

    /** 
     * Send trace messages to the java.util.logger. Don't rely on the logger alone to conditionalize whether we send this message, we don't even want to construct the message if the level is not enabled. This is used to construct verbose trace messages for individual log entry processing.
     */
    private static void trace(Level level, DatabaseImpl database, String debugType, boolean success, Node node,
	    long logLsn, IN parent, boolean found, boolean replaced, boolean inserted, long replacedLsn, long abortLsn,
	    int index) {
	new RecoveryManager_trace(level, database, debugType, success, node, logLsn, parent, found, replaced, inserted,
		replacedLsn, abortLsn, index).execute();
    }

    private void traceAndThrowException(long badLsn, String method, Exception origException) throws DatabaseException {
	String badLsnString = DbLsn.getNoFormatString(badLsn);
	Label577:           ;  //this.hook577(method, origException, badLsnString);
	throw new DatabaseException("last LSN=" + badLsnString, origException);
    }
/*
    protected void hook556() throws DatabaseException, IOException {
    }

    protected void hook557(DatabaseImpl db) throws DatabaseException {
    }

    protected void hook558() throws DatabaseException, IOException {
    }

    protected void hook559() throws DatabaseException, IOException {
    }

    protected void hook560() throws DatabaseException, IOException {
    }

    protected void hook561(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook562(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook563() throws IOException, DatabaseException {
    }

    protected void hook564(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook565(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook566(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook567(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook568(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook569(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook570(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook571(long start, long end) throws IOException, DatabaseException {
    }

    protected void hook572() throws IOException, DatabaseException {
    }

    protected void hook573() throws DatabaseException, IOException {
    }

    protected void hook574(LNFileReader reader) throws IOException, DatabaseException, Exception {
    }

    protected void hook575(IOException e) throws DatabaseException {
    }

    protected void hook576(DatabaseImpl db, long logLsn, Exception e, String trace) throws DatabaseException {
    }

    protected void hook577(String method, Exception origException, String badLsnString) throws DatabaseException {
    }

    protected void hook578(EnvironmentImpl env) throws DatabaseException {
    }

    protected void hook579(long nodeId, boolean containsDuplicates, long logLsn, boolean found, boolean deleted,
	    SearchResult result) throws DatabaseException {
    }

    protected void hook580(DatabaseImpl db, IN inFromLog, long lsn, boolean success, RootUpdater rootUpdater)
	    throws DatabaseException {
    }

    protected void hook581(DatabaseImpl db, DIN inFromLog, long lsn, boolean found, boolean inserted, boolean replaced,
	    long origLsn, IN parent, int index, boolean success) throws DatabaseException {
    }

    protected void hook582(DatabaseImpl db, IN inFromLog, long logLsn, boolean inserted, boolean replaced, long origLsn,
	    boolean success, SearchResult result) throws DatabaseException {
    }

    protected void hook583(DatabaseImpl db, TreeLocation location, LN lnFromLog, long logLsn, boolean found,
	    boolean replaced, boolean inserted, boolean success) throws DatabaseException {
    }

    protected static void hook584(Level traceLevel, DatabaseImpl db, TreeLocation location, LN lnFromLog,
	    byte[] mainKey, byte[] dupKey, long logLsn, long abortLsn, boolean abortKnownDeleted, RecoveryInfo info,
	    boolean splitsAllowed, boolean found, boolean replaced, boolean success) throws DatabaseException {
	  }
*/
  //  protected void hook585(IN in) throws DatabaseException {
    //}

/*
    protected void hook586(RecoveryInfo info, LNFileReader reader, TreeLocation location, LN ln, long logLsn,
	    long abortLsn, boolean abortKnownDeleted, DatabaseImpl db)
	    throws IOException, DatabaseException, Exception {
	undo(detailedTraceLevel, db, location, ln, reader.getKey(), reader.getDupTreeKey(), logLsn, abortLsn,
		abortKnownDeleted, info, true);
    }
*/
  //  protected void hook587(IN inFromLog, long logLsn) throws DatabaseException {
   // }

//    protected void hook588(SearchResult result) throws DatabaseException {
 //   }

  //  protected void hook589(IN parent) throws DatabaseException {
  //  }

 //   protected void hook590(SearchResult result) throws DatabaseException {
  //  }

    //protected void hook591(TreeLocation location) throws DatabaseException {
   // }

/*    protected static boolean hook592(TreeLocation location, long logLsn, long abortLsn, boolean replaced,
	    DIN duplicateRoot) throws DatabaseException {
			if (DbLsn.compareTo(logLsn, location.childLsn) == 0) {
					duplicateRoot.updateDupCountLNRefAndNullTarget(abortLsn);
					replaced = true;
			}
			return replaced;
    }
*/
    //protected void hook593(INFileReader reader) throws IOException, DatabaseException {
    //}

//    protected void hook594(DatabaseImpl db, TreeLocation location, byte[] deletedKey) throws DatabaseException {   }

//    protected static void hook595(DatabaseImpl db, TreeLocation location, byte[] deletedKey) throws DatabaseException {  }

  //  protected void hook596() throws IOException, DatabaseException {
  //  }

  //  protected void hook597() throws IOException, DatabaseException, Exception {
  //  }

  //  protected void hook598() throws IOException, DatabaseException, Exception {
  //  }

}
\00after if lock couldn't be obtained. 0means block indefinitely. Not used if nonBlockingRequest is true.
     * @param nonBlockingRequestif true, means don't block if lock can't be acquired, andignore the timeout parameter.
     * @return a LockGrantType indicating whether the request was fulfilled ornot. LockGrantType.NEW means the lock grant was fulfilled and the caller did not previously hold the lock. PROMOTION means the lock was granted and it was a promotion from READ to WRITE. EXISTING means the lock was already granted (not a promotion). DENIED means the lock was not granted either because the timeout passed without acquiring the lock or timeout was -1 and the lock was not immediately available.
     * @throws DeadlockExceptionif acquiring the lock would result in a deadlock.
     */
    public LockGrantType lock(long nodeId, Locker locker, LockType type, long timeout, boolean nonBlockingRequest,
	    DatabaseImpl database) throws DeadlockException, DatabaseException {
	assert timeout >= 0;
	synchronized (locker) {
	    Long nid = new Long(nodeId);
	    LockAttemptResult result = attemptLock(nid, locker, type, nonBlockingRequest);
	    if (result.success || result.lockGrant == LockGrantType.DENIED) {
		return result.lockGrant;
	    }
	    Label772: //this.hook772(nonBlockingRequest);
	    assert !nonBlockingRequest;
	    try {
		boolean doWait = true;
		if (locker.isTimedOut()) {
		    if (validateOwnership(nid, locker, type, true, memoryBudget)) {
			doWait = false;
		    } else {
			String errMsg = makeTimeoutMsg("Transaction", locker, nodeId, type, result.lockGrant,
				result.useLock, locker.getTxnTimeOut(), locker.getTxnStartMillis(),
				System.currentTimeMillis(), database);
			throw new DeadlockException(errMsg);
		    }
		}
		boolean keepTime = (timeout > 0);
		long startTime = (keepTime ? System.currentTimeMillis() : 0);
		while (doWait) {
		    locker.setWaitingFor(result.useLock);
		    try {
			locker.wait(timeout);
		    } catch (InterruptedException IE) {
			throw new RunRecoveryException(envImpl, IE);
		    }
		    boolean lockerTimedOut = locker.isTimedOut();
		    long now = System.currentTimeMillis();
		    boolean thisLockTimedOut = (keepTime && (now - startTime > timeout));
		    boolean isRestart = (result.lockGrant == LockGrantType.WAIT_RESTART);
		    if (validateOwnership(nid, locker, type, lockerTimedOut || thisLockTimedOut || isRestart,
			    memoryBudget)) {
			break;
		    } else {
			if (isRestart) {
			    throw rangeRestartException;
			}
			if (thisLockTimedOut) {
			    locker.setOnlyAbortable();
			    String errMsg = makeTimeoutMsg("Lock", locker, nodeId, type, result.lockGrant,
				    result.useLock, timeout, startTime, now, database);
			    throw new DeadlockException(errMsg);
			}
			if (lockerTimedOut) {
			    locker.setOnlyAbortable();
			    String errMsg = makeTimeoutMsg("Transaction", locker, nodeId, type, result.lockGrant,
				    result.useLock, locker.getTxnTimeOut(), locker.getTxnStartMillis(), now, database);
			    throw new DeadlockException(errMsg);
			}
		    }
		}
	    } finally {
		locker.setWaitingFor(null);
		assert EnvironmentImpl.maybeForceYield();
	    }
	    locker.addLock(nid, result.useLock, type, result.lockGrant);
	    return result.lockGrant;
	}
    }

    abstract protected LockAttemptResult attemptLock(Long nodeId, Locker locker, LockType type,
	    boolean nonBlockingRequest) throws DatabaseException;

    protected LockAttemptResult attemptLockInternal(Long nodeId, Locker locker, LockType type,
	    boolean nonBlockingRequest, int lockTableIndex) throws DatabaseException {
			Map lockTable = lockTables[lockTableIndex];
			Lock useLock = (Lock) lockTable.get(nodeId);
			if (useLock == null) {
					useLock = new Lock(nodeId);
					lockTable.put(nodeId, useLock);
					//this.hook780(lockTableIndex);
				  Label780:
			}
			LockGrantType lockGrant = useLock.lock(type, locker, nonBlockingRequest, memoryBudget, lockTableIndex);
			boolean success = false;
			if ((lockGrant == LockGrantType.NEW) || (lockGrant == LockGrantType.PROMOTION)) {
					locker.addLock(nodeId, useLock, type, lockGrant);
					success = true;
			} else if (lockGrant == LockGrantType.EXISTING) {
					success = true;
			} else if (lockGrant == LockGrantType.DENIED) {
			} else {
					Label775: //this.hook775();
			}
			return new LockAttemptResult(useLock, lockGrant, success);
    }

    /** 
     * Create a informative lock or txn timeout message.
     */
    protected abstract String makeTimeoutMsg(String lockOrTxn, Locker locker, long nodeId, LockType type,
	    LockGrantType grantType, Lock useLock, long timeout, long start, long now, DatabaseImpl database)
	    throws DatabaseException;

    /** 
     * Do the real work of creating an lock or txn timeout message.
     */
    protected String makeTimeoutMsgInternal(String lockOrTxn, Locker locker, long nodeId, LockType type,
	    LockGrantType grantType, Lock useLock, long timeout, long start, long now, DatabaseImpl database) {
	if (lockTableDump) {
	    System.out.println("++++++++++ begin lock table dump ++++++++++");
	    for (int i = 0; i < nLockTables; i++) {
		StringBuffer sb = new StringBuffer();
		dumpToStringNoLatch(sb, i);
		System.out.println(sb.toString());
	    }
	    System.out.println("++++++++++ end lock table dump ++++++++++");
	}
	StringBuffer sb = new StringBuffer();
	sb.append(lockOrTxn);
	sb.append(" expired. Locker ").append(locker);
	sb.append(": waited for lock");
	if (database != null) {
	    sb.append(" on database=").append(database.getDebugName());
	}
	sb.append(" node=").append(nodeId);
	sb.append(" type=").append(type);
	sb.append(" grant=").append(grantType);
	sb.append(" timeoutMillis=").append(timeout);
	sb.append(" startTime=").append(start);
	sb.append(" endTime=").append(now);
	sb.append("\nOwners: ").append(useLock.getOwnersClone());
	sb.append("\nWaiters: ").append(useLock.getWaitersListClone()).append("\n");
	StringBuffer deadlockInfo = findDeadlock(useLock, locker);
	if (deadlockInfo != null) {
	    sb.append(deadlockInfo);
	}
	return sb.toString();
    }

    /** 
     * Release a lock and possibly notify any waiters that they have been granted the lock.
     * @param nodeIdThe node ID of the lock to release.
     * @return true if the lock is released successfully, false if the lock isnot currently being held.
     */
    boolean release(long nodeId, Locker locker) throws DatabaseException {
	return release(nodeId, null, locker, true);
    }

    /** 
     * Release a lock and possibly notify any waiters that they have been granted the lock.
     * @param lockThe lock to release
     * @return true if the lock is released successfully, false if the lock isnot currently being held.
     */
    boolean release(Lock lock, Locker locker) throws DatabaseException {
	return release(-1, lock, locker, false);
    }

    /** 
     * Do the work of releasing a lock and notifying any waiters that they have been granted the lock.
     * @param lockThe lock to release. If null, use nodeId to find lock
     * @param nodeIdThe node ID of the lock to release, if lock is null. May notbe valid if lock is not null. MUST be valid if removeFromLocker is true
     * @param locker
     * @param removeFromLockertrue if we're responsible for
     * @return true if the lock is released successfully, false if the lock isnot currently being held.
     */
    private boolean release(long nodeId, Lock lock, Locker locker, boolean removeFromLocker) throws DatabaseException {
	synchronized (locker) {
	    Set newOwners = releaseAndFindNotifyTargets(nodeId, lock, locker, removeFromLocker);
	    if (newOwners == null) {
		return false;
	    }
	    if (newOwners.size() > 0) {
		Iterator iter = newOwners.iterator();
		while (iter.hasNext()) {
		    Locker lockerToNotify = (Locker) iter.next();
		    synchronized (lockerToNotify) {
			lockerToNotify.notifyAll();
		    }
		    assert EnvironmentImpl.maybeForceYield();
		}
	    }
	    return true;
	}
    }

    /** 
     * Release the lock, and return the set of new owners to notify, if any.
     * @return null if the lock does not exist or the given locker was not theowner, a non-empty set if owners should be notified after releasing, an empty set if no notification is required.
     */
    protected abstract Set releaseAndFindNotifyTargets(long nodeId, Lock lock, Locker locker, boolean removeFromLocker)
	    throws DatabaseException;

    /** 
     * Do the real work of releaseAndFindNotifyTargets
     */
    protected Set releaseAndFindNotifyTargetsInternal(long nodeId, Lock lock, Locker locker, boolean removeFromLocker,
	    int lockTableIndex) throws DatabaseException {
	Lock useLock = lock;
	Map lockTable = lockTables[lockTableIndex];
	if (useLock == null) {
	    useLock = (Lock) lockTable.get(new Long(nodeId));
	}
	if (useLock == null) {
	    return null;
	}
	Set lockersToNotify = useLock.release(locker, memoryBudget, lockTableIndex);
	if (lockersToNotify == null) {
	    return null;
	}
	if (removeFromLocker) {
	    assert nodeId != -1;
	    locker.removeLock(nodeId, useLock);
	}
	if ((useLock.nWaiters() == 0) && (useLock.nOwners() == 0)) {
	    lockTables[lockTableIndex].remove(useLock.getNodeId());
	    //this.hook781(lockTableIndex);
      Label781:
	}
	return lockersToNotify;
    }

    /** 
     * Transfer ownership a lock from one locker to another locker. We're not sending any notification to the waiters on the lock table, and the past and present owner should be ready for the transfer.
     */
    abstract void transfer(long nodeId, Locker owningLocker, Locker destLocker, boolean demoteToRead)
	    throws DatabaseException;

    /** 
     * Do the real work of transfer
     */
    protected void transferInternal(long nodeId, Locker owningLocker, Locker destLocker, boolean demoteToRead,
	    int lockTableIndex) throws DatabaseException {
	Map lockTable = lockTables[lockTableIndex];
	Lock useLock = (Lock) lockTable.get(new Long(nodeId));
	assert useLock != null : "Transfer, lock " + nodeId + " was null";
	if (demoteToRead) {
	    useLock.demote(owningLocker);
	}
	useLock.transfer(owningLocker, destLocker, memoryBudget, lockTableIndex);
	owningLocker.removeLock(nodeId, useLock);
    }

    /** 
     * Transfer ownership a lock from one locker to a set of other txns, cloning the lock as necessary. This will always be demoted to read, as we can't have multiple locker owners any other way. We're not sending any notification to the waiters on the lock table, and the past and present owners should be ready for the transfer.
     */
    abstract void transferMultiple(long nodeId, Locker owningLocker, Locker[] destLockers) throws DatabaseException;

    /** 
     * Do the real work of transferMultiple
     */
    protected void transferMultipleInternal(long nodeId, Locker owningLocker, Locker[] destLockers, int lockTableIndex)
	    throws DatabaseException {
	Map lockTable = lockTables[lockTableIndex];
	Lock useLock = (Lock) lockTable.get(new Long(nodeId));
	assert useLock != null : "Transfer, lock " + nodeId + " was null";
	useLock.demote(owningLocker);
	useLock.transferMultiple(owningLocker, destLockers, memoryBudget, lockTableIndex);
	owningLocker.removeLock(nodeId, useLock);
    }

    /** 
     * Demote a lock from write to read. Call back to the owning locker to move this to its read collection.
     * @param lockThe lock to release. If null, use nodeId to find lock
     * @param locker
     */
    abstract void demote(long nodeId, Locker locker) throws DatabaseException;

    /** 
     * Do the real work of demote.
     */
    protected void demoteInternal(long nodeId, Locker locker, int lockTableIndex) throws DatabaseException {
	Map lockTable = lockTables[lockTableIndex];
	Lock useLock = (Lock) lockTable.get(new Long(nodeId));
	useLock.demote(locker);
	locker.moveWriteToReadLock(nodeId, useLock);
    }

    /** 
     * Test the status of the lock on nodeId. If any transaction holds any lock on it, true is returned. If no transaction holds a lock on it, false is returned. This method is only used by unit tests.
     * @param nodeIdThe NodeId to check.
     * @return true if any transaction holds any lock on the nodeid. false if nolock is held by any transaction.
     */
    abstract boolean isLocked(Long nodeId) throws DatabaseException;

    /** 
     * Do the real work of isLocked.
     */
    protected boolean isLockedInternal(Long nodeId, int lockTableIndex) {
	Map lockTable = lockTables[lockTableIndex];
	Lock entry = (Lock) lockTable.get(nodeId);
	if (entry == null) {
	    return false;
	}
	return entry.nOwners() != 0;
    }

    /** 
     * Return true if this locker owns this a lock of this type on given node. This method is only used by unit tests.
     */
    abstract boolean isOwner(Long nodeId, Locker locker, LockType type) throws DatabaseException;

    /** 
     * Do the real work of isOwner.
     */
    protected boolean isOwnerInternal(Long nodeId, Locker locker, LockType type, int lockTableIndex) {
	Map lockTable = lockTables[lockTableIndex];
	Lock entry = (Lock) lockTable.get(nodeId);
	if (entry == null) {
	    return false;
	}
	return entry.isOwner(locker, type);
    }

    /** 
     * Return true if this locker is waiting on this lock. This method is only used by unit tests.
     */
    abstract boolean isWaiter(Long nodeId, Locker locker) throws DatabaseException;

    /** 
     * Do the real work of isWaiter.
     */
    protected boolean isWaiterInternal(Long nodeId, Locker locker, int lockTableIndex) {
	Map lockTable = lockTables[lockTableIndex];
	Lock entry = (Lock) lockTable.get(nodeId);
	if (entry == null) {
	    return false;
	}
	return entry.isWaiter(locker);
    }

    /** 
     * Return the number of waiters for this lock.
     */
    abstract int nWaiters(Long nodeId) throws DatabaseException;

    /** 
     * Do the real work of nWaiters.
     */
    protected int nWaitersInternal(Long nodeId, int lockTableIndex) {
	Map lockTable = lockTables[lockTableIndex];
	Lock entry = (Lock) lockTable.get(nodeId);
	if (entry == null) {
	    return -1;
	}
	return entry.nWaiters();
    }

    /** 
     * Return the number of owners of this lock.
     */
    abstract int nOwners(Long nodeId) throws DatabaseException;

    /** 
     * Do the real work of nWaiters.
     */
    protected int nOwnersInternal(Long nodeId, int lockTableIndex) {
	Map lockTable = lockTables[lockTableIndex];
	Lock entry = (Lock) lockTable.get(nodeId);
	if (entry == null) {
	    return -1;
	}
	return entry.nOwners();
    }

    /** 
     * @return the transaction that owns the write lock for this
     */
    abstract Locker getWriteOwnerLocker(Long nodeId) throws DatabaseException;

    /** 
     * Do the real work of getWriteOwnerLocker.
     */
    protected Locker getWriteOwnerLockerInternal(Long nodeId, int lockTableIndex) throws DatabaseException {
	Map lockTable = lockTables[lockTableIndex];
	Lock lock = (Lock) lockTable.get(nodeId);
	if (lock == null) {
	    return null;
	} else if (lock.nOwners() > 1) {
	    return null;
	} else {
	    return lock.getWriteOwnerLocker();
	}
    }

    abstract protected boolean validateOwnership(Long nodeId, Locker locker, LockType type, boolean flushFromWaiters,
	    MemoryBudget mb) throws DatabaseException;

    protected boolean validateOwnershipInternal(Long nodeId, Locker locker, LockType type, boolean flushFromWaiters,
	    MemoryBudget mb, int lockTableIndex) throws DatabaseException {
	if (isOwnerInternal(nodeId, locker, type, lockTableIndex)) {
	    return true;
	}
	if (flushFromWaiters) {
	    Lock entry = (Lock) lockTables[lockTableIndex].get(nodeId);
	    if (entry != null) {
		entry.flushWaiter(locker, mb, lockTableIndex);
	    }
	}
	return false;
    }

    /** 
     * Dump the lock table to the lock stats.
     */
    abstract protected void dumpLockTable(LockStats stats) throws DatabaseException;

    /** 
     * Do the real work of dumpLockTableInternal.
     */
    protected void dumpLockTableInternal(LockStats stats, int i) {
			Map lockTable = lockTables[i];
			Label776: //	this.hook776(stats, lockTable);
			Iterator iter = lockTable.values().iterator();
			while (iter.hasNext()) {
					Lock lock = (Lock) iter.next();
					Label777: //this.hook777(stats, lock);
					Iterator ownerIter = lock.getOwnersClone().iterator();
					while (ownerIter.hasNext()) {
					LockInfo info = (LockInfo) ownerIter.next();
					Label778: //this.hook778(stats, info);
					}
			}
    }

    /** 
     * Debugging
     */
    public void dump() throws DatabaseException {
	System.out.println(dumpToString());
    }

    public String dumpToString() throws DatabaseException {
	StringBuffer sb = new StringBuffer();
	for (int i = 0; i < nLockTables; i++) {
	    Label773: //this.hook773(sb, i);	
			try {
					dumpToStringNoLatch(sb, i);//original(sb, i);
			} finally {
					Label773_1: ; //
			}
			//end hook773
	}
	return sb.toString();
    }

    private void dumpToStringNoLatch(StringBuffer sb, int whichTable) {
	Map lockTable = lockTables[whichTable];
	Iterator entries = lockTable.entrySet().iterator();
	while (entries.hasNext()) {
	    Map.Entry entry = (Map.Entry) entries.next();
	    Long nid = (Long) entry.getKey();
	    Lock lock = (Lock) entry.getValue();
	    sb.append("---- Node Id: ").append(nid).append("----\n");
	    sb.append(lock);
	    sb.append('\n');
	}
    }

    private StringBuffer findDeadlock(Lock lock, Locker rootLocker) {
	Set ownerSet = new HashSet();
	ownerSet.add(rootLocker);
	StringBuffer ret = findDeadlock1(ownerSet, lock, rootLocker);
	if (ret != null) {
	    return ret;
	} else {
	    return null;
	}
    }

    private StringBuffer findDeadlock1(Set ownerSet, Lock lock, Locker rootLocker) {
	Iterator ownerIter = lock.getOwnersClone().iterator();
	while (ownerIter.hasNext()) {
	    LockInfo info = (LockInfo) ownerIter.next();
	    Locker locker = info.getLocker();
	    Lock waitsFor = locker.getWaitingFor();
	    if (ownerSet.contains(locker) || locker == rootLocker) {
		StringBuffer ret = new StringBuffer();
		ret.append("Transaction ").append(locker.toString());
		ret.append(" owns ").append(lock.getNodeId());
		ret.append(" ").append(info).append("\n");
		ret.append("Transaction ").append(locker.toString());
		ret.append(" waits for ");
		if (waitsFor == null) {
		    ret.append(" nothing");
		} else {
		    ret.append(" node ");
		    ret.append(waitsFor.getNodeId());
		}
		ret.append("\n");
		return ret;
	    }
	    if (waitsFor != null) {
		ownerSet.add(locker);
		StringBuffer sb = findDeadlock1(ownerSet, waitsFor, rootLocker);
		if (sb != null) {
		    String waitInfo = "Transaction " + locker + " waits for node " + waitsFor.getNodeId() + "\n";
		    sb.insert(0, waitInfo);
		    return sb;
		}
		ownerSet.remove(locker);
	    }
	}
	return null;
    }

    /** 
     * This is just a struct to hold a multi-value return.
     */
  //  protected void hook770() throws DatabaseException {
  //  }

 //   protected void hook771(EnvironmentImpl envImpl, int i) throws DatabaseException {
   // }

   // protected void hook772(boolean nonBlockingRequest) throws DeadlockException, DatabaseException {
   // }

//    protected void hook773(StringBuffer sb, int i) throws DatabaseException {
//	dumpToStringNoLatch(sb, i);
  //  }

  //  protected void hook774() throws DatabaseException {
  //  }

    protected void hook775() throws DatabaseException {
    }

    protected void hook776(LockStats stats, Map lockTable) {
    }

    protected void hook777(LockStats stats, Lock lock) {
    }

    protected void hook778(LockStats stats, LockInfo info) {
    }

    //protected void hook779(DbConfigManager configMgr) throws DatabaseException {
    //}

    protected void hook780(int lockTableIndex) throws DatabaseException {
    }

  //  protected void hook781(int lockTableIndex) throws DatabaseException {
  //  }

}
\00after recovery.
     */
    synchronized public void setCheckpointId(long lastCheckpointId) {
        checkpointId = lastCheckpointId;
    }

    /** 
     * @return the first active LSN point of the last completed checkpoint. Ifno checkpoint has run, return null.
     */
    public long getFirstActiveLsn() {
        return lastFirstActiveLsn;
    }

    /** 
     * Initialize the FirstActiveLsn during recovery. The cleaner needs this.
     */
    public void setFirstActiveLsn(long lastFirstActiveLsn) {
        this.lastFirstActiveLsn = lastFirstActiveLsn;
    }

    /** 
     * Determine whether a checkpoint should be run. 1. If the force parameter is specified, always checkpoint. 2. If the config object specifies time or log size, use that. 3. If the environment is configured to use log size based checkpointing, check the log. 4. Lastly, use time based checking.
     */
    private boolean isRunnable(CheckpointConfig config) throws DatabaseException {
        return new Checkpointer_isRunnable(this, config).execute();
    }

    /** 
     * The real work to do a checkpoint. This may be called by the checkpoint thread when waking up, or it may be invoked programatically through the api.
     * @param allowDeltasif true, this checkpoint may opt to log BIN deltas instead ofthe full node.
     * @param flushAllif true, this checkpoint must flush all the way to the top ofthe dbtree, instead of stopping at the highest level last modified.
     * @param invokingSourcea debug aid, to indicate who invoked this checkpoint. (i.e.recovery, the checkpointer daemon, the cleaner, programatically)
     */
    public synchronized void doCheckpoint(CheckpointConfig config, boolean flushAll, String invokingSource)
    throws DatabaseException {
        new Checkpointer_doCheckpoint(this, config, flushAll, invokingSource).execute();
    }

    /** 
     * Flush a FileSummaryLN node for each TrackedFileSummary that is currently active. Tell the UtilizationProfile about the updated file summary.
     */
    private void flushUtilizationInfo() throws DatabaseException {
        if (!DbInternal.getCheckpointUP(envImpl.getConfigManager().getEnvironmentConfig())) {
            return;
        }
        UtilizationProfile profile = envImpl.getUtilizationProfile();
        TrackedFileSummary[] activeFiles = envImpl.getUtilizationTracker().getTrackedFiles();
        for (int i = 0; i < activeFiles.length; i += 1) {
            profile.flushFileSummary(activeFiles[i]);
        }
    }

    /** 
     * Flush the nodes in order, from the lowest level to highest level. As a flush dirties its parent, add it to the dirty map, thereby cascading the writes up the tree. If flushAll wasn't specified, we need only cascade up to the highest level set at the start of checkpointing. Note that all but the top level INs and the BINDeltas are logged provisionally. That's because we don't need to process lower INs because the higher INs will end up pointing at them.
     */
    private void flushDirtyNodes(SortedMap dirtyMap, boolean flushAll, boolean allowDeltas, boolean flushExtraLevel,
        long checkpointStart) throws DatabaseException {
        while (dirtyMap.size() > 0) {
            Integer currentLevel = (Integer) dirtyMap.firstKey();
            boolean logProvisionally = (currentLevel.intValue() != highestFlushLevel);
            Set nodeSet = (Set) dirtyMap.get(currentLevel);
            Iterator iter = nodeSet.iterator();
            while (iter.hasNext()) {
                CheckpointReference targetRef = (CheckpointReference) iter.next();
                Label520: //this.hook520();
                //this.hook546(dirtyMap, allowDeltas, checkpointStart, currentLevel, logProvisionally, targetRef);
                Label546:
                    iter.remove();
            }
            dirtyMap.remove(currentLevel);
            if (currentLevel.intValue() == highestFlushLevel) {
                break;
            }
        }
    }

    /** 
     * Scan the INList for all dirty INs. Arrange them in level sorted map for level ordered flushing.
     */
    private SortedMap selectDirtyINs(boolean flushAll, boolean flushExtraLevel) throws DatabaseException {
        return new Checkpointer_selectDirtyINs(this, flushAll, flushExtraLevel).execute();
    }

    /** 
     * Flush the target IN.
     */
    private void flushIN(CheckpointReference targetRef, Map dirtyMap, int currentLevel, boolean logProvisionally,
        boolean allowDeltas, long checkpointStart) throws DatabaseException {
        Tree tree = targetRef.db.getTree();
        boolean targetWasRoot = false;
        if (targetRef.isDbRoot) {
            RootFlusher flusher = new RootFlusher(targetRef.db, logManager, targetRef.nodeId);
            tree.withRootLatchedExclusive(flusher);
            boolean flushed = flusher.getFlushed();
            targetWasRoot = flusher.stillRoot();
            if (flushed) {
                DbTree dbTree = targetRef.db.getDbEnvironment().getDbMapTree();
                dbTree.modifyDbRoot(targetRef.db);
                Label532: //this.hook532();
            }
        }
        if (!targetWasRoot) {
            SearchResult result = tree.getParentINForChildIN(targetRef.nodeId, targetRef.containsDuplicates, false,
                targetRef.mainTreeKey, targetRef.dupTreeKey, false, false, -1, null, false);
            if (result.parent != null) {
                boolean mustLogParent = false;
                //this.hook526(targetRef, dirtyMap, currentLevel, logProvisionally, allowDeltas, checkpointStart, tree, result, mustLogParent);
                try {
                    if (result.exactParentFound) {
                        IN renewedTarget = (IN) result.parent.getTarget(result.index);
                        if (renewedTarget == null) {
                            mustLogParent = true;
                        } else {
                            mustLogParent = logTargetAndUpdateParent(renewedTarget, result.parent, result.index, allowDeltas,
                                checkpointStart, logProvisionally);
                        }
                    } else {
                        if (result.childNotResident) {
                            if (result.parent.getLevel() > currentLevel) {
                                mustLogParent = true;
                            }
                        }
                    }
                    if (mustLogParent) {
                        assert checkParentChildRelationship(result, currentLevel): dumpParentChildInfo(result, result.parent,
                            targetRef.nodeId, currentLevel, tree);
                        addToDirtyMap(dirtyMap, result.parent);
                    }
                } finally {
                    Label526: //;
                }
            }
        }
    }

    /** 
     * @return true if this parent is appropriately 1 level above the child.
     */
    private boolean checkParentChildRelationship(SearchResult result, int childLevel) {
        if (result.childNotResident && !result.exactParentFound) {
            return true;
        }
        int parentLevel = result.parent.getLevel();
        boolean isMapTree = (childLevel & IN.DBMAP_LEVEL) != 0;
        boolean isMainTree = (childLevel & IN.MAIN_LEVEL) != 0;
        boolean checkOk = false;
        if (isMapTree || isMainTree) {
            if (parentLevel == (childLevel + 1)) {
                checkOk = true;
            }
        } else {
            if (childLevel == 1) {
                if (parentLevel == 2) {
                    checkOk = true;
                }
            } else {
                if ((parentLevel == IN.BIN_LEVEL) || (parentLevel == childLevel + 1)) {
                    checkOk = true;
                }
            }
        }
        return checkOk;
    }

    private String dumpParentChildInfo(SearchResult result, IN parent, long childNodeId, int currentLevel, Tree tree)
    throws DatabaseException {
        StringBuffer sb = new StringBuffer();
        sb.append("ckptId=").append(checkpointId);
        sb.append(" result=").append(result);
        sb.append(" parent node=").append(parent.getNodeId());
        sb.append(" level=").append(parent.getLevel());
        sb.append(" child node=").append(childNodeId);
        sb.append(" level=").append(currentLevel);
        return sb.toString();
    }

    private boolean logTargetAndUpdateParent(IN target, IN parent, int index, boolean allowDeltas, long checkpointStart,
        boolean logProvisionally) throws DatabaseException {
        target.latch(false);
        long newLsn = DbLsn.NULL_LSN;
        boolean mustLogParent = true;
        Label527: //this.hook527(target, parent, allowDeltas, checkpointStart, logProvisionally, newLsn, mustLogParent);
            if (target.getDirty()) {
                newLsn = target.log(logManager, allowDeltas, logProvisionally, true, parent);
                if (allowDeltas && newLsn == DbLsn.NULL_LSN) {
                    Label537: //this.hook537();
                        long lastFullLsn = target.getLastFullVersion();
                    if (DbLsn.compareTo(lastFullLsn, checkpointStart) < 0) {
                        mustLogParent = false;
                    }
                }
            }
        //End hook527
        Label527_1: //;
        if (newLsn != DbLsn.NULL_LSN) {
            Label533: //this.hook533(target);
                parent.updateEntry(index, newLsn);
        }
        return mustLogParent;
    }

    /** 
     * Add a node to the dirty map. The dirty map is keyed by level (Integers) and holds sets of IN references.
     */
    private void addToDirtyMap(Map dirtyMap, IN in ) {
        Integer inLevel = new Integer( in .getLevel());
        Set inSet = (Set) dirtyMap.get(inLevel);
        if (inSet == null) {
            inSet = new HashSet();
            dirtyMap.put(inLevel, inSet);
        }
        inSet.add(new CheckpointReference( in .getDatabase(), in .getNodeId(), in .containsDuplicates(), in .isDbRoot(), in .getMainTreeKey(), in .getDupTreeKey()));
    }

  //  protected void hook520() throws DatabaseException {}

 //   protected void hook526(CheckpointReference targetRef, Map dirtyMap, int currentLevel, boolean logProvisionally,
 //       boolean allowDeltas, long checkpointStart, Tree tree, SearchResult result, boolean mustLogParent)
  //  throws DatabaseException {

  //      if (result.exactParentFound) {
    //        IN renewedTarget = (IN) result.parent.getTarget(result.index);
      //      if (renewedTarget == null) {
        //        mustLogParent = true;
//            } else {
  //              mustLogParent = logTargetAndUpdateParent(renewedTarget, result.parent, result.index, allowDeltas,
    //                checkpointStart, logProvisionally);
      //      }
        //} else {
          //  if (result.childNotResident) {
            //    if (result.parent.getLevel() > currentLevel) {
              //      mustLogParent = true;
                //}
            //}
        //}
    //    if (mustLogParent) {
            //assert checkParentChildRelationship(result, currentLevel): dumpParentChildInfo(result, result.parent,
          //      targetRef.nodeId, currentLevel, tree);
        //    addToDirtyMap(dirtyMap, result.parent);
      //  }
    //}

    protected void hook527(IN target, IN parent, boolean allowDeltas, long checkpointStart, boolean logProvisionally,
        long newLsn, boolean mustLogParent) throws DatabaseException {

    }

    protected void hook531() throws DatabaseException {}

    protected void hook532() throws DatabaseException {}

    protected void hook533(IN target) throws DatabaseException {}

    //   protected void hook537() throws DatabaseException {
    //  }

    //  protected void hook538(EnvironmentImpl envImpl, long waitTime, String name) throws DatabaseException {
    //  }

    // protected void hook539(EnvironmentImpl envImpl) throws DatabaseException {
    // }

    protected void hook545(long waitTime) throws DatabaseException {}

    //   protected void hook546(SortedMap dirtyMap, boolean allowDeltas, long checkpointStart, Integer currentLevel,
    //	    boolean logProvisionally, CheckpointReference targetRef) throws DatabaseException {
    //   }

}
\00after instantiating the tree from the log.
     */
    void setEnvironmentImpl(EnvironmentImpl envImpl) throws DatabaseException {
	this.envImpl = envImpl;
	idDatabase.setEnvironmentImpl(envImpl);
	nameDatabase.setEnvironmentImpl(envImpl);
    }

    /** 
     * Create a database.
     */
    public synchronized DatabaseImpl createDb(Locker locker, String databaseName, DatabaseConfig dbConfig,
	    Database databaseHandle) throws DatabaseException {
	return createDb(locker, databaseName, dbConfig, databaseHandle, true);
    }

    /** 
     * Create a database.
     * @param lockerowning locker
     * @param databaseNameidentifier for database
     * @param dbConfig
     * @param allowEvictionis whether eviction is allowed during cursor operations.
     */
    public synchronized DatabaseImpl createDb(Locker locker, String databaseName, DatabaseConfig dbConfig,
	    Database databaseHandle, boolean allowEviction) throws DatabaseException {
	DatabaseId newId = new DatabaseId(getNextDbId());
	DatabaseImpl newDb = new DatabaseImpl(databaseName, newId, envImpl, dbConfig);
	CursorImpl idCursor = null;
	CursorImpl nameCursor = null;
	boolean operationOk = false;
	Locker autoTxn = null;
	try {
	    nameCursor = new CursorImpl(nameDatabase, locker);
	    //this.hook307(allowEviction, nameCursor);
      Label307:
	    LN nameLN = new NameLN(newId);
	    nameCursor.putLN(databaseName.getBytes("UTF-8"), nameLN, false);
	    if (databaseHandle != null) {
		locker.addToHandleMaps(new Long(nameLN.getNodeId()), databaseHandle);
	    }
	    autoTxn = createLocker(envImpl);
	    idCursor = new CursorImpl(idDatabase, autoTxn);
	    //this.hook306(allowEviction, idCursor);
      Label306:
	    idCursor.putLN(newId.getBytes(), new MapLN(newDb), false);
	    operationOk = true;
	} catch (UnsupportedEncodingException UEE) {
	    throw new DatabaseException(UEE);
	} finally {
	    if (idCursor != null) {
		idCursor.close();
	    }
	    if (nameCursor != null) {
		nameCursor.close();
	    }
	    if (autoTxn != null) {
		autoTxn.operationEnd(operationOk);
	    }
	}
	return newDb;
    }

    /** 
     * Called by the Tree to propagate a root change. If the tree is a data database, we will write the MapLn that represents this db to the log. If the tree is one of the mapping dbs, we'll write the dbtree to the log.
     * @param dbthe target db
     */
    public void modifyDbRoot(DatabaseImpl db) throws DatabaseException {
	if (db.getId().equals(ID_DB_ID) || db.getId().equals(NAME_DB_ID)) {
	    envImpl.logMapTreeRoot();
	} else {
	    Locker locker = createLocker(envImpl);
	    CursorImpl cursor = new CursorImpl(idDatabase, locker);
	    boolean operationOk = false;
	    try {
		DatabaseEntry keyDbt = new DatabaseEntry(db.getId().getBytes());
		MapLN mapLN = null;
		while (true) {
		    try {
			boolean searchOk = (cursor.searchAndPosition(keyDbt, new DatabaseEntry(), SearchMode.SET,
				LockType.WRITE) & CursorImpl.FOUND) != 0;
			if (!searchOk) {
			    throw new DatabaseException("can't find database " + db.getId());
			}
			mapLN = (MapLN) cursor.getCurrentLNAlreadyLatched(LockType.WRITE);
			assert mapLN != null;
		    } catch (DeadlockException DE) {
			cursor.close();
			locker.operationEnd(false);
			locker = createLocker(envImpl);
			cursor = new CursorImpl(idDatabase, locker);
			continue;
		    } finally {
			Label299: //this.hook299(cursor);
		    }
		    break;
		}
		RewriteMapLN writeMapLN = new RewriteMapLN(cursor);
		mapLN.getDatabase().getTree().withRootLatchedExclusive(writeMapLN);
		operationOk = true;
	    } finally {
		if (cursor != null) {
		    cursor.close();
		}
		locker.operationEnd(operationOk);
	    }
	}
    }

    private NameLockResult lockNameLN(Locker locker, String databaseName, String action) throws DatabaseException {
	NameLockResult result = new NameLockResult();
	result.dbImpl = getDb(locker, databaseName, null);
	if (result.dbImpl == null) {
	    throw new DatabaseNotFoundException("Attempted to " + action + " non-existent database " + databaseName);
	}
	result.nameCursor = new CursorImpl(nameDatabase, locker);
	try {
	    DatabaseEntry key = new DatabaseEntry(databaseName.getBytes("UTF-8"));
	    boolean found = (result.nameCursor.searchAndPosition(key, null, SearchMode.SET, LockType.WRITE)
		    & CursorImpl.FOUND) != 0;
	    if (!found) {
		Label300: //this.hook300(result);
		result.nameCursor.close();
		result.nameCursor = null;
		return result;
	    }
	    result.nameLN = (NameLN) result.nameCursor.getCurrentLNAlreadyLatched(LockType.WRITE);
	    assert result.nameLN != null;
	    int handleCount = result.dbImpl.getReferringHandleCount();
	    if (handleCount > 0) {
		throw new DatabaseException(
			"Can't " + action + " database " + databaseName + "," + handleCount + " open Dbs exist");
	    }
	} catch (UnsupportedEncodingException UEE) {
	    Label301: //this.hook301(result);
	    result.nameCursor.close();
	    throw new DatabaseException(UEE);
	} catch (DatabaseException e) {
	    Label302: //this.hook302(result);
	    result.nameCursor.close();
	    throw e;
	}
	return result;
    }

    void deleteMapLN(DatabaseId id) throws DatabaseException {
	Locker autoTxn = null;
	boolean operationOk = false;
	CursorImpl idCursor = null;
	try {
	    autoTxn = createLocker(envImpl);
	    idCursor = new CursorImpl(idDatabase, autoTxn);
	    boolean found = (idCursor.searchAndPosition(new DatabaseEntry(id.getBytes()), null, SearchMode.SET,
		    LockType.WRITE) & CursorImpl.FOUND) != 0;
	    if (found) {
		idCursor.delete();
	    }
	    operationOk = true;
	} finally {
	    if (idCursor != null) {
		idCursor.close();
	    }
	    if (autoTxn != null) {
		autoTxn.operationEnd(operationOk);
	    }
	}
    }

    /** 
     * Get a database object given a database name.
     */
    public DatabaseImpl getDb(Locker nameLocker, String databaseName, Database databaseHandle)
	    throws DatabaseException {
	return getDb(nameLocker, databaseName, databaseHandle, true);
    }

    /** 
     * Get a database object given a database name.
     * @param nameLockeris used to access the NameLN. As always, a NullTxn is used toaccess the MapLN.
     * @param databaseNametarget database
     * @return null if database doesn't exist
     * @param allowEvictionis whether eviction is allowed during cursor operations.
     */
    public DatabaseImpl getDb(Locker nameLocker, String databaseName, Database databaseHandle, boolean allowEviction)
	    throws DatabaseException {
	try {
	    CursorImpl nameCursor = null;
	    DatabaseId id = null;
	    try {
		nameCursor = new CursorImpl(nameDatabase, nameLocker);
		//this.hook308(allowEviction, nameCursor);
    Label308:
		DatabaseEntry keyDbt = new DatabaseEntry(databaseName.getBytes("UTF-8"));
		boolean found = (nameCursor.searchAndPosition(keyDbt, null, SearchMode.SET, LockType.READ)
			& CursorImpl.FOUND) != 0;
		if (found) {
		    NameLN nameLN = (NameLN) nameCursor.getCurrentLNAlreadyLatched(LockType.READ);
		    assert nameLN != null;
		    id = nameLN.getId();
		    if (databaseHandle != null) {
			nameLocker.addToHandleMaps(new Long(nameLN.getNodeId()), databaseHandle);
		    }
		}
	    } finally {
		if (nameCursor != null) {
		    Label303: //this.hook303(nameCursor);
		    nameCursor.close();
		}
	    }
	    if (id == null) {
		return null;
	    } else {
		return getDb(id, -1, allowEviction, databaseName);
	    }
	} catch (UnsupportedEncodingException UEE) {
	    throw new DatabaseException(UEE);
	}
    }

    /** 
     * Get a database object based on an id only. Used by recovery, cleaning and other clients who have an id in hand, and don't have a resident node, to find the matching database for a given log entry.
     */
    public DatabaseImpl getDb(DatabaseId dbId) throws DatabaseException {
	return getDb(dbId, -1);
    }

    /** 
     * Get a database object based on an id only. Specify the lock timeout to use, or -1 to use the default timeout. A timeout should normally only be specified by daemons with their own timeout configuration. public for unit tests.
     */
    public DatabaseImpl getDb(DatabaseId dbId, long lockTimeout) throws DatabaseException {
	return getDb(dbId, lockTimeout, true, null);
    }

    /** 
     * Get a database object based on an id only, caching the id-db mapping in the given map.
     */
    public DatabaseImpl getDb(DatabaseId dbId, long lockTimeout, Map dbCache) throws DatabaseException {
	if (dbCache.containsKey(dbId)) {
	    return (DatabaseImpl) dbCache.get(dbId);
	} else {
	    DatabaseImpl db = getDb(dbId, lockTimeout, true, null);
	    dbCache.put(dbId, db);
	    return db;
	}
    }

    /** 
     * Get a database object based on an id only. Specify the lock timeout to use, or -1 to use the default timeout. A timeout should normally only be specified by daemons with their own timeout configuration. public for unit tests.
     * @param allowEvictionis whether eviction is allowed during cursor operations.
     */
    public DatabaseImpl getDb(DatabaseId dbId, long lockTimeout, boolean allowEviction, String dbNameIfAvailable)
	    throws DatabaseException {
	if (dbId.equals(idDatabase.getId())) {
	    return idDatabase;
	} else if (dbId.equals(nameDatabase.getId())) {
	    return nameDatabase;
	} else {
	    Locker locker = new BasicLocker(envImpl);
	    if (lockTimeout != -1) {
		locker.setLockTimeout(lockTimeout);
	    }
	    CursorImpl idCursor = null;
	    DatabaseImpl foundDbImpl = null;
	    while (true) {
		idCursor = new CursorImpl(idDatabase, locker);
		//this.hook309(allowEviction, idCursor);
    Label309:
		try {
		    DatabaseEntry keyDbt = new DatabaseEntry(dbId.getBytes());
		    boolean found = (idCursor.searchAndPosition(keyDbt, new DatabaseEntry(), SearchMode.SET,
			    LockType.READ) & CursorImpl.FOUND) != 0;
		    if (found) {
			MapLN mapLN = (MapLN) idCursor.getCurrentLNAlreadyLatched(LockType.READ);
			assert mapLN != null;
			foundDbImpl = mapLN.getDatabase();
		    }
		    break;
		} catch (DeadlockException DE) {
		    idCursor.close();
		    locker.operationEnd(false);
		    locker = new BasicLocker(envImpl);
		    if (lockTimeout != -1) {
			locker.setLockTimeout(lockTimeout);
		    }
		    idCursor = new CursorImpl(idDatabase, locker);
		    //this.hook310(allowEviction, idCursor);
        Label310:
		    continue;
		} finally {
		    Label304: //this.hook304(idCursor);
		    idCursor.close();
		    locker.operationEnd(true);
		}
	    }
	    if (envImpl.isOpen()) {
		setDebugNameForDatabaseImpl(foundDbImpl, dbNameIfAvailable);
	    }
	    return foundDbImpl;
	}
    }

    private void setDebugNameForDatabaseImpl(DatabaseImpl dbImpl, String dbName) throws DatabaseException {
	if (dbImpl != null) {
	    if (dbName != null) {
		dbImpl.setDebugDatabaseName(dbName);
	    } else if (dbImpl.getDebugName() == null) {
		dbImpl.setDebugDatabaseName(getDbName(dbImpl.getId()));
	    }
	}
    }

    /** 
     * Rebuild the IN list after recovery.
     */
    public void rebuildINListMapDb() throws DatabaseException {
	idDatabase.getTree().rebuildINList();
    }

    /** 
     * Return the database name for a given db. Slow, must traverse. Used by truncate and for debugging.
     */
    public String getDbName(DatabaseId id) throws DatabaseException {
	if (id.equals(ID_DB_ID)) {
	    return ID_DB_NAME;
	} else if (id.equals(NAME_DB_ID)) {
	    return NAME_DB_NAME;
	}
	Locker locker = null;
	CursorImpl cursor = null;
	try {
	    locker = new BasicLocker(envImpl);
	    cursor = new CursorImpl(nameDatabase, locker);
	    DatabaseEntry keyDbt = new DatabaseEntry();
	    DatabaseEntry dataDbt = new DatabaseEntry();
	    String name = null;
	    if (cursor.positionFirstOrLast(true, null)) {
		OperationStatus status = cursor.getCurrentAlreadyLatched(keyDbt, dataDbt, LockType.NONE, true);
		do {
		    if (status == OperationStatus.SUCCESS) {
			NameLN nameLN = (NameLN) cursor.getCurrentLN(LockType.NONE);
			if (nameLN != null && nameLN.getId().equals(id)) {
			    name = new String(keyDbt.getData(), "UTF-8");
			    break;
			}
		    }
		    status = cursor.getNext(keyDbt, dataDbt, LockType.NONE, true, false);
		} while (status == OperationStatus.SUCCESS);
	    }
	    return name;
	} catch (UnsupportedEncodingException UEE) {
	    throw new DatabaseException(UEE);
	} finally {
	    if (cursor != null) {
		Label305: //this.hook305(cursor);
		cursor.close();
	    }
	    if (locker != null) {
		locker.operationEnd();
	    }
	}
    }

    /** 
     * @return a list of database names held in the environment, as strings.
     */
    public List getDbNames() throws DatabaseException {
	List nameList = new ArrayList();
	Locker locker = null;
	CursorImpl cursor = null;
	try {
	    locker = new BasicLocker(envImpl);
	    cursor = new CursorImpl(nameDatabase, locker);
	    DatabaseEntry keyDbt = new DatabaseEntry();
	    DatabaseEntry dataDbt = new DatabaseEntry();
	    if (cursor.positionFirstOrLast(true, null)) {
		OperationStatus status = cursor.getCurrentAlreadyLatched(keyDbt, dataDbt, LockType.READ, true);
		do {
		    if (status == OperationStatus.SUCCESS) {
			String name = new String(keyDbt.getData(), "UTF-8");
			if (!isReservedDbName(name)) {
			    nameList.add(name);
			}
		    }
		    status = cursor.getNext(keyDbt, dataDbt, LockType.READ, true, false);
		} while (status == OperationStatus.SUCCESS);
	    }
	    return nameList;
	} catch (UnsupportedEncodingException UEE) {
	    throw new DatabaseException(UEE);
	} finally {
	    if (cursor != null) {
		cursor.close();
	    }
	    if (locker != null) {
		locker.operationEnd();
	    }
	}
    }

    /** 
     * Returns true if the name is a reserved JE database name.
     */
    public boolean isReservedDbName(String name) {
	for (int i = 0; i < RESERVED_DB_NAMES.length; i += 1) {
	    if (RESERVED_DB_NAMES[i].equals(name)) {
		return true;
	    }
	}
	return false;
    }

    /** 
     * @return the higest level node in the environment.
     */
    public int getHighestLevel() throws DatabaseException {
	RootLevel getLevel = new RootLevel(idDatabase);
	idDatabase.getTree().withRootLatchedShared(getLevel);
	int idHighLevel = getLevel.getRootLevel();
	getLevel = new RootLevel(nameDatabase);
	nameDatabase.getTree().withRootLatchedShared(getLevel);
	int nameHighLevel = getLevel.getRootLevel();
	return (nameHighLevel > idHighLevel) ? nameHighLevel : idHighLevel;
    }

    /** 
     * @see LoggableObject#getLogType
     */
    public LogEntryType getLogType() {
	return LogEntryType.LOG_ROOT;
    }

    /** 
     * @see LoggableObject#marshallOutsideWriteLatch Can be marshalled outsidethe log write latch.
     */
    public boolean marshallOutsideWriteLatch() {
	return true;
    }

    /** 
     * @see LoggableObject#countAsObsoleteWhenLogged
     */
    public boolean countAsObsoleteWhenLogged() {
	return false;
    }

    /** 
     * @see LoggableObject#getLogSize
     */
    public int getLogSize() {
	return LogUtils.getIntLogSize() + idDatabase.getLogSize() + nameDatabase.getLogSize();
    }

    /** 
     * @see LoggableObject#writeToLog
     */
    public void writeToLog(ByteBuffer logBuffer) {
	LogUtils.writeInt(logBuffer, lastAllocatedDbId);
	idDatabase.writeToLog(logBuffer);
	nameDatabase.writeToLog(logBuffer);
    }

    /** 
     * @see LoggableObject#postLogWork
     */
    public void postLogWork(long justLoggedLsn) throws DatabaseException {
    }

    /** 
     * @see LogReadable#readFromLog
     */
    public void readFromLog(ByteBuffer itemBuffer, byte entryTypeVersion) throws LogException {
	lastAllocatedDbId = LogUtils.readInt(itemBuffer);
	idDatabase.readFromLog(itemBuffer, entryTypeVersion);
	nameDatabase.readFromLog(itemBuffer, entryTypeVersion);
    }

    /** 
     * @see LogReadable#dumpLog
     */
    public void dumpLog(StringBuffer sb, boolean verbose) {
	sb.append("<dbtree lastId = \"");
	sb.append(lastAllocatedDbId);
	sb.append("\">");
	sb.append("<idDb>");
	idDatabase.dumpLog(sb, verbose);
	sb.append("</idDb><nameDb>");
	nameDatabase.dumpLog(sb, verbose);
	sb.append("</nameDb>");
	sb.append("</dbtree>");
    }

    /** 
     * @see LogReadable#logEntryIsTransactional.
     */
    public boolean logEntryIsTransactional() {
	return false;
    }

    /** 
     * @see LogReadable#getTransactionId
     */
    public long getTransactionId() {
	return 0;
    }

    String dumpString(int nSpaces) {
	StringBuffer self = new StringBuffer();
	self.append(TreeUtils.indent(nSpaces));
	self.append("<dbTree lastDbId =\"");
	self.append(lastAllocatedDbId);
	self.append("\">");
	self.append('\n');
	self.append(idDatabase.dumpString(nSpaces + 1));
	self.append('\n');
	self.append(nameDatabase.dumpString(nSpaces + 1));
	self.append('\n');
	self.append("</dbtree>");
	return self.toString();
    }

    public String toString() {
	return dumpString(0);
    }

    /** 
     * For debugging.
     */
    public void dump() throws DatabaseException {
	idDatabase.getTree().dump();
	nameDatabase.getTree().dump();
    }

   /* protected void hook299(CursorImpl cursor) throws DatabaseException {
    }

    protected void hook300(NameLockResult result) throws DatabaseException, UnsupportedEncodingException {
    }

    protected void hook301(NameLockResult result) throws DatabaseException {
    }

    protected void hook302(NameLockResult result) throws DatabaseException {
    }

    protected void hook303(CursorImpl nameCursor) throws DatabaseException, UnsupportedEncodingException {
    }

    protected void hook304(CursorImpl idCursor) throws DatabaseException {
    }

    protected void hook305(CursorImpl cursor) throws DatabaseException {
    }
*/

  //  protected void hook306(boolean allowEviction, CursorImpl idCursor)
	//    throws DatabaseException, UnsupportedEncodingException {
  //  }

  //  protected void hook307(boolean allowEviction, CursorImpl nameCursor)
	 //   throws DatabaseException, UnsupportedEncodingException {
   // }

  //  protected void hook308(boolean allowEviction, CursorImpl nameCursor)
	//    throws DatabaseException, UnsupportedEncodingException {
  //  }

  //  protected void hook309(boolean allowEviction, CursorImpl idCursor) throws DatabaseException {
  //  }

  //  protected void hook310(boolean allowEviction, CursorImpl idCursor) throws DatabaseException {
  //  }

}
\00after the file itself has finally been deleted.
     */
    synchronized void removeDeletedFile(Long fileNum) {
	safeToDeleteFiles.remove(fileNum);
    }

    /** 
     * If there are no pending LNs or DBs outstanding, move the checkpointed files to the fully-processed set.  The check for pending LNs/DBs and the copying of the checkpointed files must be done atomically in a synchronized block.  All methods that call this method are synchronized.
     */
    private void updateProcessedFiles() {
			boolean b = pendingLNs.isEmpty();
			Label165: //b = this.hook165(b);
			if (b) {
					fullyProcessedFiles.addAll(checkpointedFiles);
					checkpointedFiles.clear();
			}
    }

//    protected void hook163() {
  //  }

    //protected void hook164() {
    //}

 //   protected boolean hook165(boolean b) {
//	return b;
 //   }

}
\00after the write lock has been obtained.
     * @see Locker#lockInternal
     * @Override
     */
    LockResult lockInternal(long nodeId, LockType lockType, boolean noWait, DatabaseImpl database)
	    throws DatabaseException {
	long timeout = 0;
	boolean useNoWait = noWait || defaultNoWait;
	synchronized (this) {
	    checkState(false);
	    if (!useNoWait) {
		timeout = lockTimeOutMillis;
	    }
	}
	LockGrantType grant = lockManager.lock(nodeId, this, lockType, timeout, useNoWait, database);
	WriteLockInfo info = null;
	if (writeInfo != null) {
	    if (grant != LockGrantType.DENIED && lockType.isWriteLock()) {
		synchronized (this) {
		    info = (WriteLockInfo) writeInfo.get(new Long(nodeId));
		    undoDatabases.put(database.getId(), database);
		}
	    }
	}
	return new LockResult(grant, info);
    }

    public int prepare(Xid xid) throws DatabaseException {
	if ((txnState & IS_PREPARED) != 0) {
	    throw new DatabaseException("prepare() has already been called for Transaction " + id + ".");
	}
	synchronized (this) {
	    checkState(false);
	    if (checkCursorsForClose()) {
		throw new DatabaseException("Transaction " + id + " prepare failed because there were open cursors.");
	    }
	    TxnPrepare prepareRecord = new TxnPrepare(id, xid);
	    LogManager logManager = envImpl.getLogManager();
	    logManager.logForceFlush(prepareRecord, true);
	}
	setPrepared(true);
	return XAResource.XA_OK;
    }

    public void commit(Xid xid) throws DatabaseException {
	commit(TXN_SYNC);
	envImpl.getTxnManager().unRegisterXATxn(xid, true);
	return;
    }

    public void abort(Xid xid) throws DatabaseException {
	abort(true);
	envImpl.getTxnManager().unRegisterXATxn(xid, false);
	return;
    }

    /** 
     * Call commit() with the default sync configuration property.
     */
    public long commit() throws DatabaseException {
	return commit(defaultFlushSyncBehavior);
    }

    /** 
     * Commit this transaction 1. Releases read locks 2. Writes a txn commit record into the log 3. Flushes the log to disk. 4. Add deleted LN info to IN compressor queue 5. Release all write locks  If any step of this fails, we must convert this transaction to an abort.
     */
    public long commit(byte flushSyncBehavior) throws DatabaseException {
			try {
					long commitLsn = DbLsn.NULL_LSN;
					synchronized (this) {
				checkState(false);
				if (checkCursorsForClose()) {
						throw new DatabaseException(
							"Transaction " + id + " commit failed because there were open cursors.");
				}
				if (handleLockToHandleMap != null) {
						Iterator handleLockIter = handleLockToHandleMap.entrySet().iterator();
						while (handleLockIter.hasNext()) {
					Map.Entry entry = (Map.Entry) handleLockIter.next();
					transferHandleLockToHandleSet((Long) entry.getKey(), (Set) entry.getValue());
						}
				}
				LogManager logManager = envImpl.getLogManager();
				int numReadLocks = clearReadLocks();
				int numWriteLocks = 0;
				if (writeInfo != null) {
						numWriteLocks = writeInfo.size();
						TxnCommit commitRecord = new TxnCommit(id, lastLoggedLsn);
						if (flushSyncBehavior == TXN_SYNC) {
					commitLsn = logManager.logForceFlush(commitRecord, true);
						} else if (flushSyncBehavior == TXN_WRITE_NOSYNC) {
					commitLsn = logManager.logForceFlush(commitRecord, false);
						} else {
					commitLsn = logManager.log(commitRecord);
						}
						Label806: //this.hook806();
						Set alreadyCountedLsnSet = new HashSet();
						Iterator iter = writeInfo.values().iterator();
						while (iter.hasNext()) {
					WriteLockInfo info = (WriteLockInfo) iter.next();
					lockManager.release(info.lock, this);
					if (info.abortLsn != DbLsn.NULL_LSN && !info.abortKnownDeleted) {
							Long longLsn = new Long(info.abortLsn);
							if (!alreadyCountedLsnSet.contains(longLsn)) {
						logManager.countObsoleteNode(info.abortLsn, null);
						alreadyCountedLsnSet.add(longLsn);
							}
					}
						}
						writeInfo = null;
						Label803: //this.hook803();
				}
				traceCommit(numWriteLocks, numReadLocks);
					}
					Label805: //this.hook805();
					close(true);
					return commitLsn;
			} catch (RunRecoveryException e) {
					throw e;
			} catch (Throwable t) {
					try {
				abortInternal(flushSyncBehavior == TXN_SYNC, !(t instanceof DatabaseException));
				Label800: //this.hook800(t);
					} catch (Throwable abortT2) {
				throw new DatabaseException("Failed while attempting to commit transaction " + id
					+ ". The attempt to abort and clean up also failed. "
					+ "The original exception seen from commit = " + t.getMessage()
					+ " The exception from the cleanup = " + abortT2.getMessage(), t);
					}
					throw new DatabaseException("Failed while attempting to commit transaction " + id
						+ ", aborted instead. Original exception = " + t.getMessage(), t);
			}
    }

    /** 
     * Abort this transaction. Steps are: 1. Release LN read locks. 2. Write a txn abort entry to the log. This is only for log file cleaning optimization and there's no need to guarantee a flush to disk.   3. Find the last LN log entry written for this txn, and use that to traverse the log looking for nodes to undo. For each node, use the same undo logic as recovery to rollback the transaction. Note that we walk the log in order to undo in reverse order of the actual operations. For example, suppose the txn did this: delete K1/D1 (in LN 10) create K1/D1 (in LN 20) If we process LN10 before LN 20, we'd inadvertently create a  duplicate tree of "K1", which would be fatal for the mapping tree. 4. Release the write lock for this LN.
     */
    public long abort(boolean forceFlush) throws DatabaseException {
	return abortInternal(forceFlush, true);
    }

    private long abortInternal(boolean forceFlush, boolean writeAbortRecord) throws DatabaseException {
	try {
	    int numReadLocks;
	    int numWriteLocks;
	    long abortLsn;
	    synchronized (this) {
		checkState(true);
		TxnAbort abortRecord = new TxnAbort(id, lastLoggedLsn);
		abortLsn = DbLsn.NULL_LSN;
		if (writeInfo != null) {
		    if (writeAbortRecord) {
			if (forceFlush) {
			    abortLsn = envImpl.getLogManager().logForceFlush(abortRecord, true);
			} else {
			    abortLsn = envImpl.getLogManager().log(abortRecord);
			}
		    }
		}
		undo();
		numReadLocks = (readLocks == null) ? 0 : clearReadLocks();
		Label808: //this.hook808();
		numWriteLocks = (writeInfo == null) ? 0 : clearWriteLocks();
		Label804: //this.hook804();
	    }
	    Label807: //this.hook807();
	    synchronized (this) {
		boolean openCursors = checkCursorsForClose();
		Label799: //this.hook799(numReadLocks, numWriteLocks, openCursors);
		if (openCursors) {
		    throw new DatabaseException("Transaction " + id + " detected open cursors while aborting");
		}
		if (handleToHandleLockMap != null) {
		    Iterator handleIter = handleToHandleLockMap.keySet().iterator();
		    while (handleIter.hasNext()) {
			Database handle = (Database) handleIter.next();
			DbInternal.dbInvalidate(handle);
		    }
		}
		return abortLsn;
	    }
	} finally {
	    close(false);
	}
    }

    /** 
     * Rollback the changes to this txn's write locked nodes.
     */
    private void undo() throws DatabaseException {
	Long nodeId = null;
	long undoLsn = lastLoggedLsn;
	LogManager logManager = envImpl.getLogManager();
	try {
	    Set alreadyUndone = new HashSet();
	    TreeLocation location = new TreeLocation();
	    while (undoLsn != DbLsn.NULL_LSN) {
		LNLogEntry undoEntry = (LNLogEntry) logManager.getLogEntry(undoLsn);
		LN undoLN = undoEntry.getLN();
		nodeId = new Long(undoLN.getNodeId());
		if (!alreadyUndone.contains(nodeId)) {
		    alreadyUndone.add(nodeId);
		    DatabaseId dbId = undoEntry.getDbId();
		    DatabaseImpl db = (DatabaseImpl) undoDatabases.get(dbId);
		    undoLN.postFetchInit(db, undoLsn);
		    long abortLsn = undoEntry.getAbortLsn();
		    boolean abortKnownDeleted = undoEntry.getAbortKnownDeleted();
		    Label802: //this.hook802(undoLsn, location, undoEntry, undoLN, db, abortLsn, abortKnownDeleted);
				RecoveryManager.undo(Level.FINER, db, location, undoLN, undoEntry.getKey(), undoEntry.getDupKey(), undoLsn, abortLsn, abortKnownDeleted, null, false);
		    Label802_1: //end of hook802
		    if (!undoLN.isDeleted()) {
			logManager.countObsoleteNode(undoLsn, null);
		    }
		}
		undoLsn = undoEntry.getUserTxn().getLastLsn();
	    }
	} catch (RuntimeException e) {
	    throw new DatabaseException("Txn undo for node=" + nodeId + " LSN=" + DbLsn.getNoFormatString(undoLsn), e);
	} catch (DatabaseException e) {
	    Label801: //this.hook801(nodeId, undoLsn, e);
	    throw e;
	}
    }

    private int clearWriteLocks() throws DatabaseException {
	int numWriteLocks = writeInfo.size();
	Iterator iter = writeInfo.values().iterator();
	while (iter.hasNext()) {
	    WriteLockInfo info = (WriteLockInfo) iter.next();
	    lockManager.release(info.lock, this);
	}
	writeInfo = null;
	return numWriteLocks;
    }

    private int clearReadLocks() throws DatabaseException {
	int numReadLocks = 0;
	if (readLocks != null) {
	    numReadLocks = readLocks.size();
	    Iterator iter = readLocks.iterator();
	    while (iter.hasNext()) {
		Lock rLock = (Lock) iter.next();
		lockManager.release(rLock, this);
	    }
	    readLocks = null;
	}
	return numReadLocks;
    }

    /** 
     * Called by the recovery manager when logging a transaction aware object. This method is synchronized by the caller, by being called within the log latch. Record the last LSN for this transaction, to create the transaction chain, and also record the LSN in the write info for abort logic.
     */
    public void addLogInfo(long lastLsn) throws DatabaseException {
	lastLoggedLsn = lastLsn;
	synchronized (this) {
	    if (firstLoggedLsn == DbLsn.NULL_LSN) {
		firstLoggedLsn = lastLsn;
	    }
	}
    }

    /** 
     * @return first logged LSN, to aid recovery rollback.
     */
    long getFirstActiveLsn() throws DatabaseException {
	synchronized (this) {
	    return firstLoggedLsn;
	}
    }

    /** 
     * Add lock to the appropriate queue.
     */
    void addLock(Long nodeId, Lock lock, LockType type, LockGrantType grantStatus) throws DatabaseException {
	new Txn_addLock(this, nodeId, lock, type, grantStatus).execute();
    }

    private void addReadLock(Lock lock) {
			int delta = 0;
			if (readLocks == null) {
					readLocks = new HashSet();
			//		delta = this.hook811(delta);
          Label811:
			}
			readLocks.add(lock);
			//this.hook810(delta);
      Label810:
    }

    /** 
     * Remove the lock from the set owned by this transaction. If specified to LockManager.release, the lock manager will call this when its releasing a lock. Usually done because the transaction doesn't need to really keep the lock, i.e for a deleted record.
     */
    void removeLock(long nodeId, Lock lock) throws DatabaseException {
			synchronized (this) {
					if ((readLocks != null) && readLocks.remove(lock)) {
				//this.hook812();
          Label812:
					} else if ((writeInfo != null) && (writeInfo.remove(new Long(nodeId)) != null)) {
				//this.hook813();
         label813: 
					}
			}
    }

    /** 
     * A lock is being demoted. Move it from the write collection into the read collection.
     */
    void moveWriteToReadLock(long nodeId, Lock lock) {
	boolean found = false;
	synchronized (this) {
	    if ((writeInfo != null) && (writeInfo.remove(new Long(nodeId)) != null)) {
		found = true;
		//this.hook814();
    Label814:
	    }
	    assert found : "Couldn't find lock for Node " + nodeId + " in writeInfo Map.";
	    addReadLock(lock);
	}
    }

    /** 
     * @return true if this transaction created this node. We know that thisis true if the node is write locked and has a null abort LSN.
     */
    public boolean createdNode(long nodeId) throws DatabaseException {
	boolean created = false;
	synchronized (this) {
	    if (writeInfo != null) {
		WriteLockInfo info = (WriteLockInfo) writeInfo.get(new Long(nodeId));
		if (info != null) {
		    created = info.createdThisTxn;
		}
	    }
	}
	return created;
    }

    /** 
     * @return the abortLsn for this node.
     */
    public long getAbortLsn(long nodeId) throws DatabaseException {
	WriteLockInfo info = null;
	synchronized (this) {
	    if (writeInfo != null) {
		info = (WriteLockInfo) writeInfo.get(new Long(nodeId));
	    }
	}
	if (info == null) {
	    return DbLsn.NULL_LSN;
	} else {
	    return info.abortLsn;
	}
    }

    /** 
     * @return the WriteLockInfo for this node.
     */
    public WriteLockInfo getWriteLockInfo(long nodeId) throws DatabaseException {
	WriteLockInfo info = WriteLockInfo.basicWriteLockInfo;
	synchronized (this) {
	    if (writeInfo != null) {
		info = (WriteLockInfo) writeInfo.get(new Long(nodeId));
	    }
	}
	return info;
    }

    /** 
     * Is always transactional.
     */
    public boolean isTransactional() {
	return true;
    }

    /** 
     * Is serializable isolation if so configured.
     */
    public boolean isSerializableIsolation() {
	return serializableIsolation;
    }

    /** 
     * Is read-committed isolation if so configured.
     */
    public boolean isReadCommittedIsolation() {
	return readCommittedIsolation;
    }

    /** 
     * This is a transactional locker.
     */
    public Txn getTxnLocker() {
	return this;
    }

    /** 
     * Returns 'this', since this locker holds no non-transactional locks.
     */
    public Locker newNonTxnLocker() throws DatabaseException {
	return this;
    }

    /** 
     * This locker holds no non-transactional locks.
     */
    public void releaseNonTxnLocks() throws DatabaseException {
    }

    /** 
     * Created transactions do nothing at the end of the operation.
     */
    public void operationEnd() throws DatabaseException {
    }

    /** 
     * Created transactions do nothing at the end of the operation.
     */
    public void operationEnd(boolean operationOK) throws DatabaseException {
    }

    /** 
     * Created transactions don't transfer locks until commit.
     */
    public void setHandleLockOwner(boolean ignore, Database dbHandle, boolean dbIsClosing) throws DatabaseException {
	if (dbIsClosing) {
	    Long handleLockId = (Long) handleToHandleLockMap.get(dbHandle);
	    if (handleLockId != null) {
		Set dbHandleSet = (Set) handleLockToHandleMap.get(handleLockId);
		boolean removed = dbHandleSet.remove(dbHandle);
		assert removed : "Can't find " + dbHandle + " from dbHandleSet";
		if (dbHandleSet.size() == 0) {
		    Object foo = handleLockToHandleMap.remove(handleLockId);
		    assert (foo != null) : "Can't find " + handleLockId + " from handleLockIdtoHandleMap.";
		}
	    }
	    unregisterHandle(dbHandle);
	} else {
	    if (dbHandle != null) {
		DbInternal.dbSetHandleLocker(dbHandle, this);
	    }
	}
    }

    /** 
     * Cursors operating under this transaction are added to the collection.
     */
    public void registerCursor(CursorImpl cursor) throws DatabaseException {
	synchronized (this) {
	    cursor.setLockerNext(cursorSet);
	    if (cursorSet != null) {
		cursorSet.setLockerPrev(cursor);
	    }
	    cursorSet = cursor;
	}
    }

    /** 
     * Remove a cursor from the collection.
     */
    public void unRegisterCursor(CursorImpl cursor) throws DatabaseException {
	synchronized (this) {
	    CursorImpl prev = cursor.getLockerPrev();
	    CursorImpl next = cursor.getLockerNext();
	    if (prev == null) {
		cursorSet = next;
	    } else {
		prev.setLockerNext(next);
	    }
	    if (next != null) {
		next.setLockerPrev(prev);
	    }
	    cursor.setLockerPrev(null);
	    cursor.setLockerNext(null);
	}
    }

    /** 
     * @return true if this txn is willing to give up the handle lock toanother txn before this txn ends.
     */
    public boolean isHandleLockTransferrable() {
	return false;
    }

    /** 
     * Check if all cursors associated with the txn are closed. If not, those open cursors will be forcibly closed.
     * @return true if open cursors exist
     */
    private boolean checkCursorsForClose() throws DatabaseException {
	CursorImpl c = cursorSet;
	while (c != null) {
	    if (!c.isClosed()) {
		return true;
	    }
	    c = c.getLockerNext();
	}
	return false;
    }

    /** 
     * Set the state of a transaction to ONLY_ABORTABLE.
     */
    public void setOnlyAbortable() {
	txnState &= ~STATE_BITS;
	txnState |= ONLY_ABORTABLE;
    }

    /** 
     * Get the state of a transaction's ONLY_ABORTABLE.
     */
    public boolean getOnlyAbortable() {
	return (txnState & ONLY_ABORTABLE) != 0;
    }

    /** 
     * Throw an exception if the transaction is not open. If calledByAbort is true, it means we're being called from abort(). Caller must invoke with "this" synchronized.
     */
    protected void checkState(boolean calledByAbort) throws DatabaseException {
	boolean ok = false;
	boolean onlyAbortable = false;
	byte state = (byte) (txnState & STATE_BITS);
	ok = (state == USABLE);
	onlyAbortable = (state == ONLY_ABORTABLE);
	if (!calledByAbort && onlyAbortable) {
	    throw new DatabaseException("Transaction " + id + " must be aborted.");
	}
	if (ok || (calledByAbort && onlyAbortable)) {
	    return;
	}
	throw new DatabaseException("Transaction " + id + " has been closed.");
    }

    /** 
     */
    private void close(boolean isCommit) throws DatabaseException {
	synchronized (this) {
	    txnState &= ~STATE_BITS;
	    txnState |= CLOSED;
	}
	envImpl.getTxnManager().unRegisterTxn(this, isCommit);
    }

    /** 
     * @see LogWritable#getLogSize
     */
    public int getLogSize() {
	return LogUtils.LONG_BYTES + LogUtils.LONG_BYTES;
    }

    /** 
     * @see LogWritable#writeToLog
     */
    public void writeToLog(ByteBuffer logBuffer) {
	LogUtils.writeLong(logBuffer, id);
	LogUtils.writeLong(logBuffer, lastLoggedLsn);
    }

    /** 
     * @see LogReadable#readFromLogIt's ok for FindBugs to whine about id not being synchronized.
     */
    public void readFromLog(ByteBuffer logBuffer, byte entryTypeVersion) {
	id = LogUtils.readLong(logBuffer);
	lastLoggedLsn = LogUtils.readLong(logBuffer);
    }

    /** 
     * @see LogReadable#dumpLog
     */
    public void dumpLog(StringBuffer sb, boolean verbose) {
	sb.append("<txn id=\"");
	sb.append(super.toString());
	sb.append("\">");
	sb.append(DbLsn.toString(lastLoggedLsn));
	sb.append("</txn>");
    }

    /** 
     * @see LogReadable#getTransactionId
     */
    public long getTransactionId() {
	return getId();
    }

    /** 
     * @see LogReadable#logEntryIsTransactional
     */
    public boolean logEntryIsTransactional() {
	return true;
    }

    /** 
     * Transfer a single handle lock to the set of corresponding handles at commit time.
     */
    private void transferHandleLockToHandleSet(Long handleLockId, Set dbHandleSet) throws DatabaseException {
	int numHandles = dbHandleSet.size();
	Database[] dbHandles = new Database[numHandles];
	dbHandles = (Database[]) dbHandleSet.toArray(dbHandles);
	Locker[] destTxns = new Locker[numHandles];
	for (int i = 0; i < numHandles; i++) {
	    destTxns[i] = new BasicLocker(envImpl);
	}
	long nodeId = handleLockId.longValue();
	lockManager.transferMultiple(nodeId, this, destTxns);
	for (int i = 0; i < numHandles; i++) {
	    destTxns[i].addToHandleMaps(handleLockId, dbHandles[i]);
	    DbInternal.dbSetHandleLocker(dbHandles[i], destTxns[i]);
	}
    }

    /** 
     * Send trace messages to the java.util.logger. Don't rely on the logger alone to conditionalize whether we send this message, we don't even want to construct the message if the level is not enabled.  The string construction can be numerous enough to show up on a performance profile.
     */
    private void traceCommit(int numWriteLocks, int numReadLocks) {
	new Txn_traceCommit(this, numWriteLocks, numReadLocks).execute();
    }

    int getInMemorySize() {
	return inMemorySize;
    }

    /** 
     * Store information about a DatabaseImpl that will have to be purged at transaction commit or abort. This handles cleanup after operations like Environment.truncateDatabase,  Environment.removeDatabase. Cleanup like this is done outside the usual transaction commit or node undo processing, because the mapping tree is always AutoTxn'ed to avoid deadlock and is  essentially  non-transactional
     */
    //protected void hook799(int numReadLocks, int numWriteLocks, boolean openCursors) throws DatabaseException {}

  //  protected void hook800(Throwable t) throws DatabaseException, Throwable {}

//    protected void hook801(Long nodeId, long undoLsn, DatabaseException e) throws DatabaseException {}

/*    protected void hook802(long undoLsn, TreeLocation location, LNLogEntry undoEntry, LN undoLN, DatabaseImpl db,
	    long abortLsn, boolean abortKnownDeleted) throws DatabaseException, RuntimeException {
	RecoveryManager.undo(Level.FINER, db, location, undoLN, undoEntry.getKey(), undoEntry.getDupKey(), undoLsn,
		abortLsn, abortKnownDeleted, null, false);
    }
  //  protected void hook803() throws DatabaseException, RunRecoveryException, Throwable {
  //  }

  //  protected void hook804() throws DatabaseException {
  //  }

  //  protected void hook805() throws DatabaseException, RunRecoveryException, Throwable {
  //  }

   // protected void hook806() throws DatabaseException, RunRecoveryException, Throwable {
   // }

   // protected void hook807() throws DatabaseException {
   // }

    //protected void hook808() throws DatabaseException {
    //}

    protected void hook809() throws DatabaseException {
    }

    protected void hook810(int delta) {
    }

    protected int hook811(int delta) {
	return delta;
    }

    protected void hook812() throws DatabaseException {
    }

    protected void hook813() throws DatabaseException {
    }

    protected void hook814() {
    }
*/
}
\00after LLLLLLL539: Checkpointer(EnvironmentImpl , long , String ){
			logSizeBytesInterval = envImpl.getConfigManager().getLong(EnvironmentParams.CHECKPOINTER_BYTES_INTERVAL);
			//original(envImpl);
    }

}
\00after LLLLLLL540:execute() {
		      if (bytePeriod == 0) {
		        //original();
		      }
      }
      after LLLLLLL541:execute() {
        bytePeriod=configManager.getLong(EnvironmentParams.CHECKPOINTER_BYTES_INTERVAL);
        //original();
      }
    }
  
  static class Checkpointer_isRunnable {
      after LLLLLLL542:execute() {
		      if (useBytesInterval != 0) {
		        nextLsn=_this.envImpl.getFileManager().getNextLsn();
		        if (DbLsn.getNoCleaningDistance(nextLsn,_this.lastCheckpointEnd,_this.logFileMax) >= useBytesInterval) {
		          throw new ReturnBoolean(true);
		        }
				 else {
						      throw new ReturnBoolean(false);
						  }
					}

				//else {
				//    original();
				//	  }
      }

      after LLLLLLL543: execute() {
				    if (config.getKBytes() != 0) {
				      useBytesInterval=config.getKBytes() << 10;
				    }
					 else {
								  throw new ReturnBoolean(false); // add hook542
					 }
      
     }

      after LLLLLLL544: execute() {
        if (_this.logSizeBytesInterval != 0) {
          useBytesInterval=_this.logSizeBytesInterval;
        }
//   else {
//          original();
//        }
      }
    }
}
\00after execute() {  // Original method: loggingBase_EnvironmentImpl_inner.ump
        //Logger result=original();
        fileHandler=null;
        try {
          if (_this.configManager.getBoolean(EnvironmentParams.JE_LOGGING_FILE)) {
            limit=_this.configManager.getInt(EnvironmentParams.JE_LOGGING_FILE_LIMIT);
            count=_this.configManager.getInt(EnvironmentParams.JE_LOGGING_FILE_COUNT);
            logFilePattern=envHome + "/" + Tracer.INFO_FILES;
            fileHandler=new FileHandler(logFilePattern,limit,count,true);
            fileHandler.setFormatter(new SimpleFormatter());
            fileHandler.setLevel(level);
            logger.addHandler(fileHandler);
          }
        }
   catch (      IOException e) {
          throw new DatabaseException(e.getMessage());
        }
//        return result;
      }
    }
}
\00after LLLLLLLExecute_loggingBase: execute() {
        //Logger result=original();
        if (_this.configManager.getBoolean(EnvironmentParams.JE_LOGGING_CONSOLE)) {
          consoleHandler=new ConsoleHandler();
          consoleHandler.setLevel(level);
          logger.addHandler(consoleHandler);
        }
        //return result;
      }
    }
}
\00after LLLLLLL291:  verify(VerifyConfig , PrintStream ) {
	cursor.releaseBINs();
//	original(cursor);
    }

}
\00after LLLLLLL62: getNext(DatabaseEntry , LockMode ) {

	priCursor.trace(Level.FINEST, "JoinCursor.getNext(key): ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL63: getNext(DatabaseEntry , DatabaseEntry , LockMode ){
	priCursor.trace(Level.FINEST, "JoinCursor.getNext(key,data): ", lockMode);
	//original(lockMode);
    }

}
\00after LLLLLLL65:  delete() {
	trace(Level.FINEST, "SecondaryCursor.delete: ", null);
	//original();
    }

    after LLLLLLL66: getCurrent(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getCurrent: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL67: getFirst(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getFirst: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL68: getLast(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getLast: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL69: getNext(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getNext: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL70: etNextDup(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getNextDup: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL71:  getNextNoDup(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode )
{

	trace(Level.FINEST, "SecondaryCursor.getNextNoDup: ", null, null, lockMode);
	//original(lockMode);
    }

    after LLLLLLL72: getPrev(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getPrev: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL73:  getPrevDup(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getPrevDup: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL74: getPrevNoDup(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getPrevNoDup: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL75:getSearchKey(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getSearchKey: ", key, null, lockMode);
	//original(key, lockMode);
    }

    after LLLLLLL76: getSearchKeyRange(DatabaseEntry , DatabaseEntry , DatabaseEntry ,	    LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getSearchKeyRange: ", key, data, lockMode);
	//original(key, data, lockMode);
    }

    after LLLLLLL77: getSearchBoth(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "SecondaryCursor.getSearchBoth: ", key, data, lockMode);
	//original(key, data, lockMode);
    }

    after LLLLLLL78: getSearchBothRange(DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ){
	trace(Level.FINEST, "SecondaryCursor.getSearchBothRange: ", key, data, lockMode);
	//original(key, data, lockMode);
    }

}
\00after LLLLLLL79: delete(Transaction , DatabaseEntry ) {
	trace(Level.FINEST, "SecondaryDatabase.delete", txn, key, null, null);
	//original(txn, key);
    }

    after LLLLLLL80: get(Transaction , DatabaseEntry , DatabaseEntry , DatabaseEntry , LockMode ) {
	trace(Level.FINEST, "SecondaryDatabase.get", txn, key, null, lockMode);
	//original(txn, key, lockMode);
    }

    after LLLLLLL81: getSearchBoth(Transaction , DatabaseEntry , DatabaseEntry , DatabaseEntry ,	    LockMode ) {
	trace(Level.FINEST, "SecondaryDatabase.getSearchBoth", txn, key, data, lockMode);
	//original(txn, key, data, lockMode);
    }

}
\00after LLLLLLL0: count() {

	trace(Level.FINEST, "Cursor.count: ", null);
	//original();
    }

    after LLLLLLL1: delete(){

	trace(Level.FINEST, "Cursor.delete: ", null);
	//original();
    }


    after LLLLLLL2: put(DatabaseEntry , DatabaseEntry ){
	trace(Level.FINEST, "Cursor.put: ", key, data, null);
	//original(key, data);
    }

    after LLLLLLL3: putNoOverwrite(DatabaseEntry , DatabaseEntry )
		{

	trace(Level.FINEST, "Cursor.putNoOverwrite: ", key, data, null);
	//original(key, data);
    }

    after LLLLLLL4: putNoDupData(DatabaseEntry , DatabaseEntry ) {
	trace(Level.FINEST, "Cursor.putNoDupData: ", key, data, null);
	//original(key, data);
    }

    after LLLLLLL5:  putCurrent(DatabaseEntry ) {
	trace(Level.FINEST, "Cursor.putCurrent: ", null, data, null);
	//original(data);
    }

    after LLLLLLL6: getCurrent(DatabaseEntry , DatabaseEntry , LockMode ){
	trace(Level.FINEST, "Cursor.getCurrent: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL7: getFirst(DatabaseEntry , DatabaseEntry , LockMode ) {
	trace(Level.FINEST, "Cursor.getFirst: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL8: getLast(DatabaseEntry , DatabaseEntry , LockMode ){
	trace(Level.FINEST, "Cursor.getLast: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL9: getNext(DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Cursor.getNext: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL10:  getNextDup(DatabaseEntry , DatabaseEntry , LockMode ){
	trace(Level.FINEST, "Cursor.getNextDup: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL11: getNextNoDup(DatabaseEntry , DatabaseEntry , LockMode ){
	trace(Level.FINEST, "Cursor.getNextNoDup: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL12: getPrev(DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Cursor.getPrev: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL13: getPrevDup(DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Cursor.getPrevDup: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL14: getPrevNoDup(DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Cursor.getPrevNoDup: ", lockMode);
	//original(lockMode);
    }

    after LLLLLLL15: getSearchKey(DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Cursor.getSearchKey: ", key, null, lockMode);
	//original(key, lockMode);
    }

    after LLLLLLL16: getSearchKeyRange(DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Cursor.getSearchKeyRange: ", key, null, lockMode);
	//original(key, lockMode);
    }

    after LLLLLLL17: getSearchBoth(DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Cursor.getSearchBoth: ", key, data, lockMode);
	//original(key, data, lockMode);
    }

    after LLLLLLL18: getSearchBothRange(DatabaseEntry , DatabaseEntry , LockMode ){
	trace(Level.FINEST, "Cursor.getSearchBothRange: ", key, data, lockMode);
	//original(key, data, lockMode);
    }

}
\00after LLLLLLL44:  close()  { //synchronized void close() 
	trace(Level.FINEST, "Database.close: ", null, null);
	//original();
    }

    after LLLLLLL45:  openSequence(Transaction , DatabaseEntry , SequenceConfig ) {
	trace(Level.FINEST, "Database.openSequence", txn, key, null, null);
	//original(txn, key);
    }

    after LLLLLLL46:  openCursor(Transaction , CursorConfig ) {
	trace(Level.FINEST, "Database.openCursor", txn, cursorConfig);
	//original(txn, cursorConfig);
    }

    after LLLLLLL47: delete(Transaction , DatabaseEntry ){
	trace(Level.FINEST, "Database.delete", txn, key, null, null);
	//original(txn, key);
    }

    after LLLLLLL48: get(Transaction , DatabaseEntry , DatabaseEntry , LockMode ){

	trace(Level.FINEST, "Database.get", txn, key, null, lockMode);
	//original(txn, key, lockMode);
    }

    after LLLLLLL49: getSearchBoth(Transaction , DatabaseEntry , DatabaseEntry , LockMode )
  {
	trace(Level.FINEST, "Database.getSearchBoth", txn, key, data, lockMode);
	//original(txn, key, data, lockMode);
  }

    after LLLLLLL50:  put(Transaction , DatabaseEntry , DatabaseEntry ){
	trace(Level.FINEST, "Database.put", txn, key, data, null);
	//original(txn, key, data);
    }

    after LLLLLLL51: putNoOverwrite(Transaction , DatabaseEntry , DatabaseEntry )
   {
	trace(Level.FINEST, "Database.putNoOverwrite", txn, key, data, null);
	//original(txn, key, data);
    }

    after LLLLLLL52: putNoDupData(Transaction , DatabaseEntry , DatabaseEntry ){

	trace(Level.FINEST, "Database.putNoDupData", txn, key, data, null);
	//original(txn, key, data);
    }

}
\00after LLLLLLL521: execute() {

        sb=new StringBuffer();
  		{
          Label517:  YYY  //this.hook517();
        }
        if (nextLsn != DbLsn.NULL_LSN) {
          sb.append(" " + "nextLsn=").append(DbLsn.getNoFormatString(nextLsn));
        }
        if (_this.lastCheckpointEnd != DbLsn.NULL_LSN) {
          sb.append(" lastCkpt=");
          sb.append(DbLsn.getNoFormatString(_this.lastCheckpointEnd));
        }
  		{
          Label518:  YYY //this.hook518();
        }
        sb.append(" force=").append(config.getForce());
        Tracer.trace(Level.FINEST,_this.envImpl,sb.toString());
        //original();
      }
    }
}
\00after LLLLLLL800: commit(byte ) {
	Tracer.trace(envImpl, "Txn", "commit", "Commit of transaction " + id + " failed", t);
	//original(t);
    }

    after LLLLLLL801: undo() {
	Tracer.trace(envImpl, "Txn", "undo", "for node=" + nodeId + " LSN=" + DbLsn.getNoFormatString(undoLsn), e);
	//original(nodeId, undoLsn, e);
    }

}
\00after LLLLLLL122: doClean(boolean , boolean , boolean ){

	Tracer.trace(env, "Cleaner", "doClean", "", IOE);
	//original(IOE);
    }

    after LLLLLLL123: doClean(boolean , boolean , boolean ) {
	Tracer.trace(Level.SEVERE, env, traceMsg);
	//original(traceMsg);
    }

}
\00after LLLLLLL575: recover(boolean ) {
	Tracer.trace(env, "RecoveryManager", "recover", "Couldn't recover", e);
	//original(e);
    }

    after LLLLLLL576: replaceOrInsert(DatabaseImpl , IN , long , long , boolean ) {

	Tracer.trace(db.getDbEnvironment(), "RecoveryManager", "replaceOrInsert",
		" lsnFromLog:" + DbLsn.getNoFormatString(logLsn) + " " + trace, e);
	//original(db, logLsn, e, trace);
    }

    after LLLLLLL577: trace(Level , DatabaseImpl , String , boolean , Node , long , IN , boolean , boolean , boolean , long , long , int ) {
	Tracer.trace(env, "RecoveryManager", method, "last LSN = " + badLsnString, origException);
	//original(method, origException, badLsnString);
    }

}
\00after LLLLLLL557: replayINDelete(DatabaseImpl , long , boolean , byte , byte , long ) {
	traceRootDeletion(Level.FINE, db);
	//original(db);
    }

}
\00after LLLLLLL799: abortInternal(boolean , boolean ) {

	Tracer.trace(Level.FINE, envImpl, "Abort:id = " + id + " numWriteLocks= " + numWriteLocks + " numReadLocks= "
		+ numReadLocks + " openCursors= " + openCursors);
	//original(numReadLocks, numWriteLocks, openCursors);
    }

}
\00after LLLLLLL318: doClose(boolean ){
	Tracer.trace(Level.FINE, this, "Env " + envHome + " daemons shutdown");
	//original();
    }

    after LLLLLLL319: doClose(boolean ){
	Tracer.trace(Level.FINE, this, "Close of environment " + envHome + " started");
	//original();
    }

}
\00after LLLLLLL617: execute(){
        _this.traceSplit(Level.FINE,parent,newSibling,parentLsn,myNewLsn,newSiblingLsn,splitIndex,idKeyIndex,childIndex);
       // original();
      }
    }
}
\00after LLLLLLL173: deleteFileSummary(Long){

	env.getEvictor().doCriticalEviction();
	//original();
    }

    after LLLLLLL174:getObsoleteDetail(Long , PackedOffsets , boolean ){
	env.getEvictor().doCriticalEviction();
	//original();
    }

    after LLLLLLL175: verifyFileSummaryDatabase() {
	env.getEvictor().doCriticalEviction();
	//original();
    }

}
\00after LLLLLLL203:  cloneCursor(boolean , CursorImpl ) { // Label203 introduced in Evictor_CursorImpl.ump

	database.getDbEnvironment().getEvictor().doCriticalEviction();
//	original();
    }

    /** 
     * Reset a cursor to an uninitialized state, but unlike close(), allow it to be used further.
     */
 after reset() {
	//original();
	if (allowEviction) {
	    database.getDbEnvironment().getEvictor().doCriticalEviction();
	}
    }

    /** 
     * Close a cursor.
     * @throws DatabaseExceptionif the cursor was previously closed.
     */
  after close()  {
	//original();
	if (allowEviction) {
	    database.getDbEnvironment().getEvictor().doCriticalEviction();
	}
    }

}
\00after LLLLLLL520: flushDirtyNodes(SortedMap , boolean , boolean , boolean ,        long ){
	envImpl.getEvictor().doCriticalEviction();
	//original();
    }

}
\00after LLLLLLL176:execute(){

        _this.env.getEvictor().doCriticalEviction();
       // original();
      }
    }
}
\00after LLLLLLL86:execute() { // Label86 introduced by CriticalEviction_Cleaner_inner.ump
        _this.env.getEvictor().doCriticalEviction();
        //original();
      }
    }
}
\00after LLLLLLL119: execute(){
        if (Cleaner.DO_CRITICAL_EVICTION) {
          _this.env.getEvictor().doCriticalEviction();
        }
        //original();
      }
      after LLLLLLL120: execute(){
        if (Cleaner.DO_CRITICAL_EVICTION) {
          _this.env.getEvictor().doCriticalEviction();
        }
        //original();
      }
    }
}
\00after LLLLLLL518: execute() { // introduced in LoggingFinest/Checkpointer_inner.ump
        sb.append(" time interval=").append(useTimeInterval);
       // original();
      }
    }
}
\00after LLLLLLL371: execute(){

        logger=_this.envImpl.getLogger();
        if (logger.isLoggable(_this.detailedTraceLevel)) {
          msg="Evictor: ";
          Label369:       YYY    ;  //this.hook369();
          msg+=" finished=" + finished + " source="+ source+ " requiredEvictBytes="+ _this.formatter.format(requiredEvictBytes)+ " evictBytes="+ _this.formatter.format(evictBytes)+ " inListSize="+ inListStartSize+ " nNodesScanned="+ _this.nNodesScannedThisRun;
          Label368:     YYY      ;  //this.hook368();
          msg+=" nBatchSets=" + nBatchSets;
          Tracer.trace(_this.detailedTraceLevel,_this.envImpl,msg);
        }
        //original();
      }
    }
   static class Evictor_isRunnable {
//      void hook370() throws DatabaseException {}

      after LLLLLLL372: execute(){
        logger=_this.envImpl.getLogger();
        if (logger.isLoggable(_this.detailedTraceLevel)) {
          r=Runtime.getRuntime();
          totalBytes=r.totalMemory();
          freeBytes=r.freeMemory();
          usedBytes=r.totalMemory() - r.freeMemory();
          sb=new StringBuffer();
          sb.append(" source=").append(source);
          Label370:    YYY       ;  //this.hook370();
          sb.append(" requiredEvict=").append(_this.formatter.format(_this.currentRequiredEvictBytes));
          sb.append(" JVMtotalBytes= ").append(_this.formatter.format(totalBytes));
          sb.append(" JVMfreeBytes= ").append(_this.formatter.format(freeBytes));
          sb.append(" JVMusedBytes= ").append(_this.formatter.format(usedBytes));
          logger.log(_this.detailedTraceLevel,sb.toString());
        }
        //original();
      }
    }
}
\00after LLLLLLLEvict_1:evict() {


	    releaseBINs();
	//}
    }

    after LLLLLLL202: evict(){
	latchBINs();
	//original();
    }

}
\00after LLLLLLL374_1: evict(INList , IN , ScanIterator )
  {

	    target.releaseLatchIfOwner();

  }

    after LLLLLLL375_1: evictIN(IN , IN , int , INList , ScanIterator , boolean ){

	    parent.releaseLatch();

    }

    after LLLLLLL378: evictIN(IN , IN , int , INList , ScanIterator , boolean ){

	assert parent.isLatchOwner();
//	original(parent);
    }

    after LLLLLLL375_1: evictIN(IN , IN , int , INList , ScanIterator , boolean ){ // Label379_1 reuses Label375_1
	    renewedChild.releaseLatch();
	
    }

}
\00after execute() {
        //long result=//original();
        assert LatchSupport.countLatchesHeld() == 0 : "latches held = " + LatchSupport.countLatchesHeld();
      //  return result;
      }
      after LLLLLLL376:  execute(){
        inList.latchMajor();
        //original();
      }
      after LLLLLLL377: execute(){
        inList.releaseMajorLatch();
        //original();
      }
    }
}
\00after LLLLLLL385: evict(INList , IN , ScanIterator ){
	envImpl.lazyCompress(target);
	//original(target);
    }

}
\00
